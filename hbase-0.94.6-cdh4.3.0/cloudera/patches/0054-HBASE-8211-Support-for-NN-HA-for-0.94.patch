From cf4b40f38c97db58e346fc51a9dda8f4cb744c56 Mon Sep 17 00:00:00 2001
From: Himanshu <himanshu@cloudera.com>
Date: Tue, 2 Apr 2013 16:36:44 -0600
Subject: [PATCH 54/96] HBASE-8211 Support for NN HA for 0.94

Ref: CDH-10625
Author: Himanshu Vashishtha
Reason: Usability
---
 .../org/apache/hadoop/hbase/HBaseFileSystem.java   |  263 ++++++++++++++++++++
 .../apache/hadoop/hbase/backup/HFileArchiver.java  |   26 +-
 .../java/org/apache/hadoop/hbase/io/Reference.java |    3 +-
 .../hadoop/hbase/master/MasterFileSystem.java      |   19 +-
 .../hadoop/hbase/master/SplitLogManager.java       |    3 +-
 .../hadoop/hbase/master/cleaner/CleanerChore.java  |    9 +-
 .../hbase/master/handler/CreateTableHandler.java   |    3 +-
 .../hbase/master/handler/DeleteTableHandler.java   |    4 +-
 .../apache/hadoop/hbase/regionserver/HRegion.java  |   19 +-
 .../hbase/regionserver/HRegionFileSystem.java      |   38 +++
 .../hbase/regionserver/SplitTransaction.java       |    8 +-
 .../apache/hadoop/hbase/regionserver/Store.java    |    8 +-
 .../hadoop/hbase/regionserver/StoreFile.java       |    7 +-
 .../apache/hadoop/hbase/regionserver/wal/HLog.java |   21 +-
 .../hbase/regionserver/wal/HLogFileSystem.java     |   69 +++++
 .../hbase/regionserver/wal/HLogSplitter.java       |   42 ++--
 .../hadoop/hbase/util/FSTableDescriptors.java      |    9 +-
 .../java/org/apache/hadoop/hbase/util/FSUtils.java |   12 +-
 .../apache/hadoop/hbase/zookeeper/ZKSplitLog.java  |    3 +-
 .../apache/hadoop/hbase/HBaseTestingUtility.java   |    5 +
 .../apache/hadoop/hbase/TestHBaseFileSystem.java   |  179 +++++++++++++
 .../hbase/master/TestOpenedRegionHandler.java      |    1 -
 22 files changed, 660 insertions(+), 91 deletions(-)
 create mode 100644 src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java

diff --git a/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java b/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
new file mode 100644
index 0000000..eebac4d
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
@@ -0,0 +1,263 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hbase.regionserver.wal.HLogFileSystem;
+import org.apache.hadoop.hbase.util.Threads;
+
+/**
+ * An abstraction of the underlying filesystem. This is used by other entities such as
+ * {@link HLogFileSystem}, to make calls to the underlying filesystem.
+ *
+ */
+public abstract class HBaseFileSystem {
+
+  public static final Log LOG = LogFactory.getLog(HBaseFileSystem.class);
+
+  /**
+   * In order to handle NN connectivity hiccups, one need to retry non-idempotent operation at the
+   * client level.
+   */
+  protected static  int hdfsClientRetriesNumber;
+  private static int baseSleepBeforeRetries;
+  private static final int DEFAULT_HDFS_CLIENT_RETRIES_NUMBER = 10;
+  private static final int DEFAULT_BASE_SLEEP_BEFORE_RETRIES = 1000;
+
+  
+  /**
+   * Deletes a file. Assumes the user has already checked for this directory existence.
+   * @param dir
+   * @param fs
+   * @param conf
+   * @return true if the directory is deleted.
+   * @throws IOException
+   */
+  public static boolean deleteFileFromFileSystem(FileSystem fs, Configuration conf, Path dir)
+      throws IOException {
+    IOException lastIOE = null;
+    int i = 0;
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.delete(dir, false);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        if (!fs.exists(dir)) return true;
+        // dir is there, retry deleting after some time.
+        sleepBeforeRetry("Delete File", i + 1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in deleteFileFromFileSystem", lastIOE);
+  }
+  
+  
+  /**
+   * Deletes a directory. Assumes the user has already checked for this directory existence.
+   * @param dir
+   * @param fs
+   * @param conf
+   * @return true if the directory is deleted.
+   * @throws IOException
+   */
+  public static boolean deleteDirFromFileSystem(FileSystem fs, Configuration conf, Path dir)
+      throws IOException {
+    IOException lastIOE = null;
+    int i = 0;
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.delete(dir, true);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        if (!fs.exists(dir)) return true;
+        // dir is there, retry deleting after some time.
+        sleepBeforeRetry("Delete Dir", i + 1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in deleteDirFromFileSystem", lastIOE);
+  }
+
+  protected static void checkAndSetRetryCounts(Configuration conf) {
+    if (hdfsClientRetriesNumber == 0) {
+      hdfsClientRetriesNumber = conf.getInt("hdfs.client.retries.number",
+        DEFAULT_HDFS_CLIENT_RETRIES_NUMBER);
+      baseSleepBeforeRetries = conf.getInt("hdfs.client.sleep.before.retries",
+        DEFAULT_BASE_SLEEP_BEFORE_RETRIES);
+    }
+  }
+  
+  /**
+   * Creates a directory for a filesystem and configuration object. Assumes the user has already
+   * checked for this directory existence.
+   * @param fs
+   * @param conf
+   * @param dir
+   * @return the result of fs.mkdirs(). In case underlying fs throws an IOException, it checks
+   *         whether the directory exists or not, and returns true if it exists.
+   * @throws IOException
+   */
+  public static boolean makeDirOnFileSystem(FileSystem fs, Configuration conf, Path dir)
+      throws IOException {
+    int i = 0;
+    IOException lastIOE = null;
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.mkdirs(dir);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        if (fs.exists(dir)) return true; // directory is present
+        sleepBeforeRetry("Create Directory", i+1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in makeDirOnFileSystem", lastIOE);
+  }
+  
+  /**
+   * Renames a directory. Assumes the user has already checked for this directory existence.
+   * @param src
+   * @param fs
+   * @param dst
+   * @param conf
+   * @return true if the directory is renamed.
+   * @throws IOException
+   */
+  public static boolean renameDirForFileSystem(FileSystem fs, Configuration conf, Path src, Path dst)
+      throws IOException {
+    IOException lastIOE = null;
+    int i = 0;
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.rename(src, dst);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        if (!fs.exists(src) && fs.exists(dst)) return true;
+        // src is there, retry renaming after some time.
+        sleepBeforeRetry("Rename Directory", i + 1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in renameDirForFileSystem", lastIOE);
+  }
+  
+/**
+ * Creates a path on the file system. Checks whether the path exists already or not, and use it
+ * for retrying in case underlying fs throws an exception.
+ * @param fs
+ * @param conf
+ * @param dir
+ * @param overwrite
+ * @return
+ * @throws IOException
+ */
+  public static FSDataOutputStream createPathOnFileSystem(FileSystem fs, Configuration conf, Path dir,
+      boolean overwrite) throws IOException {
+    int i = 0;
+    boolean existsBefore = fs.exists(dir);
+    IOException lastIOE = null;
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.create(dir, overwrite);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        // directory is present, don't overwrite
+        if (!existsBefore && fs.exists(dir)) return fs.create(dir, false);
+        sleepBeforeRetry("Create Path", i + 1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in createPathOnFileSystem", lastIOE);
+  }
+
+  /**
+   * Creates the specified file with the given permission.
+   * @param fs
+   * @param path
+   * @param perm
+   * @param overwrite
+   * @return
+   * @throws IOException 
+   */
+  public static FSDataOutputStream createPathWithPermsOnFileSystem(FileSystem fs,
+      Configuration conf, Path path, FsPermission perm, boolean overwrite) throws IOException {
+    int i = 0;
+    IOException lastIOE = null;
+    boolean existsBefore = fs.exists(path);
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.create(path, perm, overwrite, fs.getConf().getInt("io.file.buffer.size", 4096),
+          fs.getDefaultReplication(), fs.getDefaultBlockSize(), null);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        // path is present now, don't overwrite
+        if (!existsBefore && fs.exists(path)) return fs.create(path, false);
+        // if it existed before let's retry in case we get IOE, as we can't rely on fs.exists()
+        sleepBeforeRetry("Create Path with Perms", i + 1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in createPathWithPermsOnFileSystem", lastIOE);
+  }
+
+/**
+ * Creates the file. Assumes the user has already checked for this file existence.
+ * @param fs
+ * @param conf
+ * @param dir
+ * @return result true if the file is created with this call, false otherwise.
+ * @throws IOException
+ */
+  public static boolean createNewFileOnFileSystem(FileSystem fs, Configuration conf, Path file)
+      throws IOException {
+    int i = 0;
+    IOException lastIOE = null;
+    checkAndSetRetryCounts(conf);
+    do {
+      try {
+        return fs.createNewFile(file);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        if (fs.exists(file)) return true; // file exists now, return true.
+        sleepBeforeRetry("Create NewFile", i + 1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in createNewFileOnFileSystem", lastIOE);
+  }
+  
+  /**
+   * sleeping logic for static methods; handles the interrupt exception. Keeping a static version
+   * for this to avoid re-looking for the integer values.
+   */
+  protected static void sleepBeforeRetry(String msg, int sleepMultiplier) {
+    if (sleepMultiplier > hdfsClientRetriesNumber) {
+      LOG.warn(msg + ", retries exhausted");
+      return;
+    }
+    LOG.info(msg + ", sleeping " + baseSleepBeforeRetries + " times " + sleepMultiplier);
+    Threads.sleep(baseSleepBeforeRetries * sleepMultiplier);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java b/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
index 6083328..808b1f8 100644
--- a/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
+++ b/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -216,7 +217,7 @@ public class HFileArchiver {
     Path storeArchiveDir = HFileArchiveUtil.getStoreArchivePath(conf, parent, family);
 
     // make sure we don't archive if we can't and that the archive dir exists
-    if (!fs.mkdirs(storeArchiveDir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, storeArchiveDir)) {
       throw new IOException("Could not make archive directory (" + storeArchiveDir + ") for store:"
           + Bytes.toString(family) + ", deleting compacted files instead.");
     }
@@ -250,7 +251,7 @@ public class HFileArchiver {
       Configuration conf, Path tableDir, byte[] family, Path storeFile) throws IOException {
     Path storeArchiveDir = HFileArchiveUtil.getStoreArchivePath(conf, regionInfo, tableDir, family);
     // make sure we don't archive if we can't and that the archive dir exists
-    if (!fs.mkdirs(storeArchiveDir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, storeArchiveDir)) {
       throw new IOException("Could not make archive directory (" + storeArchiveDir + ") for store:"
           + Bytes.toString(family) + ", deleting compacted files instead.");
     }
@@ -318,7 +319,7 @@ public class HFileArchiver {
 
     // make sure the archive directory exists
     if (!fs.exists(baseArchiveDir)) {
-      if (!fs.mkdirs(baseArchiveDir)) {
+      if (!HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), baseArchiveDir)) {
         throw new IOException("Failed to create the archive directory:" + baseArchiveDir
             + ", quitting archive attempt.");
       }
@@ -385,11 +386,12 @@ public class HFileArchiver {
 
       // move the archive file to the stamped backup
       Path backedupArchiveFile = new Path(archiveDir, filename + SEPARATOR + archiveStartTime);
-      if (!fs.rename(archiveFile, backedupArchiveFile)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), archiveFile,
+        backedupArchiveFile)) {
         LOG.error("Could not rename archive file to backup: " + backedupArchiveFile
             + ", deleting existing file in favor of newer.");
         // try to delete the exisiting file, if we can't rename it
-        if (!fs.delete(archiveFile, false)) {
+        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, fs.getConf(), archiveFile)) {
           throw new IOException("Couldn't delete existing archive file (" + archiveFile
               + ") or rename it to the backup file (" + backedupArchiveFile
               + ") to make room for similarly named file.");
@@ -410,10 +412,9 @@ public class HFileArchiver {
         // the cleaner has removed our archive directory (HBASE-7643).
         // (we're in a retry loop, so don't worry too much about the exception)
         try {
-          if (!fs.exists(archiveDir)) {
-            if (fs.mkdirs(archiveDir)) {
-              LOG.debug("Created archive directory:" + archiveDir);
-            }
+          if (!fs.exists(archiveDir)
+              && HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), archiveDir)) {
+            LOG.debug("Created archive directory:" + archiveDir);
           }
         } catch (IOException e) {
           LOG.warn("Failed to create the archive directory: " + archiveDir, e);
@@ -476,7 +477,7 @@ public class HFileArchiver {
    */
   private static boolean deleteRegionWithoutArchiving(FileSystem fs, Path regionDir)
       throws IOException {
-    if (fs.delete(regionDir, true)) {
+    if (HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), regionDir)) {
       LOG.debug("Deleted all region files in: " + regionDir);
       return true;
     }
@@ -610,7 +611,7 @@ public class HFileArchiver {
     public boolean moveAndClose(Path dest) throws IOException {
       this.close();
       Path p = this.getPath();
-      return fs.rename(p, dest);
+      return HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), p, dest);
     }
 
     /**
@@ -641,7 +642,8 @@ public class HFileArchiver {
 
     @Override
     public void delete() throws IOException {
-      if (!fs.delete(file, true)) throw new IOException("Failed to delete:" + this.file);
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), file)) 
+        throw new IOException("Failed to delete:" + this.file);
     }
 
     @Override
diff --git a/src/main/java/org/apache/hadoop/hbase/io/Reference.java b/src/main/java/org/apache/hadoop/hbase/io/Reference.java
index 99ecb7e..1fcea84 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/Reference.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/Reference.java
@@ -27,6 +27,7 @@ import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
@@ -125,7 +126,7 @@ public class Reference implements Writable {
 
   public Path write(final FileSystem fs, final Path p)
   throws IOException {
-    FSDataOutputStream out = fs.create(p, false);
+    FSDataOutputStream out = HBaseFileSystem.createPathOnFileSystem(fs, fs.getConf(), p, false);
     try {
       write(out);
     } finally {
diff --git a/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java b/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
index 54ce8c5..fffd663 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
@@ -139,7 +140,7 @@ public class MasterFileSystem {
 
     // Make sure the region servers can archive their old logs
     if(!this.fs.exists(oldLogDir)) {
-      this.fs.mkdirs(oldLogDir);
+      HBaseFileSystem.makeDirOnFileSystem(fs, conf, oldLogDir);
     }
 
     return oldLogDir;
@@ -276,7 +277,7 @@ public class MasterFileSystem {
       Path splitDir = logDir.suffix(HLog.SPLITTING_EXT);
       // rename the directory so a rogue RS doesn't create more HLogs
       if (fs.exists(logDir)) {
-        if (!this.fs.rename(logDir, splitDir)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, logDir, splitDir)) {
           throw new IOException("Failed fs.rename for log split: " + logDir);
         }
         logDir = splitDir;
@@ -348,7 +349,7 @@ public class MasterFileSystem {
     // Filesystem is good. Go ahead and check for hbase.rootdir.
     try {
       if (!fs.exists(rd)) {
-        fs.mkdirs(rd);
+        HBaseFileSystem.makeDirOnFileSystem(fs, c, rd);
         // DFS leaves safe mode with 0 DNs when there are 0 blocks.
         // We used to handle this by checking the current DN count and waiting until
         // it is nonzero. With security, the check for datanode count doesn't work --
@@ -412,13 +413,13 @@ public class MasterFileSystem {
           HFileArchiver.archiveRegion(fs, this.rootdir, tabledir, regiondir);
         }
       }
-      if (!fs.delete(tmpdir, true)) {
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, c, tmpdir)) {
         throw new IOException("Unable to clean the temp directory: " + tmpdir);
       }
     }
 
     // Create the temp directory
-    if (!fs.mkdirs(tmpdir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, c, tmpdir)) {
       throw new IOException("HBase temp directory '" + tmpdir + "' creation failure.");
     }
   }
@@ -486,7 +487,7 @@ public class MasterFileSystem {
   }
 
   public void deleteTable(byte[] tableName) throws IOException {
-    fs.delete(new Path(rootdir, Bytes.toString(tableName)), true);
+    HBaseFileSystem.deleteDirFromFileSystem(fs, conf, new Path(rootdir, Bytes.toString(tableName)));
   }
 
   /**
@@ -499,11 +500,11 @@ public class MasterFileSystem {
     Path tempPath = new Path(this.tempdir, path.getName());
 
     // Ensure temp exists
-    if (!fs.exists(tempdir) && !fs.mkdirs(tempdir)) {
+    if (!fs.exists(tempdir) && !HBaseFileSystem.makeDirOnFileSystem(fs, conf, tempdir)) {
       throw new IOException("HBase temp directory '" + tempdir + "' creation failure.");
     }
 
-    if (!fs.rename(path, tempPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, path, tempPath)) {
       throw new IOException("Unable to move '" + path + "' to temp '" + tempPath + "'");
     }
 
@@ -535,7 +536,7 @@ public class MasterFileSystem {
     // delete the family folder
     Path familyDir = new Path(tableDir,
       new Path(region.getEncodedName(), Bytes.toString(familyName)));
-    if (fs.delete(familyDir, true) == false) {
+    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, conf, familyDir)) {
       throw new IOException("Could not delete family "
           + Bytes.toString(familyName) + " from FileSystem for region "
           + region.getRegionNameAsString() + "(" + region.getEncodedName()
diff --git a/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java b/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
index 02c9662..32d1565 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
@@ -38,6 +38,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Chore;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.Stoppable;
 import org.apache.hadoop.hbase.master.SplitLogManager.TaskFinisher.Status;
@@ -278,7 +279,7 @@ public class SplitLogManager extends ZooKeeperListener {
     for(Path logDir: logDirs){
       status.setStatus("Cleaning up log directory...");
       try {
-        if (fs.exists(logDir) && !fs.delete(logDir, false)) {
+        if (fs.exists(logDir) && !HBaseFileSystem.deleteFileFromFileSystem(fs, conf, logDir)) {
           LOG.warn("Unable to delete log src dir. Ignoring. " + logDir);
         }
       } catch (IOException ioe) {
diff --git a/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java b/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
index e586342..6d6da76 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
@@ -28,6 +28,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Chore;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.Stoppable;
 import org.apache.hadoop.hbase.util.FSUtils;
@@ -153,7 +154,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     // if the directory doesn't exist, then we are done
     if (children == null) {
       try {
-        return fs.delete(toCheck, false);
+        return HBaseFileSystem.deleteFileFromFileSystem(fs, conf, toCheck);
       } catch (IOException e) {
         if (LOG.isTraceEnabled()) {
           LOG.trace("Couldn't delete directory: " + toCheck, e);
@@ -185,7 +186,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     // delete this directory. However, don't do so recursively so we don't delete files that have
     // been added since we last checked.
     try {
-      return fs.delete(toCheck, false);
+      return HBaseFileSystem.deleteFileFromFileSystem(fs, conf, toCheck);
     } catch (IOException e) {
       if (LOG.isTraceEnabled()) {
         LOG.trace("Couldn't delete directory: " + toCheck, e);
@@ -207,7 +208,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     // first check to see if the path is valid
     if (!validate(filePath)) {
       LOG.warn("Found a wrongly formatted file: " + filePath.getName() + " deleting it.");
-      boolean success = this.fs.delete(filePath, true);
+      boolean success = HBaseFileSystem.deleteDirFromFileSystem(fs, conf, filePath);
       if (!success) LOG.warn("Attempted to delete:" + filePath
           + ", but couldn't. Run cleaner chain and attempt to delete on next pass.");
 
@@ -233,7 +234,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     if (LOG.isTraceEnabled()) {
       LOG.trace("Removing:" + filePath + " from archive");
     }
-    boolean success = this.fs.delete(filePath, false);
+    boolean success = HBaseFileSystem.deleteFileFromFileSystem(fs, conf, filePath);
     if (!success) {
       LOG.warn("Attempted to delete:" + filePath
           + ", but couldn't. Run cleaner chain and attempt to delete on next pass.");
diff --git a/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java b/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
index 717880d..e40c54c 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
@@ -38,6 +38,7 @@ import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException;
@@ -177,7 +178,7 @@ public class CreateTableHandler extends EventHandler {
     List<HRegionInfo> regionInfos = handleCreateHdfsRegions(tempdir, tableName);
 
     // 3. Move Table temp directory to the hbase root location
-    if (!fs.rename(tempTableDir, tableDir)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, tempTableDir, tableDir)) {
       throw new IOException("Unable to move table from temp=" + tempTableDir +
         " to hbase root=" + tableDir);
     }
diff --git a/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java b/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
index 957e531..db91aa7 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
@@ -26,6 +26,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.backup.HFileArchiver;
@@ -89,10 +90,9 @@ public class DeleteTableHandler extends TableEventHandler {
       }
 
       // 5. Delete table from FS (temp directory)
-      if (!fs.delete(tempTableDir, true)) {
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), tempTableDir)) {
         LOG.error("Couldn't delete " + tempTableDir);
       }
-
       LOG.debug("Table '" + Bytes.toString(tableName) + "' archived!");
     } finally {
       // 6. Update table descriptor cache
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 9261961..71a6d4c 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -71,6 +71,7 @@ import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
 import org.apache.hadoop.hbase.DroppedSnapshotException;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HConstants.OperationStatusCode;
@@ -657,7 +658,7 @@ public class HRegion implements HeapSize { // , Writable{
     final Path initialFiles, final Path regiondir)
   throws IOException {
     if (initialFiles != null && fs.exists(initialFiles)) {
-      if (!fs.rename(initialFiles, regiondir)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), initialFiles, regiondir)) {
         LOG.warn("Unable to rename " + initialFiles + " to " + regiondir);
       }
     }
@@ -819,7 +820,7 @@ public class HRegion implements HeapSize { // , Writable{
     } finally {
       out.close();
     }
-    if (!fs.rename(tmpPath, regioninfoPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, tmpPath, regioninfoPath)) {
       throw new IOException("Unable to rename " + tmpPath + " to " +
         regioninfoPath);
     }
@@ -2663,7 +2664,7 @@ public class HRegion implements HeapSize { // , Writable{
         // open and read the files as well).
         LOG.debug("Creating reference for file (" + (i+1) + "/" + sz + ") : " + file);
         Path referenceFile = new Path(dstStoreDir, file.getName());
-        boolean success = fs.createNewFile(referenceFile);
+        boolean success = HBaseFileSystem.createNewFileOnFileSystem(fs, conf, referenceFile);
         if (!success) {
           throw new IOException("Failed to create reference file:" + referenceFile);
         }
@@ -3072,7 +3073,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     // Now delete the content of recovered edits.  We're done w/ them.
     for (Path file: files) {
-      if (!this.fs.delete(file, false)) {
+      if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, file)) {
         LOG.error("Failed delete of " + file);
       } else {
         LOG.debug("Deleted recovered.edits file=" + file);
@@ -3264,7 +3265,7 @@ public class HRegion implements HeapSize { // , Writable{
     FileStatus stat = fs.getFileStatus(p);
     if (stat.getLen() > 0) return false;
     LOG.warn("File " + p + " is zero-length, deleting.");
-    fs.delete(p, false);
+    HBaseFileSystem.deleteFileFromFileSystem(fs, fs.getConf(), p);
     return true;
   }
 
@@ -4179,7 +4180,7 @@ public class HRegion implements HeapSize { // , Writable{
         HTableDescriptor.getTableDir(rootDir, info.getTableName());
     Path regionDir = HRegion.getRegionDir(tableDir, info.getEncodedName());
     FileSystem fs = FileSystem.get(conf);
-    fs.mkdirs(regionDir);
+    HBaseFileSystem.makeDirOnFileSystem(fs, conf, regionDir);
     // Write HRI to a file in case we need to recover .META.
     writeRegioninfoOnFilesystem(info, regionDir, fs, conf);
     HLog effectiveHLog = hlog;
@@ -4375,7 +4376,7 @@ public class HRegion implements HeapSize { // , Writable{
     if (LOG.isDebugEnabled()) {
       LOG.debug("DELETING region " + regiondir.toString());
     }
-    if (!fs.delete(regiondir, true)) {
+    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), regiondir)) {
       LOG.warn("Failed delete of " + regiondir);
     }
   }
@@ -4421,7 +4422,7 @@ public class HRegion implements HeapSize { // , Writable{
     final HRegionInfo hri, byte [] colFamily)
   throws IOException {
     Path dir = Store.getStoreHomedir(tabledir, hri.getEncodedName(), colFamily);
-    if (!fs.mkdirs(dir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), dir)) {
       LOG.warn("Failed to create " + dir);
     }
   }
@@ -4531,7 +4532,7 @@ public class HRegion implements HeapSize { // , Writable{
       throw new IOException("Cannot merge; target file collision at " +
           newRegionDir);
     }
-    fs.mkdirs(newRegionDir);
+    HBaseFileSystem.makeDirOnFileSystem(fs, conf, newRegionDir);
 
     LOG.info("starting merge of regions: " + a + " and " + b +
       " into new region " + newRegionInfo.toString() +
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
new file mode 100644
index 0000000..b2a7a6f
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseFileSystem;
+
+/**
+ * Acts as an abstraction layer b/w HBase and underlying fs. Used for making non-idempotent calls.
+ * This is useful as it can have a retry logic for such operations, as they are not retried at
+ * hdfs level.
+ * Region specific methods that access fs should be added here.
+ *
+ */
+public class HRegionFileSystem extends HBaseFileSystem {
+  public static final Log LOG = LogFactory.getLog(HRegionFileSystem.class);
+
+  public HRegionFileSystem(Configuration conf) {
+    checkAndSetRetryCounts(conf);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java b/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
index 058393d..aaa2dd6 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
@@ -37,6 +37,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.ServerName;
@@ -551,12 +552,13 @@ public class SplitTransaction {
     if (fs.exists(splitdir)) {
       LOG.info("The " + splitdir
           + " directory exists.  Hence deleting it to recreate it");
-      if (!fs.delete(splitdir, true)) {
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), splitdir)) {
         throw new IOException("Failed deletion of " + splitdir
             + " before creating them again.");
       }
     }
-    if (!fs.mkdirs(splitdir)) throw new IOException("Failed create of " + splitdir);
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), splitdir))
+        throw new IOException("Failed create of " + splitdir);
   }
 
   private static void cleanupSplitDir(final FileSystem fs, final Path splitdir)
@@ -577,7 +579,7 @@ public class SplitTransaction {
   throws IOException {
     if (!fs.exists(dir)) {
       if (mustPreExist) throw new IOException(dir.toString() + " does not exist!");
-    } else if (!fs.delete(dir, true)) {
+    } else if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), dir)) {
       throw new IOException("Failed delete of " + dir);
     }
   }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java b/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
index 90f7fd0..b01f716 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
@@ -45,6 +45,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
@@ -276,8 +277,7 @@ public class Store extends SchemaConfigured implements HeapSize {
    */
   Path createStoreHomeDir(final FileSystem fs,
       final Path homedir) throws IOException {
-    if (!fs.exists(homedir)) {
-      if (!fs.mkdirs(homedir))
+    if (!fs.exists(homedir) && !HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), homedir)) {
         throw new IOException("Failed create of: " + homedir.toString());
     }
     return homedir;
@@ -867,7 +867,7 @@ public class Store extends SchemaConfigured implements HeapSize {
     String msg = "Renaming flushed file at " + path + " to " + dstPath;
     LOG.debug(msg);
     status.setStatus("Flushing " + this + ": " + msg);
-    if (!fs.rename(path, dstPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, path, dstPath)) {
       LOG.warn("Unable to rename " + path + " to " + dstPath);
     }
 
@@ -1636,7 +1636,7 @@ public class Store extends SchemaConfigured implements HeapSize {
       Path origPath = compactedFile.getPath();
       Path destPath = new Path(homedir, origPath.getName());
       LOG.info("Renaming compacted file at " + origPath + " to " + destPath);
-      if (!fs.rename(origPath, destPath)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, origPath, destPath)) {
         LOG.error("Failed move of compacted file " + origPath + " to " +
             destPath);
         throw new IOException("Failed move of compacted file " + origPath +
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java b/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
index bc0bd9d..f84569c 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
@@ -41,6 +41,7 @@ import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HDFSBlocksDistribution;
 import org.apache.hadoop.hbase.HTableDescriptor;
@@ -675,7 +676,7 @@ public class StoreFile extends SchemaConfigured {
    */
   public void deleteReader() throws IOException {
     closeReader(true);
-    this.fs.delete(getPath(), true);
+    HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), getPath());
   }
 
   @Override
@@ -718,7 +719,7 @@ public class StoreFile extends SchemaConfigured {
     if (!fs.exists(src)) {
       throw new FileNotFoundException(src.toString());
     }
-    if (!fs.rename(src, tgt)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), src, tgt)) {
       throw new IOException("Failed rename of " + src + " to " + tgt);
     }
     return tgt;
@@ -841,7 +842,7 @@ public class StoreFile extends SchemaConfigured {
       }
 
       if (!fs.exists(dir)) {
-        fs.mkdirs(dir);
+        HBaseFileSystem.makeDirOnFileSystem(fs, conf, dir);
       }
 
       if (filePath == null) {
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index 57fc520..e636726 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -59,6 +59,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.fs.Syncable;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
@@ -132,6 +133,7 @@ public class HLog implements Syncable {
   private final FileSystem fs;
   private final Path dir;
   private final Configuration conf;
+  private final HLogFileSystem hlogFs;
   // Listeners that are called on WAL events.
   private List<WALActionsListener> listeners =
     new CopyOnWriteArrayList<WALActionsListener>();
@@ -390,6 +392,7 @@ public class HLog implements Syncable {
     this.fs = fs;
     this.dir = dir;
     this.conf = conf;
+    this.hlogFs = new HLogFileSystem(conf);
     if (listeners != null) {
       for (WALActionsListener i: listeners) {
         registerWALActionsListener(i);
@@ -405,14 +408,12 @@ public class HLog implements Syncable {
     if (failIfLogDirExists && fs.exists(dir)) {
       throw new IOException("Target HLog directory already exists: " + dir);
     }
-    if (!fs.mkdirs(dir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, dir)) {
       throw new IOException("Unable to mkdir " + dir);
     }
     this.oldLogDir = oldLogDir;
-    if (!fs.exists(oldLogDir)) {
-      if (!fs.mkdirs(this.oldLogDir)) {
-        throw new IOException("Unable to mkdir " + this.oldLogDir);
-      }
+    if (!fs.exists(oldLogDir) && !HBaseFileSystem.makeDirOnFileSystem(fs, conf, oldLogDir)) {
+      throw new IOException("Unable to mkdir " + this.oldLogDir);
     }
     this.maxLogs = conf.getInt("hbase.regionserver.maxlogs", 32);
     this.minTolerableReplication = conf.getInt(
@@ -698,7 +699,7 @@ public class HLog implements Syncable {
    */
   protected Writer createWriterInstance(final FileSystem fs, final Path path,
       final Configuration conf) throws IOException {
-    return createWriter(fs, path, conf);
+    return this.hlogFs.createWriter(fs, conf, path);
   }
 
   /**
@@ -917,7 +918,7 @@ public class HLog implements Syncable {
         i.preLogArchive(p, newPath);
       }
     }
-    if (!this.fs.rename(p, newPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, p, newPath)) {
       throw new IOException("Unable to rename " + p + " to " + newPath);
     }
     // Tell our listeners that a log has been archived.
@@ -969,7 +970,7 @@ public class HLog implements Syncable {
         }
       }
 
-      if (!fs.rename(file.getPath(),p)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, file.getPath(),p)) {
         throw new IOException("Unable to rename " + file.getPath() + " to " + p);
       }
       // Tell our listeners that a log was archived.
@@ -981,7 +982,7 @@ public class HLog implements Syncable {
     }
     LOG.debug("Moved " + files.length + " log files to " +
       FSUtils.getPath(this.oldLogDir));
-    if (!fs.delete(dir, true)) {
+    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, conf, dir)) {
       LOG.info("Unable to delete " + dir);
     }
   }
@@ -1863,7 +1864,7 @@ public class HLog implements Syncable {
   throws IOException {
     Path moveAsideName = new Path(edits.getParent(), edits.getName() + "." +
       System.currentTimeMillis());
-    if (!fs.rename(edits, moveAsideName)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), edits, moveAsideName)) {
       LOG.warn("Rename failed from " + edits + " to " + moveAsideName);
     }
     return moveAsideName;
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
new file mode 100644
index 0000000..29efe4f
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver.wal;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
+import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
+import org.apache.hadoop.hbase.regionserver.wal.HLog.Writer;
+
+/**
+ * Acts as an abstraction between the HLog and the underlying filesystem. This is analogous to the
+ * {@link HRegionFileSystem} class.
+ */
+public class HLogFileSystem extends HBaseFileSystem {
+  public static final Log LOG = LogFactory.getLog(HLogFileSystem.class);
+
+  /**
+   * In order to handle NN connectivity hiccups, one need to retry non-idempotent operation at the
+   * client level.
+   */
+
+  public HLogFileSystem(Configuration conf) {
+    checkAndSetRetryCounts(conf);
+  }
+
+  /**
+   * Creates writer for the given path.
+   * @param fs
+   * @param conf
+   * @param hlogFile
+   * @return an init'ed writer for the given path.
+   * @throws IOException
+   */
+  public Writer createWriter(FileSystem fs, Configuration conf, Path hlogFile) throws IOException {
+    int i = 0;
+    IOException lastIOE = null;
+    do {
+      try {
+        return HLog.createWriter(fs, hlogFile, conf);
+      } catch (IOException ioe) {
+        lastIOE = ioe;
+        sleepBeforeRetry("Create Writer", i+1);
+      }
+    } while (++i <= hdfsClientRetriesNumber);
+    throw new IOException("Exception in createWriter", lastIOE);
+
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
index a0e19c4..3aae3ac 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
@@ -41,6 +41,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
@@ -90,6 +91,7 @@ public class HLogSplitter {
   protected final Path oldLogDir;
   protected final FileSystem fs;
   protected final Configuration conf;
+  private final HLogFileSystem hlogFs;
 
   // Major subcomponents of the split process.
   // These are separated into inner classes to make testing easier.
@@ -162,6 +164,7 @@ public class HLogSplitter {
         conf.getInt("hbase.regionserver.hlog.splitlog.buffersize",
             128*1024*1024));
     outputSink = new OutputSink();
+    this.hlogFs = new HLogFileSystem(conf);
   }
 
   /**
@@ -476,7 +479,7 @@ public class HLogSplitter {
             LOG.warn("Found existing old edits file. It could be the "
                 + "result of a previous failed split attempt. Deleting " + dst + ", length="
                 + fs.getFileStatus(dst).getLen());
-            if (!fs.delete(dst, false)) {
+            if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, dst)) {
               LOG.warn("Failed deleting of old " + dst);
               throw new IOException("Failed deleting of old " + dst);
             }
@@ -485,7 +488,7 @@ public class HLogSplitter {
           // data without touching disk. TestHLogSplit#testThreading is an
           // example.
           if (fs.exists(wap.p)) {
-            if (!fs.rename(wap.p, dst)) {
+            if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, wap.p, dst)) {
               throw new IOException("Failed renaming " + wap.p + " to " + dst);
             }
             LOG.debug("Rename " + wap.p + " to " + dst);
@@ -552,7 +555,7 @@ public class HLogSplitter {
     }
     archiveLogs(null, corruptedLogs, processedLogs, oldLogDir, fs, conf);
     Path stagingDir = ZKSplitLog.getSplitLogDir(rootdir, logPath.getName());
-    fs.delete(stagingDir, true);
+    HBaseFileSystem.deleteDirFromFileSystem(fs, conf, stagingDir);
   }
 
   /**
@@ -575,17 +578,17 @@ public class HLogSplitter {
     final Path corruptDir = new Path(conf.get(HConstants.HBASE_DIR), conf.get(
         "hbase.regionserver.hlog.splitlog.corrupt.dir",  HConstants.CORRUPT_DIR_NAME));
 
-    if (!fs.mkdirs(corruptDir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, corruptDir)) {
       LOG.info("Unable to mkdir " + corruptDir);
     }
-    fs.mkdirs(oldLogDir);
+    HBaseFileSystem.makeDirOnFileSystem(fs, conf, oldLogDir);
 
     // this method can get restarted or called multiple times for archiving
     // the same log files.
     for (Path corrupted : corruptedLogs) {
       Path p = new Path(corruptDir, corrupted.getName());
       if (fs.exists(corrupted)) {
-        if (!fs.rename(corrupted, p)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, corrupted, p)) {
           LOG.warn("Unable to move corrupted log " + corrupted + " to " + p);
         } else {
           LOG.warn("Moving corrupted log " + corrupted + " to " + p);
@@ -596,7 +599,7 @@ public class HLogSplitter {
     for (Path p : processedLogs) {
       Path newPath = HLog.getHLogArchivePath(oldLogDir, p);
       if (fs.exists(p)) {
-        if (!fs.rename(p, newPath)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, p, newPath)) {
           LOG.warn("Unable to move  " + p + " to " + newPath);
         } else {
           LOG.debug("Archived processed log " + p + " to " + newPath);
@@ -606,7 +609,7 @@ public class HLogSplitter {
 
     // distributed log splitting removes the srcDir (region's log dir) later
     // when all the log files in that srcDir have been successfully processed
-    if (srcDir != null && !fs.delete(srcDir, true)) {
+    if (srcDir != null && !HBaseFileSystem.deleteDirFromFileSystem(fs, conf, srcDir)) {
       throw new IOException("Unable to delete src dir: " + srcDir);
     }
   }
@@ -640,20 +643,21 @@ public class HLogSplitter {
     if (fs.exists(dir) && fs.isFile(dir)) {
       Path tmp = new Path("/tmp");
       if (!fs.exists(tmp)) {
-        fs.mkdirs(tmp);
+        HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), tmp);
       }
       tmp = new Path(tmp,
         HLog.RECOVERED_EDITS_DIR + "_" + encodedRegionName);
       LOG.warn("Found existing old file: " + dir + ". It could be some "
         + "leftover of an old installation. It should be a folder instead. "
         + "So moving it to " + tmp);
-      if (!fs.rename(dir, tmp)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), dir, tmp)) {
         LOG.warn("Failed to sideline old file " + dir);
       }
     }
 
-    if (isCreate && !fs.exists(dir)) {
-      if (!fs.mkdirs(dir)) LOG.warn("mkdir failed on " + dir);
+    if (isCreate && !fs.exists(dir) && 
+        !HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), dir)) {
+      LOG.warn("mkdir failed on " + dir);
     }
     // Append file name ends with RECOVERED_LOG_TMPFILE_SUFFIX to ensure
     // region's replayRecoveredEdits will not delete it
@@ -824,7 +828,7 @@ public class HLogSplitter {
    */
   protected Writer createWriter(FileSystem fs, Path logfile, Configuration conf)
       throws IOException {
-    return HLog.createWriter(fs, logfile, conf);
+    return hlogFs.createWriter(fs, conf, logfile);
   }
 
   /**
@@ -1071,7 +1075,7 @@ public class HLogSplitter {
           + "result of a previous failed split attempt. Deleting "
           + regionedits + ", length="
           + fs.getFileStatus(regionedits).getLen());
-      if (!fs.delete(regionedits, false)) {
+      if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, regionedits)) {
         LOG.warn("Failed delete of old " + regionedits);
       }
     }
@@ -1097,13 +1101,13 @@ public class HLogSplitter {
             + "result of a previous failed split attempt. Deleting "
             + ret + ", length="
             + fs.getFileStatus(ret).getLen());
-        if (!fs.delete(ret, false)) {
+        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, ret)) {
           LOG.warn("Failed delete of old " + ret);
         }
       }
       Path dir = ret.getParent();
-      if (!fs.exists(dir)) {
-        if (!fs.mkdirs(dir)) LOG.warn("mkdir failed on " + dir);
+      if (!fs.exists(dir) && !HBaseFileSystem.makeDirOnFileSystem(fs, conf, dir)) { 
+          LOG.warn("mkdir failed on " + dir);
       }
     } catch (IOException e) {
       LOG.warn("Could not prepare temp staging area ", e);
@@ -1196,7 +1200,7 @@ public class HLogSplitter {
             LOG.warn("Found existing old edits file. It could be the "
                 + "result of a previous failed split attempt. Deleting " + dst
                 + ", length=" + fs.getFileStatus(dst).getLen());
-            if (!fs.delete(dst, false)) {
+            if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, dst)) {
               LOG.warn("Failed deleting of old " + dst);
               throw new IOException("Failed deleting of old " + dst);
             }
@@ -1205,7 +1209,7 @@ public class HLogSplitter {
           // the data without touching disk. TestHLogSplit#testThreading is an
           // example.
           if (fs.exists(wap.p)) {
-            if (!fs.rename(wap.p, dst)) {
+            if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, wap.p, dst)) {
               throw new IOException("Failed renaming " + wap.p + " to " + dst);
             }
             LOG.debug("Rename " + wap.p + " to " + dst);
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
index 786b334..fae73dd 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
@@ -38,6 +38,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableDescriptors;
@@ -224,7 +225,7 @@ public class FSTableDescriptors implements TableDescriptors {
     if (!this.fsreadonly) {
       Path tabledir = FSUtils.getTablePath(this.rootdir, tablename);
       if (this.fs.exists(tabledir)) {
-        if (!this.fs.delete(tabledir, true)) {
+        if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), tabledir)) {
           throw new IOException("Failed delete of " + tabledir.toString());
         }
       }
@@ -280,7 +281,7 @@ public class FSTableDescriptors implements TableDescriptors {
       for (int i = 1; i < status.length; i++) {
         Path p = status[i].getPath();
         // Clean up old versions
-        if (!fs.delete(p, false)) {
+        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, fs.getConf(), p)) {
           LOG.warn("Failed cleanup of " + status);
         } else {
           LOG.debug("Cleaned up old tableinfo file " + p);
@@ -504,7 +505,7 @@ public class FSTableDescriptors implements TableDescriptors {
       try {
         writeHTD(fs, p, hTableDescriptor);
         tableInfoPath = getTableInfoFileName(tableDir, sequenceid);
-        if (!fs.rename(p, tableInfoPath)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), p, tableInfoPath)) {
           throw new IOException("Failed rename of " + p + " to " + tableInfoPath);
         }
       } catch (IOException ioe) {
@@ -530,7 +531,7 @@ public class FSTableDescriptors implements TableDescriptors {
   private static void writeHTD(final FileSystem fs, final Path p,
       final HTableDescriptor htd)
   throws IOException {
-    FSDataOutputStream out = fs.create(p, false);
+    FSDataOutputStream out = HBaseFileSystem.createPathOnFileSystem(fs, fs.getConf(), p, false);
     try {
       htd.write(out);
       out.write('\n');
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
index 1242088..9ab974e 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
@@ -44,6 +44,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HDFSBlocksDistribution;
@@ -106,7 +107,7 @@ public abstract class FSUtils {
    */
   public Path checkdir(final FileSystem fs, final Path dir) throws IOException {
     if (!fs.exists(dir)) {
-      fs.mkdirs(dir);
+      HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), dir);
     }
     return dir;
   }
@@ -151,13 +152,10 @@ public abstract class FSUtils {
    * @return output stream to the created file
    * @throws IOException if the file cannot be created
    */
-  public static FSDataOutputStream create(FileSystem fs, Path path,
-      FsPermission perm, boolean overwrite) throws IOException {
+  public static FSDataOutputStream create(FileSystem fs, Path path, FsPermission perm,
+      boolean overwrite) throws IOException {
     LOG.debug("Creating file=" + path + " with permission=" + perm);
-
-    return fs.create(path, perm, overwrite,
-        fs.getConf().getInt("io.file.buffer.size", 4096),
-        fs.getDefaultReplication(), fs.getDefaultBlockSize(), null);
+    return HBaseFileSystem.createPathWithPermsOnFileSystem(fs, fs.getConf(), path, perm, overwrite);
   }
 
   /**
diff --git a/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java b/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
index f02c178..f5d5f85 100644
--- a/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
@@ -33,6 +33,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.master.SplitLogManager;
 import org.apache.hadoop.hbase.regionserver.SplitLogWorker;
@@ -164,7 +165,7 @@ public class ZKSplitLog {
       FileSystem fs) {
     Path file = new Path(getSplitLogDir(rootdir, logFileName), "corrupt");
     try {
-      fs.createNewFile(file);
+      HBaseFileSystem.createNewFileOnFileSystem(fs, fs.getConf(), file);
     } catch (IOException e) {
       LOG.warn("Could not flag a log file as corrupted. Failed to create " +
           file, e);
diff --git a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 7e4bffa..a8a6b86 100644
--- a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -197,6 +197,11 @@ public class HBaseTestingUtility {
 
     // a hbase checksum verification failure will cause unit tests to fail
     ChecksumUtil.generateExceptionForChecksumFailureForTest(true);
+    setHDFSClientRetryProperty();
+  }
+
+  private void setHDFSClientRetryProperty() {
+    this.conf.setInt("hdfs.client.retries.number", 1);
   }
 
   /**
diff --git a/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java b/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java
new file mode 100644
index 0000000..f59c2d4
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.util.Progressable;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(MediumTests.class)
+public class TestHBaseFileSystem {
+  public static final Log LOG = LogFactory.getLog(TestHBaseFileSystem.class);
+
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private static Configuration conf;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    conf = TEST_UTIL.getConfiguration();
+    conf.setBoolean("dfs.support.append", true);
+    // The below config supported by 0.20-append and CDH3b2
+    conf.setInt("dfs.client.block.recovery.retries", 2);
+    TEST_UTIL.startMiniDFSCluster(3);
+    Path hbaseRootDir =
+      TEST_UTIL.getDFSCluster().getFileSystem().makeQualified(new Path("/hbase"));
+    LOG.info("hbase.rootdir=" + hbaseRootDir);
+    conf.set(HConstants.HBASE_DIR, hbaseRootDir.toString());
+    conf.setInt("hdfs.client.retries.number", 10);
+  }
+
+  
+  @Test
+  public void testNonIdempotentOpsWithRetries() throws IOException {
+    LOG.info("testNonIdempotentOpsWithRetries");
+
+    Path rootDir = new Path(conf.get(HConstants.HBASE_DIR));
+    FileSystem fs = TEST_UTIL.getTestFileSystem();
+    // Create a Region
+    assertTrue(HBaseFileSystem.createPathOnFileSystem(fs, TestHBaseFileSystem.conf, rootDir, true) != null);
+
+    boolean result = HBaseFileSystem.makeDirOnFileSystem(new MockFileSystemForCreate(), TestHBaseFileSystem.conf, new Path("/a"));
+    assertTrue("Couldn't create the directory", result);
+
+
+    result = HBaseFileSystem.renameDirForFileSystem(new MockFileSystem(), TestHBaseFileSystem.conf, new Path("/a"), new Path("/b"));
+    assertTrue("Couldn't rename the directory", result);
+
+    result = HBaseFileSystem.deleteDirFromFileSystem(new MockFileSystem(), TestHBaseFileSystem.conf, new Path("/a"));
+
+    assertTrue("Couldn't delete the directory", result);
+    fs.delete(rootDir, true);
+  }
+
+  static class MockFileSystemForCreate extends MockFileSystem {
+    @Override
+    public boolean exists(Path path) {
+      return false;
+    }
+  }
+
+  /**
+   * a mock fs which throws exception for first 3 times, and then process the call (returns the
+   * excepted result).
+   */
+  static class MockFileSystem extends FileSystem {
+    int retryCount;
+    final static int successRetryCount = 3;
+
+    public MockFileSystem() {
+      retryCount = 0;
+    }
+
+    @Override
+    public FSDataOutputStream append(Path arg0, int arg1, Progressable arg2) throws IOException {
+      throw new IOException("");
+    }
+
+    @Override
+    public FSDataOutputStream create(Path arg0, FsPermission arg1, boolean arg2, int arg3,
+        short arg4, long arg5, Progressable arg6) throws IOException {
+      LOG.debug("Create, " + retryCount);
+      if (retryCount++ < successRetryCount) throw new IOException("Something bad happen");
+      return null;
+    }
+
+    @Override
+    public boolean delete(Path arg0) throws IOException {
+      if (retryCount++ < successRetryCount) throw new IOException("Something bad happen");
+      return true;
+    }
+
+    @Override
+    public boolean delete(Path arg0, boolean arg1) throws IOException {
+      if (retryCount++ < successRetryCount) throw new IOException("Something bad happen");
+      return true;
+    }
+
+    @Override
+    public FileStatus getFileStatus(Path arg0) throws IOException {
+      FileStatus fs = new FileStatus();
+      return fs;
+    }
+
+    @Override
+    public boolean exists(Path path) {
+      return true;
+    }
+
+    @Override
+    public URI getUri() {
+      throw new RuntimeException("Something bad happen");
+    }
+
+    @Override
+    public Path getWorkingDirectory() {
+      throw new RuntimeException("Something bad happen");
+    }
+
+    @Override
+    public FileStatus[] listStatus(Path arg0) throws IOException {
+      throw new IOException("Something bad happen");
+    }
+
+    @Override
+    public boolean mkdirs(Path arg0, FsPermission arg1) throws IOException {
+      LOG.debug("mkdirs, " + retryCount);
+      if (retryCount++ < successRetryCount) throw new IOException("Something bad happen");
+      return true;
+    }
+
+    @Override
+    public FSDataInputStream open(Path arg0, int arg1) throws IOException {
+      throw new IOException("Something bad happen");
+    }
+
+    @Override
+    public boolean rename(Path arg0, Path arg1) throws IOException {
+      LOG.debug("rename, " + retryCount);
+      if (retryCount++ < successRetryCount) throw new IOException("Something bad happen");
+      return true;
+    }
+
+    @Override
+    public void setWorkingDirectory(Path arg0) {
+      throw new RuntimeException("Something bad happen");
+    }
+  }
+
+  
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java b/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
index f0e7e0c..a38b49d 100644
--- a/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
+++ b/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
@@ -73,7 +73,6 @@ public class TestOpenedRegionHandler {
   public void tearDown() throws Exception {
     // Stop the cluster
     TEST_UTIL.shutdownMiniCluster();
-    TEST_UTIL = new HBaseTestingUtility(resetConf);
   }
 
   @Test
-- 
1.7.0.4

