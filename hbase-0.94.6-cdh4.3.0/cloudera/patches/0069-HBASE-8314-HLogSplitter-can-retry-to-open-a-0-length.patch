From c72f6044f6b0d23c4d2497ac7aea13309ca34c66 Mon Sep 17 00:00:00 2001
From: Jimmy Xiang <jxiang@cloudera.com>
Date: Tue, 16 Apr 2013 13:48:44 -0700
Subject: [PATCH 69/96] HBASE-8314 HLogSplitter can retry to open a 0-length hlog file

Reason: Bug
Author: Jimmy Xiang
Ref: CDH-11180
---
 .../apache/hadoop/hbase/regionserver/wal/HLog.java |   57 +++++++++++++++-----
 .../hbase/regionserver/wal/TestHLogSplit.java      |   38 +++++++++++++
 2 files changed, 81 insertions(+), 14 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index e942fae..b32d100 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -23,6 +23,7 @@ import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.FileNotFoundException;
 import java.io.IOException;
+import java.io.InterruptedIOException;
 import java.io.OutputStream;
 import java.io.UnsupportedEncodingException;
 import java.lang.reflect.InvocationTargetException;
@@ -716,22 +717,50 @@ public class HLog implements Syncable {
   public static Reader getReader(final FileSystem fs, final Path path,
                                  Configuration conf)
       throws IOException {
-    try {
-
-      if (logReaderClass == null) {
+    if (logReaderClass == null) {
+      logReaderClass = conf.getClass("hbase.regionserver.hlog.reader.impl",
+        SequenceFileLogReader.class, Reader.class);
+    }
 
-        logReaderClass = conf.getClass("hbase.regionserver.hlog.reader.impl",
-            SequenceFileLogReader.class, Reader.class);
+    try {
+      // A hlog file could be under recovery, so it may take several
+      // tries to get it open. Instead of claiming it is corrupted, retry
+      // to open it up to 5 minutes by default.
+      long startWaiting = System.currentTimeMillis();
+      long openTimeout = conf.getInt("hbase.hlog.open.timeout", 300000) + startWaiting;
+      int nbAttempt = 0;
+      while (true) {
+        try {
+          HLog.Reader reader = logReaderClass.newInstance();
+          reader.init(fs, path, conf);
+          return reader;
+        } catch (IOException e) {
+          String msg = e.getMessage();
+          if (msg != null && msg.contains("Cannot obtain block length")) {
+            if (++nbAttempt == 1) {
+              LOG.warn("Lease should have recovered. This is not expected. Will retry", e);
+            }
+            if (nbAttempt > 2 && openTimeout < System.currentTimeMillis()) {
+              LOG.error("Can't open after " + nbAttempt + " attempts and "
+                + (System.currentTimeMillis() - startWaiting)
+                + "ms " + " for " + path);
+            } else {
+              try {
+                Thread.sleep(nbAttempt < 3 ? 500 : 1000);
+                continue; // retry
+              } catch (InterruptedException ie) {
+                InterruptedIOException iioe = new InterruptedIOException();
+                iioe.initCause(ie);
+                throw iioe;
+              }
+            }
+          }
+          throw e;
+        }
       }
-
-
-      HLog.Reader reader = logReaderClass.newInstance();
-      reader.init(fs, path, conf);
-      return reader;
-    } catch (IOException e) {
-      throw e;
-    }
-    catch (Exception e) {
+    } catch (IOException ie) {
+      throw ie;
+    } catch (Exception e) {
       throw new IOException("Cannot get log reader", e);
     }
   }
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
index d780391..a7e4fd8 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
@@ -737,6 +737,44 @@ public class TestHLogSplit {
     }
   }
 
+  @Test
+  public void testRetryOpenDuringRecovery() throws Exception {
+    generateHLogs(-1);
+
+    fs.initialize(fs.getUri(), conf);
+
+    FileSystem spiedFs = Mockito.spy(fs);
+    // The "Cannot obtain block length" part is very important,
+    // that's how it comes out of HDFS. If HDFS changes the exception
+    // message, this test needs to be adjusted accordingly.
+    //
+    // When DFSClient tries to open a file, HDFS needs to locate
+    // the last block of the file and get its length. However, if the
+    // last block is under recovery, HDFS may have problem to obtain
+    // the block length, in which case, retry may help.
+    Mockito.doAnswer(new Answer<FSDataInputStream>() {
+      private int count = 0;
+
+      public FSDataInputStream answer(InvocationOnMock invocation) throws Throwable {
+            if (count++ < 3) {
+                throw new IOException("Cannot obtain block length");
+            }
+            return (FSDataInputStream)invocation.callRealMethod();
+        }
+    }).when(spiedFs).open(Mockito.<Path>any(), Mockito.anyInt());
+
+    HLogSplitter logSplitter = new HLogSplitter(
+        conf, hbaseDir, hlogDir, oldLogDir, spiedFs);
+
+    try {
+      logSplitter.splitLog();
+      assertEquals(NUM_WRITERS, fs.listStatus(oldLogDir).length);
+      assertFalse(fs.exists(hlogDir));
+    } catch (IOException e) {
+      fail("There shouldn't be any exception but: " + e.toString());
+    }
+  }
+
   /**
    * Test log split process with fake data and lots of edits to trigger threading
    * issues.
-- 
1.7.0.4

