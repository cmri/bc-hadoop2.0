From e9d8b10509106176a60793dc236846d3f5787fa0 Mon Sep 17 00:00:00 2001
From: Jimmy Xiang <jxiang@apache.org>
Date: Sun, 14 Apr 2013 15:18:32 +0000
Subject: [PATCH 66/96] HBASE-1936 ClassLoader that loads from hdfs; useful adding filters to classpath without having to restart services

Reason: Improvement
Author: Jimmy Xiang
Ref: CDH-10307

git-svn-id: https://svn.apache.org/repos/asf/hbase/branches/0.94@1467789 13f79535-47bb-0310-9956-ffa450edef68

Conflicts:

	src/test/java/org/apache/hadoop/hbase/client/TestGet.java
---
 .../java/org/apache/hadoop/hbase/client/Get.java   |   18 +--
 .../java/org/apache/hadoop/hbase/client/Scan.java  |   17 +--
 .../org/apache/hadoop/hbase/filter/SkipFilter.java |   15 +-
 .../hadoop/hbase/filter/WhileMatchFilter.java      |   15 +-
 .../java/org/apache/hadoop/hbase/util/Classes.java |   57 +++++
 .../hadoop/hbase/util/DynamicClassLoader.java      |  218 ++++++++++++++++++++
 .../org/apache/hadoop/hbase/client/TestGet.java    |   61 ++++++
 .../hadoop/hbase/util/TestDynamicClassLoader.java  |  216 +++++++++++++++++++
 8 files changed, 565 insertions(+), 52 deletions(-)
 create mode 100644 src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/util/TestDynamicClassLoader.java

diff --git a/src/main/java/org/apache/hadoop/hbase/client/Get.java b/src/main/java/org/apache/hadoop/hbase/client/Get.java
index 0fb6838..ad9a058 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/Get.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/Get.java
@@ -19,14 +19,13 @@
  */
 package org.apache.hadoop.hbase.client;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.filter.Filter;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Classes;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableFactories;
 
 import java.io.DataInput;
 import java.io.DataOutput;
@@ -248,6 +247,7 @@ public class Get extends OperationWithAttributes
    * Method for retrieving the get's RowLock
    * @return RowLock
    */
+  @SuppressWarnings("deprecation")
   public RowLock getRowLock() {
     return new RowLock(this.row, this.lockId);
   }
@@ -400,7 +400,8 @@ public class Get extends OperationWithAttributes
     this.maxVersions = in.readInt();
     boolean hasFilter = in.readBoolean();
     if (hasFilter) {
-      this.filter = (Filter)createForName(Bytes.toString(Bytes.readByteArray(in)));
+      this.filter = Classes.createWritableForName(
+        Bytes.toString(Bytes.readByteArray(in)));
       this.filter.readFields(in);
     }
     this.cacheBlocks = in.readBoolean();
@@ -458,15 +459,4 @@ public class Get extends OperationWithAttributes
     }
     writeAttributes(out);
   }
-
-  @SuppressWarnings("unchecked")
-  private Writable createForName(String className) {
-    try {
-      Class<? extends Writable> clazz =
-        (Class<? extends Writable>) Class.forName(className);
-      return WritableFactories.newInstance(clazz, new Configuration());
-    } catch (ClassNotFoundException e) {
-      throw new RuntimeException("Can't find class " + className);
-    }
-  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/client/Scan.java b/src/main/java/org/apache/hadoop/hbase/client/Scan.java
index c56500f..9b2aad7 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/Scan.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/Scan.java
@@ -20,14 +20,13 @@
 
 package org.apache.hadoop.hbase.client;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.filter.Filter;
 import org.apache.hadoop.hbase.filter.IncompatibleFilterException;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Classes;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableFactories;
 
 import java.io.DataInput;
 import java.io.DataOutput;
@@ -573,17 +572,6 @@ public class Scan extends OperationWithAttributes implements Writable {
     return map;
   }
 
-  @SuppressWarnings("unchecked")
-  private Writable createForName(String className) {
-    try {
-      Class<? extends Writable> clazz =
-        (Class<? extends Writable>) Class.forName(className);
-      return WritableFactories.newInstance(clazz, new Configuration());
-    } catch (ClassNotFoundException e) {
-      throw new RuntimeException("Can't find class " + className);
-    }
-  }
-
   //Writable
   public void readFields(final DataInput in)
   throws IOException {
@@ -598,7 +586,8 @@ public class Scan extends OperationWithAttributes implements Writable {
     this.caching = in.readInt();
     this.cacheBlocks = in.readBoolean();
     if(in.readBoolean()) {
-      this.filter = (Filter)createForName(Bytes.toString(Bytes.readByteArray(in)));
+      this.filter = Classes.createWritableForName(
+        Bytes.toString(Bytes.readByteArray(in)));
       this.filter.readFields(in);
     }
     this.tr = new TimeRange();
diff --git a/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java b/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
index c27ac17..5fe17df 100644
--- a/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
+++ b/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
@@ -21,11 +21,11 @@
 package org.apache.hadoop.hbase.filter;
 
 import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Classes;
 
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
-import java.util.List;
 
 /**
  * A wrapper filter that filters an entire row if any of the KeyValue checks do
@@ -91,17 +91,8 @@ public class SkipFilter extends FilterBase {
   }
 
   public void readFields(DataInput in) throws IOException {
-    String className = in.readUTF();
-    try {
-      this.filter = (Filter)(Class.forName(className).newInstance());
-      this.filter.readFields(in);
-    } catch (InstantiationException e) {
-      throw new RuntimeException("Failed deserialize.", e);
-    } catch (IllegalAccessException e) {
-      throw new RuntimeException("Failed deserialize.", e);
-    } catch (ClassNotFoundException e) {
-      throw new RuntimeException("Failed deserialize.", e);
-    }
+    this.filter = Classes.createForName(in.readUTF());
+    this.filter.readFields(in);
   }
 
   public boolean isFamilyEssential(byte[] name) {
diff --git a/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java b/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
index 591a247..bed8d58 100644
--- a/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
+++ b/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
@@ -21,11 +21,11 @@
 package org.apache.hadoop.hbase.filter;
 
 import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Classes;
 
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
-import java.util.List;
 
 /**
  * A wrapper filter that returns true from {@link #filterAllRemaining()} as soon
@@ -92,17 +92,8 @@ public class WhileMatchFilter extends FilterBase {
   }
 
   public void readFields(DataInput in) throws IOException {
-    String className = in.readUTF();
-    try {
-      this.filter = (Filter)(Class.forName(className).newInstance());
-      this.filter.readFields(in);
-    } catch (InstantiationException e) {
-      throw new RuntimeException("Failed deserialize.", e);
-    } catch (IllegalAccessException e) {
-      throw new RuntimeException("Failed deserialize.", e);
-    } catch (ClassNotFoundException e) {
-      throw new RuntimeException("Failed deserialize.", e);
-    }
+    this.filter = Classes.createForName(in.readUTF());
+    this.filter.readFields(in);
   }
 
   public boolean isFamilyEssential(byte[] name) {
diff --git a/src/main/java/org/apache/hadoop/hbase/util/Classes.java b/src/main/java/org/apache/hadoop/hbase/util/Classes.java
index 2b353e7..beca9c2 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/Classes.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/Classes.java
@@ -20,12 +20,28 @@
 
 package org.apache.hadoop.hbase.util;
 
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.filter.Filter;
+import org.apache.hadoop.io.WritableFactories;
+
 /**
  * Utilities for class manipulation.
  */
 public class Classes {
 
   /**
+   * Dynamic class loader to load filter/comparators
+   */
+  private final static ClassLoader CLASS_LOADER;
+
+  static {
+    ClassLoader parent = Classes.class.getClassLoader();
+    Configuration conf = HBaseConfiguration.create();
+    CLASS_LOADER = new DynamicClassLoader(conf, parent);
+  }
+
+  /**
    * Equivalent of {@link Class#forName(String)} which also returns classes for
    * primitives like <code>boolean</code>, etc.
    * 
@@ -61,6 +77,7 @@ public class Classes {
     return valueType;
   }
 
+  @SuppressWarnings("rawtypes")
   public static String stringify(Class[] classes) {
     StringBuilder buf = new StringBuilder();
     if (classes != null) {
@@ -75,4 +92,44 @@ public class Classes {
     }
     return buf.toString();
   }
+
+  /**
+   * Used to dynamically load a filter class, and create a Writable filter.
+   * This filter class most likely extends Configurable.
+   *
+   * @param className the filter class name.
+   * @return a filter
+   */
+  @SuppressWarnings("unchecked")
+  public static Filter createWritableForName(String className) {
+    try {
+      Class<? extends Filter> clazz =
+        (Class<? extends Filter>) Class.forName(className, true, CLASS_LOADER);
+      return (Filter)WritableFactories.newInstance(clazz, new Configuration());
+    } catch (ClassNotFoundException e) {
+      throw new RuntimeException("Can't find class " + className);
+    }
+  }
+
+  /**
+   * This method is almost the same as #createWritableForName, except
+   * that this one doesn't expect the filter class to extends Configurable.
+   *
+   * @param className the filter class name.
+   * @return a filter
+   */
+  @SuppressWarnings("unchecked")
+  public static Filter createForName(String className) {
+    try {
+      Class<? extends Filter> clazz =
+        (Class<? extends Filter>)Class.forName(className, true, CLASS_LOADER);
+      return (Filter)clazz.newInstance();
+    } catch (ClassNotFoundException e) {
+      throw new RuntimeException("Can't find class " + className);
+    } catch (InstantiationException e) {
+      throw new RuntimeException("Couldn't instantiate " + className, e);
+    } catch (IllegalAccessException e) {
+      throw new RuntimeException("No access to " + className, e);
+    }
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java b/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
new file mode 100644
index 0000000..01132ae
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
@@ -0,0 +1,218 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.util.HashMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * This is a class loader that can load classes dynamically from new
+ * jar files under a configured folder. It always uses its parent class
+ * loader to load a class at first. Only if its parent class loader
+ * can not load a class, we will try to load it using the logic here.
+ * <p>
+ * We can't unload a class already loaded. So we will use the existing
+ * jar files we already know to load any class which can't be loaded
+ * using the parent class loader. If we still can't load the class from
+ * the existing jar files, we will check if any new jar file is added,
+ * if so, we will load the new jar file and try to load the class again.
+ * If still failed, a class not found exception will be thrown.
+ * <p>
+ * Be careful in uploading new jar files and make sure all classes
+ * are consistent, otherwise, we may not be able to load your
+ * classes properly.
+ */
+@InterfaceAudience.Private
+public class DynamicClassLoader extends URLClassLoader {
+  private static final Log LOG =
+      LogFactory.getLog(DynamicClassLoader.class);
+
+  // Dynamic jars are put under ${hbase.local.dir}/dynamic/jars/
+  private static final String DYNAMIC_JARS_DIR = File.separator
+    + "dynamic" + File.separator + "jars" + File.separator;
+
+  /**
+   * Parent class loader used to load any class at first.
+   */
+  private final ClassLoader parent;
+
+  private File localDir;
+
+  // FileSystem of the remote path, set only if remoteDir != null
+  private FileSystem remoteDirFs;
+  private Path remoteDir;
+
+  // Last modified time of local jars
+  private HashMap<String, Long> jarModifiedTime;
+
+  /**
+   * Creates a DynamicClassLoader that can load classes dynamically
+   * from jar files under a specific folder.
+   *
+   * @param conf the configuration for the cluster.
+   * @param parent the parent ClassLoader to set.
+   */
+  public DynamicClassLoader(
+      final Configuration conf, final ClassLoader parent) {
+    super(new URL[]{}, parent);
+    this.parent = parent;
+
+    jarModifiedTime = new HashMap<String, Long>();
+    String localDirPath = conf.get("hbase.local.dir") + DYNAMIC_JARS_DIR;
+    localDir = new File(localDirPath);
+    if (!localDir.mkdirs() && !localDir.isDirectory()) {
+      throw new RuntimeException("Failed to create local dir " + localDir.getPath()
+        + ", DynamicClassLoader failed to init");
+    }
+
+    String remotePath = conf.get("hbase.dynamic.jars.dir");
+    if (remotePath == null || remotePath.equals(localDirPath)) {
+      remoteDir = null;  // ignore if it is the same as the local path
+    } else {
+      remoteDir = new Path(remotePath);
+      try {
+        remoteDirFs = remoteDir.getFileSystem(conf);
+      } catch (IOException ioe) {
+        LOG.warn("Failed to identify the fs of dir "
+          + remoteDir + ", ignored", ioe);
+        remoteDir = null;
+      }
+    }
+  }
+
+  @Override
+  public Class<?> loadClass(String name)
+      throws ClassNotFoundException {
+    try {
+      return parent.loadClass(name);
+    } catch (ClassNotFoundException e) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Class " + name + " not found - using dynamical class loader");
+      }
+
+      // Check whether the class has already been loaded:
+      Class<?> clasz = findLoadedClass(name);
+      if (clasz != null) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Class " + name + " already loaded");
+        }
+      }
+      else {
+        try {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Finding class: " + name);
+          }
+          clasz = findClass(name);
+        } catch (ClassNotFoundException cnfe) {
+          // Load new jar files if any
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Loading new jar files, if any");
+          }
+          loadNewJars();
+
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Finding class again: " + name);
+          }
+          clasz = findClass(name);
+        }
+      }
+      return clasz;
+    }
+  }
+
+  private synchronized void loadNewJars() {
+    // Refresh local jar file lists
+    for (File file: localDir.listFiles()) {
+      String fileName = file.getName();
+      if (jarModifiedTime.containsKey(fileName)) {
+        continue;
+      }
+      if (file.isFile() && fileName.endsWith(".jar")) {
+        jarModifiedTime.put(fileName, Long.valueOf(file.lastModified()));
+        try {
+          URL url = file.toURI().toURL();
+          addURL(url);
+        } catch (MalformedURLException mue) {
+          // This should not happen, just log it
+          LOG.warn("Failed to load new jar " + fileName, mue);
+        }
+      }
+    }
+
+    // Check remote files
+    FileStatus[] statuses = null;
+    if (remoteDir != null) {
+      try {
+        statuses = remoteDirFs.listStatus(remoteDir);
+      } catch (IOException ioe) {
+        LOG.warn("Failed to check remote dir status " + remoteDir, ioe);
+      }
+    }
+    if (statuses == null || statuses.length == 0) {
+      return; // no remote files at all
+    }
+
+    for (FileStatus status: statuses) {
+      if (status.isDir()) continue; // No recursive lookup
+      Path path = status.getPath();
+      String fileName = path.getName();
+      if (!fileName.endsWith(".jar")) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Ignored non-jar file " + fileName);
+        }
+        continue; // Ignore non-jar files
+      }
+      Long cachedLastModificationTime = jarModifiedTime.get(fileName);
+      if (cachedLastModificationTime != null) {
+        long lastModified = status.getModificationTime();
+        if (lastModified < cachedLastModificationTime.longValue()) {
+          // There could be some race, for example, someone uploads
+          // a new one right in the middle the old one is copied to
+          // local. We can check the size as well. But it is still
+          // not guaranteed. This should be rare. Most likely,
+          // we already have the latest one.
+          // If you are unlucky to hit this race issue, you have
+          // to touch the remote jar to update its last modified time
+          continue;
+        }
+      }
+      try {
+        // Copy it to local
+        File dst = new File(localDir, fileName);
+        remoteDirFs.copyToLocalFile(path, new Path(dst.getPath()));
+        jarModifiedTime.put(fileName, Long.valueOf(dst.lastModified()));
+        URL url = dst.toURI().toURL();
+        addURL(url);
+      } catch (IOException ioe) {
+        LOG.warn("Failed to load new jar " + fileName, ioe);
+      }
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/client/TestGet.java b/src/test/java/org/apache/hadoop/hbase/client/TestGet.java
index 9cf7ecf..9846774 100644
--- a/src/test/java/org/apache/hadoop/hbase/client/TestGet.java
+++ b/src/test/java/org/apache/hadoop/hbase/client/TestGet.java
@@ -20,24 +20,56 @@
 
 package org.apache.hadoop.hbase.client;
 
+import static org.junit.Assert.fail;
+
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
+import java.io.DataInput;
 import java.io.DataInputStream;
 import java.io.DataOutput;
 import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileOutputStream;
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Set;
 
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.util.Base64;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.Assert;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import com.google.common.io.ByteStreams;
+
 // TODO: cover more test cases
 @Category(SmallTests.class)
 public class TestGet {
+
+  private static final String WRITABLE_GET =
+    "AgD//////////wAAAAEBD3Rlc3QuTW9ja0ZpbHRlcgEAAAAAAAAAAH//////////AQAAAAAAAAAA";
+
+  private static final String MOCK_FILTER_JAR =
+    "UEsDBBQACAgIACmBi0IAAAAAAAAAAAAAAAAJAAQATUVUQS1JTkYv/soAAAMAUEsHCAAAAAACAAAA" +
+    "AAAAAFBLAwQUAAgICAApgYtCAAAAAAAAAAAAAAAAFAAAAE1FVEEtSU5GL01BTklGRVNULk1G803M" +
+    "y0xLLS7RDUstKs7Mz7NSMNQz4OVyLkpNLElN0XWqBAmY6xnEG1gqaPgXJSbnpCo45xcV5BcllgCV" +
+    "a/Jy8XIBAFBLBwgxyqRbQwAAAEQAAABQSwMECgAACAAAbICLQgAAAAAAAAAAAAAAAAUAAAB0ZXN0" +
+    "L1BLAwQUAAgICAAcgItCAAAAAAAAAAAAAAAAFQAAAHRlc3QvTW9ja0ZpbHRlci5jbGFzc41Qy07C" +
+    "QBS9A4VKBZGHoO7cgQvHmLjCuPBBQlJloWE/tCMdLZ1mOlV/y5WJCz/AjzLeDqCRYOIs7uuce87N" +
+    "fHy+vQPAEezakCNQ1TzR9Ep6D30Raq5ssAh0pZpQFjMv4DRgvpQxDcYs4fTOcOiMeoYTAsUTEQl9" +
+    "SiDf6Y4IWOfS5w7koVSGAhTRwBURv06nY65u2TjEjborPRaOmBJZPx9aOhAJgZq7dE+PgKM48/uC" +
+    "hz4SWh33nj0yKiS9YJoNojjVvczYuXz2eKyFjBIb6gQaC9pg+I2gDVOTQwRXiBAoPCmh8Zb2b49h" +
+    "qhcmzVUAet/IVHkcL8bt6s/xBxkb9gA/B7KXxwo/BaONHcVMMBf2X2HtBYscOBiLZliCdYzlGQFz" +
+    "BTOBDagiaxNrC7uakTk2m4guS1SMRGsGziWyqgFN47xlsH+K1f4UaxuxbcPf+QJQSwcI8UIYqlEB" +
+    "AABeAgAAUEsBAhQAFAAICAgAKYGLQgAAAAACAAAAAAAAAAkABAAAAAAAAAAAAAAAAAAAAE1FVEEt" +
+    "SU5GL/7KAABQSwECFAAUAAgICAApgYtCMcqkW0MAAABEAAAAFAAAAAAAAAAAAAAAAAA9AAAATUVU" +
+    "QS1JTkYvTUFOSUZFU1QuTUZQSwECCgAKAAAIAABsgItCAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAA" +
+    "AADCAAAAdGVzdC9QSwECFAAUAAgICAAcgItC8UIYqlEBAABeAgAAFQAAAAAAAAAAAAAAAADlAAAA" +
+    "dGVzdC9Nb2NrRmlsdGVyLmNsYXNzUEsFBgAAAAAEAAQA8wAAAHkCAAAAAA==";
+
   @Test
   public void testAttributesSerialization() throws IOException {
     Get get = new Get();
@@ -117,6 +149,35 @@ public class TestGet {
     Assert.assertEquals(1, qualifiers.size());
   }
 
+  @Test
+  public void testDynamicFilter() throws Exception {
+    DataInput dis = ByteStreams.newDataInput(Base64.decode(WRITABLE_GET));
+    Get get = new Get();
+    try {
+      get.readFields(dis);
+      fail("Should not be able to load the filter class");
+    } catch (RuntimeException re) {
+      String msg = re.getMessage();
+      Assert.assertTrue(msg != null
+        && msg.contains("Can't find class test.MockFilter"));
+    }
+
+    Configuration conf = HBaseConfiguration.create();
+    String localPath = conf.get("hbase.local.dir") + File.separator
+      + "dynamic" + File.separator + "jars" + File.separator;
+    File jarFile = new File(localPath, "MockFilter.jar");
+    jarFile.deleteOnExit();
+
+    FileOutputStream fos = new FileOutputStream(jarFile);
+    fos.write(Base64.decode(MOCK_FILTER_JAR));
+    fos.close();
+
+    dis = ByteStreams.newDataInput(Base64.decode(WRITABLE_GET));
+    get.readFields(dis);
+    Assert.assertEquals("test.MockFilter",
+      get.getFilter().getClass().getName());
+  }
+
   @org.junit.Rule
   public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =
     new org.apache.hadoop.hbase.ResourceCheckerJUnitRule();
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestDynamicClassLoader.java b/src/test/java/org/apache/hadoop/hbase/util/TestDynamicClassLoader.java
new file mode 100644
index 0000000..ab5e788
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestDynamicClassLoader.java
@@ -0,0 +1,216 @@
+/*
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.FileWriter;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.jar.JarEntry;
+import java.util.jar.JarOutputStream;
+import java.util.jar.Manifest;
+
+import javax.tools.JavaCompiler;
+import javax.tools.JavaFileObject;
+import javax.tools.StandardJavaFileManager;
+import javax.tools.ToolProvider;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.SmallTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+/**
+ * Test TestDynamicClassLoader
+ */
+@Category(SmallTests.class)
+public class TestDynamicClassLoader {
+  private static final Log LOG = LogFactory.getLog(TestDynamicClassLoader.class);
+
+  private static final Configuration conf = HBaseConfiguration.create();
+
+  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+
+  static {
+    conf.set("hbase.dynamic.jars.dir", TEST_UTIL.getDataTestDir().toString());
+  }
+
+  // generate jar file
+  private boolean createJarArchive(File archiveFile, File[] tobeJared) {
+    try {
+      byte buffer[] = new byte[4096];
+      // Open archive file
+      FileOutputStream stream = new FileOutputStream(archiveFile);
+      JarOutputStream out = new JarOutputStream(stream, new Manifest());
+
+      for (int i = 0; i < tobeJared.length; i++) {
+        if (tobeJared[i] == null || !tobeJared[i].exists()
+            || tobeJared[i].isDirectory()) {
+          continue;
+        }
+
+        // Add archive entry
+        JarEntry jarAdd = new JarEntry(tobeJared[i].getName());
+        jarAdd.setTime(tobeJared[i].lastModified());
+        out.putNextEntry(jarAdd);
+
+        // Write file to archive
+        FileInputStream in = new FileInputStream(tobeJared[i]);
+        while (true) {
+          int nRead = in.read(buffer, 0, buffer.length);
+          if (nRead <= 0)
+            break;
+          out.write(buffer, 0, nRead);
+        }
+        in.close();
+      }
+      out.close();
+      stream.close();
+      LOG.info("Adding classes to jar file completed");
+      return true;
+    } catch (Exception ex) {
+      LOG.error("Error: " + ex.getMessage());
+      return false;
+    }
+  }
+
+  private File buildJar(
+      String className, String folder) throws Exception {
+    String javaCode = "public class " + className + " {}";
+    Path srcDir = new Path(TEST_UTIL.getDataTestDir(), "src");
+    File srcDirPath = new File(srcDir.toString());
+    srcDirPath.mkdirs();
+    File sourceCodeFile = new File(srcDir.toString(), className + ".java");
+    BufferedWriter bw = new BufferedWriter(new FileWriter(sourceCodeFile));
+    bw.write(javaCode);
+    bw.close();
+
+    // compile it by JavaCompiler
+    JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();
+    ArrayList<String> srcFileNames = new ArrayList<String>();
+    srcFileNames.add(sourceCodeFile.toString());
+    StandardJavaFileManager fm = compiler.getStandardFileManager(null, null,
+      null);
+    Iterable<? extends JavaFileObject> cu =
+      fm.getJavaFileObjects(sourceCodeFile);
+    List<String> options = new ArrayList<String>();
+    options.add("-classpath");
+    // only add hbase classes to classpath. This is a little bit tricky: assume
+    // the classpath is {hbaseSrc}/target/classes.
+    String currentDir = new File(".").getAbsolutePath();
+    String classpath =
+        currentDir + File.separator + "target"+ File.separator + "classes" +
+        System.getProperty("path.separator") + System.getProperty("java.class.path");
+    options.add(classpath);
+    LOG.debug("Setting classpath to: "+classpath);
+
+    JavaCompiler.CompilationTask task = compiler.getTask(null, fm, null,
+      options, null, cu);
+    assertTrue("Compile file " + sourceCodeFile + " failed.", task.call());
+
+    // build a jar file by the classes files
+    String jarFileName = className + ".jar";
+    File jarFile = new File(folder, jarFileName);
+    if (!createJarArchive(jarFile,
+        new File[]{new File(srcDir.toString(), className + ".class")})){
+      assertTrue("Build jar file failed.", false);
+    }
+    return jarFile;
+  }
+
+  @Test
+  public void testLoadClassFromLocalPath() throws Exception {
+    ClassLoader parent = TestDynamicClassLoader.class.getClassLoader();
+    DynamicClassLoader classLoader = new DynamicClassLoader(conf, parent);
+
+    String className = "TestLoadClassFromLocalPath";
+    try {
+      classLoader.loadClass(className);
+      fail("Should not be able to load class " + className);
+    } catch (ClassNotFoundException cnfe) {
+      // expected, move on
+    }
+
+    try {
+      buildJar(className, localDirPath());
+      classLoader.loadClass(className);
+    } catch (ClassNotFoundException cnfe) {
+      LOG.error("Should be able to load class " + className, cnfe);
+      fail(cnfe.getMessage());
+    } finally {
+      deleteClass(className);
+    }
+  }
+
+  @Test
+  public void testLoadClassFromAnotherPath() throws Exception {
+    ClassLoader parent = TestDynamicClassLoader.class.getClassLoader();
+    DynamicClassLoader classLoader = new DynamicClassLoader(conf, parent);
+
+    String className = "TestLoadClassFromAnotherPath";
+    try {
+      classLoader.loadClass(className);
+      fail("Should not be able to load class " + className);
+    } catch (ClassNotFoundException cnfe) {
+      // expected, move on
+    }
+
+    try {
+      buildJar(className, TEST_UTIL.getDataTestDir().toString());
+      classLoader.loadClass(className);
+    } catch (ClassNotFoundException cnfe) {
+      LOG.error("Should be able to load class " + className, cnfe);
+      fail(cnfe.getMessage());
+    } finally {
+      deleteClass(className);
+    }
+  }
+
+  private String localDirPath() {
+    return conf.get("hbase.local.dir") + File.separator
+      + "dynamic" + File.separator + "jars" + File.separator;
+  }
+
+  private void deleteClass(String className) throws Exception {
+    String jarFileName = className + ".jar";
+    File file = new File(TEST_UTIL.getDataTestDir().toString(), jarFileName);
+    file.deleteOnExit();
+
+    file = new File(conf.get("hbase.dynamic.jars.dir"), jarFileName);
+    file.deleteOnExit();
+
+    file = new File(localDirPath(), jarFileName);
+    file.deleteOnExit();
+  }
+
+  @org.junit.Rule
+  public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =
+    new org.apache.hadoop.hbase.ResourceCheckerJUnitRule();
+}
-- 
1.7.0.4

