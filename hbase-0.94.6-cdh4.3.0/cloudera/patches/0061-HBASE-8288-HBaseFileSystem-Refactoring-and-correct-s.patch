From d3e8a9bbd2c938dd0304774d1eb263ddcd9604e1 Mon Sep 17 00:00:00 2001
From: Himanshu <himanshu@cloudera.com>
Date: Mon, 8 Apr 2013 15:30:42 -0600
Subject: [PATCH 61/96] HBASE-8288 HBaseFileSystem: Refactoring and correct semantics for createPath methods

Reference: CDH-10625
Author: Himanshu Vashishtha
Reason: Usability
---
 .../org/apache/hadoop/hbase/HBaseFileSystem.java   |   86 +++++++++-----------
 .../apache/hadoop/hbase/backup/HFileArchiver.java  |   19 ++---
 .../java/org/apache/hadoop/hbase/io/Reference.java |    2 +-
 .../hadoop/hbase/master/MasterFileSystem.java      |   18 ++--
 .../hadoop/hbase/master/SplitLogManager.java       |    2 +-
 .../hadoop/hbase/master/cleaner/CleanerChore.java  |    8 +-
 .../hbase/master/handler/CreateTableHandler.java   |    2 +-
 .../hbase/master/handler/DeleteTableHandler.java   |    2 +-
 .../apache/hadoop/hbase/regionserver/HRegion.java  |   18 ++--
 .../hbase/regionserver/HRegionFileSystem.java      |    2 +-
 .../hbase/regionserver/SplitTransaction.java       |    6 +-
 .../apache/hadoop/hbase/regionserver/Store.java    |    6 +-
 .../hadoop/hbase/regionserver/StoreFile.java       |    6 +-
 .../apache/hadoop/hbase/regionserver/wal/HLog.java |   12 ++--
 .../hbase/regionserver/wal/HLogFileSystem.java     |    2 +-
 .../hbase/regionserver/wal/HLogSplitter.java       |   32 ++++----
 .../hadoop/hbase/util/FSTableDescriptors.java      |    8 +-
 .../java/org/apache/hadoop/hbase/util/FSUtils.java |    4 +-
 .../apache/hadoop/hbase/zookeeper/ZKSplitLog.java  |    2 +-
 .../apache/hadoop/hbase/HBaseTestingUtility.java   |    1 +
 .../apache/hadoop/hbase/TestHBaseFileSystem.java   |   20 ++++-
 21 files changed, 130 insertions(+), 128 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java b/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
index eebac4d..5f83231 100644
--- a/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
+++ b/src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
@@ -46,21 +46,25 @@ public abstract class HBaseFileSystem {
   private static int baseSleepBeforeRetries;
   private static final int DEFAULT_HDFS_CLIENT_RETRIES_NUMBER = 10;
   private static final int DEFAULT_BASE_SLEEP_BEFORE_RETRIES = 1000;
+  // This static block is added for performance reasons. This is to ensure we are not checking
+  // in the method calls whether retry properties are set or not. Refer to HBase-8288 for more
+  // context.
+  static {
+    setRetryCounts(HBaseConfiguration.create());
+  }
 
   
   /**
    * Deletes a file. Assumes the user has already checked for this directory existence.
-   * @param dir
    * @param fs
-   * @param conf
+   * @param dir
    * @return true if the directory is deleted.
    * @throws IOException
    */
-  public static boolean deleteFileFromFileSystem(FileSystem fs, Configuration conf, Path dir)
+  public static boolean deleteFileFromFileSystem(FileSystem fs, Path dir)
       throws IOException {
     IOException lastIOE = null;
     int i = 0;
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.delete(dir, false);
@@ -77,17 +81,15 @@ public abstract class HBaseFileSystem {
   
   /**
    * Deletes a directory. Assumes the user has already checked for this directory existence.
-   * @param dir
    * @param fs
-   * @param conf
+   * @param dir
    * @return true if the directory is deleted.
    * @throws IOException
    */
-  public static boolean deleteDirFromFileSystem(FileSystem fs, Configuration conf, Path dir)
+  public static boolean deleteDirFromFileSystem(FileSystem fs, Path dir)
       throws IOException {
     IOException lastIOE = null;
     int i = 0;
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.delete(dir, true);
@@ -101,30 +103,26 @@ public abstract class HBaseFileSystem {
     throw new IOException("Exception in deleteDirFromFileSystem", lastIOE);
   }
 
-  protected static void checkAndSetRetryCounts(Configuration conf) {
-    if (hdfsClientRetriesNumber == 0) {
-      hdfsClientRetriesNumber = conf.getInt("hdfs.client.retries.number",
-        DEFAULT_HDFS_CLIENT_RETRIES_NUMBER);
-      baseSleepBeforeRetries = conf.getInt("hdfs.client.sleep.before.retries",
-        DEFAULT_BASE_SLEEP_BEFORE_RETRIES);
-    }
+  protected static void setRetryCounts(Configuration conf) {
+    hdfsClientRetriesNumber = conf.getInt("hdfs.client.retries.number",
+      DEFAULT_HDFS_CLIENT_RETRIES_NUMBER);
+    baseSleepBeforeRetries = conf.getInt("hdfs.client.sleep.before.retries",
+      DEFAULT_BASE_SLEEP_BEFORE_RETRIES);
   }
   
   /**
    * Creates a directory for a filesystem and configuration object. Assumes the user has already
    * checked for this directory existence.
    * @param fs
-   * @param conf
    * @param dir
    * @return the result of fs.mkdirs(). In case underlying fs throws an IOException, it checks
    *         whether the directory exists or not, and returns true if it exists.
    * @throws IOException
    */
-  public static boolean makeDirOnFileSystem(FileSystem fs, Configuration conf, Path dir)
+  public static boolean makeDirOnFileSystem(FileSystem fs, Path dir)
       throws IOException {
     int i = 0;
     IOException lastIOE = null;
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.mkdirs(dir);
@@ -139,18 +137,16 @@ public abstract class HBaseFileSystem {
   
   /**
    * Renames a directory. Assumes the user has already checked for this directory existence.
-   * @param src
    * @param fs
+   * @param src
    * @param dst
-   * @param conf
    * @return true if the directory is renamed.
    * @throws IOException
    */
-  public static boolean renameDirForFileSystem(FileSystem fs, Configuration conf, Path src, Path dst)
+  public static boolean renameDirForFileSystem(FileSystem fs, Path src, Path dst)
       throws IOException {
     IOException lastIOE = null;
     int i = 0;
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.rename(src, dst);
@@ -163,30 +159,29 @@ public abstract class HBaseFileSystem {
     } while (++i <= hdfsClientRetriesNumber);
     throw new IOException("Exception in renameDirForFileSystem", lastIOE);
   }
-  
-/**
- * Creates a path on the file system. Checks whether the path exists already or not, and use it
- * for retrying in case underlying fs throws an exception.
- * @param fs
- * @param conf
- * @param dir
- * @param overwrite
- * @return
- * @throws IOException
- */
-  public static FSDataOutputStream createPathOnFileSystem(FileSystem fs, Configuration conf, Path dir,
-      boolean overwrite) throws IOException {
+
+  /**
+   * Creates a path on the file system. Checks whether the path exists already or not, and use it
+   * for retrying in case underlying fs throws an exception. If the dir already exists and overwrite
+   * flag is false, the underlying FileSystem throws an IOE. It is not retried and the IOE is
+   * re-thrown to the caller.
+   * @param fs
+   * @param dir
+   * @param overwrite
+   * @return
+   * @throws IOException
+   */
+  public static FSDataOutputStream createPathOnFileSystem(FileSystem fs, Path dir, boolean overwrite)
+      throws IOException {
     int i = 0;
     boolean existsBefore = fs.exists(dir);
     IOException lastIOE = null;
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.create(dir, overwrite);
       } catch (IOException ioe) {
         lastIOE = ioe;
-        // directory is present, don't overwrite
-        if (!existsBefore && fs.exists(dir)) return fs.create(dir, false);
+        if (existsBefore && !overwrite) throw ioe;// a legitimate exception
         sleepBeforeRetry("Create Path", i + 1);
       }
     } while (++i <= hdfsClientRetriesNumber);
@@ -195,6 +190,8 @@ public abstract class HBaseFileSystem {
 
   /**
    * Creates the specified file with the given permission.
+   * If the dir already exists and the overwrite flag is false, underlying FileSystem throws
+   * an IOE. It is not retried and the IOE is re-thrown to the caller.
    * @param fs
    * @param path
    * @param perm
@@ -202,21 +199,18 @@ public abstract class HBaseFileSystem {
    * @return
    * @throws IOException 
    */
-  public static FSDataOutputStream createPathWithPermsOnFileSystem(FileSystem fs,
-      Configuration conf, Path path, FsPermission perm, boolean overwrite) throws IOException {
+  public static FSDataOutputStream createPathWithPermsOnFileSystem(FileSystem fs, Path path,
+      FsPermission perm, boolean overwrite) throws IOException {
     int i = 0;
     IOException lastIOE = null;
     boolean existsBefore = fs.exists(path);
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.create(path, perm, overwrite, fs.getConf().getInt("io.file.buffer.size", 4096),
           fs.getDefaultReplication(), fs.getDefaultBlockSize(), null);
       } catch (IOException ioe) {
         lastIOE = ioe;
-        // path is present now, don't overwrite
-        if (!existsBefore && fs.exists(path)) return fs.create(path, false);
-        // if it existed before let's retry in case we get IOE, as we can't rely on fs.exists()
+        if (existsBefore && !overwrite) throw ioe;// a legitimate exception
         sleepBeforeRetry("Create Path with Perms", i + 1);
       }
     } while (++i <= hdfsClientRetriesNumber);
@@ -226,16 +220,14 @@ public abstract class HBaseFileSystem {
 /**
  * Creates the file. Assumes the user has already checked for this file existence.
  * @param fs
- * @param conf
  * @param dir
  * @return result true if the file is created with this call, false otherwise.
  * @throws IOException
  */
-  public static boolean createNewFileOnFileSystem(FileSystem fs, Configuration conf, Path file)
+  public static boolean createNewFileOnFileSystem(FileSystem fs, Path file)
       throws IOException {
     int i = 0;
     IOException lastIOE = null;
-    checkAndSetRetryCounts(conf);
     do {
       try {
         return fs.createNewFile(file);
diff --git a/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java b/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
index 808b1f8..8a14b11 100644
--- a/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
+++ b/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
@@ -217,7 +217,7 @@ public class HFileArchiver {
     Path storeArchiveDir = HFileArchiveUtil.getStoreArchivePath(conf, parent, family);
 
     // make sure we don't archive if we can't and that the archive dir exists
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, storeArchiveDir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, storeArchiveDir)) {
       throw new IOException("Could not make archive directory (" + storeArchiveDir + ") for store:"
           + Bytes.toString(family) + ", deleting compacted files instead.");
     }
@@ -251,7 +251,7 @@ public class HFileArchiver {
       Configuration conf, Path tableDir, byte[] family, Path storeFile) throws IOException {
     Path storeArchiveDir = HFileArchiveUtil.getStoreArchivePath(conf, regionInfo, tableDir, family);
     // make sure we don't archive if we can't and that the archive dir exists
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, storeArchiveDir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, storeArchiveDir)) {
       throw new IOException("Could not make archive directory (" + storeArchiveDir + ") for store:"
           + Bytes.toString(family) + ", deleting compacted files instead.");
     }
@@ -319,7 +319,7 @@ public class HFileArchiver {
 
     // make sure the archive directory exists
     if (!fs.exists(baseArchiveDir)) {
-      if (!HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), baseArchiveDir)) {
+      if (!HBaseFileSystem.makeDirOnFileSystem(fs, baseArchiveDir)) {
         throw new IOException("Failed to create the archive directory:" + baseArchiveDir
             + ", quitting archive attempt.");
       }
@@ -386,12 +386,11 @@ public class HFileArchiver {
 
       // move the archive file to the stamped backup
       Path backedupArchiveFile = new Path(archiveDir, filename + SEPARATOR + archiveStartTime);
-      if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), archiveFile,
-        backedupArchiveFile)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, archiveFile, backedupArchiveFile)) {
         LOG.error("Could not rename archive file to backup: " + backedupArchiveFile
             + ", deleting existing file in favor of newer.");
         // try to delete the exisiting file, if we can't rename it
-        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, fs.getConf(), archiveFile)) {
+        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, archiveFile)) {
           throw new IOException("Couldn't delete existing archive file (" + archiveFile
               + ") or rename it to the backup file (" + backedupArchiveFile
               + ") to make room for similarly named file.");
@@ -413,7 +412,7 @@ public class HFileArchiver {
         // (we're in a retry loop, so don't worry too much about the exception)
         try {
           if (!fs.exists(archiveDir)
-              && HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), archiveDir)) {
+              && HBaseFileSystem.makeDirOnFileSystem(fs, archiveDir)) {
             LOG.debug("Created archive directory:" + archiveDir);
           }
         } catch (IOException e) {
@@ -477,7 +476,7 @@ public class HFileArchiver {
    */
   private static boolean deleteRegionWithoutArchiving(FileSystem fs, Path regionDir)
       throws IOException {
-    if (HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), regionDir)) {
+    if (HBaseFileSystem.deleteDirFromFileSystem(fs, regionDir)) {
       LOG.debug("Deleted all region files in: " + regionDir);
       return true;
     }
@@ -611,7 +610,7 @@ public class HFileArchiver {
     public boolean moveAndClose(Path dest) throws IOException {
       this.close();
       Path p = this.getPath();
-      return HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), p, dest);
+      return HBaseFileSystem.renameDirForFileSystem(fs, p, dest);
     }
 
     /**
@@ -642,7 +641,7 @@ public class HFileArchiver {
 
     @Override
     public void delete() throws IOException {
-      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), file)) 
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, file)) 
         throw new IOException("Failed to delete:" + this.file);
     }
 
diff --git a/src/main/java/org/apache/hadoop/hbase/io/Reference.java b/src/main/java/org/apache/hadoop/hbase/io/Reference.java
index 1fcea84..06dc504 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/Reference.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/Reference.java
@@ -126,7 +126,7 @@ public class Reference implements Writable {
 
   public Path write(final FileSystem fs, final Path p)
   throws IOException {
-    FSDataOutputStream out = HBaseFileSystem.createPathOnFileSystem(fs, fs.getConf(), p, false);
+    FSDataOutputStream out = HBaseFileSystem.createPathOnFileSystem(fs, p, false);
     try {
       write(out);
     } finally {
diff --git a/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java b/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
index fffd663..4dae2c9 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
@@ -140,7 +140,7 @@ public class MasterFileSystem {
 
     // Make sure the region servers can archive their old logs
     if(!this.fs.exists(oldLogDir)) {
-      HBaseFileSystem.makeDirOnFileSystem(fs, conf, oldLogDir);
+      HBaseFileSystem.makeDirOnFileSystem(fs, oldLogDir);
     }
 
     return oldLogDir;
@@ -277,7 +277,7 @@ public class MasterFileSystem {
       Path splitDir = logDir.suffix(HLog.SPLITTING_EXT);
       // rename the directory so a rogue RS doesn't create more HLogs
       if (fs.exists(logDir)) {
-        if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, logDir, splitDir)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, logDir, splitDir)) {
           throw new IOException("Failed fs.rename for log split: " + logDir);
         }
         logDir = splitDir;
@@ -349,7 +349,7 @@ public class MasterFileSystem {
     // Filesystem is good. Go ahead and check for hbase.rootdir.
     try {
       if (!fs.exists(rd)) {
-        HBaseFileSystem.makeDirOnFileSystem(fs, c, rd);
+        HBaseFileSystem.makeDirOnFileSystem(fs, rd);
         // DFS leaves safe mode with 0 DNs when there are 0 blocks.
         // We used to handle this by checking the current DN count and waiting until
         // it is nonzero. With security, the check for datanode count doesn't work --
@@ -413,13 +413,13 @@ public class MasterFileSystem {
           HFileArchiver.archiveRegion(fs, this.rootdir, tabledir, regiondir);
         }
       }
-      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, c, tmpdir)) {
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, tmpdir)) {
         throw new IOException("Unable to clean the temp directory: " + tmpdir);
       }
     }
 
     // Create the temp directory
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, c, tmpdir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, tmpdir)) {
       throw new IOException("HBase temp directory '" + tmpdir + "' creation failure.");
     }
   }
@@ -487,7 +487,7 @@ public class MasterFileSystem {
   }
 
   public void deleteTable(byte[] tableName) throws IOException {
-    HBaseFileSystem.deleteDirFromFileSystem(fs, conf, new Path(rootdir, Bytes.toString(tableName)));
+    HBaseFileSystem.deleteDirFromFileSystem(fs, new Path(rootdir, Bytes.toString(tableName)));
   }
 
   /**
@@ -500,11 +500,11 @@ public class MasterFileSystem {
     Path tempPath = new Path(this.tempdir, path.getName());
 
     // Ensure temp exists
-    if (!fs.exists(tempdir) && !HBaseFileSystem.makeDirOnFileSystem(fs, conf, tempdir)) {
+    if (!fs.exists(tempdir) && !HBaseFileSystem.makeDirOnFileSystem(fs, tempdir)) {
       throw new IOException("HBase temp directory '" + tempdir + "' creation failure.");
     }
 
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, path, tempPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, path, tempPath)) {
       throw new IOException("Unable to move '" + path + "' to temp '" + tempPath + "'");
     }
 
@@ -536,7 +536,7 @@ public class MasterFileSystem {
     // delete the family folder
     Path familyDir = new Path(tableDir,
       new Path(region.getEncodedName(), Bytes.toString(familyName)));
-    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, conf, familyDir)) {
+    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, familyDir)) {
       throw new IOException("Could not delete family "
           + Bytes.toString(familyName) + " from FileSystem for region "
           + region.getRegionNameAsString() + "(" + region.getEncodedName()
diff --git a/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java b/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
index 32d1565..c2ed381 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
@@ -279,7 +279,7 @@ public class SplitLogManager extends ZooKeeperListener {
     for(Path logDir: logDirs){
       status.setStatus("Cleaning up log directory...");
       try {
-        if (fs.exists(logDir) && !HBaseFileSystem.deleteFileFromFileSystem(fs, conf, logDir)) {
+        if (fs.exists(logDir) && !HBaseFileSystem.deleteFileFromFileSystem(fs, logDir)) {
           LOG.warn("Unable to delete log src dir. Ignoring. " + logDir);
         }
       } catch (IOException ioe) {
diff --git a/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java b/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
index 6d6da76..0fa53c0 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
@@ -154,7 +154,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     // if the directory doesn't exist, then we are done
     if (children == null) {
       try {
-        return HBaseFileSystem.deleteFileFromFileSystem(fs, conf, toCheck);
+        return HBaseFileSystem.deleteFileFromFileSystem(fs, toCheck);
       } catch (IOException e) {
         if (LOG.isTraceEnabled()) {
           LOG.trace("Couldn't delete directory: " + toCheck, e);
@@ -186,7 +186,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     // delete this directory. However, don't do so recursively so we don't delete files that have
     // been added since we last checked.
     try {
-      return HBaseFileSystem.deleteFileFromFileSystem(fs, conf, toCheck);
+      return HBaseFileSystem.deleteFileFromFileSystem(fs, toCheck);
     } catch (IOException e) {
       if (LOG.isTraceEnabled()) {
         LOG.trace("Couldn't delete directory: " + toCheck, e);
@@ -208,7 +208,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     // first check to see if the path is valid
     if (!validate(filePath)) {
       LOG.warn("Found a wrongly formatted file: " + filePath.getName() + " deleting it.");
-      boolean success = HBaseFileSystem.deleteDirFromFileSystem(fs, conf, filePath);
+      boolean success = HBaseFileSystem.deleteDirFromFileSystem(fs, filePath);
       if (!success) LOG.warn("Attempted to delete:" + filePath
           + ", but couldn't. Run cleaner chain and attempt to delete on next pass.");
 
@@ -234,7 +234,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Chore
     if (LOG.isTraceEnabled()) {
       LOG.trace("Removing:" + filePath + " from archive");
     }
-    boolean success = HBaseFileSystem.deleteFileFromFileSystem(fs, conf, filePath);
+    boolean success = HBaseFileSystem.deleteFileFromFileSystem(fs, filePath);
     if (!success) {
       LOG.warn("Attempted to delete:" + filePath
           + ", but couldn't. Run cleaner chain and attempt to delete on next pass.");
diff --git a/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java b/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
index e40c54c..df76e65 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
@@ -178,7 +178,7 @@ public class CreateTableHandler extends EventHandler {
     List<HRegionInfo> regionInfos = handleCreateHdfsRegions(tempdir, tableName);
 
     // 3. Move Table temp directory to the hbase root location
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, tempTableDir, tableDir)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, tempTableDir, tableDir)) {
       throw new IOException("Unable to move table from temp=" + tempTableDir +
         " to hbase root=" + tableDir);
     }
diff --git a/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java b/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
index db91aa7..c7fcbc1 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
@@ -90,7 +90,7 @@ public class DeleteTableHandler extends TableEventHandler {
       }
 
       // 5. Delete table from FS (temp directory)
-      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), tempTableDir)) {
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, tempTableDir)) {
         LOG.error("Couldn't delete " + tempTableDir);
       }
       LOG.debug("Table '" + Bytes.toString(tableName) + "' archived!");
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index c027582..fa952fd 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -658,7 +658,7 @@ public class HRegion implements HeapSize { // , Writable{
     final Path initialFiles, final Path regiondir)
   throws IOException {
     if (initialFiles != null && fs.exists(initialFiles)) {
-      if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), initialFiles, regiondir)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, initialFiles, regiondir)) {
         LOG.warn("Unable to rename " + initialFiles + " to " + regiondir);
       }
     }
@@ -820,7 +820,7 @@ public class HRegion implements HeapSize { // , Writable{
     } finally {
       out.close();
     }
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, tmpPath, regioninfoPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, tmpPath, regioninfoPath)) {
       throw new IOException("Unable to rename " + tmpPath + " to " +
         regioninfoPath);
     }
@@ -2664,7 +2664,7 @@ public class HRegion implements HeapSize { // , Writable{
         // open and read the files as well).
         LOG.debug("Creating reference for file (" + (i+1) + "/" + sz + ") : " + file);
         Path referenceFile = new Path(dstStoreDir, file.getName());
-        boolean success = HBaseFileSystem.createNewFileOnFileSystem(fs, conf, referenceFile);
+        boolean success = HBaseFileSystem.createNewFileOnFileSystem(fs, referenceFile);
         if (!success) {
           throw new IOException("Failed to create reference file:" + referenceFile);
         }
@@ -3073,7 +3073,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     // Now delete the content of recovered edits.  We're done w/ them.
     for (Path file: files) {
-      if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, file)) {
+      if (!HBaseFileSystem.deleteFileFromFileSystem(fs, file)) {
         LOG.error("Failed delete of " + file);
       } else {
         LOG.debug("Deleted recovered.edits file=" + file);
@@ -3265,7 +3265,7 @@ public class HRegion implements HeapSize { // , Writable{
     FileStatus stat = fs.getFileStatus(p);
     if (stat.getLen() > 0) return false;
     LOG.warn("File " + p + " is zero-length, deleting.");
-    HBaseFileSystem.deleteFileFromFileSystem(fs, fs.getConf(), p);
+    HBaseFileSystem.deleteFileFromFileSystem(fs, p);
     return true;
   }
 
@@ -4179,7 +4179,7 @@ public class HRegion implements HeapSize { // , Writable{
         HTableDescriptor.getTableDir(rootDir, info.getTableName());
     Path regionDir = HRegion.getRegionDir(tableDir, info.getEncodedName());
     FileSystem fs = FileSystem.get(conf);
-    HBaseFileSystem.makeDirOnFileSystem(fs, conf, regionDir);
+    HBaseFileSystem.makeDirOnFileSystem(fs, regionDir);
     // Write HRI to a file in case we need to recover .META.
     writeRegioninfoOnFilesystem(info, regionDir, fs, conf);
     HLog effectiveHLog = hlog;
@@ -4375,7 +4375,7 @@ public class HRegion implements HeapSize { // , Writable{
     if (LOG.isDebugEnabled()) {
       LOG.debug("DELETING region " + regiondir.toString());
     }
-    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), regiondir)) {
+    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, regiondir)) {
       LOG.warn("Failed delete of " + regiondir);
     }
   }
@@ -4421,7 +4421,7 @@ public class HRegion implements HeapSize { // , Writable{
     final HRegionInfo hri, byte [] colFamily)
   throws IOException {
     Path dir = Store.getStoreHomedir(tabledir, hri.getEncodedName(), colFamily);
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), dir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, dir)) {
       LOG.warn("Failed to create " + dir);
     }
   }
@@ -4531,7 +4531,7 @@ public class HRegion implements HeapSize { // , Writable{
       throw new IOException("Cannot merge; target file collision at " +
           newRegionDir);
     }
-    HBaseFileSystem.makeDirOnFileSystem(fs, conf, newRegionDir);
+    HBaseFileSystem.makeDirOnFileSystem(fs, newRegionDir);
 
     LOG.info("starting merge of regions: " + a + " and " + b +
       " into new region " + newRegionInfo.toString() +
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
index b2a7a6f..fe4cb68 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
@@ -33,6 +33,6 @@ public class HRegionFileSystem extends HBaseFileSystem {
   public static final Log LOG = LogFactory.getLog(HRegionFileSystem.class);
 
   public HRegionFileSystem(Configuration conf) {
-    checkAndSetRetryCounts(conf);
+    setRetryCounts(conf);
   }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java b/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
index aaa2dd6..5500912 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
@@ -552,12 +552,12 @@ public class SplitTransaction {
     if (fs.exists(splitdir)) {
       LOG.info("The " + splitdir
           + " directory exists.  Hence deleting it to recreate it");
-      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), splitdir)) {
+      if (!HBaseFileSystem.deleteDirFromFileSystem(fs, splitdir)) {
         throw new IOException("Failed deletion of " + splitdir
             + " before creating them again.");
       }
     }
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), splitdir))
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, splitdir))
         throw new IOException("Failed create of " + splitdir);
   }
 
@@ -579,7 +579,7 @@ public class SplitTransaction {
   throws IOException {
     if (!fs.exists(dir)) {
       if (mustPreExist) throw new IOException(dir.toString() + " does not exist!");
-    } else if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), dir)) {
+    } else if (!HBaseFileSystem.deleteDirFromFileSystem(fs, dir)) {
       throw new IOException("Failed delete of " + dir);
     }
   }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java b/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
index b01f716..097aae0 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
@@ -277,7 +277,7 @@ public class Store extends SchemaConfigured implements HeapSize {
    */
   Path createStoreHomeDir(final FileSystem fs,
       final Path homedir) throws IOException {
-    if (!fs.exists(homedir) && !HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), homedir)) {
+    if (!fs.exists(homedir) && !HBaseFileSystem.makeDirOnFileSystem(fs, homedir)) {
         throw new IOException("Failed create of: " + homedir.toString());
     }
     return homedir;
@@ -867,7 +867,7 @@ public class Store extends SchemaConfigured implements HeapSize {
     String msg = "Renaming flushed file at " + path + " to " + dstPath;
     LOG.debug(msg);
     status.setStatus("Flushing " + this + ": " + msg);
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, path, dstPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, path, dstPath)) {
       LOG.warn("Unable to rename " + path + " to " + dstPath);
     }
 
@@ -1636,7 +1636,7 @@ public class Store extends SchemaConfigured implements HeapSize {
       Path origPath = compactedFile.getPath();
       Path destPath = new Path(homedir, origPath.getName());
       LOG.info("Renaming compacted file at " + origPath + " to " + destPath);
-      if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, origPath, destPath)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, origPath, destPath)) {
         LOG.error("Failed move of compacted file " + origPath + " to " +
             destPath);
         throw new IOException("Failed move of compacted file " + origPath +
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java b/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
index f84569c..235629c 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
@@ -676,7 +676,7 @@ public class StoreFile extends SchemaConfigured {
    */
   public void deleteReader() throws IOException {
     closeReader(true);
-    HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), getPath());
+    HBaseFileSystem.deleteDirFromFileSystem(fs, getPath());
   }
 
   @Override
@@ -719,7 +719,7 @@ public class StoreFile extends SchemaConfigured {
     if (!fs.exists(src)) {
       throw new FileNotFoundException(src.toString());
     }
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), src, tgt)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, src, tgt)) {
       throw new IOException("Failed rename of " + src + " to " + tgt);
     }
     return tgt;
@@ -842,7 +842,7 @@ public class StoreFile extends SchemaConfigured {
       }
 
       if (!fs.exists(dir)) {
-        HBaseFileSystem.makeDirOnFileSystem(fs, conf, dir);
+        HBaseFileSystem.makeDirOnFileSystem(fs, dir);
       }
 
       if (filePath == null) {
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index e636726..e942fae 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -408,11 +408,11 @@ public class HLog implements Syncable {
     if (failIfLogDirExists && fs.exists(dir)) {
       throw new IOException("Target HLog directory already exists: " + dir);
     }
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, dir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, dir)) {
       throw new IOException("Unable to mkdir " + dir);
     }
     this.oldLogDir = oldLogDir;
-    if (!fs.exists(oldLogDir) && !HBaseFileSystem.makeDirOnFileSystem(fs, conf, oldLogDir)) {
+    if (!fs.exists(oldLogDir) && !HBaseFileSystem.makeDirOnFileSystem(fs, oldLogDir)) {
       throw new IOException("Unable to mkdir " + this.oldLogDir);
     }
     this.maxLogs = conf.getInt("hbase.regionserver.maxlogs", 32);
@@ -918,7 +918,7 @@ public class HLog implements Syncable {
         i.preLogArchive(p, newPath);
       }
     }
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, p, newPath)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, p, newPath)) {
       throw new IOException("Unable to rename " + p + " to " + newPath);
     }
     // Tell our listeners that a log has been archived.
@@ -970,7 +970,7 @@ public class HLog implements Syncable {
         }
       }
 
-      if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, file.getPath(),p)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, file.getPath(), p)) {
         throw new IOException("Unable to rename " + file.getPath() + " to " + p);
       }
       // Tell our listeners that a log was archived.
@@ -982,7 +982,7 @@ public class HLog implements Syncable {
     }
     LOG.debug("Moved " + files.length + " log files to " +
       FSUtils.getPath(this.oldLogDir));
-    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, conf, dir)) {
+    if (!HBaseFileSystem.deleteDirFromFileSystem(fs, dir)) {
       LOG.info("Unable to delete " + dir);
     }
   }
@@ -1864,7 +1864,7 @@ public class HLog implements Syncable {
   throws IOException {
     Path moveAsideName = new Path(edits.getParent(), edits.getName() + "." +
       System.currentTimeMillis());
-    if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), edits, moveAsideName)) {
+    if (!HBaseFileSystem.renameDirForFileSystem(fs, edits, moveAsideName)) {
       LOG.warn("Rename failed from " + edits + " to " + moveAsideName);
     }
     return moveAsideName;
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
index 29efe4f..9bcf1db 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
@@ -41,7 +41,7 @@ public class HLogFileSystem extends HBaseFileSystem {
    */
 
   public HLogFileSystem(Configuration conf) {
-    checkAndSetRetryCounts(conf);
+    setRetryCounts(conf);
   }
 
   /**
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
index 3aae3ac..d745dce 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
@@ -479,7 +479,7 @@ public class HLogSplitter {
             LOG.warn("Found existing old edits file. It could be the "
                 + "result of a previous failed split attempt. Deleting " + dst + ", length="
                 + fs.getFileStatus(dst).getLen());
-            if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, dst)) {
+            if (!HBaseFileSystem.deleteFileFromFileSystem(fs, dst)) {
               LOG.warn("Failed deleting of old " + dst);
               throw new IOException("Failed deleting of old " + dst);
             }
@@ -488,7 +488,7 @@ public class HLogSplitter {
           // data without touching disk. TestHLogSplit#testThreading is an
           // example.
           if (fs.exists(wap.p)) {
-            if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, wap.p, dst)) {
+            if (!HBaseFileSystem.renameDirForFileSystem(fs, wap.p, dst)) {
               throw new IOException("Failed renaming " + wap.p + " to " + dst);
             }
             LOG.debug("Rename " + wap.p + " to " + dst);
@@ -555,7 +555,7 @@ public class HLogSplitter {
     }
     archiveLogs(null, corruptedLogs, processedLogs, oldLogDir, fs, conf);
     Path stagingDir = ZKSplitLog.getSplitLogDir(rootdir, logPath.getName());
-    HBaseFileSystem.deleteDirFromFileSystem(fs, conf, stagingDir);
+    HBaseFileSystem.deleteDirFromFileSystem(fs, stagingDir);
   }
 
   /**
@@ -578,17 +578,17 @@ public class HLogSplitter {
     final Path corruptDir = new Path(conf.get(HConstants.HBASE_DIR), conf.get(
         "hbase.regionserver.hlog.splitlog.corrupt.dir",  HConstants.CORRUPT_DIR_NAME));
 
-    if (!HBaseFileSystem.makeDirOnFileSystem(fs, conf, corruptDir)) {
+    if (!HBaseFileSystem.makeDirOnFileSystem(fs, corruptDir)) {
       LOG.info("Unable to mkdir " + corruptDir);
     }
-    HBaseFileSystem.makeDirOnFileSystem(fs, conf, oldLogDir);
+    HBaseFileSystem.makeDirOnFileSystem(fs, oldLogDir);
 
     // this method can get restarted or called multiple times for archiving
     // the same log files.
     for (Path corrupted : corruptedLogs) {
       Path p = new Path(corruptDir, corrupted.getName());
       if (fs.exists(corrupted)) {
-        if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, corrupted, p)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, corrupted, p)) {
           LOG.warn("Unable to move corrupted log " + corrupted + " to " + p);
         } else {
           LOG.warn("Moving corrupted log " + corrupted + " to " + p);
@@ -599,7 +599,7 @@ public class HLogSplitter {
     for (Path p : processedLogs) {
       Path newPath = HLog.getHLogArchivePath(oldLogDir, p);
       if (fs.exists(p)) {
-        if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, p, newPath)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, p, newPath)) {
           LOG.warn("Unable to move  " + p + " to " + newPath);
         } else {
           LOG.debug("Archived processed log " + p + " to " + newPath);
@@ -609,7 +609,7 @@ public class HLogSplitter {
 
     // distributed log splitting removes the srcDir (region's log dir) later
     // when all the log files in that srcDir have been successfully processed
-    if (srcDir != null && !HBaseFileSystem.deleteDirFromFileSystem(fs, conf, srcDir)) {
+    if (srcDir != null && !HBaseFileSystem.deleteDirFromFileSystem(fs, srcDir)) {
       throw new IOException("Unable to delete src dir: " + srcDir);
     }
   }
@@ -643,20 +643,20 @@ public class HLogSplitter {
     if (fs.exists(dir) && fs.isFile(dir)) {
       Path tmp = new Path("/tmp");
       if (!fs.exists(tmp)) {
-        HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), tmp);
+        HBaseFileSystem.makeDirOnFileSystem(fs, tmp);
       }
       tmp = new Path(tmp,
         HLog.RECOVERED_EDITS_DIR + "_" + encodedRegionName);
       LOG.warn("Found existing old file: " + dir + ". It could be some "
         + "leftover of an old installation. It should be a folder instead. "
         + "So moving it to " + tmp);
-      if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), dir, tmp)) {
+      if (!HBaseFileSystem.renameDirForFileSystem(fs, dir, tmp)) {
         LOG.warn("Failed to sideline old file " + dir);
       }
     }
 
     if (isCreate && !fs.exists(dir) && 
-        !HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), dir)) {
+        !HBaseFileSystem.makeDirOnFileSystem(fs, dir)) {
       LOG.warn("mkdir failed on " + dir);
     }
     // Append file name ends with RECOVERED_LOG_TMPFILE_SUFFIX to ensure
@@ -1075,7 +1075,7 @@ public class HLogSplitter {
           + "result of a previous failed split attempt. Deleting "
           + regionedits + ", length="
           + fs.getFileStatus(regionedits).getLen());
-      if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, regionedits)) {
+      if (!HBaseFileSystem.deleteFileFromFileSystem(fs, regionedits)) {
         LOG.warn("Failed delete of old " + regionedits);
       }
     }
@@ -1101,12 +1101,12 @@ public class HLogSplitter {
             + "result of a previous failed split attempt. Deleting "
             + ret + ", length="
             + fs.getFileStatus(ret).getLen());
-        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, ret)) {
+        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, ret)) {
           LOG.warn("Failed delete of old " + ret);
         }
       }
       Path dir = ret.getParent();
-      if (!fs.exists(dir) && !HBaseFileSystem.makeDirOnFileSystem(fs, conf, dir)) { 
+      if (!fs.exists(dir) && !HBaseFileSystem.makeDirOnFileSystem(fs, dir)) { 
           LOG.warn("mkdir failed on " + dir);
       }
     } catch (IOException e) {
@@ -1200,7 +1200,7 @@ public class HLogSplitter {
             LOG.warn("Found existing old edits file. It could be the "
                 + "result of a previous failed split attempt. Deleting " + dst
                 + ", length=" + fs.getFileStatus(dst).getLen());
-            if (!HBaseFileSystem.deleteFileFromFileSystem(fs, conf, dst)) {
+            if (!HBaseFileSystem.deleteFileFromFileSystem(fs, dst)) {
               LOG.warn("Failed deleting of old " + dst);
               throw new IOException("Failed deleting of old " + dst);
             }
@@ -1209,7 +1209,7 @@ public class HLogSplitter {
           // the data without touching disk. TestHLogSplit#testThreading is an
           // example.
           if (fs.exists(wap.p)) {
-            if (!HBaseFileSystem.renameDirForFileSystem(fs, conf, wap.p, dst)) {
+            if (!HBaseFileSystem.renameDirForFileSystem(fs, wap.p, dst)) {
               throw new IOException("Failed renaming " + wap.p + " to " + dst);
             }
             LOG.debug("Rename " + wap.p + " to " + dst);
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
index fae73dd..6b4c64b 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
@@ -225,7 +225,7 @@ public class FSTableDescriptors implements TableDescriptors {
     if (!this.fsreadonly) {
       Path tabledir = FSUtils.getTablePath(this.rootdir, tablename);
       if (this.fs.exists(tabledir)) {
-        if (!HBaseFileSystem.deleteDirFromFileSystem(fs, fs.getConf(), tabledir)) {
+        if (!HBaseFileSystem.deleteDirFromFileSystem(fs, tabledir)) {
           throw new IOException("Failed delete of " + tabledir.toString());
         }
       }
@@ -281,7 +281,7 @@ public class FSTableDescriptors implements TableDescriptors {
       for (int i = 1; i < status.length; i++) {
         Path p = status[i].getPath();
         // Clean up old versions
-        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, fs.getConf(), p)) {
+        if (!HBaseFileSystem.deleteFileFromFileSystem(fs, p)) {
           LOG.warn("Failed cleanup of " + status);
         } else {
           LOG.debug("Cleaned up old tableinfo file " + p);
@@ -505,7 +505,7 @@ public class FSTableDescriptors implements TableDescriptors {
       try {
         writeHTD(fs, p, hTableDescriptor);
         tableInfoPath = getTableInfoFileName(tableDir, sequenceid);
-        if (!HBaseFileSystem.renameDirForFileSystem(fs, fs.getConf(), p, tableInfoPath)) {
+        if (!HBaseFileSystem.renameDirForFileSystem(fs, p, tableInfoPath)) {
           throw new IOException("Failed rename of " + p + " to " + tableInfoPath);
         }
       } catch (IOException ioe) {
@@ -531,7 +531,7 @@ public class FSTableDescriptors implements TableDescriptors {
   private static void writeHTD(final FileSystem fs, final Path p,
       final HTableDescriptor htd)
   throws IOException {
-    FSDataOutputStream out = HBaseFileSystem.createPathOnFileSystem(fs, fs.getConf(), p, false);
+    FSDataOutputStream out = HBaseFileSystem.createPathOnFileSystem(fs, p, false);
     try {
       htd.write(out);
       out.write('\n');
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
index 9ab974e..0f5d468 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
@@ -107,7 +107,7 @@ public abstract class FSUtils {
    */
   public Path checkdir(final FileSystem fs, final Path dir) throws IOException {
     if (!fs.exists(dir)) {
-      HBaseFileSystem.makeDirOnFileSystem(fs, fs.getConf(), dir);
+      HBaseFileSystem.makeDirOnFileSystem(fs, dir);
     }
     return dir;
   }
@@ -155,7 +155,7 @@ public abstract class FSUtils {
   public static FSDataOutputStream create(FileSystem fs, Path path, FsPermission perm,
       boolean overwrite) throws IOException {
     LOG.debug("Creating file=" + path + " with permission=" + perm);
-    return HBaseFileSystem.createPathWithPermsOnFileSystem(fs, fs.getConf(), path, perm, overwrite);
+    return HBaseFileSystem.createPathWithPermsOnFileSystem(fs, path, perm, overwrite);
   }
 
   /**
diff --git a/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java b/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
index f5d5f85..8545243 100644
--- a/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
@@ -165,7 +165,7 @@ public class ZKSplitLog {
       FileSystem fs) {
     Path file = new Path(getSplitLogDir(rootdir, logFileName), "corrupt");
     try {
-      HBaseFileSystem.createNewFileOnFileSystem(fs, fs.getConf(), file);
+      HBaseFileSystem.createNewFileOnFileSystem(fs, file);
     } catch (IOException e) {
       LOG.warn("Could not flag a log file as corrupted. Failed to create " +
           file, e);
diff --git a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index a8a6b86..f1aaa95 100644
--- a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -202,6 +202,7 @@ public class HBaseTestingUtility {
 
   private void setHDFSClientRetryProperty() {
     this.conf.setInt("hdfs.client.retries.number", 1);
+    HBaseFileSystem.setRetryCounts(conf);
   }
 
   /**
diff --git a/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java b/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java
index f59c2d4..0d833ab 100644
--- a/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java
+++ b/src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java
@@ -55,6 +55,7 @@ public class TestHBaseFileSystem {
     LOG.info("hbase.rootdir=" + hbaseRootDir);
     conf.set(HConstants.HBASE_DIR, hbaseRootDir.toString());
     conf.setInt("hdfs.client.retries.number", 10);
+    HBaseFileSystem.setRetryCounts(conf);
   }
 
   
@@ -65,16 +66,24 @@ public class TestHBaseFileSystem {
     Path rootDir = new Path(conf.get(HConstants.HBASE_DIR));
     FileSystem fs = TEST_UTIL.getTestFileSystem();
     // Create a Region
-    assertTrue(HBaseFileSystem.createPathOnFileSystem(fs, TestHBaseFileSystem.conf, rootDir, true) != null);
-
-    boolean result = HBaseFileSystem.makeDirOnFileSystem(new MockFileSystemForCreate(), TestHBaseFileSystem.conf, new Path("/a"));
+    assertTrue(HBaseFileSystem.createPathOnFileSystem(fs, rootDir, true) != null);
+    
+    try {
+      HBaseFileSystem.createPathOnFileSystem(new MockFileSystemForCreate(), 
+        new Path("/A"), false);
+     assertTrue(false);// control should not come here.
+    } catch (Exception e) {
+      LOG.info(e);
+    }
+    
+    boolean result = HBaseFileSystem.makeDirOnFileSystem(new MockFileSystemForCreate(), new Path("/a"));
     assertTrue("Couldn't create the directory", result);
 
 
-    result = HBaseFileSystem.renameDirForFileSystem(new MockFileSystem(), TestHBaseFileSystem.conf, new Path("/a"), new Path("/b"));
+    result = HBaseFileSystem.renameDirForFileSystem(new MockFileSystem(), new Path("/a"), new Path("/b"));
     assertTrue("Couldn't rename the directory", result);
 
-    result = HBaseFileSystem.deleteDirFromFileSystem(new MockFileSystem(), TestHBaseFileSystem.conf, new Path("/a"));
+    result = HBaseFileSystem.deleteDirFromFileSystem(new MockFileSystem(), new Path("/a"));
 
     assertTrue("Couldn't delete the directory", result);
     fs.delete(rootDir, true);
@@ -83,6 +92,7 @@ public class TestHBaseFileSystem {
   static class MockFileSystemForCreate extends MockFileSystem {
     @Override
     public boolean exists(Path path) {
+      if ("/A".equals(path.toString())) return true;
       return false;
     }
   }
-- 
1.7.0.4

