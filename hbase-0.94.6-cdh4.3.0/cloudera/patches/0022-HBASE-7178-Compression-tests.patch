From af233279577922c21f3d475eeacae7c3c2f75774 Mon Sep 17 00:00:00 2001
From: David S. Wang <dsw@cloudera.com>
Date: Wed, 27 Mar 2013 09:34:57 -0700
Subject: [PATCH 22/96] HBASE-7178 Compression tests

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1410496 13f79535-47bb-0310-9956-ffa450edef68

Reason: Test
Ref: CDH-8687
Author: Matteo Bertozzi
(cherry picked from commit 22a64fb365dc96d9066a9bbe8b793b347d03bf6b)
---
 .../hadoop/hbase/regionserver/TestStore.java       |   56 ++++++++---
 .../hadoop/hbase/util/TestCompressionTest.java     |  104 +++++++++++---------
 2 files changed, 100 insertions(+), 60 deletions(-)

diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java b/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
index 121d277..42dbef8 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
@@ -52,7 +52,10 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
+import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
+import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
@@ -130,7 +133,7 @@ public class TestStore extends TestCase {
     hcd.setMaxVersions(4);
     init(methodName, conf, hcd);
   }
-  
+
   private void init(String methodName, Configuration conf,
       HColumnDescriptor hcd) throws IOException {
     //Setting up a Store
@@ -150,10 +153,39 @@ public class TestStore extends TestCase {
     store = new Store(basedir, region, hcd, fs, conf);
   }
 
+  /**
+   * Verify that compression and data block encoding are respected by the
+   * Store.createWriterInTmp() method, used on store flush.
+   */
+  public void testCreateWriter() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    FileSystem fs = FileSystem.get(conf);
+
+    HColumnDescriptor hcd = new HColumnDescriptor(family);
+    hcd.setCompressionType(Compression.Algorithm.GZ);
+    hcd.setDataBlockEncoding(DataBlockEncoding.DIFF);
+    init(getName(), conf, hcd);
+
+    // Test createWriterInTmp()
+    StoreFile.Writer writer = store.createWriterInTmp(4, hcd.getCompression(), false);
+    Path path = writer.getPath();
+    writer.append(new KeyValue(row, family, qf1, Bytes.toBytes(1)));
+    writer.append(new KeyValue(row, family, qf2, Bytes.toBytes(2)));
+    writer.append(new KeyValue(row2, family, qf1, Bytes.toBytes(3)));
+    writer.append(new KeyValue(row2, family, qf2, Bytes.toBytes(4)));
+    writer.close();
+
+    // Verify that compression and encoding settings are respected
+    HFile.Reader reader = HFile.createReader(fs, path, new CacheConfig(conf));
+    assertEquals(hcd.getCompressionType(), reader.getCompressionAlgorithm());
+    assertEquals(hcd.getDataBlockEncoding(), reader.getEncodingOnDisk());
+    reader.close();
+  }
+
   public void testDeleteExpiredStoreFiles() throws Exception {
     int storeFileNum = 4;
     int ttl = 4;
-    
+
     Configuration conf = HBaseConfiguration.create();
     // Enable the expired store file deletion
     conf.setBoolean("hbase.store.delete.expired.storefile", true);
@@ -184,7 +216,7 @@ public class TestStore extends TestCase {
       // verify the expired store file.
       CompactionRequest cr = this.store.requestCompaction();
       assertEquals(1, cr.getFiles().size());
-      assertTrue(cr.getFiles().get(0).getReader().getMaxTimestamp() < 
+      assertTrue(cr.getFiles().get(0).getReader().getMaxTimestamp() <
           (System.currentTimeMillis() - this.store.scanInfo.getTtl()));
       // Verify that the expired the store has been deleted.
       this.store.compact(cr);
@@ -200,7 +232,7 @@ public class TestStore extends TestCase {
     FileSystem fs = FileSystem.get(conf);
     // Initialize region
     init(getName(), conf);
-    
+
     int storeFileNum = 4;
     for (int i = 1; i <= storeFileNum; i++) {
       LOG.info("Adding some data for the store file #"+i);
@@ -210,30 +242,30 @@ public class TestStore extends TestCase {
       flush(i);
     }
     // after flush; check the lowest time stamp
-    long lowestTimeStampFromStore = 
+    long lowestTimeStampFromStore =
         Store.getLowestTimestamp(store.getStorefiles());
-    long lowestTimeStampFromFS = 
+    long lowestTimeStampFromFS =
       getLowestTimeStampFromFS(fs,store.getStorefiles());
     assertEquals(lowestTimeStampFromStore,lowestTimeStampFromFS);
-    
+
     // after compact; check the lowest time stamp
     store.compact(store.requestCompaction());
     lowestTimeStampFromStore = Store.getLowestTimestamp(store.getStorefiles());
     lowestTimeStampFromFS = getLowestTimeStampFromFS(fs,store.getStorefiles());
-    assertEquals(lowestTimeStampFromStore,lowestTimeStampFromFS); 
+    assertEquals(lowestTimeStampFromStore,lowestTimeStampFromFS);
   }
-  
-  private static long getLowestTimeStampFromFS(FileSystem fs, 
+
+  private static long getLowestTimeStampFromFS(FileSystem fs,
       final List<StoreFile> candidates) throws IOException {
     long minTs = Long.MAX_VALUE;
     if (candidates.isEmpty()) {
-      return minTs; 
+      return minTs;
     }
     Path[] p = new Path[candidates.size()];
     for (int i = 0; i < candidates.size(); ++i) {
       p[i] = candidates.get(i).getPath();
     }
-    
+
     FileStatus[] stats = fs.listStatus(p);
     if (stats == null || stats.length == 0) {
       return minTs;
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java b/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
index df78395..e291d9d 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
@@ -20,6 +20,8 @@
 
 package org.apache.hadoop.hbase.util;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.SmallTests;
 import org.apache.hadoop.hbase.io.hfile.Compression;
@@ -39,10 +41,10 @@ import static org.junit.Assert.*;
 
 @Category(SmallTests.class)
 public class TestCompressionTest {
+  static final Log LOG = LogFactory.getLog(TestCompressionTest.class);
 
   @Test
   public void testTestCompression() {
-
     // This test will fail if you run the tests with LZO compression available.
     try {
       CompressionTest.testCompression(Compression.Algorithm.LZO);
@@ -61,70 +63,76 @@ public class TestCompressionTest {
       assertNull(e.getCause());
     }
 
-
     assertFalse(CompressionTest.testCompression("LZO"));
     assertTrue(CompressionTest.testCompression("NONE"));
     assertTrue(CompressionTest.testCompression("GZ"));
 
-    if (isCompressionAvailable("org.apache.hadoop.io.compress.SnappyCodec")) {
-      if (NativeCodeLoader.isNativeCodeLoaded()) {
-        try {
-          System.loadLibrary("snappy");
+    if (NativeCodeLoader.isNativeCodeLoaded()) {
+      nativeCodecTest("LZO", "lzo2", "com.hadoop.compression.lzo.LzoCodec");
+      nativeCodecTest("LZ4", null, "org.apache.hadoop.io.compress.Lz4Codec");
+      nativeCodecTest("SNAPPY", "snappy", "org.apache.hadoop.io.compress.SnappyCodec");
+    } else {
+      // Hadoop nativelib is not available
+      LOG.debug("Native code not loaded");
+      assertFalse(CompressionTest.testCompression("LZO"));
+      assertFalse(CompressionTest.testCompression("LZ4"));
+      assertFalse(CompressionTest.testCompression("SNAPPY"));
+    }
+  }
+
+  private boolean isCompressionAvailable(String codecClassName) {
+    try {
+      Thread.currentThread().getContextClassLoader().loadClass(codecClassName);
+      return true;
+    } catch (Exception ex) {
+      return false;
+    }
+  }
+
+  /**
+   * Verify CompressionTest.testCompression() on a native codec.
+   */
+  private void nativeCodecTest(String codecName, String libName, String codecClassName) {
+    if (isCompressionAvailable(codecClassName)) {
+      try {
+        if (libName != null) {
+          System.loadLibrary(libName);
+        }
 
-          try {
+        try {
             Configuration conf = new Configuration();
             CompressionCodec codec = (CompressionCodec)
-              ReflectionUtils.newInstance(
-                conf.getClassByName("org.apache.hadoop.io.compress.SnappyCodec"), conf);
+              ReflectionUtils.newInstance(conf.getClassByName(codecClassName), conf);
 
             DataOutputBuffer compressedDataBuffer = new DataOutputBuffer();
-            CompressionOutputStream deflateFilter =
-              codec.createOutputStream(compressedDataBuffer);
+            CompressionOutputStream deflateFilter = codec.createOutputStream(compressedDataBuffer);
 
             byte[] data = new byte[1024];
-            DataOutputStream deflateOut = new DataOutputStream(
-              new BufferedOutputStream(deflateFilter));
+            DataOutputStream deflateOut = new DataOutputStream(new BufferedOutputStream(deflateFilter));
             deflateOut.write(data, 0, data.length);
             deflateOut.flush();
             deflateFilter.finish();
 
-            // Snappy Codec class, Snappy nativelib and Hadoop nativelib with 
-            // Snappy JNIs are present
-            assertTrue(CompressionTest.testCompression("SNAPPY"));
-          }
-          catch (UnsatisfiedLinkError ex) {
-            // Hadoop nativelib does not have Snappy JNIs
-            
-            // cannot assert the codec here because the current logic of 
-            // CompressionTest checks only classloading, not the codec
-            // usage.
-          }
-          catch (Exception ex) {
-          }
+            // Codec class, codec nativelib and Hadoop nativelib with codec JNIs are present
+            assertTrue(CompressionTest.testCompression(codecName));
+        } catch (UnsatisfiedLinkError e) {
+          // Hadoop nativelib does not have codec JNIs.
+          // cannot assert the codec here because the current logic of
+          // CompressionTest checks only classloading, not the codec
+          // usage.
+          LOG.debug("No JNI for codec '" + codecName + "' " + e.getMessage());
+        } catch (Exception e) {
+          LOG.error(codecName, e);
         }
-        catch (UnsatisfiedLinkError ex) {
-          // Snappy nativelib is not available
-          assertFalse(CompressionTest.testCompression("SNAPPY"));
-        }
-      }
-      else {
-        // Hadoop nativelib is not available
-        assertFalse(CompressionTest.testCompression("SNAPPY"));
+      } catch (UnsatisfiedLinkError e) {
+        // nativelib is not available
+        LOG.debug("Native lib not available: " + codecName);
+        assertFalse(CompressionTest.testCompression(codecName));
       }
-    }
-    else {
-      // Snappy Codec class is not available
-      assertFalse(CompressionTest.testCompression("SNAPPY"));
-    }
-  }
-
-  private boolean isCompressionAvailable(String codecClassName) {
-    try {
-      Thread.currentThread().getContextClassLoader().loadClass(codecClassName);
-      return true;
-    }
-    catch (Exception ex) {
-      return false;
+    } else {
+      // Compression Codec class is not available
+      LOG.debug("Codec class not available: " + codecName);
+      assertFalse(CompressionTest.testCompression(codecName));
     }
   }
 
-- 
1.7.0.4

