From 23dffbf944a99a007779cdd161789a4462b2e935 Mon Sep 17 00:00:00 2001
From: Tom White <tom@cloudera.com>
Date: Mon, 12 Dec 2011 16:15:12 -0800
Subject: [PATCH 1069/1357] MR1: MAPREDUCE-954. The new interface's Context objects should be interfaces.

Author: Arun C Murthy
Reason: Support 0.23 API in MR1
Ref: CDH-3861
---
 .../mrunit/mapreduce/mock/MockMapContext.java      |   96 +++++
 .../mrunit/mapreduce/mock/MockReduceContext.java   |  158 ++++++++
 .../org/apache/hadoop/mapred/FileOutputFormat.java |    2 +-
 src/mapred/org/apache/hadoop/mapred/JobClient.java |    2 +-
 .../org/apache/hadoop/mapred/JobContext.java       |   24 +-
 .../org/apache/hadoop/mapred/JobContextImpl.java   |   60 +++
 .../org/apache/hadoop/mapred/LocalJobRunner.java   |    2 +-
 src/mapred/org/apache/hadoop/mapred/MapTask.java   |   68 ++--
 .../org/apache/hadoop/mapred/ReduceTask.java       |    2 +-
 src/mapred/org/apache/hadoop/mapred/Task.java      |   66 ++--
 .../apache/hadoop/mapred/TaskAttemptContext.java   |   44 +--
 .../hadoop/mapred/TaskAttemptContextImpl.java      |   63 +++
 .../hadoop/mapred/lib/db/DBOutputFormat.java       |    3 +-
 src/mapred/org/apache/hadoop/mapreduce/Job.java    |   25 +-
 .../org/apache/hadoop/mapreduce/JobContext.java    |  232 ++++++-----
 .../org/apache/hadoop/mapreduce/MapContext.java    |   40 +--
 src/mapred/org/apache/hadoop/mapreduce/Mapper.java |   16 +-
 .../org/apache/hadoop/mapreduce/ReduceContext.java |  164 +-------
 .../org/apache/hadoop/mapreduce/Reducer.java       |   22 +-
 .../hadoop/mapreduce/TaskAttemptContext.java       |   30 +--
 .../hadoop/mapreduce/TaskInputOutputContext.java   |   73 ++---
 .../mapreduce/lib/map/MultithreadedMapper.java     |   18 +-
 .../hadoop/mapreduce/lib/map/WrappedMapper.java    |  310 ++++++++++++++
 .../mapreduce/lib/output/MultipleOutputs.java      |    8 +-
 .../mapreduce/lib/reduce/WrappedReducer.java       |  314 +++++++++++++++
 .../hadoop/mapreduce/task/JobContextImpl.java      |  425 ++++++++++++++++++++
 .../hadoop/mapreduce/task/MapContextImpl.java      |   80 ++++
 .../hadoop/mapreduce/task/ReduceContextImpl.java   |  248 ++++++++++++
 .../mapreduce/task/TaskAttemptContextImpl.java     |   68 ++++
 .../mapreduce/task/TaskInputOutputContextImpl.java |  102 +++++
 .../hadoop/mapred/TestFileOutputCommitter.java     |    6 +-
 .../org/apache/hadoop/mapred/TestTaskCommit.java   |    3 +-
 .../apache/hadoop/mapreduce/MapReduceTestUtil.java |    3 +-
 .../lib/input/TestCombineFileInputFormat.java      |    7 +-
 .../lib/input/TestMRKeyValueTextInputFormat.java   |    5 +-
 35 files changed, 2229 insertions(+), 560 deletions(-)
 create mode 100644 src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockMapContext.java
 create mode 100644 src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockReduceContext.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/JobContextImpl.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/TaskAttemptContextImpl.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/task/JobContextImpl.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/task/MapContextImpl.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/task/ReduceContextImpl.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl.java
 create mode 100644 src/mapred/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java

diff --git a/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockMapContext.java b/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockMapContext.java
new file mode 100644
index 0000000..8f5dd49
--- /dev/null
+++ b/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockMapContext.java
@@ -0,0 +1,96 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mrunit.mapreduce.mock;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Counters;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
+import org.apache.hadoop.mrunit.mock.MockOutputCollector;
+import org.apache.hadoop.mrunit.types.Pair;
+
+public class MockMapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> 
+    extends MapContextImpl<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+
+  private Iterator<Pair<KEYIN, VALUEIN>> inputIter;
+  private Pair<KEYIN, VALUEIN> curInput;
+  private MockOutputCollector<KEYOUT, VALUEOUT> output;
+  
+  public MockMapContext(final List<Pair<KEYIN, VALUEIN>> in, 
+                        final Counters counters) {
+    super(new Configuration(),
+        new TaskAttemptID("mrunit-jt", 0, TaskType.MAP, 0, 0),
+        null, null, new MockOutputCommitter(), new MockReporter(counters), null);
+    this.inputIter = in.iterator();
+    this.output = new MockOutputCollector<KEYOUT, VALUEOUT>();
+  }
+
+  @Override
+  public InputSplit getInputSplit() {
+    return new MockInputSplit();
+  }
+
+  @Override
+  public KEYIN getCurrentKey() {
+    return curInput.getFirst();
+  }
+
+  @Override
+  public VALUEIN getCurrentValue() {
+    return curInput.getSecond();
+  }
+
+  @Override
+  public boolean nextKeyValue() throws IOException {
+    if (this.inputIter.hasNext()) {
+      this.curInput = this.inputIter.next();
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  public void write(KEYOUT key, VALUEOUT value) throws IOException {
+    output.collect(key, value);
+  }
+
+  @Override
+  /** This method does nothing in the mock version. */
+  public void progress() {
+  }
+
+  @Override
+  /** This method does nothing in the mock version. */
+  public void setStatus(String status) {
+  }
+
+  /**
+   * @return the outputs from the MockOutputCollector back to
+   * the test harness.
+   */
+  public List<Pair<KEYOUT, VALUEOUT>> getOutputs() {
+    return output.getOutputs();
+  }
+}
diff --git a/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockReduceContext.java b/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockReduceContext.java
new file mode 100644
index 0000000..606670f
--- /dev/null
+++ b/src/contrib/mrunit/src/java/org/apache/hadoop/mrunit/mapreduce/mock/MockReduceContext.java
@@ -0,0 +1,158 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mrunit.mapreduce.mock;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Counters;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.task.ReduceContextImpl;
+import org.apache.hadoop.mrunit.mock.MockOutputCollector;
+import org.apache.hadoop.mrunit.types.Pair;
+
+public class MockReduceContext <KEYIN, VALUEIN, KEYOUT, VALUEOUT>
+    extends ReduceContextImpl<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+
+  // The iterator over the input key, list(val).
+  private Iterator<Pair<KEYIN, List<VALUEIN>>> inputIter;
+
+  // The current key and list of values.
+  private KEYIN curKey;
+  private InspectableIterable curValueIterable;
+
+  private MockOutputCollector<KEYOUT, VALUEOUT> output;
+
+  public MockReduceContext(final List<Pair<KEYIN, List<VALUEIN>>> in, 
+                           final Counters counters) 
+  throws IOException, InterruptedException {
+    super(new Configuration(),
+          new TaskAttemptID("mrunit-jt", 0, TaskType.REDUCE, 0, 0),
+          new MockRawKeyValueIterator(), null, null, null,
+          new MockOutputCommitter(), new MockReporter(counters), null,
+          (Class) Text.class, (Class) Text.class);
+    this.inputIter = in.iterator();
+    this.output = new MockOutputCollector<KEYOUT, VALUEOUT>();
+  }
+
+
+  /**
+   * A private iterable/iterator implementation that wraps around the 
+   * underlying iterable/iterator used by the input value list. This
+   * memorizes the last value we saw so that we can return it in getCurrentValue().
+   */
+  private class InspectableIterable implements Iterable<VALUEIN> {
+    private Iterable<VALUEIN> base;
+    private VALUEIN lastVal;
+
+    public InspectableIterable(final Iterable<VALUEIN> baseCollection) {
+      this.base = baseCollection;
+    }
+
+    public Iterator<VALUEIN> iterator() {
+      return new InspectableIterator(this.base.iterator());
+    }
+
+    public VALUEIN getLastVal() {
+      return lastVal;
+    }
+
+    private class InspectableIterator 
+        extends ReduceContextImpl<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.ValueIterator
+        implements Iterator<VALUEIN> {
+      private Iterator<VALUEIN> iter;
+      public InspectableIterator(final Iterator<VALUEIN> baseIter) {
+        iter = baseIter;
+      }
+
+      public VALUEIN next() {
+        InspectableIterable.this.lastVal = iter.next();
+        return InspectableIterable.this.lastVal;
+      }
+
+      public boolean hasNext() {
+        return iter.hasNext();
+      }
+
+      public void remove() {
+        iter.remove();
+      }
+    }
+  }
+
+  @Override
+  public boolean nextKey() {
+    if (inputIter.hasNext()) {
+      // Advance to the next key and list of values
+      Pair<KEYIN, List<VALUEIN>> p = inputIter.next();
+      curKey = p.getFirst();
+
+      // Reset the value iterator
+      curValueIterable = new InspectableIterable(p.getSecond());
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  @Override
+  public boolean nextKeyValue() {
+    return nextKey();
+  }
+
+  @Override
+  public KEYIN getCurrentKey() {
+    return curKey;
+  }
+
+  @Override
+  public VALUEIN getCurrentValue() {
+    return curValueIterable.getLastVal();
+  }
+
+  @Override
+  public Iterable<VALUEIN> getValues() {
+    return curValueIterable;
+  }
+
+  public void write(KEYOUT key, VALUEOUT value) throws IOException {
+    output.collect(key, value);
+  }
+
+  @Override
+  /** This method does nothing in the mock version. */
+  public void progress() {
+  }
+
+  @Override
+  /** This method does nothing in the mock version. */
+  public void setStatus(String status) {
+  }
+
+  /**
+   * @return the outputs from the MockOutputCollector back to
+   * the test harness.
+   */
+  public List<Pair<KEYOUT, VALUEOUT>> getOutputs() {
+    return output.getOutputs();
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java b/src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java
index 9438e77..d12f016 100644
--- a/src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java
+++ b/src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java
@@ -234,7 +234,7 @@ public abstract class FileOutputFormat<K, V> implements OutputFormat<K, V> {
 
     OutputCommitter committer = conf.getOutputCommitter();
     Path workPath = outputPath;
-    TaskAttemptContext context = new TaskAttemptContext(conf,
+    TaskAttemptContext context = new TaskAttemptContextImpl(conf,
                 TaskAttemptID.forName(conf.get("mapred.task.id")));
     if (committer instanceof FileOutputCommitter) {
       workPath = ((FileOutputCommitter)committer).getWorkPath(context,
diff --git a/src/mapred/org/apache/hadoop/mapred/JobClient.java b/src/mapred/org/apache/hadoop/mapred/JobClient.java
index 9008f3c..f7592bb 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobClient.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobClient.java
@@ -859,7 +859,7 @@ public class JobClient extends Configured implements MRConstants, Tool  {
             job.setJobSubmitHostAddress(ip.getHostAddress());
             job.setJobSubmitHostName(ip.getHostName());
           }
-          JobContext context = new JobContext(jobCopy, jobId);
+          JobContext context = new JobContextImpl(jobCopy, jobId);
 
           jobCopy = (JobConf)context.getConfiguration();
 
diff --git a/src/mapred/org/apache/hadoop/mapred/JobContext.java b/src/mapred/org/apache/hadoop/mapred/JobContext.java
index fc441a5..7388060 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobContext.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobContext.java
@@ -19,36 +19,18 @@ package org.apache.hadoop.mapred;
 
 import org.apache.hadoop.util.Progressable;
 
-public class JobContext extends org.apache.hadoop.mapreduce.JobContext {
-  private JobConf job;
-  private Progressable progress;
-
-  JobContext(JobConf conf, org.apache.hadoop.mapreduce.JobID jobId, 
-             Progressable progress) {
-    super(conf, jobId);
-    this.job = conf;
-    this.progress = progress;
-  }
-
-  JobContext(JobConf conf, org.apache.hadoop.mapreduce.JobID jobId) {
-    this(conf, jobId, Reporter.NULL);
-  }
-  
+public interface JobContext extends org.apache.hadoop.mapreduce.JobContext {
   /**
    * Get the job Configuration
    * 
    * @return JobConf
    */
-  public JobConf getJobConf() {
-    return job;
-  }
+  public JobConf getJobConf();
   
   /**
    * Get the progress mechanism for reporting progress.
    * 
    * @return progress mechanism 
    */
-  public Progressable getProgressible() {
-    return progress;
-  }
+  public Progressable getProgressible();
 }
diff --git a/src/mapred/org/apache/hadoop/mapred/JobContextImpl.java b/src/mapred/org/apache/hadoop/mapred/JobContextImpl.java
new file mode 100644
index 0000000..d443bc0
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/JobContextImpl.java
@@ -0,0 +1,60 @@
+/* Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * @deprecated Use {@link org.apache.hadoop.mapreduce.JobContext} instead.
+ */
+@Deprecated
+public class JobContextImpl 
+    extends org.apache.hadoop.mapreduce.task.JobContextImpl 
+    implements JobContext {
+  private JobConf job;
+  private Progressable progress;
+
+  JobContextImpl(JobConf conf, org.apache.hadoop.mapreduce.JobID jobId, 
+                 Progressable progress) {
+    super(conf, jobId);
+    this.job = conf;
+    this.progress = progress;
+  }
+
+  JobContextImpl(JobConf conf, org.apache.hadoop.mapreduce.JobID jobId) {
+    this(conf, jobId, Reporter.NULL);
+  }
+  
+  /**
+   * Get the job Configuration
+   * 
+   * @return JobConf
+   */
+  public JobConf getJobConf() {
+    return job;
+  }
+  
+  /**
+   * Get the progress mechanism for reporting progress.
+   * 
+   * @return progress mechanism 
+   */
+  public Progressable getProgressible() {
+    return progress;
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java b/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
index 74b09f8..737a26b 100644
--- a/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -185,7 +185,7 @@ class LocalJobRunner implements JobSubmissionProtocol {
     @Override
     public void run() {
       JobID jobId = profile.getJobID();
-      JobContext jContext = new JobContext(conf, jobId);
+      JobContext jContext = new JobContextImpl(conf, jobId);
       OutputCommitter outputCommitter = job.getOutputCommitter();
       try {
         TaskSplitMetaInfo[] taskSplitMetaInfos =
diff --git a/src/mapred/org/apache/hadoop/mapred/MapTask.java b/src/mapred/org/apache/hadoop/mapred/MapTask.java
index 7fdd4f4..06615bf 100644
--- a/src/mapred/org/apache/hadoop/mapred/MapTask.java
+++ b/src/mapred/org/apache/hadoop/mapred/MapTask.java
@@ -60,6 +60,8 @@ import org.apache.hadoop.io.serializer.Serializer;
 import org.apache.hadoop.mapred.IFile.Writer;
 import org.apache.hadoop.mapred.Merger.Segment;
 import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.mapreduce.lib.map.WrappedMapper;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
 import org.apache.hadoop.mapreduce.split.JobSplit;
 import org.apache.hadoop.mapreduce.split.JobSplit.SplitMetaInfo;
 import org.apache.hadoop.mapreduce.split.JobSplit.TaskSplitIndex;
@@ -597,7 +599,8 @@ class MapTask extends Task {
                              InterruptedException {
     // make a task context so we can get the classes
     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
-      new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID());
+      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, 
+                                                                  getTaskID());
     // make a mapper
     org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =
       (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)
@@ -617,45 +620,32 @@ class MapTask extends Task {
     
     job.setBoolean("mapred.skip.on", isSkipping());
     org.apache.hadoop.mapreduce.RecordWriter output = null;
-    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context 
-         mapperContext = null;
-    try {
-      Constructor<org.apache.hadoop.mapreduce.Mapper.Context> contextConstructor =
-        org.apache.hadoop.mapreduce.Mapper.Context.class.getConstructor
-        (new Class[]{org.apache.hadoop.mapreduce.Mapper.class,
-                     Configuration.class,
-                     org.apache.hadoop.mapreduce.TaskAttemptID.class,
-                     org.apache.hadoop.mapreduce.RecordReader.class,
-                     org.apache.hadoop.mapreduce.RecordWriter.class,
-                     org.apache.hadoop.mapreduce.OutputCommitter.class,
-                     org.apache.hadoop.mapreduce.StatusReporter.class,
-                     org.apache.hadoop.mapreduce.InputSplit.class});
-
-      // get an output object
-      if (job.getNumReduceTasks() == 0) {
-         output =
-           new NewDirectOutputCollector(taskContext, job, umbilical, reporter);
-      } else {
-        output = new NewOutputCollector(taskContext, job, umbilical, reporter);
-      }
-
-      mapperContext = contextConstructor.newInstance(mapper, job, getTaskID(),
-                                                     input, output, committer,
-                                                     reporter, split);
-
-      input.initialize(split, mapperContext);
-      mapper.run(mapperContext);
-      input.close();
-      output.close(mapperContext);
-    } catch (NoSuchMethodException e) {
-      throw new IOException("Can't find Context constructor", e);
-    } catch (InstantiationException e) {
-      throw new IOException("Can't create Context", e);
-    } catch (InvocationTargetException e) {
-      throw new IOException("Can't invoke Context constructor", e);
-    } catch (IllegalAccessException e) {
-      throw new IOException("Can't invoke Context constructor", e);
+    
+    // get an output object
+    if (job.getNumReduceTasks() == 0) {
+      output = 
+        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);
+    } else {
+      output = new NewOutputCollector(taskContext, job, umbilical, reporter);
     }
+      
+    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> 
+    mapContext = 
+      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), 
+          input, output, 
+          committer, 
+          reporter, split);
+      
+    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context 
+        mapperContext = 
+          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(
+              mapContext);
+
+    input.initialize(split, mapperContext);
+    mapper.run(mapperContext);
+    statusUpdate(umbilical);
+    input.close();
+    output.close(mapperContext);
   }
 
   interface MapOutputCollector<K, V> {
diff --git a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
index 1140a93..4481b0b 100644
--- a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
@@ -551,7 +551,7 @@ class ReduceTask extends Task {
     };
     // make a task context so we can get the classes
     org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
-      new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID());
+      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, getTaskID());
     // make a reducer
     org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =
       (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)
diff --git a/src/mapred/org/apache/hadoop/mapred/Task.java b/src/mapred/org/apache/hadoop/mapred/Task.java
index 2d8a2ef..4f2b49c 100644
--- a/src/mapred/org/apache/hadoop/mapred/Task.java
+++ b/src/mapred/org/apache/hadoop/mapred/Task.java
@@ -49,6 +49,8 @@ import org.apache.hadoop.io.serializer.Deserializer;
 import org.apache.hadoop.io.serializer.SerializationFactory;
 import org.apache.hadoop.mapred.IFile.Writer;
 import org.apache.hadoop.mapreduce.JobStatus;
+import org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer;
+import org.apache.hadoop.mapreduce.task.ReduceContextImpl;
 import org.apache.hadoop.net.NetUtils;
 import org.apache.hadoop.util.Progress;
 import org.apache.hadoop.util.Progressable;
@@ -497,8 +499,8 @@ abstract public class Task implements Writable, Configurable {
                          boolean useNewApi) throws IOException, 
                                                    ClassNotFoundException,
                                                    InterruptedException {
-    jobContext = new JobContext(job, id, reporter);
-    taskContext = new TaskAttemptContext(job, taskId, reporter);
+    jobContext = new JobContextImpl(job, id, reporter);
+    taskContext = new TaskAttemptContextImpl(job, taskId, reporter);
     if (getState() == TaskStatus.State.UNASSIGNED) {
       setState(TaskStatus.State.RUNNING);
     }
@@ -1234,29 +1236,6 @@ abstract public class Task implements Writable, Configurable {
     }
   }
 
-  private static final Constructor<org.apache.hadoop.mapreduce.Reducer.Context> 
-  contextConstructor;
-  static {
-    try {
-      contextConstructor = 
-        org.apache.hadoop.mapreduce.Reducer.Context.class.getConstructor
-        (new Class[]{org.apache.hadoop.mapreduce.Reducer.class,
-            Configuration.class,
-            org.apache.hadoop.mapreduce.TaskAttemptID.class,
-            RawKeyValueIterator.class,
-            org.apache.hadoop.mapreduce.Counter.class,
-            org.apache.hadoop.mapreduce.Counter.class,
-            org.apache.hadoop.mapreduce.RecordWriter.class,
-            org.apache.hadoop.mapreduce.OutputCommitter.class,
-            org.apache.hadoop.mapreduce.StatusReporter.class,
-            RawComparator.class,
-            Class.class,
-            Class.class});
-    } catch (NoSuchMethodException nme) {
-      throw new IllegalArgumentException("Can't find constructor");
-    }
-  }
-
   @SuppressWarnings("unchecked")
   protected static <INKEY,INVALUE,OUTKEY,OUTVALUE> 
   org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context
@@ -1272,21 +1251,26 @@ abstract public class Task implements Writable, Configurable {
                       org.apache.hadoop.mapreduce.StatusReporter reporter,
                       RawComparator<INKEY> comparator,
                       Class<INKEY> keyClass, Class<INVALUE> valueClass
-  ) throws IOException, ClassNotFoundException {
-    try {
-
-      return contextConstructor.newInstance(reducer, job, taskId,
-                                            rIter, inputKeyCounter, 
-                                            inputValueCounter, output, 
-                                            committer, reporter, comparator, 
-                                            keyClass, valueClass);
-    } catch (InstantiationException e) {
-      throw new IOException("Can't create Context", e);
-    } catch (InvocationTargetException e) {
-      throw new IOException("Can't invoke Context constructor", e);
-    } catch (IllegalAccessException e) {
-      throw new IOException("Can't invoke Context constructor", e);
-    }
+  ) throws IOException, InterruptedException {
+    org.apache.hadoop.mapreduce.ReduceContext<INKEY, INVALUE, OUTKEY, OUTVALUE> 
+    reduceContext = 
+      new ReduceContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, taskId, 
+                                                              rIter, 
+                                                              inputKeyCounter, 
+                                                              inputValueCounter, 
+                                                              output, 
+                                                              committer, 
+                                                              reporter, 
+                                                              comparator, 
+                                                              keyClass, 
+                                                              valueClass);
+
+    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context 
+        reducerContext = 
+          new WrappedReducer<INKEY, INVALUE, OUTKEY, OUTVALUE>().getReducerContext(
+              reduceContext);
+
+    return reducerContext;
   }
 
   protected static abstract class CombinerRunner<K,V> {
@@ -1328,7 +1312,7 @@ abstract public class Task implements Writable, Configurable {
       }
       // make a task context so we can get the classes
       org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
-        new org.apache.hadoop.mapreduce.TaskAttemptContext(job, taskId);
+        new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, taskId);
       Class<? extends org.apache.hadoop.mapreduce.Reducer<K,V,K,V>> newcls = 
         (Class<? extends org.apache.hadoop.mapreduce.Reducer<K,V,K,V>>)
            taskContext.getCombinerClass();
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskAttemptContext.java b/src/mapred/org/apache/hadoop/mapred/TaskAttemptContext.java
index 072f53c..39e3357 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskAttemptContext.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskAttemptContext.java
@@ -19,39 +19,17 @@ package org.apache.hadoop.mapred;
 
 import org.apache.hadoop.util.Progressable;
 
-public class TaskAttemptContext 
-       extends org.apache.hadoop.mapreduce.TaskAttemptContext {
-  private Progressable progress;
+/**
+ * @deprecated Use {@link org.apache.hadoop.mapreduce.TaskAttemptContext}
+ *   instead.
+ */
+@Deprecated
+public interface TaskAttemptContext 
+    extends org.apache.hadoop.mapreduce.TaskAttemptContext {
 
-  TaskAttemptContext(JobConf conf, TaskAttemptID taskid) {
-    this(conf, taskid, Reporter.NULL);
-  }
-  
-  TaskAttemptContext(JobConf conf, TaskAttemptID taskid,
-                     Progressable progress) {
-    super(conf, taskid);
-    this.progress = progress;
-  }
-  
-  /**
-   * Get the taskAttemptID.
-   *  
-   * @return TaskAttemptID
-   */
-  public TaskAttemptID getTaskAttemptID() {
-    return (TaskAttemptID) super.getTaskAttemptID();
-  }
-  
-  public Progressable getProgressible() {
-    return progress;
-  }
-  
-  public JobConf getJobConf() {
-    return (JobConf) getConfiguration();
-  }
+  public TaskAttemptID getTaskAttemptID();
 
-  @Override
-  public void progress() {
-    progress.progress();
-  }
+  public Progressable getProgressible();
+  
+  public JobConf getJobConf();
 }
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskAttemptContextImpl.java b/src/mapred/org/apache/hadoop/mapred/TaskAttemptContextImpl.java
new file mode 100644
index 0000000..a0903cb
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/TaskAttemptContextImpl.java
@@ -0,0 +1,63 @@
+/* Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * @deprecated Use {@link org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl}
+ *   instead.
+ */
+@Deprecated
+public class TaskAttemptContextImpl
+       extends org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl 
+       implements TaskAttemptContext {
+  private Progressable progress;
+
+  TaskAttemptContextImpl(JobConf conf, TaskAttemptID taskid) {
+    this(conf, taskid, Reporter.NULL);
+  }
+  
+  TaskAttemptContextImpl(JobConf conf, TaskAttemptID taskid,
+                         Progressable progress) {
+    super(conf, taskid);
+    this.progress = progress;
+  }
+  
+  /**
+   * Get the taskAttemptID.
+   *  
+   * @return TaskAttemptID
+   */
+  public TaskAttemptID getTaskAttemptID() {
+    return (TaskAttemptID) super.getTaskAttemptID();
+  }
+  
+  public Progressable getProgressible() {
+    return progress;
+  }
+  
+  public JobConf getJobConf() {
+    return (JobConf) getConfiguration();
+  }
+
+  @Override
+  public void progress() {
+    progress.progress();
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java b/src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java
index f42c57f..c88f03b 100644
--- a/src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java
+++ b/src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java
@@ -30,6 +30,7 @@ import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 import org.apache.hadoop.util.Progressable;
 
 public class DBOutputFormat<K  extends DBWritable, V> 
@@ -64,7 +65,7 @@ public class DBOutputFormat<K  extends DBWritable, V>
   public RecordWriter<K, V> getRecordWriter(FileSystem filesystem,
       JobConf job, String name, Progressable progress) throws IOException {
     org.apache.hadoop.mapreduce.RecordWriter<K, V> w = super.getRecordWriter(
-      new TaskAttemptContext(job, 
+      new TaskAttemptContextImpl(job, 
             TaskAttemptID.forName(job.get("mapred.task.id"))));
     org.apache.hadoop.mapreduce.lib.db.DBOutputFormat.DBRecordWriter writer = 
      (org.apache.hadoop.mapreduce.lib.db.DBOutputFormat.DBRecordWriter) w;
diff --git a/src/mapred/org/apache/hadoop/mapreduce/Job.java b/src/mapred/org/apache/hadoop/mapreduce/Job.java
index 86b2b56..51802fe 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/Job.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/Job.java
@@ -29,6 +29,7 @@ import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.TaskCompletionEvent;
+import org.apache.hadoop.mapreduce.task.JobContextImpl;
 
 /**
  * The job submitter's view of the Job. It allows the user to configure the
@@ -36,8 +37,9 @@ import org.apache.hadoop.mapred.TaskCompletionEvent;
  * only work until the job is submitted, afterwards they will throw an 
  * IllegalStateException.
  */
-public class Job extends JobContext {  
+public class Job extends JobContextImpl implements JobContext {  
   public static enum JobState {DEFINE, RUNNING};
+
   private JobState state = JobState.DEFINE;
   private JobClient jobClient;
   private RunningJob info;
@@ -100,7 +102,8 @@ public class Job extends JobContext {
   public void setInputFormatClass(Class<? extends InputFormat> cls
                                   ) throws IllegalStateException {
     ensureState(JobState.DEFINE);
-    conf.setClass(INPUT_FORMAT_CLASS_ATTR, cls, InputFormat.class);
+    conf.setClass(INPUT_FORMAT_CLASS_ATTR, cls, 
+                  InputFormat.class);
   }
 
   /**
@@ -111,7 +114,8 @@ public class Job extends JobContext {
   public void setOutputFormatClass(Class<? extends OutputFormat> cls
                                    ) throws IllegalStateException {
     ensureState(JobState.DEFINE);
-    conf.setClass(OUTPUT_FORMAT_CLASS_ATTR, cls, OutputFormat.class);
+    conf.setClass(OUTPUT_FORMAT_CLASS_ATTR, cls, 
+                  OutputFormat.class);
   }
 
   /**
@@ -171,7 +175,8 @@ public class Job extends JobContext {
   public void setPartitionerClass(Class<? extends Partitioner> cls
                                   ) throws IllegalStateException {
     ensureState(JobState.DEFINE);
-    conf.setClass(PARTITIONER_CLASS_ATTR, cls, Partitioner.class);
+    conf.setClass(PARTITIONER_CLASS_ATTR, cls, 
+                  Partitioner.class);
   }
 
   /**
@@ -439,12 +444,12 @@ public class Job extends JobContext {
       }      
     } else {
       String mode = "map compatability";
-      ensureNotSet(JobContext.INPUT_FORMAT_CLASS_ATTR, mode);
-      ensureNotSet(JobContext.MAP_CLASS_ATTR, mode);
+      ensureNotSet(INPUT_FORMAT_CLASS_ATTR, mode);
+      ensureNotSet(MAP_CLASS_ATTR, mode);
       if (numReduces != 0) {
-        ensureNotSet(JobContext.PARTITIONER_CLASS_ATTR, mode);
+        ensureNotSet(PARTITIONER_CLASS_ATTR, mode);
        } else {
-        ensureNotSet(JobContext.OUTPUT_FORMAT_CLASS_ATTR, mode);
+        ensureNotSet(OUTPUT_FORMAT_CLASS_ATTR, mode);
       }
     }
     if (numReduces != 0) {
@@ -456,8 +461,8 @@ public class Job extends JobContext {
         ensureNotSet(oldReduceClass, mode);   
       } else {
         String mode = "reduce compatability";
-        ensureNotSet(JobContext.OUTPUT_FORMAT_CLASS_ATTR, mode);
-        ensureNotSet(JobContext.REDUCE_CLASS_ATTR, mode);   
+        ensureNotSet(OUTPUT_FORMAT_CLASS_ATTR, mode);
+        ensureNotSet(REDUCE_CLASS_ATTR, mode);   
       }
     }   
   }
diff --git a/src/mapred/org/apache/hadoop/mapreduce/JobContext.java b/src/mapred/org/apache/hadoop/mapreduce/JobContext.java
index eb752a7..2498107 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/JobContext.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/JobContext.java
@@ -19,14 +19,12 @@
 package org.apache.hadoop.mapreduce;
 
 import java.io.IOException;
+import java.net.URI;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
 import org.apache.hadoop.security.Credentials;
 import org.apache.hadoop.security.UserGroupInformation;
 
@@ -34,24 +32,20 @@ import org.apache.hadoop.security.UserGroupInformation;
  * A read-only view of the job that is provided to the tasks while they
  * are running.
  */
-public class JobContext {
+public interface JobContext {
   // Put all of the attribute names in here so that Job and JobContext are
   // consistent.
-  protected static final String INPUT_FORMAT_CLASS_ATTR = 
+  public static final String INPUT_FORMAT_CLASS_ATTR = 
     "mapreduce.inputformat.class";
-  protected static final String MAP_CLASS_ATTR = "mapreduce.map.class";
-  protected static final String COMBINE_CLASS_ATTR = "mapreduce.combine.class";
-  protected static final String REDUCE_CLASS_ATTR = "mapreduce.reduce.class";
-  protected static final String OUTPUT_FORMAT_CLASS_ATTR = 
+  public static final String MAP_CLASS_ATTR = "mapreduce.map.class";
+  public static final String COMBINE_CLASS_ATTR = "mapreduce.combine.class";
+  public static final String REDUCE_CLASS_ATTR = "mapreduce.reduce.class";
+  public static final String OUTPUT_FORMAT_CLASS_ATTR = 
     "mapreduce.outputformat.class";
-  protected static final String PARTITIONER_CLASS_ATTR = 
+  public static final String PARTITIONER_CLASS_ATTR = 
     "mapreduce.partitioner.class";
   public static final String JAR_UNPACK_PATTERN = "mapreduce.job.jar.unpack.pattern";
 
-  protected final org.apache.hadoop.mapred.JobConf conf;
-  protected final Credentials credentials;
-  private JobID jobId;
-
   public static final String JOB_NAMENODES = "mapreduce.job.hdfs-servers";
 
   public static final String JOB_ACL_VIEW_JOB = "mapreduce.job.acl-view-job";
@@ -69,84 +63,50 @@ public class JobContext {
     "mapred.userlog.retain.hours";
   public static final String MAPREDUCE_TASK_CLASSPATH_PRECEDENCE = 
     "mapreduce.task.classpath.user.precedence";
-  
-  /**
-   * The UserGroupInformation object that has a reference to the current user
-   */
-  protected UserGroupInformation ugi;
-  
-  public JobContext(Configuration conf, JobID jobId) {
-    this.conf = new org.apache.hadoop.mapred.JobConf(conf);
-    this.credentials = this.conf.getCredentials();
-    this.jobId = jobId;
-    try {
-      this.ugi = UserGroupInformation.getCurrentUser();
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  void setJobID(JobID jobId) {
-    this.jobId = jobId;
-  }
 
   /**
    * Return the configuration for the job.
    * @return the shared configuration object
    */
-  public Configuration getConfiguration() {
-    return conf;
-  }
-
+  public Configuration getConfiguration();
+  
   /**
    * Get credentials for the job.
    * @return credentials for the job
    */
-  public Credentials getCredentials() {
-    return credentials;
-  }
+  public Credentials getCredentials();
 
   /**
    * Get the unique ID for the job.
    * @return the object with the job id
    */
-  public JobID getJobID() {
-    return jobId;
-  }
+  public JobID getJobID();
   
   /**
    * Get configured the number of reduce tasks for this job. Defaults to 
    * <code>1</code>.
    * @return the number of reduce tasks for this job.
    */
-  public int getNumReduceTasks() {
-    return conf.getNumReduceTasks();
-  }
+  public int getNumReduceTasks();
   
   /**
    * Get the current working directory for the default file system.
    * 
    * @return the directory name.
    */
-  public Path getWorkingDirectory() throws IOException {
-    return conf.getWorkingDirectory();
-  }
-
+  public Path getWorkingDirectory() throws IOException;
+  
   /**
    * Get the key class for the job output data.
    * @return the key class for the job output data.
    */
-  public Class<?> getOutputKeyClass() {
-    return conf.getOutputKeyClass();
-  }
+  public Class<?> getOutputKeyClass();
   
   /**
    * Get the value class for job outputs.
    * @return the value class for job outputs.
    */
-  public Class<?> getOutputValueClass() {
-    return conf.getOutputValueClass();
-  }
+  public Class<?> getOutputValueClass();
 
   /**
    * Get the key class for the map output data. If it is not set, use the
@@ -154,9 +114,7 @@ public class JobContext {
    * different than the final output key class.
    * @return the map output key class.
    */
-  public Class<?> getMapOutputKeyClass() {
-    return conf.getMapOutputKeyClass();
-  }
+  public Class<?> getMapOutputKeyClass();
 
   /**
    * Get the value class for the map output data. If it is not set, use the
@@ -165,9 +123,7 @@ public class JobContext {
    *  
    * @return the map output value class.
    */
-  public Class<?> getMapOutputValueClass() {
-    return conf.getMapOutputValueClass();
-  }
+  public Class<?> getMapOutputValueClass();
 
   /**
    * Get the user-specified job name. This is only used to identify the 
@@ -175,9 +131,7 @@ public class JobContext {
    * 
    * @return the job's name, defaulting to "".
    */
-  public String getJobName() {
-    return conf.getJobName();
-  }
+  public String getJobName();
   
   /**
    * Get the boolean value for the property that specifies which classpath
@@ -185,9 +139,7 @@ public class JobContext {
    * precedence. False - system's classes takes precedence.
    * @return true if user's classes should take precedence
    */
-  public boolean userClassesTakesPrecedence() {
-    return conf.userClassesTakesPrecedence();
-  }
+  public boolean userClassesTakesPrecedence();
 
   /**
    * Get the {@link InputFormat} class for the job.
@@ -196,10 +148,7 @@ public class JobContext {
    */
   @SuppressWarnings("unchecked")
   public Class<? extends InputFormat<?,?>> getInputFormatClass() 
-     throws ClassNotFoundException {
-    return (Class<? extends InputFormat<?,?>>) 
-      conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);
-  }
+     throws ClassNotFoundException;
 
   /**
    * Get the {@link Mapper} class for the job.
@@ -208,10 +157,7 @@ public class JobContext {
    */
   @SuppressWarnings("unchecked")
   public Class<? extends Mapper<?,?,?,?>> getMapperClass() 
-     throws ClassNotFoundException {
-    return (Class<? extends Mapper<?,?,?,?>>) 
-      conf.getClass(MAP_CLASS_ATTR, Mapper.class);
-  }
+     throws ClassNotFoundException;
 
   /**
    * Get the combiner class for the job.
@@ -220,10 +166,7 @@ public class JobContext {
    */
   @SuppressWarnings("unchecked")
   public Class<? extends Reducer<?,?,?,?>> getCombinerClass() 
-     throws ClassNotFoundException {
-    return (Class<? extends Reducer<?,?,?,?>>) 
-      conf.getClass(COMBINE_CLASS_ATTR, null);
-  }
+     throws ClassNotFoundException;
 
   /**
    * Get the {@link Reducer} class for the job.
@@ -232,10 +175,7 @@ public class JobContext {
    */
   @SuppressWarnings("unchecked")
   public Class<? extends Reducer<?,?,?,?>> getReducerClass() 
-     throws ClassNotFoundException {
-    return (Class<? extends Reducer<?,?,?,?>>) 
-      conf.getClass(REDUCE_CLASS_ATTR, Reducer.class);
-  }
+     throws ClassNotFoundException;
 
   /**
    * Get the {@link OutputFormat} class for the job.
@@ -244,10 +184,7 @@ public class JobContext {
    */
   @SuppressWarnings("unchecked")
   public Class<? extends OutputFormat<?,?>> getOutputFormatClass() 
-     throws ClassNotFoundException {
-    return (Class<? extends OutputFormat<?,?>>) 
-      conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);
-  }
+     throws ClassNotFoundException;
 
   /**
    * Get the {@link Partitioner} class for the job.
@@ -256,27 +193,20 @@ public class JobContext {
    */
   @SuppressWarnings("unchecked")
   public Class<? extends Partitioner<?,?>> getPartitionerClass() 
-     throws ClassNotFoundException {
-    return (Class<? extends Partitioner<?,?>>) 
-      conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);
-  }
+     throws ClassNotFoundException;
 
   /**
    * Get the {@link RawComparator} comparator used to compare keys.
    * 
    * @return the {@link RawComparator} comparator used to compare keys.
    */
-  public RawComparator<?> getSortComparator() {
-    return conf.getOutputKeyComparator();
-  }
+  public RawComparator<?> getSortComparator();
 
   /**
    * Get the pathname of the job's jar.
    * @return the pathname
    */
-  public String getJar() {
-    return conf.getJar();
-  }
+  public String getJar();
 
   /** 
    * Get the user defined {@link RawComparator} comparator for 
@@ -285,7 +215,105 @@ public class JobContext {
    * @return comparator set by the user for grouping values.
    * @see Job#setGroupingComparatorClass(Class) for details.  
    */
-  public RawComparator<?> getGroupingComparator() {
-    return conf.getOutputValueGroupingComparator();
-  }
+  public RawComparator<?> getGroupingComparator();
+  
+  /**
+   * Get whether job-setup and job-cleanup is needed for the job 
+   * 
+   * @return boolean 
+   */
+  public boolean getJobSetupCleanupNeeded();
+
+  /**
+   * Get whether the task profiling is enabled.
+   * @return true if some tasks will be profiled
+   */
+  public boolean getProfileEnabled();
+  
+  /**
+   * 
+   * @return the parameters to pass to the task child to configure profiling
+   */
+  public String getProfileParams();
+
+ /**
+  * Get the reported username for this job.
+  * 
+  * @return the username
+  */
+  public String getUser();
+ 
+ /**
+  * This method checks to see if symlinks are to be create for the 
+  * localized cache files in the current working directory 
+  * @return true if symlinks are to be created- else return false
+  */
+  public boolean getSymlink();
+ 
+ /**
+  * Get the archive entries in classpath as an array of Path
+  */
+  public Path[] getArchiveClassPaths();
+
+ /**
+  * Get cache archives set in the Configuration
+  * @return A URI array of the caches set in the Configuration
+  * @throws IOException
+  */
+  public URI[] getCacheArchives() throws IOException;
+  
+  /**
+   * Get cache files set in the Configuration
+   * @throws IOException
+   */
+  
+  public URI[] getCacheFiles() throws IOException;
+
+  /**
+   * Return the path array of the localized caches
+   * @return A path array of localized caches
+   * @throws IOException
+   */
+  public Path[] getLocalCacheArchives() throws IOException;
+
+  /**
+   * Return the path array of the localized files
+   * @return A path array of localized files
+   * @throws IOException
+   */
+  public Path[] getLocalCacheFiles() throws IOException;
+
+  /**
+   * Get the file entries in classpath as an array of Path
+   */
+  public Path[] getFileClassPaths();
+  
+  /**
+   * Get the timestamps of the archives.  Used by internal
+   * @return a string array of timestamps 
+   * @throws IOException
+   */
+  public String[] getArchiveTimestamps();
+  
+  /**
+   * Get the timestamps of the files.  Used by internal
+   * @return a string array of timestamps 
+   * @throws IOException
+   */
+  public String[] getFileTimestamps();
+
+  /** 
+   * Get the configured number of maximum attempts that will be made to run a
+   *  
+   * @return the max number of attempts per map task.
+   */
+  public int getMaxMapAttempts();
+
+  /** 
+   * Get the configured number of maximum attempts  that will be made to run a
+   * 
+   * @return the max number of attempts per reduce task.
+   */
+  public int getMaxReduceAttempts();
+
 }
diff --git a/src/mapred/org/apache/hadoop/mapreduce/MapContext.java b/src/mapred/org/apache/hadoop/mapreduce/MapContext.java
index 2f3990f..aecbbc7 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/MapContext.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/MapContext.java
@@ -18,10 +18,6 @@
 
 package org.apache.hadoop.mapreduce;
 
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-
 /**
  * The context that is given to the {@link Mapper}.
  * @param <KEYIN> the key input type to the Mapper
@@ -29,43 +25,13 @@ import org.apache.hadoop.conf.Configuration;
  * @param <KEYOUT> the key output type from the Mapper
  * @param <VALUEOUT> the value output type from the Mapper
  */
-public class MapContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
+public interface MapContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
   extends TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
-  private RecordReader<KEYIN,VALUEIN> reader;
-  private InputSplit split;
-
-  public MapContext(Configuration conf, TaskAttemptID taskid,
-                    RecordReader<KEYIN,VALUEIN> reader,
-                    RecordWriter<KEYOUT,VALUEOUT> writer,
-                    OutputCommitter committer,
-                    StatusReporter reporter,
-                    InputSplit split) {
-    super(conf, taskid, writer, committer, reporter);
-    this.reader = reader;
-    this.split = split;
-  }
 
   /**
    * Get the input split for this map.
    */
-  public InputSplit getInputSplit() {
-    return split;
-  }
-
-  @Override
-  public KEYIN getCurrentKey() throws IOException, InterruptedException {
-    return reader.getCurrentKey();
-  }
-
-  @Override
-  public VALUEIN getCurrentValue() throws IOException, InterruptedException {
-    return reader.getCurrentValue();
-  }
-
-  @Override
-  public boolean nextKeyValue() throws IOException, InterruptedException {
-    return reader.nextKeyValue();
-  }
-
+  public InputSplit getInputSplit();
+  
 }
      
\ No newline at end of file
diff --git a/src/mapred/org/apache/hadoop/mapreduce/Mapper.java b/src/mapred/org/apache/hadoop/mapreduce/Mapper.java
index a49d29c..23e8ad9 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/Mapper.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/Mapper.java
@@ -23,6 +23,7 @@ import java.io.IOException;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
 
 /** 
  * Maps input key/value pairs to a set of intermediate key/value pairs.  
@@ -94,16 +95,11 @@ import org.apache.hadoop.io.compress.CompressionCodec;
  */
 public class Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
 
-  public class Context 
-    extends MapContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
-    public Context(Configuration conf, TaskAttemptID taskid,
-                   RecordReader<KEYIN,VALUEIN> reader,
-                   RecordWriter<KEYOUT,VALUEOUT> writer,
-                   OutputCommitter committer,
-                   StatusReporter reporter,
-                   InputSplit split) throws IOException, InterruptedException {
-      super(conf, taskid, reader, writer, committer, reporter, split);
-    }
+  /**
+   * The <code>Context</code> passed on to the {@link Mapper} implementations.
+   */
+  public abstract class Context
+    implements MapContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
   }
   
   /**
diff --git a/src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java b/src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java
index 8f08f0a..855bae9 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java
@@ -22,14 +22,7 @@ import java.io.IOException;
 import java.util.Iterator;
 import java.util.NoSuchElementException;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.serializer.Deserializer;
-import org.apache.hadoop.io.serializer.SerializationFactory;
 import org.apache.hadoop.mapred.RawKeyValueIterator;
-import org.apache.hadoop.util.Progressable;
 
 /**
  * The context passed to the {@link Reducer}.
@@ -38,161 +31,26 @@ import org.apache.hadoop.util.Progressable;
  * @param <KEYOUT> the class of the output keys
  * @param <VALUEOUT> the class of the output values
  */
-public class ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
+public interface ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
     extends TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
-  private RawKeyValueIterator input;
-  private Counter inputKeyCounter;
-  private Counter inputValueCounter;
-  private RawComparator<KEYIN> comparator;
-  private KEYIN key;                                  // current key
-  private VALUEIN value;                              // current value
-  private boolean firstValue = false;                 // first value in key
-  private boolean nextKeyIsSame = false;              // more w/ this key
-  private boolean hasMore;                            // more in file
-  protected Progressable reporter;
-  private Deserializer<KEYIN> keyDeserializer;
-  private Deserializer<VALUEIN> valueDeserializer;
-  private DataInputBuffer buffer = new DataInputBuffer();
-  private BytesWritable currentRawKey = new BytesWritable();
-  private ValueIterable iterable = new ValueIterable();
-
-  public ReduceContext(Configuration conf, TaskAttemptID taskid,
-                       RawKeyValueIterator input, 
-                       Counter inputKeyCounter,
-                       Counter inputValueCounter,
-                       RecordWriter<KEYOUT,VALUEOUT> output,
-                       OutputCommitter committer,
-                       StatusReporter reporter,
-                       RawComparator<KEYIN> comparator,
-                       Class<KEYIN> keyClass,
-                       Class<VALUEIN> valueClass
-                       ) throws InterruptedException, IOException{
-    super(conf, taskid, output, committer, reporter);
-    this.input = input;
-    this.inputKeyCounter = inputKeyCounter;
-    this.inputValueCounter = inputValueCounter;
-    this.comparator = comparator;
-    SerializationFactory serializationFactory = new SerializationFactory(conf);
-    this.keyDeserializer = serializationFactory.getDeserializer(keyClass);
-    this.keyDeserializer.open(buffer);
-    this.valueDeserializer = serializationFactory.getDeserializer(valueClass);
-    this.valueDeserializer.open(buffer);
-    hasMore = input.next();
-  }
 
   /** Start processing next unique key. */
-  public boolean nextKey() throws IOException,InterruptedException {
-    while (hasMore && nextKeyIsSame) {
-      nextKeyValue();
-    }
-    if (hasMore) {
-      if (inputKeyCounter != null) {
-        inputKeyCounter.increment(1);
-      }
-      return nextKeyValue();
-    } else {
-      return false;
-    }
-  }
+  public boolean nextKey() throws IOException,InterruptedException;
 
   /**
-   * Advance to the next key/value pair.
-   */
-  @Override
-  public boolean nextKeyValue() throws IOException, InterruptedException {
-    if (!hasMore) {
-      key = null;
-      value = null;
-      return false;
-    }
-    firstValue = !nextKeyIsSame;
-    DataInputBuffer next = input.getKey();
-    currentRawKey.set(next.getData(), next.getPosition(), 
-                      next.getLength() - next.getPosition());
-    buffer.reset(currentRawKey.getBytes(), 0, currentRawKey.getLength());
-    key = keyDeserializer.deserialize(key);
-    next = input.getValue();
-    buffer.reset(next.getData(), next.getPosition(), next.getLength());
-    value = valueDeserializer.deserialize(value);
-    hasMore = input.next();
-    if (hasMore) {
-      next = input.getKey();
-      nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0, 
-                                         currentRawKey.getLength(),
-                                         next.getData(),
-                                         next.getPosition(),
-                                         next.getLength() - next.getPosition()
-                                         ) == 0;
-    } else {
-      nextKeyIsSame = false;
-    }
-    inputValueCounter.increment(1);
-    return true;
-  }
-
-  public KEYIN getCurrentKey() {
-    return key;
-  }
-
-  @Override
-  public VALUEIN getCurrentValue() {
-    return value;
-  }
-
-  protected class ValueIterator implements Iterator<VALUEIN> {
-
-    @Override
-    public boolean hasNext() {
-      return firstValue || nextKeyIsSame;
-    }
-
-    @Override
-    public VALUEIN next() {
-      // if this is the first record, we don't need to advance
-      if (firstValue) {
-        firstValue = false;
-        return value;
-      }
-      // if this isn't the first record and the next key is different, they
-      // can't advance it here.
-      if (!nextKeyIsSame) {
-        throw new NoSuchElementException("iterate past last value");
-      }
-      // otherwise, go to the next key/value pair
-      try {
-        nextKeyValue();
-        return value;
-      } catch (IOException ie) {
-        throw new RuntimeException("next value iterator failed", ie);
-      } catch (InterruptedException ie) {
-        // this is bad, but we can't modify the exception list of java.util
-        throw new RuntimeException("next value iterator interrupted", ie);        
-      }
-    }
-
-    @Override
-    public void remove() {
-      throw new UnsupportedOperationException("remove not implemented");
-    }
-    
-  }
-
-  protected class ValueIterable implements Iterable<VALUEIN> {
-    private ValueIterator iterator = new ValueIterator();
-    @Override
-    public Iterator<VALUEIN> iterator() {
-      return iterator;
-    } 
-  }
-  
-  /**
    * Iterate through the values for the current key, reusing the same value 
    * object, which is stored in the context.
    * @return the series of values associated with the current key. All of the 
    * objects returned directly and indirectly from this method are reused.
    */
-  public 
-  Iterable<VALUEIN> getValues() throws IOException, InterruptedException {
-    return iterable;
+  public Iterable<VALUEIN> getValues() throws IOException, InterruptedException;
+
+  /**
+   * {@link Iterator} to iterate over values for a given group of records.
+   */
+  interface ValueIterator<VALUEIN> extends Iterator<VALUEIN> {
+
   }
 }
+
+
diff --git a/src/mapred/org/apache/hadoop/mapreduce/Reducer.java b/src/mapred/org/apache/hadoop/mapreduce/Reducer.java
index 583135a..b336fef 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/Reducer.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/Reducer.java
@@ -117,23 +117,11 @@ import org.apache.hadoop.mapred.RawKeyValueIterator;
  */
 public class Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
 
-  public class Context 
-    extends ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
-    public Context(Configuration conf, TaskAttemptID taskid,
-                   RawKeyValueIterator input, 
-                   Counter inputKeyCounter,
-                   Counter inputValueCounter,
-                   RecordWriter<KEYOUT,VALUEOUT> output,
-                   OutputCommitter committer,
-                   StatusReporter reporter,
-                   RawComparator<KEYIN> comparator,
-                   Class<KEYIN> keyClass,
-                   Class<VALUEIN> valueClass
-                   ) throws IOException, InterruptedException {
-      super(conf, taskid, input, inputKeyCounter, inputValueCounter,
-            output, committer, reporter, 
-            comparator, keyClass, valueClass);
-    }
+  /**
+   * The <code>Context</code> passed on to the {@link Reducer} implementations.
+   */
+  public abstract class Context 
+    implements ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
   }
 
   /**
diff --git a/src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java b/src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java
index db6a2f5..e49affb 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java
@@ -18,49 +18,27 @@
 
 package org.apache.hadoop.mapreduce;
 
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.util.Progressable;
 
 /**
  * The context for task attempts.
  */
-public class TaskAttemptContext extends JobContext implements Progressable {
-  private final TaskAttemptID taskId;
-  private String status = "";
-  
-  public TaskAttemptContext(Configuration conf, 
-                            TaskAttemptID taskId) {
-    super(conf, taskId.getJobID());
-    this.taskId = taskId;
-  }
+public interface TaskAttemptContext extends JobContext, Progressable {
 
   /**
    * Get the unique name for this task attempt.
    */
-  public TaskAttemptID getTaskAttemptID() {
-    return taskId;
-  }
+  public TaskAttemptID getTaskAttemptID();
 
   /**
    * Set the current status of the task to the given string.
    */
-  public void setStatus(String msg) throws IOException {
-    status = msg;
-  }
+  public void setStatus(String msg);
 
   /**
    * Get the last set status message.
    * @return the current status message
    */
-  public String getStatus() {
-    return status;
-  }
+  public String getStatus();
 
-  /**
-   * Report progress. The subtypes actually do work in this method.
-   */
-  public void progress() { 
-  }
 }
\ No newline at end of file
diff --git a/src/mapred/org/apache/hadoop/mapreduce/TaskInputOutputContext.java b/src/mapred/org/apache/hadoop/mapreduce/TaskInputOutputContext.java
index 1d2bb06..0147061 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/TaskInputOutputContext.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/TaskInputOutputContext.java
@@ -20,9 +20,6 @@ package org.apache.hadoop.mapreduce;
 
 import java.io.IOException;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.util.Progressable;
-
 /**
  * A context object that allows input and output from the task. It is only
  * supplied to the {@link Mapper} or {@link Reducer}.
@@ -31,28 +28,14 @@ import org.apache.hadoop.util.Progressable;
  * @param <KEYOUT> the output key type for the task
  * @param <VALUEOUT> the output value type for the task
  */
-public abstract class TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
-       extends TaskAttemptContext implements Progressable {
-  private RecordWriter<KEYOUT,VALUEOUT> output;
-  private StatusReporter reporter;
-  private OutputCommitter committer;
-
-  public TaskInputOutputContext(Configuration conf, TaskAttemptID taskid,
-                                RecordWriter<KEYOUT,VALUEOUT> output,
-                                OutputCommitter committer,
-                                StatusReporter reporter) {
-    super(conf, taskid);
-    this.output = output;
-    this.reporter = reporter;
-    this.committer = committer;
-  }
+public interface TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
+       extends TaskAttemptContext {
 
   /**
    * Advance to the next key, value pair, returning null if at end.
    * @return the key object that was read into, or null if no more
    */
-  public abstract 
-  boolean nextKeyValue() throws IOException, InterruptedException;
+  public boolean nextKeyValue() throws IOException, InterruptedException;
  
   /**
    * Get the current key.
@@ -60,8 +43,7 @@ public abstract class TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
    * @throws IOException
    * @throws InterruptedException
    */
-  public abstract 
-  KEYIN getCurrentKey() throws IOException, InterruptedException;
+  public KEYIN getCurrentKey() throws IOException, InterruptedException;
 
   /**
    * Get the current value.
@@ -69,36 +51,33 @@ public abstract class TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
    * @throws IOException
    * @throws InterruptedException
    */
-  public abstract VALUEIN getCurrentValue() throws IOException, 
-                                                   InterruptedException;
+  public VALUEIN getCurrentValue() throws IOException, InterruptedException;
 
   /**
    * Generate an output key/value pair.
    */
-  public void write(KEYOUT key, VALUEOUT value
-                    ) throws IOException, InterruptedException {
-    output.write(key, value);
-  }
-
-  public Counter getCounter(Enum<?> counterName) {
-    return reporter.getCounter(counterName);
-  }
+  public void write(KEYOUT key, VALUEOUT value) 
+      throws IOException, InterruptedException;
 
-  public Counter getCounter(String groupName, String counterName) {
-    return reporter.getCounter(groupName, counterName);
-  }
+  /**
+   * Get the {@link Counter} for the given <code>counterName</code>.
+   * @param counterName counter name
+   * @return the <code>Counter</code> for the given <code>counterName</code>
+   */
+  public Counter getCounter(Enum<?> counterName);
 
-  @Override
-  public void progress() {
-    reporter.progress();
-  }
+  /**
+   * Get the {@link Counter} for the given <code>groupName</code> and 
+   * <code>counterName</code>.
+   * @param counterName counter name
+   * @return the <code>Counter</code> for the given <code>groupName</code> and 
+   *         <code>counterName</code>
+   */
+  public Counter getCounter(String groupName, String counterName);
 
-  @Override
-  public void setStatus(String status) {
-    reporter.setStatus(status);
-  }
-  
-  public OutputCommitter getOutputCommitter() {
-    return committer;
-  }
+  /**
+   * Get the {@link OutputCommitter} for the task-attempt.
+   * @return the <code>OutputCommitter</code> for the task-attempt
+   */
+  public OutputCommitter getOutputCommitter();
 }
diff --git a/src/mapred/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java b/src/mapred/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java
index 95530f9..47f138e 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java
@@ -24,11 +24,13 @@ import org.apache.hadoop.mapreduce.Counter;
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.MapContext;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.RecordWriter;
 import org.apache.hadoop.mapreduce.StatusReporter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
@@ -245,13 +247,15 @@ public class MultithreadedMapper<K1, V1, K2, V2>
     MapRunner(Context context) throws IOException, InterruptedException {
       mapper = ReflectionUtils.newInstance(mapClass, 
                                            context.getConfiguration());
-      subcontext = new Context(outer.getConfiguration(), 
-                            outer.getTaskAttemptID(),
-                            new SubMapRecordReader(),
-                            new SubMapRecordWriter(), 
-                            context.getOutputCommitter(),
-                            new SubMapStatusReporter(),
-                            outer.getInputSplit());
+      MapContext<K1, V1, K2, V2> mapContext = 
+        new MapContextImpl<K1, V1, K2, V2>(outer.getConfiguration(), 
+                                           outer.getTaskAttemptID(),
+                                           new SubMapRecordReader(),
+                                           new SubMapRecordWriter(), 
+                                           context.getOutputCommitter(),
+                                           new SubMapStatusReporter(),
+                                           outer.getInputSplit());
+      subcontext = new WrappedMapper<K1, V1, K2, V2>().getMapContext(mapContext);
     }
 
     public Throwable getThrowable() {
diff --git a/src/mapred/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java b/src/mapred/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java
new file mode 100644
index 0000000..ab011e5
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java
@@ -0,0 +1,310 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.lib.map;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.MapContext;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.Partitioner;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.security.Credentials;
+
+/**
+ * A {@link Mapper} which wraps a given one to allow custom 
+ * {@link Mapper.Context} implementations.
+ */
+public class WrappedMapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> 
+    extends Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+  
+  /**
+   * Get a wrapped {@link Mapper.Context} for custom implementations.
+   * @param mapContext <code>MapContext</code> to be wrapped
+   * @return a wrapped <code>Mapper.Context</code> for custom implementations
+   */
+  public Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context
+  getMapContext(MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> mapContext) {
+    return new Context(mapContext);
+  }
+  
+  public class Context 
+      extends Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context {
+
+    protected MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> mapContext;
+
+    public Context(MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> mapContext) {
+      this.mapContext = mapContext;
+    }
+
+    /**
+     * Get the input split for this map.
+     */
+    public InputSplit getInputSplit() {
+      return mapContext.getInputSplit();
+    }
+
+    @Override
+    public KEYIN getCurrentKey() throws IOException, InterruptedException {
+      return mapContext.getCurrentKey();
+    }
+
+    @Override
+    public VALUEIN getCurrentValue() throws IOException, InterruptedException {
+      return mapContext.getCurrentValue();
+    }
+
+    @Override
+    public boolean nextKeyValue() throws IOException, InterruptedException {
+      return mapContext.nextKeyValue();
+    }
+
+    @Override
+    public Counter getCounter(Enum<?> counterName) {
+      return mapContext.getCounter(counterName);
+    }
+
+    @Override
+    public Counter getCounter(String groupName, String counterName) {
+      return mapContext.getCounter(groupName, counterName);
+    }
+
+    @Override
+    public OutputCommitter getOutputCommitter() {
+      return mapContext.getOutputCommitter();
+    }
+
+    @Override
+    public void write(KEYOUT key, VALUEOUT value) throws IOException,
+        InterruptedException {
+      mapContext.write(key, value);
+    }
+
+    @Override
+    public String getStatus() {
+      return mapContext.getStatus();
+    }
+
+    @Override
+    public TaskAttemptID getTaskAttemptID() {
+      return mapContext.getTaskAttemptID();
+    }
+
+    @Override
+    public void setStatus(String msg) {
+      mapContext.setStatus(msg);
+    }
+
+    @Override
+    public Path[] getArchiveClassPaths() {
+      return mapContext.getArchiveClassPaths();
+    }
+
+    @Override
+    public String[] getArchiveTimestamps() {
+      return mapContext.getArchiveTimestamps();
+    }
+
+    @Override
+    public URI[] getCacheArchives() throws IOException {
+      return mapContext.getCacheArchives();
+    }
+
+    @Override
+    public URI[] getCacheFiles() throws IOException {
+      return mapContext.getCacheArchives();
+    }
+
+    @Override
+    public Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass()
+        throws ClassNotFoundException {
+      return mapContext.getCombinerClass();
+    }
+
+    @Override
+    public Configuration getConfiguration() {
+      return mapContext.getConfiguration();
+    }
+
+    @Override
+    public Path[] getFileClassPaths() {
+      return mapContext.getFileClassPaths();
+    }
+
+    @Override
+    public String[] getFileTimestamps() {
+      return mapContext.getFileTimestamps();
+    }
+
+    @Override
+    public RawComparator<?> getGroupingComparator() {
+      return mapContext.getGroupingComparator();
+    }
+
+    @Override
+    public Class<? extends InputFormat<?, ?>> getInputFormatClass()
+        throws ClassNotFoundException {
+      return mapContext.getInputFormatClass();
+    }
+
+    @Override
+    public String getJar() {
+      return mapContext.getJar();
+    }
+
+    @Override
+    public JobID getJobID() {
+      return mapContext.getJobID();
+    }
+
+    @Override
+    public String getJobName() {
+      return mapContext.getJobName();
+    }
+    
+    @Override
+    public boolean userClassesTakesPrecedence() {
+      return mapContext.userClassesTakesPrecedence();
+    }
+
+    @Override
+    public boolean getJobSetupCleanupNeeded() {
+      return mapContext.getJobSetupCleanupNeeded();
+    }
+
+    @Override
+    public Path[] getLocalCacheArchives() throws IOException {
+      return mapContext.getLocalCacheArchives();
+    }
+
+    @Override
+    public Path[] getLocalCacheFiles() throws IOException {
+      return mapContext.getLocalCacheFiles();
+    }
+
+    @Override
+    public Class<?> getMapOutputKeyClass() {
+      return mapContext.getMapOutputKeyClass();
+    }
+
+    @Override
+    public Class<?> getMapOutputValueClass() {
+      return mapContext.getMapOutputValueClass();
+    }
+
+    @Override
+    public Class<? extends Mapper<?, ?, ?, ?>> getMapperClass()
+        throws ClassNotFoundException {
+      return mapContext.getMapperClass();
+    }
+
+    @Override
+    public int getMaxMapAttempts() {
+      return mapContext.getMaxMapAttempts();
+    }
+
+    @Override
+    public int getMaxReduceAttempts() {
+      return mapContext.getMaxReduceAttempts();
+    }
+
+    @Override
+    public int getNumReduceTasks() {
+      return mapContext.getNumReduceTasks();
+    }
+
+    @Override
+    public Class<? extends OutputFormat<?, ?>> getOutputFormatClass()
+        throws ClassNotFoundException {
+      return mapContext.getOutputFormatClass();
+    }
+
+    @Override
+    public Class<?> getOutputKeyClass() {
+      return mapContext.getOutputKeyClass();
+    }
+
+    @Override
+    public Class<?> getOutputValueClass() {
+      return mapContext.getOutputValueClass();
+    }
+
+    @Override
+    public Class<? extends Partitioner<?, ?>> getPartitionerClass()
+        throws ClassNotFoundException {
+      return mapContext.getPartitionerClass();
+    }
+
+    @Override
+    public Class<? extends Reducer<?, ?, ?, ?>> getReducerClass()
+        throws ClassNotFoundException {
+      return mapContext.getReducerClass();
+    }
+
+    @Override
+    public RawComparator<?> getSortComparator() {
+      return mapContext.getSortComparator();
+    }
+
+    @Override
+    public boolean getSymlink() {
+      return mapContext.getSymlink();
+    }
+
+    @Override
+    public Path getWorkingDirectory() throws IOException {
+      return mapContext.getWorkingDirectory();
+    }
+
+    @Override
+    public void progress() {
+      mapContext.progress();
+    }
+
+    @Override
+    public boolean getProfileEnabled() {
+      return mapContext.getProfileEnabled();
+    }
+
+    @Override
+    public String getProfileParams() {
+      return mapContext.getProfileParams();
+    }
+
+    @Override
+    public String getUser() {
+      return mapContext.getUser();
+    }
+    
+    @Override
+    public Credentials getCredentials() {
+      return mapContext.getCredentials();
+    }
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java b/src/mapred/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java
index f3e34f5..5145c25 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java
@@ -21,6 +21,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapreduce.*;
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 import org.apache.hadoop.util.ReflectionUtils;
 
 import java.io.IOException;
@@ -379,8 +380,9 @@ public class MultipleOutputs<KEYOUT, VALUEOUT> {
   public void write(KEYOUT key, VALUEOUT value, String baseOutputPath) 
       throws IOException, InterruptedException {
     checkBaseOutputPath(baseOutputPath);
-    TaskAttemptContext taskContext = new TaskAttemptContext(
-      context.getConfiguration(), context.getTaskAttemptID());
+    TaskAttemptContext taskContext = 
+      new TaskAttemptContextImpl(context.getConfiguration(), 
+                                 context.getTaskAttemptID());
     getRecordWriter(taskContext, baseOutputPath).write(key, value);
   }
 
@@ -434,7 +436,7 @@ public class MultipleOutputs<KEYOUT, VALUEOUT> {
     job.setOutputFormatClass(getNamedOutputFormatClass(context, nameOutput));
     job.setOutputKeyClass(getNamedOutputKeyClass(context, nameOutput));
     job.setOutputValueClass(getNamedOutputValueClass(context, nameOutput));
-    taskContext = new TaskAttemptContext(
+    taskContext = new TaskAttemptContextImpl(
       job.getConfiguration(), context.getTaskAttemptID());
     
     taskContexts.put(nameOutput, taskContext);
diff --git a/src/mapred/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java b/src/mapred/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java
new file mode 100644
index 0000000..b990b62
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java
@@ -0,0 +1,314 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.lib.reduce;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.Partitioner;
+import org.apache.hadoop.mapreduce.ReduceContext;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.security.Credentials;
+
+/**
+ * A {@link Reducer} which wraps a given one to allow for custom 
+ * {@link Reducer.Context} implementations.
+ */
+public class WrappedReducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT> 
+    extends Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+
+  /**
+   * A a wrapped {@link Reducer.Context} for custom implementations.
+   * @param reduceContext <code>ReduceContext</code> to be wrapped
+   * @return a wrapped <code>Reducer.Context</code> for custom implementations
+   */
+  public Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context 
+  getReducerContext(ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> reduceContext) {
+    return new Context(reduceContext);
+  }
+  
+  public class Context 
+      extends Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context {
+
+    protected ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> reduceContext;
+
+    public Context(ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> reduceContext)
+    {
+      this.reduceContext = reduceContext; 
+    }
+
+    @Override
+    public KEYIN getCurrentKey() throws IOException, InterruptedException {
+      return reduceContext.getCurrentKey();
+    }
+
+    @Override
+    public VALUEIN getCurrentValue() throws IOException, InterruptedException {
+      return reduceContext.getCurrentValue();
+    }
+
+    @Override
+    public boolean nextKeyValue() throws IOException, InterruptedException {
+      return reduceContext.nextKeyValue();
+    }
+
+    @Override
+    public Counter getCounter(Enum counterName) {
+      return reduceContext.getCounter(counterName);
+    }
+
+    @Override
+    public Counter getCounter(String groupName, String counterName) {
+      return reduceContext.getCounter(groupName, counterName);
+    }
+
+    @Override
+    public OutputCommitter getOutputCommitter() {
+      return reduceContext.getOutputCommitter();
+    }
+
+    @Override
+    public void write(KEYOUT key, VALUEOUT value) throws IOException,
+        InterruptedException {
+      reduceContext.write(key, value);
+    }
+
+    @Override
+    public String getStatus() {
+      return reduceContext.getStatus();
+    }
+
+    @Override
+    public TaskAttemptID getTaskAttemptID() {
+      return reduceContext.getTaskAttemptID();
+    }
+
+    @Override
+    public void setStatus(String msg) {
+      reduceContext.setStatus(msg);
+    }
+
+    @Override
+    public Path[] getArchiveClassPaths() {
+      return reduceContext.getArchiveClassPaths();
+    }
+
+    @Override
+    public String[] getArchiveTimestamps() {
+      return reduceContext.getArchiveTimestamps();
+    }
+
+    @Override
+    public URI[] getCacheArchives() throws IOException {
+      return reduceContext.getCacheArchives();
+    }
+
+    @Override
+    public URI[] getCacheFiles() throws IOException {
+      return reduceContext.getCacheArchives();
+    }
+
+    @Override
+    public Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass()
+        throws ClassNotFoundException {
+      return reduceContext.getCombinerClass();
+    }
+
+    @Override
+    public Configuration getConfiguration() {
+      return reduceContext.getConfiguration();
+    }
+
+    @Override
+    public Path[] getFileClassPaths() {
+      return reduceContext.getFileClassPaths();
+    }
+
+    @Override
+    public String[] getFileTimestamps() {
+      return reduceContext.getFileTimestamps();
+    }
+
+    @Override
+    public RawComparator<?> getGroupingComparator() {
+      return reduceContext.getGroupingComparator();
+    }
+
+    @Override
+    public Class<? extends InputFormat<?, ?>> getInputFormatClass()
+        throws ClassNotFoundException {
+      return reduceContext.getInputFormatClass();
+    }
+
+    @Override
+    public String getJar() {
+      return reduceContext.getJar();
+    }
+
+    @Override
+    public JobID getJobID() {
+      return reduceContext.getJobID();
+    }
+
+    @Override
+    public String getJobName() {
+      return reduceContext.getJobName();
+    }
+    
+    @Override
+    public boolean userClassesTakesPrecedence() {
+      return reduceContext.userClassesTakesPrecedence();
+    }
+
+    @Override
+    public boolean getJobSetupCleanupNeeded() {
+      return reduceContext.getJobSetupCleanupNeeded();
+    }
+
+    @Override
+    public Path[] getLocalCacheArchives() throws IOException {
+      return reduceContext.getLocalCacheArchives();
+    }
+
+    @Override
+    public Path[] getLocalCacheFiles() throws IOException {
+      return reduceContext.getLocalCacheFiles();
+    }
+
+    @Override
+    public Class<?> getMapOutputKeyClass() {
+      return reduceContext.getMapOutputKeyClass();
+    }
+
+    @Override
+    public Class<?> getMapOutputValueClass() {
+      return reduceContext.getMapOutputValueClass();
+    }
+
+    @Override
+    public Class<? extends Mapper<?, ?, ?, ?>> getMapperClass()
+        throws ClassNotFoundException {
+      return reduceContext.getMapperClass();
+    }
+
+    @Override
+    public int getMaxMapAttempts() {
+      return reduceContext.getMaxMapAttempts();
+    }
+
+    @Override
+    public int getMaxReduceAttempts() {
+      return reduceContext.getMaxReduceAttempts();
+    }
+
+    @Override
+    public int getNumReduceTasks() {
+      return reduceContext.getNumReduceTasks();
+    }
+
+    @Override
+    public Class<? extends OutputFormat<?, ?>> getOutputFormatClass()
+        throws ClassNotFoundException {
+      return reduceContext.getOutputFormatClass();
+    }
+
+    @Override
+    public Class<?> getOutputKeyClass() {
+      return reduceContext.getOutputKeyClass();
+    }
+
+    @Override
+    public Class<?> getOutputValueClass() {
+      return reduceContext.getOutputValueClass();
+    }
+
+    @Override
+    public Class<? extends Partitioner<?, ?>> getPartitionerClass()
+        throws ClassNotFoundException {
+      return reduceContext.getPartitionerClass();
+    }
+
+    @Override
+    public Class<? extends Reducer<?, ?, ?, ?>> getReducerClass()
+        throws ClassNotFoundException {
+      return reduceContext.getReducerClass();
+    }
+
+    @Override
+    public RawComparator<?> getSortComparator() {
+      return reduceContext.getSortComparator();
+    }
+
+    @Override
+    public boolean getSymlink() {
+      return reduceContext.getSymlink();
+    }
+
+    @Override
+    public Path getWorkingDirectory() throws IOException {
+      return reduceContext.getWorkingDirectory();
+    }
+
+    @Override
+    public void progress() {
+      reduceContext.progress();
+    }
+
+    @Override
+    public Iterable<VALUEIN> getValues() throws IOException,
+        InterruptedException {
+      return reduceContext.getValues();
+    }
+
+    @Override
+    public boolean nextKey() throws IOException, InterruptedException {
+      return reduceContext.nextKey();
+    }
+    
+    @Override
+    public boolean getProfileEnabled() {
+      return reduceContext.getProfileEnabled();
+    }
+
+    @Override
+    public String getProfileParams() {
+      return reduceContext.getProfileParams();
+    }
+
+    @Override
+    public String getUser() {
+      return reduceContext.getUser();
+    }
+    
+    @Override
+    public Credentials getCredentials() {
+      return reduceContext.getCredentials();
+    }
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapreduce/task/JobContextImpl.java b/src/mapred/org/apache/hadoop/mapreduce/task/JobContextImpl.java
new file mode 100644
index 0000000..f0637b0
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/task/JobContextImpl.java
@@ -0,0 +1,425 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.task;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configuration.IntegerRanges;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.Partitioner;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
+import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * A read-only view of the job that is provided to the tasks while they
+ * are running.
+ */
+public class JobContextImpl implements JobContext {
+
+  protected final org.apache.hadoop.mapred.JobConf conf;
+  private JobID jobId;
+  /**
+   * The UserGroupInformation object that has a reference to the current user
+   */
+  protected UserGroupInformation ugi;
+  protected final Credentials credentials;
+  
+  public JobContextImpl(Configuration conf, JobID jobId) {
+    if (conf instanceof JobConf) {
+      this.conf = (JobConf)conf;
+    } else {
+      this.conf = new JobConf(conf);
+    }
+    this.jobId = jobId;
+    this.credentials = this.conf.getCredentials();
+    try {
+      this.ugi = UserGroupInformation.getCurrentUser();
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  /**
+   * Return the configuration for the job.
+   * @return the shared configuration object
+   */
+  public Configuration getConfiguration() {
+    return conf;
+  }
+
+  /**
+   * Get the unique ID for the job.
+   * @return the object with the job id
+   */
+  public JobID getJobID() {
+    return jobId;
+  }
+  
+  /**
+   * Set the JobID.
+   */
+  public void setJobID(JobID jobId) {
+    this.jobId = jobId;
+  }
+  
+  /**
+   * Get configured the number of reduce tasks for this job. Defaults to 
+   * <code>1</code>.
+   * @return the number of reduce tasks for this job.
+   */
+  public int getNumReduceTasks() {
+    return conf.getNumReduceTasks();
+  }
+  
+  /**
+   * Get the current working directory for the default file system.
+   * 
+   * @return the directory name.
+   */
+  public Path getWorkingDirectory() throws IOException {
+    return conf.getWorkingDirectory();
+  }
+
+  /**
+   * Get the key class for the job output data.
+   * @return the key class for the job output data.
+   */
+  public Class<?> getOutputKeyClass() {
+    return conf.getOutputKeyClass();
+  }
+  
+  /**
+   * Get the value class for job outputs.
+   * @return the value class for job outputs.
+   */
+  public Class<?> getOutputValueClass() {
+    return conf.getOutputValueClass();
+  }
+
+  /**
+   * Get the key class for the map output data. If it is not set, use the
+   * (final) output key class. This allows the map output key class to be
+   * different than the final output key class.
+   * @return the map output key class.
+   */
+  public Class<?> getMapOutputKeyClass() {
+    return conf.getMapOutputKeyClass();
+  }
+
+  /**
+   * Get the value class for the map output data. If it is not set, use the
+   * (final) output value class This allows the map output value class to be
+   * different than the final output value class.
+   *  
+   * @return the map output value class.
+   */
+  public Class<?> getMapOutputValueClass() {
+    return conf.getMapOutputValueClass();
+  }
+
+  /**
+   * Get the user-specified job name. This is only used to identify the 
+   * job to the user.
+   * 
+   * @return the job's name, defaulting to "".
+   */
+  public String getJobName() {
+    return conf.getJobName();
+  }
+  
+  /**
+   * Get the boolean value for the property that specifies which classpath
+   * takes precedence when tasks are launched. True - user's classes takes
+   * precedence. False - system's classes takes precedence.
+   * @return true if user's classes should take precedence
+   */
+  public boolean userClassesTakesPrecedence() {
+    return conf.userClassesTakesPrecedence();
+  }
+
+  /**
+   * Get the {@link InputFormat} class for the job.
+   * 
+   * @return the {@link InputFormat} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends InputFormat<?,?>> getInputFormatClass() 
+     throws ClassNotFoundException {
+    return (Class<? extends InputFormat<?,?>>) 
+      conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);
+  }
+
+  /**
+   * Get the {@link Mapper} class for the job.
+   * 
+   * @return the {@link Mapper} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Mapper<?,?,?,?>> getMapperClass() 
+     throws ClassNotFoundException {
+    return (Class<? extends Mapper<?,?,?,?>>) 
+      conf.getClass(MAP_CLASS_ATTR, Mapper.class);
+  }
+
+  /**
+   * Get the combiner class for the job.
+   * 
+   * @return the combiner class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Reducer<?,?,?,?>> getCombinerClass() 
+     throws ClassNotFoundException {
+    return (Class<? extends Reducer<?,?,?,?>>) 
+      conf.getClass(COMBINE_CLASS_ATTR, null);
+  }
+
+  /**
+   * Get the {@link Reducer} class for the job.
+   * 
+   * @return the {@link Reducer} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Reducer<?,?,?,?>> getReducerClass() 
+     throws ClassNotFoundException {
+    return (Class<? extends Reducer<?,?,?,?>>) 
+      conf.getClass(REDUCE_CLASS_ATTR, Reducer.class);
+  }
+
+  /**
+   * Get the {@link OutputFormat} class for the job.
+   * 
+   * @return the {@link OutputFormat} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends OutputFormat<?,?>> getOutputFormatClass() 
+     throws ClassNotFoundException {
+    return (Class<? extends OutputFormat<?,?>>) 
+      conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);
+  }
+
+  /**
+   * Get the {@link Partitioner} class for the job.
+   * 
+   * @return the {@link Partitioner} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Partitioner<?,?>> getPartitionerClass() 
+     throws ClassNotFoundException {
+    return (Class<? extends Partitioner<?,?>>) 
+      conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);
+  }
+
+  /**
+   * Get the {@link RawComparator} comparator used to compare keys.
+   * 
+   * @return the {@link RawComparator} comparator used to compare keys.
+   */
+  public RawComparator<?> getSortComparator() {
+    return conf.getOutputKeyComparator();
+  }
+
+  /**
+   * Get the pathname of the job's jar.
+   * @return the pathname
+   */
+  public String getJar() {
+    return conf.getJar();
+  }
+
+  /** 
+   * Get the user defined {@link RawComparator} comparator for 
+   * grouping keys of inputs to the reduce.
+   * 
+   * @return comparator set by the user for grouping values.
+   * @see Job#setGroupingComparatorClass(Class) for details.  
+   */
+  public RawComparator<?> getGroupingComparator() {
+    return conf.getOutputValueGroupingComparator();
+  }
+  
+  /**
+   * Get whether job-setup and job-cleanup is needed for the job 
+   * 
+   * @return boolean 
+   */
+  public boolean getJobSetupCleanupNeeded() {
+    return conf.getBoolean("mapred.committer.job.setup.cleanup.needed", true);
+  }
+
+  /**
+   * This method checks to see if symlinks are to be create for the 
+   * localized cache files in the current working directory 
+   * @return true if symlinks are to be created- else return false
+   */
+  public boolean getSymlink() {
+    return DistributedCache.getSymlink(conf);
+  }
+  
+  /**
+   * Get the archive entries in classpath as an array of Path
+   */
+  public Path[] getArchiveClassPaths() {
+    return DistributedCache.getArchiveClassPaths(conf);
+  }
+
+  /**
+   * Get cache archives set in the Configuration
+   * @return A URI array of the caches set in the Configuration
+   * @throws IOException
+   */
+  public URI[] getCacheArchives() throws IOException {
+    return DistributedCache.getCacheArchives(conf);
+  }
+
+  /**
+   * Get cache files set in the Configuration
+   * @return A URI array of the files set in the Configuration
+   * @throws IOException
+   */
+
+  public URI[] getCacheFiles() throws IOException {
+    return DistributedCache.getCacheFiles(conf);
+  }
+
+  /**
+   * Return the path array of the localized caches
+   * @return A path array of localized caches
+   * @throws IOException
+   */
+  public Path[] getLocalCacheArchives()
+    throws IOException {
+    return DistributedCache.getLocalCacheArchives(conf);
+  }
+
+  /**
+   * Return the path array of the localized files
+   * @return A path array of localized files
+   * @throws IOException
+   */
+  public Path[] getLocalCacheFiles()
+    throws IOException {
+    return DistributedCache.getLocalCacheFiles(conf);
+  }
+
+  /**
+   * Get the file entries in classpath as an array of Path
+   */
+  public Path[] getFileClassPaths() {
+    return DistributedCache.getFileClassPaths(conf);
+  }
+  
+  /**
+   * Get the timestamps of the archives.  Used by internal
+   * DistributedCache and MapReduce code.
+   * @return a string array of timestamps 
+   * @throws IOException
+   */
+  public String[] getArchiveTimestamps() {
+    return conf.getStrings(DistributedCache.CACHE_ARCHIVES_TIMESTAMPS);
+  }
+
+  /**
+   * Get the timestamps of the files.  Used by internal
+   * DistributedCache and MapReduce code.
+   * @return a string array of timestamps 
+   * @throws IOException
+   */
+  public String[] getFileTimestamps() {
+    return conf.getStrings(DistributedCache.CACHE_FILES_TIMESTAMPS);
+  }
+
+  /** 
+   * Get the configured number of maximum attempts that will be made to run a
+   * map task, as specified by the <code>mapred.map.max.attempts</code>
+   * property. If this property is not already set, the default is 4 attempts.
+   *  
+   * @return the max number of attempts per map task.
+   */
+  public int getMaxMapAttempts() {
+    return conf.getMaxMapAttempts();
+  }
+
+  /** 
+   * Get the configured number of maximum attempts  that will be made to run a
+   * reduce task, as specified by the <code>mapred.reduce.max.attempts</code>
+   * property. If this property is not already set, the default is 4 attempts.
+   * 
+   * @return the max number of attempts per reduce task.
+   */
+  public int getMaxReduceAttempts() {
+    return conf.getMaxReduceAttempts();
+  }
+
+  /**
+   * Get whether the task profiling is enabled.
+   * @return true if some tasks will be profiled
+   */
+  public boolean getProfileEnabled() {
+    return conf.getProfileEnabled();
+  }
+
+  /**
+   * Get the profiler configuration arguments.
+   *
+   * The default value for this property is
+   * "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s"
+   * 
+   * @return the parameters to pass to the task child to configure profiling
+   */
+  public String getProfileParams() {
+    return conf.getProfileParams();
+  }
+
+  /**
+   * Get the range of maps or reduces to profile.
+   * @param isMap is the task a map?
+   * @return the task ranges
+   */
+  public IntegerRanges getProfileTaskRange(boolean isMap) {
+    return conf.getProfileTaskRange(isMap);
+  }
+
+  /**
+   * Get the reported username for this job.
+   * 
+   * @return the username
+   */
+  public String getUser() {
+    return conf.getUser();
+  }
+
+  public Credentials getCredentials() {
+    return credentials;
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapreduce/task/MapContextImpl.java b/src/mapred/org/apache/hadoop/mapreduce/task/MapContextImpl.java
new file mode 100644
index 0000000..59a544a
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/task/MapContextImpl.java
@@ -0,0 +1,80 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.task;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.MapContext;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+
+/**
+ * The context that is given to the {@link Mapper}.
+ * @param <KEYIN> the key input type to the Mapper
+ * @param <VALUEIN> the value input type to the Mapper
+ * @param <KEYOUT> the key output type from the Mapper
+ * @param <VALUEOUT> the value output type from the Mapper
+ */
+public class MapContextImpl<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
+    extends TaskInputOutputContextImpl<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
+    implements MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+  private RecordReader<KEYIN,VALUEIN> reader;
+  private InputSplit split;
+
+  public MapContextImpl(Configuration conf, TaskAttemptID taskid,
+                        RecordReader<KEYIN,VALUEIN> reader,
+                        RecordWriter<KEYOUT,VALUEOUT> writer,
+                        OutputCommitter committer,
+                        StatusReporter reporter,
+                        InputSplit split) {
+    super(conf, taskid, writer, committer, reporter);
+    this.reader = reader;
+    this.split = split;
+  }
+
+  /**
+   * Get the input split for this map.
+   */
+  public InputSplit getInputSplit() {
+    return split;
+  }
+
+  @Override
+  public KEYIN getCurrentKey() throws IOException, InterruptedException {
+    return reader.getCurrentKey();
+  }
+
+  @Override
+  public VALUEIN getCurrentValue() throws IOException, InterruptedException {
+    return reader.getCurrentValue();
+  }
+
+  @Override
+  public boolean nextKeyValue() throws IOException, InterruptedException {
+    return reader.nextKeyValue();
+  }
+
+}
+     
\ No newline at end of file
diff --git a/src/mapred/org/apache/hadoop/mapreduce/task/ReduceContextImpl.java b/src/mapred/org/apache/hadoop/mapreduce/task/ReduceContextImpl.java
new file mode 100644
index 0000000..0dba950
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/task/ReduceContextImpl.java
@@ -0,0 +1,248 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.task;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.io.serializer.Deserializer;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.hadoop.io.serializer.Serializer;
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.ReduceContext;
+import org.apache.hadoop.mapreduce.StatusReporter;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * The context passed to the {@link Reducer}.
+ * @param <KEYIN> the class of the input keys
+ * @param <VALUEIN> the class of the input values
+ * @param <KEYOUT> the class of the output keys
+ * @param <VALUEOUT> the class of the output values
+ */
+public class ReduceContextImpl<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
+    extends TaskInputOutputContextImpl<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
+    implements ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+  private RawKeyValueIterator input;
+  private Counter inputValueCounter;
+  private Counter inputKeyCounter;
+  private RawComparator<KEYIN> comparator;
+  private KEYIN key;                                  // current key
+  private VALUEIN value;                              // current value
+  private boolean firstValue = false;                 // first value in key
+  private boolean nextKeyIsSame = false;              // more w/ this key
+  private boolean hasMore;                            // more in file
+  protected Progressable reporter;
+  private Deserializer<KEYIN> keyDeserializer;
+  private Deserializer<VALUEIN> valueDeserializer;
+  private DataInputBuffer buffer = new DataInputBuffer();
+  private BytesWritable currentRawKey = new BytesWritable();
+  private ValueIterable iterable = new ValueIterable();
+  private final SerializationFactory serializationFactory;
+  private final Class<KEYIN> keyClass;
+  private final Class<VALUEIN> valueClass;
+  private final Configuration conf;
+  private final TaskAttemptID taskid;
+  private int currentKeyLength = -1;
+  private int currentValueLength = -1;
+  
+  public ReduceContextImpl(Configuration conf, TaskAttemptID taskid,
+                           RawKeyValueIterator input, 
+                           Counter inputKeyCounter,
+                           Counter inputValueCounter,
+                           RecordWriter<KEYOUT,VALUEOUT> output,
+                           OutputCommitter committer,
+                           StatusReporter reporter,
+                           RawComparator<KEYIN> comparator,
+                           Class<KEYIN> keyClass,
+                           Class<VALUEIN> valueClass
+                          ) throws InterruptedException, IOException{
+    super(conf, taskid, output, committer, reporter);
+    this.input = input;
+    this.inputKeyCounter = inputKeyCounter;
+    this.inputValueCounter = inputValueCounter;
+    this.comparator = comparator;
+    this.serializationFactory = new SerializationFactory(conf);
+    this.keyDeserializer = serializationFactory.getDeserializer(keyClass);
+    this.keyDeserializer.open(buffer);
+    this.valueDeserializer = serializationFactory.getDeserializer(valueClass);
+    this.valueDeserializer.open(buffer);
+    hasMore = input.next();
+    this.keyClass = keyClass;
+    this.valueClass = valueClass;
+    this.conf = conf;
+    this.taskid = taskid;
+  }
+
+  /** Start processing next unique key. */
+  public boolean nextKey() throws IOException,InterruptedException {
+    while (hasMore && nextKeyIsSame) {
+      nextKeyValue();
+    }
+    if (hasMore) {
+      if (inputKeyCounter != null) {
+        inputKeyCounter.increment(1);
+      }
+      return nextKeyValue();
+    } else {
+      return false;
+    }
+  }
+
+  /**
+   * Advance to the next key/value pair.
+   */
+  @Override
+  public boolean nextKeyValue() throws IOException, InterruptedException {
+    if (!hasMore) {
+      key = null;
+      value = null;
+      return false;
+    }
+    firstValue = !nextKeyIsSame;
+    DataInputBuffer nextKey = input.getKey();
+    currentRawKey.set(nextKey.getData(), nextKey.getPosition(), 
+                      nextKey.getLength() - nextKey.getPosition());
+    buffer.reset(currentRawKey.getBytes(), 0, currentRawKey.getLength());
+    key = keyDeserializer.deserialize(key);
+    DataInputBuffer nextVal = input.getValue();
+    buffer.reset(nextVal.getData(), nextVal.getPosition(), nextVal.getLength());
+    value = valueDeserializer.deserialize(value);
+
+    currentKeyLength = nextKey.getLength() - nextKey.getPosition();
+    currentValueLength = nextVal.getLength() - nextVal.getPosition();
+
+    hasMore = input.next();
+    if (hasMore) {
+      nextKey = input.getKey();
+      nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0, 
+                                     currentRawKey.getLength(),
+                                     nextKey.getData(),
+                                     nextKey.getPosition(),
+                                     nextKey.getLength() - nextKey.getPosition()
+                                         ) == 0;
+    } else {
+      nextKeyIsSame = false;
+    }
+    inputValueCounter.increment(1);
+    return true;
+  }
+
+  public KEYIN getCurrentKey() {
+    return key;
+  }
+
+  @Override
+  public VALUEIN getCurrentValue() {
+    return value;
+  }
+  
+  protected class ValueIterator implements ReduceContext.ValueIterator<VALUEIN> {
+
+    @Override
+    public boolean hasNext() {
+      return firstValue || nextKeyIsSame;
+    }
+
+    @Override
+    public VALUEIN next() {
+
+      // if this is the first record, we don't need to advance
+      if (firstValue) {
+        firstValue = false;
+        return value;
+      }
+      // if this isn't the first record and the next key is different, they
+      // can't advance it here.
+      if (!nextKeyIsSame) {
+        throw new NoSuchElementException("iterate past last value");
+      }
+      // otherwise, go to the next key/value pair
+      try {
+        nextKeyValue();
+        return value;
+      } catch (IOException ie) {
+        throw new RuntimeException("next value iterator failed", ie);
+      } catch (InterruptedException ie) {
+        // this is bad, but we can't modify the exception list of java.util
+        throw new RuntimeException("next value iterator interrupted", ie);        
+      }
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException("remove not implemented");
+    }
+
+    /**
+     * This method is called to write the record that was most recently
+     * served (before a call to the mark). Since the framework reads one
+     * record in advance, to get this record, we serialize the current key
+     * and value
+     * @param out
+     * @throws IOException
+     */
+    private void writeFirstKeyValueBytes(DataOutputStream out) 
+    throws IOException {
+      assert (getCurrentKey() != null && getCurrentValue() != null);
+      WritableUtils.writeVInt(out, currentKeyLength);
+      WritableUtils.writeVInt(out, currentValueLength);
+      Serializer<KEYIN> keySerializer = 
+        serializationFactory.getSerializer(keyClass);
+      keySerializer.open(out);
+      keySerializer.serialize(getCurrentKey());
+
+      Serializer<VALUEIN> valueSerializer = 
+        serializationFactory.getSerializer(valueClass);
+      valueSerializer.open(out);
+      valueSerializer.serialize(getCurrentValue());
+    }
+  }
+
+  protected class ValueIterable implements Iterable<VALUEIN> {
+    private ValueIterator iterator = new ValueIterator();
+    @Override
+    public Iterator<VALUEIN> iterator() {
+      return iterator;
+    } 
+  }
+  
+  /**
+   * Iterate through the values for the current key, reusing the same value 
+   * object, which is stored in the context.
+   * @return the series of values associated with the current key. All of the 
+   * objects returned directly and indirectly from this method are reused.
+   */
+  public 
+  Iterable<VALUEIN> getValues() throws IOException, InterruptedException {
+    return iterable;
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl.java b/src/mapred/org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl.java
new file mode 100644
index 0000000..3e4c029
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.task;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+
+/**
+ * The context for task attempts.
+ */
+public class TaskAttemptContextImpl extends JobContextImpl 
+    implements TaskAttemptContext {
+  private final TaskAttemptID taskId;
+  private String status = "";
+  
+  public TaskAttemptContextImpl(Configuration conf, 
+                                TaskAttemptID taskId) {
+    super(conf, taskId.getJobID());
+    this.taskId = taskId;
+  }
+
+  /**
+   * Get the unique name for this task attempt.
+   */
+  public TaskAttemptID getTaskAttemptID() {
+    return taskId;
+  }
+
+  /**
+   * Set the current status of the task to the given string.
+   */
+  public void setStatus(String msg) {
+    status = msg;
+  }
+
+  /**
+   * Get the last set status message.
+   * @return the current status message
+   */
+  public String getStatus() {
+    return status;
+  }
+
+  /**
+   * Report progress. The subtypes actually do work in this method.
+   */
+  @Override
+  public void progress() { 
+  }
+    
+}
\ No newline at end of file
diff --git a/src/mapred/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java b/src/mapred/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java
new file mode 100644
index 0000000..33ce0b1
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java
@@ -0,0 +1,102 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.task;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskInputOutputContext;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+
+/**
+ * A context object that allows input and output from the task. It is only
+ * supplied to the {@link Mapper} or {@link Reducer}.
+ * @param <KEYIN> the input key type for the task
+ * @param <VALUEIN> the input value type for the task
+ * @param <KEYOUT> the output key type for the task
+ * @param <VALUEOUT> the output value type for the task
+ */
+public abstract class TaskInputOutputContextImpl<KEYIN,VALUEIN,KEYOUT,VALUEOUT> 
+       extends TaskAttemptContextImpl 
+       implements TaskInputOutputContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+  private RecordWriter<KEYOUT,VALUEOUT> output;
+  private StatusReporter reporter;
+  private OutputCommitter committer;
+
+  public TaskInputOutputContextImpl(Configuration conf, TaskAttemptID taskid,
+                                    RecordWriter<KEYOUT,VALUEOUT> output,
+                                    OutputCommitter committer,
+                                    StatusReporter reporter) {
+    super(conf, taskid);
+    this.output = output;
+    this.reporter = reporter;
+    this.committer = committer;
+  }
+
+  /**
+   * Advance to the next key, value pair, returning null if at end.
+   * @return the key object that was read into, or null if no more
+   */
+  public abstract 
+  boolean nextKeyValue() throws IOException, InterruptedException;
+ 
+  /**
+   * Get the current key.
+   * @return the current key object or null if there isn't one
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  public abstract 
+  KEYIN getCurrentKey() throws IOException, InterruptedException;
+
+  /**
+   * Get the current value.
+   * @return the value object that was read into
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  public abstract VALUEIN getCurrentValue() throws IOException, 
+                                                   InterruptedException;
+
+  /**
+   * Generate an output key/value pair.
+   */
+  public void write(KEYOUT key, VALUEOUT value
+                    ) throws IOException, InterruptedException {
+    output.write(key, value);
+  }
+
+  public Counter getCounter(Enum<?> counterName) {
+    return reporter.getCounter(counterName);
+  }
+
+  public Counter getCounter(String groupName, String counterName) {
+    return reporter.getCounter(groupName, counterName);
+  }
+
+  public OutputCommitter getOutputCommitter() {
+    return committer;
+  }
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestFileOutputCommitter.java b/src/test/org/apache/hadoop/mapred/TestFileOutputCommitter.java
index b90a810..2615d17 100644
--- a/src/test/org/apache/hadoop/mapred/TestFileOutputCommitter.java
+++ b/src/test/org/apache/hadoop/mapred/TestFileOutputCommitter.java
@@ -23,6 +23,8 @@ import junit.framework.TestCase;
 
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.io.*;
+import org.apache.hadoop.mapred.JobContextImpl;
+import org.apache.hadoop.mapred.TaskAttemptContextImpl;
 
 public class TestFileOutputCommitter extends TestCase {
   private static Path outDir = new Path(
@@ -38,8 +40,8 @@ public class TestFileOutputCommitter extends TestCase {
     job.set("mapred.task.id", attempt);
     job.setOutputCommitter(FileOutputCommitter.class);
     FileOutputFormat.setOutputPath(job, outDir);
-    JobContext jContext = new JobContext(job, taskID.getJobID());
-    TaskAttemptContext tContext = new TaskAttemptContext(job, taskID);
+    JobContext jContext = new JobContextImpl(job, taskID.getJobID());
+    TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);
     FileOutputCommitter committer = new FileOutputCommitter();
     FileOutputFormat.setWorkOutputPath(job, 
       committer.getTempTaskOutputPath(tContext));
diff --git a/src/test/org/apache/hadoop/mapred/TestTaskCommit.java b/src/test/org/apache/hadoop/mapred/TestTaskCommit.java
index cd1112d..04db97d 100644
--- a/src/test/org/apache/hadoop/mapred/TestTaskCommit.java
+++ b/src/test/org/apache/hadoop/mapred/TestTaskCommit.java
@@ -26,6 +26,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.ipc.ProtocolSignature;
+import org.apache.hadoop.mapred.JobContextImpl;
 import org.apache.hadoop.mapred.SortedRanges.Range;
 import org.apache.hadoop.mapreduce.TaskType;
 
@@ -210,7 +211,7 @@ public class TestTaskCommit extends HadoopTestCase {
     String dummyAttemptID = "attempt_200707121733_0001_m_000000_0";
     TaskAttemptID attemptID = TaskAttemptID.forName(dummyAttemptID);
     OutputCommitter committer = new CommitterWithoutCleanup();
-    JobContext jContext = new JobContext(job, attemptID.getJobID());
+    JobContext jContext = new JobContextImpl(job, attemptID.getJobID());
     committer.setupJob(jContext);
     
 
diff --git a/src/test/org/apache/hadoop/mapreduce/MapReduceTestUtil.java b/src/test/org/apache/hadoop/mapreduce/MapReduceTestUtil.java
index 4dc663c..9b24661 100644
--- a/src/test/org/apache/hadoop/mapreduce/MapReduceTestUtil.java
+++ b/src/test/org/apache/hadoop/mapreduce/MapReduceTestUtil.java
@@ -50,6 +50,7 @@ import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.Reducer;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -375,7 +376,7 @@ public class MapReduceTestUtil {
   public static TaskAttemptContext createDummyMapTaskAttemptContext(
       Configuration conf) {
     TaskAttemptID tid = new TaskAttemptID("jt", 1, true, 0, 0);
-    return new TaskAttemptContext(conf, tid);    
+    return new TaskAttemptContextImpl(conf, tid);    
   }
 
   public static StatusReporter createDummyReporter() {
diff --git a/src/test/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java b/src/test/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java
index 6814025..bf89286 100644
--- a/src/test/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java
+++ b/src/test/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 
 public class TestCombineFileInputFormat extends TestCase {
 
@@ -171,7 +172,7 @@ public class TestCombineFileInputFormat extends TestCase {
     TaskAttemptID taskId = new TaskAttemptID("jt", 0, true, 0, 0);
     Configuration conf1 = new Configuration();
     conf1.set(DUMMY_KEY, "STATE1");
-    TaskAttemptContext context1 = new TaskAttemptContext(conf1, taskId);
+    TaskAttemptContext context1 = new TaskAttemptContextImpl(conf1, taskId);
 
     // This will create a CombineFileRecordReader that itself contains a
     // DummyRecordReader.
@@ -193,7 +194,7 @@ public class TestCombineFileInputFormat extends TestCase {
     // Switch the active context for the RecordReader...
     Configuration conf2 = new Configuration();
     conf2.set(DUMMY_KEY, "STATE2");
-    TaskAttemptContext context2 = new TaskAttemptContext(conf2, taskId);
+    TaskAttemptContext context2 = new TaskAttemptContextImpl(conf2, taskId);
     rr.initialize(split, context2);
 
     // And verify that the new context is updated into the child record reader.
@@ -207,7 +208,7 @@ public class TestCombineFileInputFormat extends TestCase {
     // called a second time.
     TaskAttemptID taskId = new TaskAttemptID("jt", 0, true, 0, 0);
     Configuration conf = new Configuration();
-    TaskAttemptContext context = new TaskAttemptContext(conf, taskId);
+    TaskAttemptContext context = new TaskAttemptContextImpl(conf, taskId);
 
     // This will create a CombineFileRecordReader that itself contains a
     // DummyRecordReader.
diff --git a/src/test/org/apache/hadoop/mapreduce/lib/input/TestMRKeyValueTextInputFormat.java b/src/test/org/apache/hadoop/mapreduce/lib/input/TestMRKeyValueTextInputFormat.java
index dc505cf..b917fcc 100644
--- a/src/test/org/apache/hadoop/mapreduce/lib/input/TestMRKeyValueTextInputFormat.java
+++ b/src/test/org/apache/hadoop/mapreduce/lib/input/TestMRKeyValueTextInputFormat.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.mapreduce.MapContext;
 import org.apache.hadoop.mapreduce.MapReduceTestUtil;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
 import org.apache.hadoop.util.LineReader;
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -105,7 +106,7 @@ public class TestMRKeyValueTextInputFormat extends TestCase {
           assertEquals("reader class is KeyValueLineRecordReader.", 
             KeyValueLineRecordReader.class, clazz);
           MapContext<Text, Text, Text, Text> mcontext = 
-            new MapContext<Text, Text, Text, Text>(job.getConfiguration(), 
+            new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), 
             context.getTaskAttemptID(), reader, null, null, 
             MapReduceTestUtil.createDummyReporter(), splits.get(j));
           reader.initialize(splits.get(j), mcontext);
@@ -194,7 +195,7 @@ public class TestMRKeyValueTextInputFormat extends TestCase {
     RecordReader<Text, Text> reader = format.createRecordReader(split, 
       MapReduceTestUtil.createDummyMapTaskAttemptContext(conf));
     MapContext<Text, Text, Text, Text> mcontext = 
-      new MapContext<Text, Text, Text, Text>(conf, 
+      new MapContextImpl<Text, Text, Text, Text>(conf, 
       context.getTaskAttemptID(), reader, null, null,
       MapReduceTestUtil.createDummyReporter(), 
       split);
-- 
1.7.0.4

