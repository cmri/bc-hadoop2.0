From 997bef062587e8b0d45aa81b84d39c18c642ceba Mon Sep 17 00:00:00 2001
From: David S. Wang <dsw@cloudera.com>
Date: Wed, 13 Feb 2013 14:18:14 -0800
Subject: [PATCH 0923/1357] Merge -r 1407657:1407658 from trunk to branch-2. Fixes: HADOOP-8860. Split MapReduce and YARN sections in documentation navigation.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-2@1408290 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 07b3f85080186b5bf3c2ce3f4a8552511f554335)
---
 .../src/site/apt/CLIMiniCluster.apt.vm             |   84 +
 .../hadoop-common/src/site/apt/ClusterSetup.apt.vm | 1126 +++++++++++++
 .../src/site/apt/SingleCluster.apt.vm              |  194 +++
 .../hadoop-common/src/site/resources/css/site.css  |   30 +
 .../hadoop-common/src/site/site.xml                |   28 +
 .../src/site/resources/css/site.css                |   30 +
 .../hadoop-hdfs/src/site/apt/Federation.apt.vm     |  342 ++++
 .../site/apt/HDFSHighAvailabilityWithNFS.apt.vm    |  880 ++++++++++
 .../site/apt/HDFSHighAvailabilityWithQJM.apt.vm    |  767 +++++++++
 .../hadoop-hdfs/src/site/apt/WebHDFS.apt.vm        | 1769 ++++++++++++++++++++
 .../hadoop-hdfs/src/site/resources/css/site.css    |   30 +
 hadoop-hdfs-project/hadoop-hdfs/src/site/site.xml  |   28 +
 .../src/site/apt/EncryptedShuffle.apt.vm           |  320 ++++
 .../src/site/resources/css/site.css                |   30 +
 .../hadoop-mapreduce-client-core/src/site/site.xml |   28 +
 hadoop-project/src/site/apt/index.apt.vm           |    6 +-
 hadoop-project/src/site/site.xml                   |   36 +-
 .../src/site/apt/CLIMiniCluster.apt.vm             |   84 -
 .../src/site/apt/ClusterSetup.apt.vm               | 1126 -------------
 .../src/site/apt/EncryptedShuffle.apt.vm           |  320 ----
 .../src/site/apt/Federation.apt.vm                 |  342 ----
 .../site/apt/HDFSHighAvailabilityWithNFS.apt.vm    |  880 ----------
 .../site/apt/HDFSHighAvailabilityWithQJM.apt.vm    |  767 ---------
 .../src/site/apt/SingleCluster.apt.vm              |  194 ---
 .../hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm   | 1769 --------------------
 .../hadoop-yarn-site/src/site/apt/index.apt.vm     |    4 +-
 26 files changed, 5710 insertions(+), 5504 deletions(-)
 create mode 100644 hadoop-common-project/hadoop-common/src/site/apt/CLIMiniCluster.apt.vm
 create mode 100644 hadoop-common-project/hadoop-common/src/site/apt/ClusterSetup.apt.vm
 create mode 100644 hadoop-common-project/hadoop-common/src/site/apt/SingleCluster.apt.vm
 create mode 100644 hadoop-common-project/hadoop-common/src/site/resources/css/site.css
 create mode 100644 hadoop-common-project/hadoop-common/src/site/site.xml
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/resources/css/site.css
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/apt/Federation.apt.vm
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/apt/WebHDFS.apt.vm
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/resources/css/site.css
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/site.xml
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/EncryptedShuffle.apt.vm
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/resources/css/site.css
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/site.xml
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/ClusterSetup.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/EncryptedShuffle.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/Federation.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/SingleCluster.apt.vm
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm

diff --git a/hadoop-common-project/hadoop-common/src/site/apt/CLIMiniCluster.apt.vm b/hadoop-common-project/hadoop-common/src/site/apt/CLIMiniCluster.apt.vm
new file mode 100644
index 0000000..957b994
--- /dev/null
+++ b/hadoop-common-project/hadoop-common/src/site/apt/CLIMiniCluster.apt.vm
@@ -0,0 +1,84 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop MapReduce Next Generation ${project.version} - CLI MiniCluster.
+  ---
+  ---
+  ${maven.build.timestamp}
+
+Hadoop MapReduce Next Generation - CLI MiniCluster.
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+* {Purpose}
+
+  Using the CLI MiniCluster, users can simply start and stop a single-node
+  Hadoop cluster with a single command, and without the need to set any
+  environment variables or manage configuration files. The CLI MiniCluster
+  starts both a <<<YARN>>>/<<<MapReduce>>> & <<<HDFS>>> clusters.
+
+  This is useful for cases where users want to quickly experiment with a real
+  Hadoop cluster or test non-Java programs that rely on significant Hadoop
+  functionality.
+
+* {Hadoop Tarball}
+
+  You should be able to obtain the Hadoop tarball from the release. Also, you
+  can directly create a tarball from the source:
+
++---+
+$ mvn clean install -DskipTests
+$ mvn package -Pdist -Dtar -DskipTests -Dmaven.javadoc.skip
++---+
+  <<NOTE:>> You will need protoc installed of version 2.4.1 or greater.
+
+  The tarball should be available in <<<hadoop-dist/target/>>> directory. 
+
+* {Running the MiniCluster}
+
+  From inside the root directory of the extracted tarball, you can start the CLI
+  MiniCluster using the following command:
+
++---+
+$ bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${project.version}-tests.jar minicluster -rmport RM_PORT -jhsport JHS_PORT
++---+
+
+  In the example command above, <<<RM_PORT>>> and <<<JHS_PORT>>> should be
+  replaced by the user's choice of these port numbers. If not specified, random
+  free ports will be used.
+
+  There are a number of command line arguments that the users can use to control
+  which services to start, and to pass other configuration properties.
+  The available command line arguments:
+
++---+
+$ -D <property=value>    Options to pass into configuration object
+$ -datanodes <arg>       How many datanodes to start (default 1)
+$ -format                Format the DFS (default false)
+$ -help                  Prints option help.
+$ -jhsport <arg>         JobHistoryServer port (default 0--we choose)
+$ -namenode <arg>        URL of the namenode (default is either the DFS
+$                        cluster or a temporary dir)
+$ -nnport <arg>          NameNode port (default 0--we choose)
+$ -nodemanagers <arg>    How many nodemanagers to start (default 1)
+$ -nodfs                 Don't start a mini DFS cluster
+$ -nomr                  Don't start a mini MR cluster
+$ -rmport <arg>          ResourceManager port (default 0--we choose)
+$ -writeConfig <path>    Save configuration to this XML file.
+$ -writeDetails <path>   Write basic information to this JSON file.
++---+
+
+  To display this full list of available arguments, the user can pass the
+  <<<-help>>> argument to the above command.
diff --git a/hadoop-common-project/hadoop-common/src/site/apt/ClusterSetup.apt.vm b/hadoop-common-project/hadoop-common/src/site/apt/ClusterSetup.apt.vm
new file mode 100644
index 0000000..b0b3831
--- /dev/null
+++ b/hadoop-common-project/hadoop-common/src/site/apt/ClusterSetup.apt.vm
@@ -0,0 +1,1126 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop Map Reduce Next Generation-${project.version} - Cluster Setup
+  ---
+  ---
+  ${maven.build.timestamp}
+
+Hadoop MapReduce Next Generation - Cluster Setup
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+* {Purpose}
+
+  This document describes how to install, configure and manage non-trivial 
+  Hadoop clusters ranging from a few nodes to extremely large clusters 
+  with thousands of nodes.
+
+  To play with Hadoop, you may first want to install it on a single 
+  machine (see {{{SingleCluster}Single Node Setup}}).
+  
+* {Prerequisites}
+
+  Download a stable version of Hadoop from Apache mirrors.
+  
+* {Installation}
+
+  Installing a Hadoop cluster typically involves unpacking the software on all 
+  the machines in the cluster or installing RPMs.
+
+  Typically one machine in the cluster is designated as the NameNode and 
+  another machine the as ResourceManager, exclusively. These are the masters. 
+  
+  The rest of the machines in the cluster act as both DataNode and NodeManager. 
+  These are the slaves.
+
+* {Running Hadoop in Non-Secure Mode}
+
+  The following sections describe how to configure a Hadoop cluster.
+
+  * {Configuration Files}
+  
+    Hadoop configuration is driven by two types of important configuration files:
+
+      * Read-only default configuration - <<<core-default.xml>>>, 
+        <<<hdfs-default.xml>>>, <<<yarn-default.xml>>> and 
+        <<<mapred-default.xml>>>.
+        
+      * Site-specific configuration - <<conf/core-site.xml>>, 
+        <<conf/hdfs-site.xml>>, <<conf/yarn-site.xml>> and 
+        <<conf/mapred-site.xml>>.
+
+
+    Additionally, you can control the Hadoop scripts found in the bin/ 
+    directory of the distribution, by setting site-specific values via the 
+    <<conf/hadoop-env.sh>> and <<yarn-env.sh>>.
+
+  * {Site Configuration}
+  
+  To configure the Hadoop cluster you will need to configure the 
+  <<<environment>>> in which the Hadoop daemons execute as well as the 
+  <<<configuration parameters>>> for the Hadoop daemons.
+
+  The Hadoop daemons are NameNode/DataNode and ResourceManager/NodeManager.
+
+
+    * {Configuring Environment of Hadoop Daemons}
+    
+    Administrators should use the <<conf/hadoop-env.sh>> and 
+    <<conf/yarn-env.sh>> script to do site-specific customization of the 
+    Hadoop daemons' process environment.
+
+    At the very least you should specify the <<<JAVA_HOME>>> so that it is 
+    correctly defined on each remote node.
+
+    In most cases you should also specify <<<HADOOP_PID_DIR>>> and 
+    <<<HADOOP_SECURE_DN_PID_DIR>>> to point to directories that can only be
+    written to by the users that are going to run the hadoop daemons.  
+    Otherwise there is the potential for a symlink attack.
+
+    Administrators can configure individual daemons using the configuration 
+    options shown below in the table:
+
+*--------------------------------------+--------------------------------------+
+|| Daemon                              || Environment Variable                |
+*--------------------------------------+--------------------------------------+
+| NameNode                             | HADOOP_NAMENODE_OPTS                 |
+*--------------------------------------+--------------------------------------+
+| DataNode                             | HADOOP_DATANODE_OPTS                 |
+*--------------------------------------+--------------------------------------+
+| Secondary NameNode                   | HADOOP_SECONDARYNAMENODE_OPTS        |
+*--------------------------------------+--------------------------------------+
+| ResourceManager                      | YARN_RESOURCEMANAGER_OPTS            |
+*--------------------------------------+--------------------------------------+
+| NodeManager                          | YARN_NODEMANAGER_OPTS                |
+*--------------------------------------+--------------------------------------+
+| WebAppProxy                          | YARN_PROXYSERVER_OPTS                |
+*--------------------------------------+--------------------------------------+
+| Map Reduce Job History Server        | HADOOP_JOB_HISTORYSERVER_OPTS        |
+*--------------------------------------+--------------------------------------+
+
+
+    For example, To configure Namenode to use parallelGC, the following 
+    statement should be added in hadoop-env.sh :
+     
+----
+    export HADOOP_NAMENODE_OPTS="-XX:+UseParallelGC ${HADOOP_NAMENODE_OPTS}" 
+----
+    
+    Other useful configuration parameters that you can customize include:
+
+      * <<<HADOOP_LOG_DIR>>> / <<<YARN_LOG_DIR>>> - The directory where the 
+        daemons' log files are stored. They are automatically created if they 
+        don't exist.
+        
+      * <<<HADOOP_HEAPSIZE>>> / <<<YARN_HEAPSIZE>>> - The maximum amount of 
+        heapsize to use, in MB e.g. if the varibale is set to 1000 the heap
+        will be set to 1000MB.  This is used to configure the heap 
+        size for the daemon. By default, the value is 1000.  If you want to
+        configure the values separately for each deamon you can use.
+*--------------------------------------+--------------------------------------+
+|| Daemon                              || Environment Variable                |
+*--------------------------------------+--------------------------------------+
+| ResourceManager                      | YARN_RESOURCEMANAGER_HEAPSIZE        |
+*--------------------------------------+--------------------------------------+
+| NodeManager                          | YARN_NODEMANAGER_HEAPSIZE            |
+*--------------------------------------+--------------------------------------+
+| WebAppProxy                          | YARN_PROXYSERVER_HEAPSIZE            |
+*--------------------------------------+--------------------------------------+
+| Map Reduce Job History Server        | HADOOP_JOB_HISTORYSERVER_HEAPSIZE    |
+*--------------------------------------+--------------------------------------+
+ 
+    * {Configuring the Hadoop Daemons in Non-Secure Mode}
+
+      This section deals with important parameters to be specified in 
+      the given configuration files:
+       
+      * <<<conf/core-site.xml>>>
+      
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<fs.defaultFS>>>      | NameNode URI            | <hdfs://host:port/>    |
+*-------------------------+-------------------------+------------------------+
+| <<<io.file.buffer.size>>> | 131072 |  |
+| | | Size of read/write buffer used in SequenceFiles. |
+*-------------------------+-------------------------+------------------------+
+
+      * <<<conf/hdfs-site.xml>>>
+      
+        * Configurations for NameNode:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.name.dir>>> | | | 
+| | Path on the local filesystem where the NameNode stores the namespace | | 
+| | and transactions logs persistently. | | 
+| | | If this is a comma-delimited list of directories then the name table is  |
+| | | replicated in all of the directories, for redundancy. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.hosts>>> / <<<dfs.namenode.hosts.exclude>>> | | |
+| | List of permitted/excluded DataNodes. | |
+| | | If necessary, use these files to control the list of allowable |
+| | | datanodes. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.blocksize>>> | 268435456 | |
+| | | HDFS blocksize of 256MB for large file-systems. |      
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.handler.count>>> | 100 | |
+| | | More NameNode server threads to handle RPCs from large number of |
+| | | DataNodes. |
+*-------------------------+-------------------------+------------------------+
+        
+        * Configurations for DataNode:
+      
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.data.dir>>> | | |
+| | Comma separated list of paths on the local filesystem of a | | 
+| | <<<DataNode>>> where it should store its blocks. | |
+| | | If this is a comma-delimited list of directories, then data will be | 
+| | | stored in all named directories, typically on different devices. |
+*-------------------------+-------------------------+------------------------+
+      
+      * <<<conf/yarn-site.xml>>>
+
+        * Configurations for ResourceManager and NodeManager:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.acl.enable>>> | | |
+| | <<<true>>> / <<<false>>> | |
+| | | Enable ACLs? Defaults to <false>. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.admin.acl>>> | | |
+| | Admin ACL | |
+| | | ACL to set admins on the cluster. |
+| | | ACLs are of for <comma-separated-users><space><comma-separated-groups>. |
+| | | Defaults to special value of <<*>> which means <anyone>. |
+| | | Special value of just <space> means no one has access. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.log-aggregation-enable>>> | | |
+| | <false> | |
+| | | Configuration to enable or disable log aggregation |
+*-------------------------+-------------------------+------------------------+
+
+
+        * Configurations for ResourceManager:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.address>>> | | | 
+| | <<<ResourceManager>>> host:port for clients to submit jobs. | |
+| | | <host:port> |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.scheduler.address>>> | | | 
+| | <<<ResourceManager>>> host:port for ApplicationMasters to talk to | |
+| | Scheduler to obtain resources. | |
+| | | <host:port> |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.resource-tracker.address>>> | | | 
+| | <<<ResourceManager>>> host:port for NodeManagers. | |
+| | | <host:port> |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.admin.address>>> | | | 
+| | <<<ResourceManager>>> host:port for administrative commands. | |
+| | | <host:port> |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.webapp.address>>> | | | 
+| | <<<ResourceManager>>> web-ui host:port. | |
+| | | <host:port> |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.scheduler.class>>> | | |
+| | <<<ResourceManager>>> Scheduler class. | |
+| | | <<<CapacityScheduler>>> (recommended) or <<<FifoScheduler>>> |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.scheduler.minimum-allocation-mb>>> | | |
+| | Minimum limit of memory to allocate to each container request at the <<<Resource Manager>>>. | |
+| | | In MBs |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.scheduler.maximum-allocation-mb>>> | | |
+| | Maximum limit of memory to allocate to each container request at the <<<Resource Manager>>>. | |
+| | | In MBs |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.nodes.include-path>>> / | | | 
+| <<<yarn.resourcemanager.nodes.exclude-path>>> | | |  
+| | List of permitted/excluded NodeManagers. | |
+| | | If necessary, use these files to control the list of allowable | 
+| | | NodeManagers. |
+*-------------------------+-------------------------+------------------------+
+ 
+        * Configurations for NodeManager:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.resource.memory-mb>>> | | |
+| | Resource i.e. available physical memory, in MB, for given <<<NodeManager>>> | |
+| | | Defines total available resources on the <<<NodeManager>>> to be made |
+| | | available to running containers |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.vmem-pmem-ratio>>> | | |
+| | Maximum ratio by which virtual memory usage of tasks may exceed |
+| | physical memory | |
+| | | The virtual memory usage of each task may exceed its physical memory |
+| | | limit by this ratio. The total amount of virtual memory used by tasks |
+| | | on the NodeManager may exceed its physical memory usage by this ratio. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.local-dirs>>> | | |
+| | Comma-separated list of paths on the local filesystem where | |
+| | intermediate data is written. ||
+| | | Multiple paths help spread disk i/o. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.log-dirs>>> | | |
+| | Comma-separated list of paths on the local filesystem where logs  | |
+| | are written. | |
+| | | Multiple paths help spread disk i/o. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.log.retain-seconds>>> | | |
+| | <10800> | |
+| | | Default time (in seconds) to retain log files on the NodeManager |
+| | | Only applicable if log-aggregation is disabled. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.remote-app-log-dir>>> | | |
+| | </logs> | |
+| | | HDFS directory where the application logs are moved on application |
+| | | completion. Need to set appropriate permissions. |
+| | | Only applicable if log-aggregation is enabled. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.remote-app-log-dir-suffix>>> | | |
+| | <logs> | |
+| | | Suffix appended to the remote log dir. Logs will be aggregated to  |
+| | | $\{yarn.nodemanager.remote-app-log-dir\}/$\{user\}/$\{thisParam\} |
+| | | Only applicable if log-aggregation is enabled. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.aux-services>>> | | |
+| | mapreduce.shuffle  | |
+| | | Shuffle service that needs to be set for Map Reduce applications. |
+*-------------------------+-------------------------+------------------------+
+
+        * Configurations for History Server (Needs to be moved elsewhere):
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.log-aggregation.retain-seconds>>> | | |
+| | <-1> | |
+| | | How long to keep aggregation logs before deleting them. -1 disables. |
+| | | Be careful, set this too small and you will spam the name node. |
+*-------------------------+-------------------------+------------------------+
+
+
+
+      * <<<conf/mapred-site.xml>>>
+
+        * Configurations for MapReduce Applications:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.framework.name>>> | | |
+| | yarn | |
+| | | Execution framework set to Hadoop YARN. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.map.memory.mb>>> | 1536 | |
+| | | Larger resource limit for maps. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.map.java.opts>>> | -Xmx1024M | |
+| | | Larger heap-size for child jvms of maps. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.reduce.memory.mb>>> | 3072 | |
+| | | Larger resource limit for reduces. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.reduce.java.opts>>> | -Xmx2560M | |
+| | | Larger heap-size for child jvms of reduces. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.task.io.sort.mb>>> | 512 | |
+| | | Higher memory-limit while sorting data for efficiency. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.task.io.sort.factor>>> | 100 | |
+| | | More streams merged at once while sorting files. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.reduce.shuffle.parallelcopies>>> | 50 | |
+| | | Higher number of parallel copies run by reduces to fetch outputs |
+| | | from very large number of maps. |
+*-------------------------+-------------------------+------------------------+
+
+        * Configurations for MapReduce JobHistory Server:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.address>>> | | |
+| | MapReduce JobHistory Server <host:port> | Default port is 10020. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.webapp.address>>> | | |
+| | MapReduce JobHistory Server Web UI <host:port> | Default port is 19888. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.intermediate-done-dir>>> | /mr-history/tmp | |
+|  | | Directory where history files are written by MapReduce jobs. | 
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.done-dir>>> | /mr-history/done| |
+| | | Directory where history files are managed by the MR JobHistory Server. | 
+*-------------------------+-------------------------+------------------------+
+
+      * Hadoop Rack Awareness
+      
+      The HDFS and the YARN components are rack-aware.
+
+      The NameNode and the ResourceManager obtains the rack information of the 
+      slaves in the cluster by invoking an API <resolve> in an administrator 
+      configured module. 
+      
+      The API resolves the DNS name (also IP address) to a rack id. 
+      
+      The site-specific module to use can be configured using the configuration 
+      item <<<topology.node.switch.mapping.impl>>>. The default implementation 
+      of the same runs a script/command configured using 
+      <<<topology.script.file.name>>>. If <<<topology.script.file.name>>> is 
+      not set, the rack id </default-rack> is returned for any passed IP address. 
+
+      * Monitoring Health of NodeManagers
+      
+      Hadoop provides a mechanism by which administrators can configure the 
+      NodeManager to run an administrator supplied script periodically to 
+      determine if a node is healthy or not. 
+      
+      Administrators can determine if the node is in a healthy state by 
+      performing any checks of their choice in the script. If the script 
+      detects the node to be in an unhealthy state, it must print a line to 
+      standard output beginning with the string ERROR. The NodeManager spawns 
+      the script periodically and checks its output. If the script's output 
+      contains the string ERROR, as described above, the node's status is 
+      reported as <<<unhealthy>>> and the node is black-listed by the 
+      ResourceManager. No further tasks will be assigned to this node. 
+      However, the NodeManager continues to run the script, so that if the 
+      node becomes healthy again, it will be removed from the blacklisted nodes
+      on the ResourceManager automatically. The node's health along with the 
+      output of the script, if it is unhealthy, is available to the 
+      administrator in the ResourceManager web interface. The time since the 
+      node was healthy is also displayed on the web interface.
+
+      The following parameters can be used to control the node health 
+      monitoring script in <<<conf/yarn-site.xml>>>.
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.health-checker.script.path>>> | | |
+| | Node health script  | |
+| | | Script to check for node's health status. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.health-checker.script.opts>>> | | |
+| | Node health script options  | |
+| | | Options for script to check for node's health status. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.health-checker.script.interval-ms>>> | | |
+| | Node health script interval  | |
+| | | Time interval for running health script. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.health-checker.script.timeout-ms>>> | | |
+| | Node health script timeout interval  | |
+| | | Timeout for health script execution. |
+*-------------------------+-------------------------+------------------------+
+
+    The health checker script is not supposed to give ERROR if only some of the
+    local disks become bad. NodeManager has the ability to periodically check
+    the health of the local disks (specifically checks nodemanager-local-dirs
+    and nodemanager-log-dirs) and after reaching the threshold of number of
+    bad directories based on the value set for the config property
+    yarn.nodemanager.disk-health-checker.min-healthy-disks, the whole node is
+    marked unhealthy and this info is sent to resource manager also. The boot
+    disk is either raided or a failure in the boot disk is identified by the
+    health checker script.
+
+    * {Slaves file}
+      
+    Typically you choose one machine in the cluster to act as the NameNode and 
+    one machine as to act as the ResourceManager, exclusively. The rest of the 
+    machines act as both a DataNode and NodeManager and are referred to as 
+    <slaves>.
+
+    List all slave hostnames or IP addresses in your <<<conf/slaves>>> file, 
+    one per line.
+
+    * {Logging}
+    
+    Hadoop uses the Apache log4j via the Apache Commons Logging framework for 
+    logging. Edit the <<<conf/log4j.properties>>> file to customize the 
+    Hadoop daemons' logging configuration (log-formats and so on).
+    
+  * {Operating the Hadoop Cluster}
+
+  Once all the necessary configuration is complete, distribute the files to the 
+  <<<HADOOP_CONF_DIR>>> directory on all the machines.
+
+    * Hadoop Startup
+  
+    To start a Hadoop cluster you will need to start both the HDFS and YARN 
+    cluster.
+
+    Format a new distributed filesystem:
+  
+----
+  $ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>
+----
+
+    Start the HDFS with the following command, run on the designated NameNode:
+  
+----
+  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
+----    	  
+
+    Run a script to start DataNodes on all slaves:
+
+----
+  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
+----    	  
+  
+    Start the YARN with the following command, run on the designated 
+    ResourceManager:
+  
+----
+  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager 
+----    	  
+
+    Run a script to start NodeManagers on all slaves:
+
+----
+  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager 
+----    	  
+
+    Start a standalone WebAppProxy server.  If multiple servers
+    are used with load balancing it should be run on each of them:
+
+----
+  $ $YARN_HOME/bin/yarn start proxyserver --config $HADOOP_CONF_DIR  
+----
+
+    Start the MapReduce JobHistory Server with the following command, run on the  
+    designated server:
+  
+----
+  $ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh start historyserver --config $HADOOP_CONF_DIR  
+----    	  
+
+    * Hadoop Shutdown      
+
+    Stop the NameNode with the following command, run on the designated 
+    NameNode:
+  
+----
+  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
+----    	  
+
+    Run a script to stop DataNodes on all slaves:
+
+----
+  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
+----    	  
+  
+    Stop the ResourceManager with the following command, run on the designated 
+    ResourceManager:
+  
+----
+  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager 
+----    	  
+
+    Run a script to stop NodeManagers on all slaves:
+
+----
+  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager 
+----    	  
+
+    Stop the WebAppProxy server. If multiple servers are used with load
+    balancing it should be run on each of them:
+
+----
+  $ $YARN_HOME/bin/yarn stop proxyserver --config $HADOOP_CONF_DIR  
+----
+
+
+    Stop the MapReduce JobHistory Server with the following command, run on the  
+    designated server:
+  
+----
+  $ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh stop historyserver --config $HADOOP_CONF_DIR  
+----    	  
+
+    
+* {Running Hadoop in Secure Mode}
+
+  This section deals with important parameters to be specified in 
+  to run Hadoop in <<secure mode>> with strong, Kerberos-based
+  authentication.
+      
+  * <<<User Accounts for Hadoop Daemons>>>
+      
+  Ensure that HDFS and YARN daemons run as different Unix users, for e.g.
+  <<<hdfs>>> and <<<yarn>>>. Also, ensure that the MapReduce JobHistory
+  server runs as user <<<mapred>>>. 
+      
+  It's recommended to have them share a Unix group, for e.g. <<<hadoop>>>.
+      
+*--------------------------------------+----------------------------------------------------------------------+
+|| User:Group                          || Daemons                                                             |
+*--------------------------------------+----------------------------------------------------------------------+
+| hdfs:hadoop                          | NameNode, Secondary NameNode, Checkpoint Node, Backup Node, DataNode |
+*--------------------------------------+----------------------------------------------------------------------+
+| yarn:hadoop                          | ResourceManager, NodeManager                                         |
+*--------------------------------------+----------------------------------------------------------------------+
+| mapred:hadoop                        | MapReduce JobHistory Server                                          |
+*--------------------------------------+----------------------------------------------------------------------+
+      
+  * <<<Permissions for both HDFS and local fileSystem paths>>>
+     
+  The following table lists various paths on HDFS and local filesystems (on
+  all nodes) and recommended permissions:
+   
+*-------------------+-------------------+------------------+------------------+
+|| Filesystem       || Path             || User:Group      || Permissions     |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<dfs.namenode.name.dir>>> | hdfs:hadoop | drwx------ | 
+*-------------------+-------------------+------------------+------------------+
+| local | <<<dfs.datanode.data.dir>>> | hdfs:hadoop | drwx------ |
+*-------------------+-------------------+------------------+------------------+
+| local | $HADOOP_LOG_DIR | hdfs:hadoop | drwxrwxr-x |
+*-------------------+-------------------+------------------+------------------+
+| local | $YARN_LOG_DIR | yarn:hadoop | drwxrwxr-x |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<yarn.nodemanager.local-dirs>>> | yarn:hadoop | drwxr-xr-x |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<yarn.nodemanager.log-dirs>>> | yarn:hadoop | drwxr-xr-x |
+*-------------------+-------------------+------------------+------------------+
+| local | container-executor | root:hadoop | --Sr-s--- |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<conf/container-executor.cfg>>> | root:hadoop | r-------- |
+*-------------------+-------------------+------------------+------------------+
+| hdfs | / | hdfs:hadoop | drwxr-xr-x |
+*-------------------+-------------------+------------------+------------------+
+| hdfs | /tmp | hdfs:hadoop | drwxrwxrwxt |
+*-------------------+-------------------+------------------+------------------+
+| hdfs | /user | hdfs:hadoop | drwxr-xr-x |
+*-------------------+-------------------+------------------+------------------+
+| hdfs | <<<yarn.nodemanager.remote-app-log-dir>>> | yarn:hadoop | drwxrwxrwxt |
+*-------------------+-------------------+------------------+------------------+
+| hdfs | <<<mapreduce.jobhistory.intermediate-done-dir>>> | mapred:hadoop | |
+| | | | drwxrwxrwxt |      
+*-------------------+-------------------+------------------+------------------+
+| hdfs | <<<mapreduce.jobhistory.done-dir>>> | mapred:hadoop | |
+| | | | drwxr-x--- |      
+*-------------------+-------------------+------------------+------------------+
+
+  * Kerberos Keytab files
+  
+    * HDFS
+    
+    The NameNode keytab file, on the NameNode host, should look like the 
+    following:
+    
+----
+
+$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/nn.service.keytab 
+Keytab name: FILE:/etc/security/keytab/nn.service.keytab
+KVNO Timestamp         Principal
+   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+
+----
+
+    The Secondary NameNode keytab file, on that host, should look like the 
+    following:
+    
+----
+
+$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/sn.service.keytab 
+Keytab name: FILE:/etc/security/keytab/sn.service.keytab
+KVNO Timestamp         Principal
+   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+
+----
+
+    The DataNode keytab file, on each host, should look like the following:
+    
+----
+
+$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/dn.service.keytab 
+Keytab name: FILE:/etc/security/keytab/dn.service.keytab
+KVNO Timestamp         Principal
+   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+
+----
+    
+    * YARN
+    
+    The ResourceManager keytab file, on the ResourceManager host, should look  
+    like the following:
+    
+----
+
+$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/rm.service.keytab 
+Keytab name: FILE:/etc/security/keytab/rm.service.keytab
+KVNO Timestamp         Principal
+   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+
+----
+
+    The NodeManager keytab file, on each host, should look like the following:
+    
+----
+
+$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/nm.service.keytab 
+Keytab name: FILE:/etc/security/keytab/nm.service.keytab
+KVNO Timestamp         Principal
+   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+
+----
+    
+    * MapReduce JobHistory Server
+
+    The MapReduce JobHistory Server keytab file, on that host, should look  
+    like the following:
+    
+----
+
+$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/jhs.service.keytab 
+Keytab name: FILE:/etc/security/keytab/jhs.service.keytab
+KVNO Timestamp         Principal
+   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
+   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
+
+----
+    
+  * Configuration in Secure Mode
+  
+    * <<<conf/core-site.xml>>>
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<hadoop.security.authentication>>> | <kerberos> | <simple> is non-secure. |
+*-------------------------+-------------------------+------------------------+
+| <<<hadoop.security.authorization>>> | <true> | |
+| | | Enable RPC service-level authorization. |
+*-------------------------+-------------------------+------------------------+
+
+    * <<<conf/hdfs-site.xml>>>
+    
+      * Configurations for NameNode:
+    
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.block.access.token.enable>>> | <true> |  |
+| | | Enable HDFS block access tokens for secure operations. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.https.enable>>> | <true> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.https-address>>> | <nn_host_fqdn:50470> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.https.port>>> | <50470> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.keytab.file>>> | </etc/security/keytab/nn.service.keytab> | |
+| | | Kerberos keytab file for the NameNode. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.kerberos.principal>>> | nn/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the NameNode. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.kerberos.https.principal>>> | host/_HOST@REALM.TLD | |
+| | | HTTPS Kerberos principal name for the NameNode. |
+*-------------------------+-------------------------+------------------------+
+
+      * Configurations for Secondary NameNode:
+    
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.secondary.http-address>>> | <c_nn_host_fqdn:50090> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.secondary.https-port>>> | <50470> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.secondary.keytab.file>>> | | | 
+| | </etc/security/keytab/sn.service.keytab> | |
+| | | Kerberos keytab file for the NameNode. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.secondary.kerberos.principal>>> | sn/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the Secondary NameNode. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.namenode.secondary.kerberos.https.principal>>> | | |
+| | host/_HOST@REALM.TLD | |
+| | | HTTPS Kerberos principal name for the Secondary NameNode. |
+*-------------------------+-------------------------+------------------------+
+
+      * Configurations for DataNode:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.data.dir.perm>>> | 700 | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.address>>> | <0.0.0.0:2003> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.https.address>>> | <0.0.0.0:2005> | |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.keytab.file>>> | </etc/security/keytab/dn.service.keytab> | |
+| | | Kerberos keytab file for the DataNode. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.kerberos.principal>>> | dn/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the DataNode. |
+*-------------------------+-------------------------+------------------------+
+| <<<dfs.datanode.kerberos.https.principal>>> | | |
+| | host/_HOST@REALM.TLD | |
+| | | HTTPS Kerberos principal name for the DataNode. |
+*-------------------------+-------------------------+------------------------+
+
+    * <<<conf/yarn-site.xml>>>
+    
+      * WebAppProxy
+
+      The <<<WebAppProxy>>> provides a proxy between the web applications
+      exported by an application and an end user.  If security is enabled
+      it will warn users before accessing a potentially unsafe web application.
+      Authentication and authorization using the proxy is handled just like
+      any other privileged web application.
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.web-proxy.address>>> | | |
+| | <<<WebAppProxy>>> host:port for proxy to AM web apps. | |
+| | | <host:port> if this is the same as <<<yarn.resourcemanager.webapp.address>>>|
+| | | or it is not defined then the <<<ResourceManager>>> will run the proxy|
+| | | otherwise a standalone proxy server will need to be launched.|
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.web-proxy.keytab>>> | | |
+| | </etc/security/keytab/web-app.service.keytab> | |
+| | | Kerberos keytab file for the WebAppProxy. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.web-proxy.principal>>> | wap/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the WebAppProxy. |
+*-------------------------+-------------------------+------------------------+
+
+      * LinuxContainerExecutor
+      
+      A <<<ContainerExecutor>>> used by YARN framework which define how any
+      <container> launched and controlled. 
+      
+      The following are the available in Hadoop YARN:
+
+*--------------------------------------+--------------------------------------+
+|| ContainerExecutor                   || Description                         |
+*--------------------------------------+--------------------------------------+
+| <<<DefaultContainerExecutor>>>             | |
+| | The default executor which YARN uses to manage container execution. |
+| | The container process has the same Unix user as the NodeManager.  |
+*--------------------------------------+--------------------------------------+
+| <<<LinuxContainerExecutor>>>               | |
+| | Supported only on GNU/Linux, this executor runs the containers as the | 
+| | user who submitted the application. It requires all user accounts to be |
+| | created on the cluster nodes where the containers are launched. It uses |
+| | a <setuid> executable that is included in the Hadoop distribution. |
+| | The NodeManager uses this executable to launch and kill containers. |
+| | The setuid executable switches to the user who has submitted the |
+| | application and launches or kills the containers. For maximum security, |
+| | this executor sets up restricted permissions and user/group ownership of |
+| | local files and directories used by the containers such as the shared |
+| | objects, jars, intermediate files, log files etc. Particularly note that, |
+| | because of this, except the application owner and NodeManager, no other |
+| | user can access any of the local files/directories including those |
+| | localized as part of the distributed cache. |
+*--------------------------------------+--------------------------------------+
+
+      To build the LinuxContainerExecutor executable run:
+        
+----
+ $ mvn package -Dcontainer-executor.conf.dir=/etc/hadoop/
+----
+        
+      The path passed in <<<-Dcontainer-executor.conf.dir>>> should be the 
+      path on the cluster nodes where a configuration file for the setuid 
+      executable should be located. The executable should be installed in
+      $YARN_HOME/bin.
+
+      The executable must have specific permissions: 6050 or --Sr-s--- 
+      permissions user-owned by <root> (super-user) and group-owned by a 
+      special group (e.g. <<<hadoop>>>) of which the NodeManager Unix user is 
+      the group member and no ordinary application user is. If any application 
+      user belongs to this special group, security will be compromised. This 
+      special group name should be specified for the configuration property 
+      <<<yarn.nodemanager.linux-container-executor.group>>> in both 
+      <<<conf/yarn-site.xml>>> and <<<conf/container-executor.cfg>>>. 
+      
+      For example, let's say that the NodeManager is run as user <yarn> who is 
+      part of the groups users and <hadoop>, any of them being the primary group.
+      Let also be that <users> has both <yarn> and another user 
+      (application submitter) <alice> as its members, and <alice> does not 
+      belong to <hadoop>. Going by the above description, the setuid/setgid 
+      executable should be set 6050 or --Sr-s--- with user-owner as <yarn> and 
+      group-owner as <hadoop> which has <yarn> as its member (and not <users> 
+      which has <alice> also as its member besides <yarn>).
+
+      The LinuxTaskController requires that paths including and leading up to 
+      the directories specified in <<<yarn.nodemanager.local-dirs>>> and 
+      <<<yarn.nodemanager.log-dirs>>> to be set 755 permissions as described 
+      above in the table on permissions on directories.
+
+        * <<<conf/container-executor.cfg>>>
+        
+        The executable requires a configuration file called 
+        <<<container-executor.cfg>>> to be present in the configuration 
+        directory passed to the mvn target mentioned above. 
+
+        The configuration file must be owned by the user running NodeManager 
+        (user <<<yarn>>> in the above example), group-owned by anyone and 
+        should have the permissions 0400 or r--------.
+
+        The executable requires following configuration items to be present 
+        in the <<<conf/container-executor.cfg>>> file. The items should be 
+        mentioned as simple key=value pairs, one per-line:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.linux-container-executor.group>>> | <hadoop> | |
+| | | Unix group of the NodeManager. The group owner of the |
+| | |<container-executor> binary should be this group. Should be same as the |
+| | | value with which the NodeManager is configured. This configuration is |
+| | | required for validating the secure access of the <container-executor> |
+| | | binary. |        
+*-------------------------+-------------------------+------------------------+
+| <<<banned.users>>> | hfds,yarn,mapred,bin | Banned users. |
+*-------------------------+-------------------------+------------------------+
+| <<<min.user.id>>> | 1000 | Prevent other super-users. |      
+*-------------------------+-------------------------+------------------------+
+
+      To re-cap, here are the local file-ssytem permissions required for the 
+      various paths related to the <<<LinuxContainerExecutor>>>:
+      
+*-------------------+-------------------+------------------+------------------+
+|| Filesystem       || Path             || User:Group      || Permissions     |
+*-------------------+-------------------+------------------+------------------+
+| local | container-executor | root:hadoop | --Sr-s--- |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<conf/container-executor.cfg>>> | root:hadoop | r-------- |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<yarn.nodemanager.local-dirs>>> | yarn:hadoop | drwxr-xr-x |
+*-------------------+-------------------+------------------+------------------+
+| local | <<<yarn.nodemanager.log-dirs>>> | yarn:hadoop | drwxr-xr-x |
+*-------------------+-------------------+------------------+------------------+
+      
+      * Configurations for ResourceManager:
+    
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.keytab>>> | | |
+| | </etc/security/keytab/rm.service.keytab> | |
+| | | Kerberos keytab file for the ResourceManager. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.resourcemanager.principal>>> | rm/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the ResourceManager. |
+*-------------------------+-------------------------+------------------------+
+      
+      * Configurations for NodeManager:
+      
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.keytab>>> | </etc/security/keytab/nm.service.keytab> | |
+| | | Kerberos keytab file for the NodeManager. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.principal>>> | nm/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the NodeManager. |
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.container-executor.class>>> | | |
+| | <<<org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor>>> | 
+| | | Use LinuxContainerExecutor. | 
+*-------------------------+-------------------------+------------------------+
+| <<<yarn.nodemanager.linux-container-executor.group>>> | <hadoop> | |
+| | | Unix group of the NodeManager. |
+*-------------------------+-------------------------+------------------------+
+
+    * <<<conf/mapred-site.xml>>>
+    
+      * Configurations for MapReduce JobHistory Server:
+
+*-------------------------+-------------------------+------------------------+
+|| Parameter              || Value                  || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.address>>> | | |
+| | MapReduce JobHistory Server <host:port> | Default port is 10020. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.keytab>>> | |
+| | </etc/security/keytab/jhs.service.keytab> | |
+| | | Kerberos keytab file for the MapReduce JobHistory Server. |
+*-------------------------+-------------------------+------------------------+
+| <<<mapreduce.jobhistory.principal>>> | mapred/_HOST@REALM.TLD | |
+| | | Kerberos principal name for the MapReduce JobHistory Server. |
+*-------------------------+-------------------------+------------------------+
+
+        
+  * {Operating the Hadoop Cluster}
+
+  Once all the necessary configuration is complete, distribute the files to the 
+  <<<HADOOP_CONF_DIR>>> directory on all the machines.
+
+  This section also describes the various Unix users who should be starting the
+  various components and uses the same Unix accounts and groups used previously:
+  
+    * Hadoop Startup
+  
+    To start a Hadoop cluster you will need to start both the HDFS and YARN 
+    cluster.
+
+    Format a new distributed filesystem as <hdfs>:
+  
+----
+[hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>
+----
+
+    Start the HDFS with the following command, run on the designated NameNode
+    as <hdfs>:
+  
+----
+[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
+----    	  
+
+    Run a script to start DataNodes on all slaves as <root> with a special
+    environment variable <<<HADOOP_SECURE_DN_USER>>> set to <hdfs>:
+
+----
+[root]$ HADOOP_SECURE_DN_USER=hdfs $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
+----    	  
+  
+    Start the YARN with the following command, run on the designated 
+    ResourceManager as <yarn>:
+  
+----
+[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager 
+----    	  
+
+    Run a script to start NodeManagers on all slaves as <yarn>:
+
+----
+[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager 
+----    	  
+
+    Start a standalone WebAppProxy server. Run on the WebAppProxy 
+    server as <yarn>.  If multiple servers are used with load balancing
+    it should be run on each of them:
+
+----
+[yarn]$ $YARN_HOME/bin/yarn start proxyserver --config $HADOOP_CONF_DIR  
+----    	  
+
+    Start the MapReduce JobHistory Server with the following command, run on the  
+    designated server as <mapred>:
+  
+----
+[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh start historyserver --config $HADOOP_CONF_DIR  
+----    	  
+
+    * Hadoop Shutdown      
+
+    Stop the NameNode with the following command, run on the designated NameNode
+    as <hdfs>:
+  
+----
+[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
+----    	  
+
+    Run a script to stop DataNodes on all slaves as <root>:
+
+----
+[root]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
+----    	  
+  
+    Stop the ResourceManager with the following command, run on the designated 
+    ResourceManager as <yarn>:
+  
+----
+[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager 
+----    	  
+
+    Run a script to stop NodeManagers on all slaves as <yarn>:
+
+----
+[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager 
+----    	  
+
+    Stop the WebAppProxy server. Run on the WebAppProxy  server as
+    <yarn>.  If multiple servers are used with load balancing it
+    should be run on each of them:
+
+----
+[yarn]$ $YARN_HOME/bin/yarn stop proxyserver --config $HADOOP_CONF_DIR  
+----
+
+    Stop the MapReduce JobHistory Server with the following command, run on the  
+    designated server as <mapred>:
+
+----
+[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh stop historyserver --config $HADOOP_CONF_DIR  
+----    	  
+    
+* {Web Interfaces}      
+
+    Once the Hadoop cluster is up and running check the web-ui of the 
+    components as described below:
+    
+*-------------------------+-------------------------+------------------------+
+|| Daemon                 || Web Interface          || Notes                 |
+*-------------------------+-------------------------+------------------------+
+| NameNode | http://<nn_host:port>/ | Default HTTP port is 50070. |
+*-------------------------+-------------------------+------------------------+
+| ResourceManager | http://<rm_host:port>/ | Default HTTP port is 8088. |
+*-------------------------+-------------------------+------------------------+
+| MapReduce JobHistory Server | http://<jhs_host:port>/ | |
+| | | Default HTTP port is 19888. |
+*-------------------------+-------------------------+------------------------+
+    
+    
diff --git a/hadoop-common-project/hadoop-common/src/site/apt/SingleCluster.apt.vm b/hadoop-common-project/hadoop-common/src/site/apt/SingleCluster.apt.vm
new file mode 100644
index 0000000..7ea4285
--- /dev/null
+++ b/hadoop-common-project/hadoop-common/src/site/apt/SingleCluster.apt.vm
@@ -0,0 +1,194 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop MapReduce Next Generation ${project.version} - Setting up a Single Node Cluster.
+  ---
+  ---
+  ${maven.build.timestamp}
+
+Hadoop MapReduce Next Generation - Setting up a Single Node Cluster.
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+* Mapreduce Tarball
+
+  You should be able to obtain the MapReduce tarball from the release.
+  If not, you should be able to create a tarball from the source.
+
++---+
+$ mvn clean install -DskipTests
+$ cd hadoop-mapreduce-project
+$ mvn clean install assembly:assembly -Pnative
++---+
+  <<NOTE:>> You will need protoc installed of version 2.4.1 or greater.
+
+  To ignore the native builds in mapreduce you can omit the <<<-Pnative>>> argument
+  for maven. The tarball should be available in <<<target/>>> directory. 
+
+  
+* Setting up the environment.
+
+  Assuming you have installed hadoop-common/hadoop-hdfs and exported
+  <<$HADOOP_COMMON_HOME>>/<<$HADOOP_HDFS_HOME>>, untar hadoop mapreduce 
+  tarball and set environment variable <<$HADOOP_MAPRED_HOME>> to the 
+  untarred directory. Set <<$YARN_HOME>> the same as <<$HADOOP_MAPRED_HOME>>. 
+ 
+  <<NOTE:>> The following instructions assume you have hdfs running.
+
+* Setting up Configuration.
+
+  To start the ResourceManager and NodeManager, you will have to update the configs.
+  Assuming your $HADOOP_CONF_DIR is the configuration directory and has the installed
+  configs for HDFS and <<<core-site.xml>>>. There are 2 config files you will have to setup
+  <<<mapred-site.xml>>> and <<<yarn-site.xml>>>.
+
+** Setting up <<<mapred-site.xml>>>
+
+  Add the following configs to your <<<mapred-site.xml>>>.
+
++---+
+  <property>
+    <name>mapreduce.cluster.temp.dir</name>
+    <value></value>
+    <description>No description</description>
+    <final>true</final>
+  </property>
+
+  <property>
+    <name>mapreduce.cluster.local.dir</name>
+    <value></value>
+    <description>No description</description>
+    <final>true</final>
+  </property>
++---+
+
+** Setting up <<<yarn-site.xml>>>
+
+Add the following configs to your <<<yarn-site.xml>>>
+
++---+
+  <property>
+    <name>yarn.resourcemanager.resource-tracker.address</name>
+    <value>host:port</value>
+    <description>host is the hostname of the resource manager and 
+    port is the port on which the NodeManagers contact the Resource Manager.
+    </description>
+  </property>
+
+  <property>
+    <name>yarn.resourcemanager.scheduler.address</name>
+    <value>host:port</value>
+    <description>host is the hostname of the resourcemanager and port is the port
+    on which the Applications in the cluster talk to the Resource Manager.
+    </description>
+  </property>
+
+  <property>
+    <name>yarn.resourcemanager.scheduler.class</name>
+    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler</value>
+    <description>In case you do not want to use the default scheduler</description>
+  </property>
+
+  <property>
+    <name>yarn.resourcemanager.address</name>
+    <value>host:port</value>
+    <description>the host is the hostname of the ResourceManager and the port is the port on
+    which the clients can talk to the Resource Manager. </description>
+  </property>
+
+  <property>
+    <name>yarn.nodemanager.local-dirs</name>
+    <value></value>
+    <description>the local directories used by the nodemanager</description>
+  </property>
+
+  <property>
+    <name>yarn.nodemanager.address</name>
+    <value>0.0.0.0:port</value>
+    <description>the nodemanagers bind to this port</description>
+  </property>  
+
+  <property>
+    <name>yarn.nodemanager.resource.memory-mb</name>
+    <value>10240</value>
+    <description>the amount of memory on the NodeManager in GB</description>
+  </property>
+ 
+  <property>
+    <name>yarn.nodemanager.remote-app-log-dir</name>
+    <value>/app-logs</value>
+    <description>directory on hdfs where the application logs are moved to </description>
+  </property>
+
+   <property>
+    <name>yarn.nodemanager.log-dirs</name>
+    <value></value>
+    <description>the directories used by Nodemanagers as log directories</description>
+  </property>
+
+  <property>
+    <name>yarn.nodemanager.aux-services</name>
+    <value>mapreduce.shuffle</value>
+    <description>shuffle service that needs to be set for Map Reduce to run </description>
+  </property>
++---+
+
+* Setting up <<<capacity-scheduler.xml>>>
+
+   Make sure you populate the root queues in <<<capacity-scheduler.xml>>>.
+
++---+
+  <property>
+    <name>yarn.scheduler.capacity.root.queues</name>
+    <value>unfunded,default</value>
+  </property>
+  
+  <property>
+    <name>yarn.scheduler.capacity.root.capacity</name>
+    <value>100</value>
+  </property>
+  
+  <property>
+    <name>yarn.scheduler.capacity.root.unfunded.capacity</name>
+    <value>50</value>
+  </property>
+  
+  <property>
+    <name>yarn.scheduler.capacity.root.default.capacity</name>
+    <value>50</value>
+  </property>
++---+
+
+* Running daemons.
+
+  Assuming that the environment variables <<$HADOOP_COMMON_HOME>>, <<$HADOOP_HDFS_HOME>>, <<$HADOO_MAPRED_HOME>>,
+  <<$YARN_HOME>>, <<$JAVA_HOME>> and <<$HADOOP_CONF_DIR>> have been set appropriately.
+  Set $<<$YARN_CONF_DIR>> the same as $<<HADOOP_CONF_DIR>>
+ 
+  Run ResourceManager and NodeManager as:
+  
++---+
+$ cd $HADOOP_MAPRED_HOME
+$ sbin/yarn-daemon.sh start resourcemanager
+$ sbin/yarn-daemon.sh start nodemanager
++---+
+
+  You should be up and running. You can run randomwriter as:
+
++---+
+$ $HADOOP_COMMON_HOME/bin/hadoop jar hadoop-examples.jar randomwriter out
++---+
+
+Good luck.
diff --git a/hadoop-common-project/hadoop-common/src/site/resources/css/site.css b/hadoop-common-project/hadoop-common/src/site/resources/css/site.css
new file mode 100644
index 0000000..f830baa
--- /dev/null
+++ b/hadoop-common-project/hadoop-common/src/site/resources/css/site.css
@@ -0,0 +1,30 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the "License"); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+#banner {
+  height: 93px;
+  background: none;
+}
+
+#bannerLeft img {
+  margin-left: 30px;
+  margin-top: 10px;
+}
+
+#bannerRight img {
+  margin: 17px;
+}
+
diff --git a/hadoop-common-project/hadoop-common/src/site/site.xml b/hadoop-common-project/hadoop-common/src/site/site.xml
new file mode 100644
index 0000000..1296cea
--- /dev/null
+++ b/hadoop-common-project/hadoop-common/src/site/site.xml
@@ -0,0 +1,28 @@
+<!--
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+
+   http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License. See accompanying LICENSE file.
+-->
+<project name="Apache Hadoop ${project.version}">
+
+  <skin>
+    <groupId>org.apache.maven.skins</groupId>
+    <artifactId>maven-stylus-skin</artifactId>
+    <version>1.2</version>
+  </skin>
+
+  <body>
+    <links>
+      <item name="Apache Hadoop" href="http://hadoop.apache.org/"/>
+    </links>
+  </body>
+
+</project>
diff --git a/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/resources/css/site.css b/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/resources/css/site.css
new file mode 100644
index 0000000..f830baa
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/resources/css/site.css
@@ -0,0 +1,30 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the "License"); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+#banner {
+  height: 93px;
+  background: none;
+}
+
+#bannerLeft img {
+  margin-left: 30px;
+  margin-top: 10px;
+}
+
+#bannerRight img {
+  margin: 17px;
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/Federation.apt.vm b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/Federation.apt.vm
new file mode 100644
index 0000000..c7c8770
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/Federation.apt.vm
@@ -0,0 +1,342 @@
+
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop Distributed File System-${project.version} - Federation
+  ---
+  ---
+  ${maven.build.timestamp}
+
+HDFS Federation
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+  This guide provides an overview of the HDFS Federation feature and
+  how to configure and manage the federated cluster.
+
+* {Background}
+
+[./federation-background.gif] HDFS Layers
+
+  HDFS has two main layers:
+
+  * <<Namespace>>
+
+    * Consists of directories, files and blocks
+
+    * It supports all the namespace related file system operations such as 
+      create, delete, modify and list files and directories.
+
+  * <<Block Storage Service>> has two parts
+
+    * Block Management (which is done in Namenode)
+
+      * Provides datanode cluster membership by handling registrations, and 
+        periodic heart beats.
+
+      * Processes block reports and maintains location of blocks.
+
+      * Supports block related operations such as create, delete, modify and 
+        get block location.
+
+      * Manages replica placement and replication of a block for under 
+        replicated blocks and deletes blocks that are over replicated.
+
+    * Storage - is provided by datanodes by storing blocks on the local file 
+      system and allows read/write access.
+
+  The prior HDFS architecture allows only a single namespace for the 
+  entire cluster. A single Namenode manages this namespace. HDFS 
+  Federation addresses limitation of the prior architecture by adding 
+  support multiple Namenodes/namespaces to HDFS file system.
+    
+* {Multiple Namenodes/Namespaces}
+
+  In order to scale the name service horizontally, federation uses multiple 
+  independent Namenodes/namespaces. The Namenodes are federated, that is, the 
+  Namenodes are independent and don’t require coordination with each other. 
+  The datanodes are used as common storage for blocks by all the Namenodes. 
+  Each datanode registers with all the Namenodes in the cluster. Datanodes 
+  send periodic heartbeats and block reports and handles commands from the 
+  Namenodes.
+
+[./federation.gif] HDFS Federation Architecture
+
+
+  <<Block Pool>>
+
+  A Block Pool is a set of blocks that belong to a single namespace. 
+  Datanodes store blocks for all the block pools in the cluster.
+  It is managed independently of other block pools. This allows a namespace 
+  to generate Block IDs for new blocks without the need for coordination 
+  with the other namespaces. The failure of a Namenode does not prevent 
+  the datanode from serving other Namenodes in the cluster.
+
+  A Namespace and its block pool together are called Namespace Volume. 
+  It is a self-contained unit of management. When a Namenode/namespace 
+  is deleted, the corresponding block pool at the datanodes is deleted.
+  Each namespace volume is upgraded as a unit, during cluster upgrade.
+
+  <<ClusterID>>
+
+  A new identifier <<ClusterID>> is added to identify all the nodes in 
+  the cluster.  When a Namenode is formatted, this identifier is provided 
+  or auto generated. This ID should be used for formatting the other 
+  Namenodes into the cluster.
+
+** Key Benefits
+
+  * Namespace Scalability - HDFS cluster storage scales horizontally but 
+    the namespace does not. Large deployments or deployments using lot 
+    of small files benefit from scaling the namespace by adding more 
+    Namenodes to the cluster
+
+  * Performance - File system operation throughput is limited by a single
+    Namenode in the prior architecture. Adding more Namenodes to the cluster
+    scales the file system read/write operations throughput.
+
+  * Isolation - A single Namenode offers no isolation in multi user 
+    environment. An experimental application can overload the Namenode 
+    and slow down production critical applications. With multiple Namenodes, 
+    different categories of applications and users can be isolated to 
+    different namespaces.
+
+* {Federation Configuration}
+
+  Federation configuration is <<backward compatible>> and allows existing 
+  single Namenode configuration to work without any change. The new 
+  configuration is designed such that all the nodes in the cluster have 
+  same configuration without the need for deploying different configuration 
+  based on the type of the node in the cluster.
+
+  A new abstraction called <<<NameServiceID>>> is added with
+  federation. The Namenode and its corresponding secondary/backup/checkpointer
+  nodes belong to this. To support single configuration file, the Namenode and
+  secondary/backup/checkpointer configuration parameters are suffixed with
+  <<<NameServiceID>>> and are added to the same configuration file.
+
+
+** Configuration:
+
+  <<Step 1>>: Add the following parameters to your configuration:
+  <<<dfs.nameservices>>>: Configure with list of comma separated 
+  NameServiceIDs. This will be used by Datanodes to determine all the 
+  Namenodes in the cluster.
+  
+  <<Step 2>>: For each Namenode and Secondary Namenode/BackupNode/Checkpointer 
+  add the following configuration suffixed with the corresponding 
+  <<<NameServiceID>>> into the common configuration file.
+
+*---------------------+--------------------------------------------+
+|| Daemon             || Configuration Parameter                   |
+*---------------------+--------------------------------------------+
+| Namenode            | <<<dfs.namenode.rpc-address>>>             |
+|                     | <<<dfs.namenode.servicerpc-address>>>      |
+|                     | <<<dfs.namenode.http-address>>>            |
+|                     | <<<dfs.namenode.https-address>>>           |
+|                     | <<<dfs.namenode.keytab.file>>>             |
+|                     | <<<dfs.namenode.name.dir>>>                |
+|                     | <<<dfs.namenode.edits.dir>>>               |
+|                     | <<<dfs.namenode.checkpoint.dir>>>          |
+|                     | <<<dfs.namenode.checkpoint.edits.dir>>>    |
+*---------------------+--------------------------------------------+
+| Secondary Namenode  | <<<dfs.namenode.secondary.http-address>>>  |
+|                     | <<<dfs.secondary.namenode.keytab.file>>>   |
+*---------------------+--------------------------------------------+
+| BackupNode          | <<<dfs.namenode.backup.address>>>          |
+|                     | <<<dfs.secondary.namenode.keytab.file>>>   |
+*---------------------+--------------------------------------------+
+    
+  Here is an example configuration with two namenodes:
+
+----
+<configuration>
+  <property>
+    <name>dfs.nameservices</name>
+    <value>ns1,ns2</value>
+  </property>
+  <property>
+    <name>dfs.namenode.rpc-address.ns1</name>
+    <value>nn-host1:rpc-port</value>
+  </property>
+  <property>
+    <name>dfs.namenode.http-address.ns1</name>
+    <value>nn-host1:http-port</value>
+  </property>
+  <property>
+    <name>dfs.namenode.secondaryhttp-address.ns1</name>
+    <value>snn-host1:http-port</value>
+  </property>
+  <property>
+    <name>dfs.namenode.rpc-address.ns2</name>
+    <value>nn-host2:rpc-port</value>
+  </property>
+  <property>
+    <name>dfs.namenode.http-address.ns2</name>
+    <value>nn-host2:http-port</value>
+  </property>
+  <property>
+    <name>dfs.namenode.secondaryhttp-address.ns2</name>
+    <value>snn-host2:http-port</value>
+  </property>
+
+  .... Other common configuration ...
+</configuration>
+----
+
+** Formatting Namenodes
+
+  <<Step 1>>: Format a namenode using the following command:
+  
+----
+> $HADOOP_PREFIX_HOME/bin/hdfs namenode -format [-clusterId <cluster_id>]
+----
+  Choose a unique cluster_id, which will not conflict other clusters in 
+  your environment. If it is not provided, then a unique ClusterID is 
+  auto generated.
+
+  <<Step 2>>: Format additional namenode using the following command:
+
+----
+> $HADOOP_PREFIX_HOME/bin/hdfs namenode -format -clusterId <cluster_id>
+----
+  Note that the cluster_id in step 2 must be same as that of the 
+  cluster_id in step 1. If they are different, the additional Namenodes 
+  will not be part of the federated cluster.
+
+** Upgrading from older release to 0.23 and configuring federation
+
+  Older releases supported a single Namenode. Here are the steps enable 
+  federation:
+
+  Step 1: Upgrade the cluster to newer release. During upgrade you can 
+  provide a ClusterID as follows:
+
+----
+> $HADOOP_PREFIX_HOME/bin/hdfs start namenode --config $HADOOP_CONF_DIR  -upgrade -clusterId <cluster_ID>
+----
+  If ClusterID is not provided, it is auto generated.
+
+** Adding a new Namenode to an existing HDFS cluster
+
+  Follow the following steps:
+
+  * Add configuration parameter <<<dfs.nameservices>>> to the configuration.
+
+  * Update the configuration with NameServiceID suffix. Configuration 
+    key names have changed post release 0.20. You must use new configuration 
+    parameter names, for federation.
+
+  * Add new Namenode related config to the configuration files.
+
+  * Propagate the configuration file to the all the nodes in the cluster.
+
+  * Start the new Namenode, Secondary/Backup.
+
+  * Refresh the datanodes to pickup the newly added Namenode by running 
+    the following command:
+
+----
+> $HADOOP_PREFIX_HOME/bin/hdfs dfadmin -refreshNameNode <datanode_host_name>:<datanode_rpc_port>
+----
+
+  * The above command must be run against all the datanodes in the cluster.
+
+* {Managing the cluster}
+
+**  Starting and stopping cluster
+
+  To start the cluster run the following command:
+
+----
+> $HADOOP_PREFIX_HOME/bin/start-dfs.sh
+----
+
+  To stop the cluster run the following command:
+
+----
+> $HADOOP_PREFIX_HOME/bin/stop-dfs.sh
+----
+
+  These commands can be run from any node where the HDFS configuration is 
+  available.  The command uses configuration to determine the Namenodes 
+  in the cluster and starts the Namenode process on those nodes. The 
+  datanodes are started on nodes specified in the <<<slaves>>> file. The 
+  script can be used as reference for building your own scripts for 
+  starting and stopping the cluster.
+
+**  Balancer
+
+  Balancer has been changed to work with multiple Namenodes in the cluster to 
+  balance the cluster. Balancer can be run using the command:
+
+----
+"$HADOOP_PREFIX"/bin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script "$bin"/hdfs start balancer [-policy <policy>]
+----
+
+  Policy could be:
+
+  * <<<node>>> - this is the <default> policy. This balances the storage at 
+    the datanode level. This is similar to balancing policy from prior releases.
+
+  * <<<blockpool>>> - this balances the storage at the block pool level. 
+    Balancing at block pool level balances storage at the datanode level also.
+
+  Note that Balander only balances the data and does not balance the namespace.
+
+** Decommissioning
+
+  Decommissioning is similar to prior releases. The nodes that need to be 
+  decomissioned are added to the exclude file at all the Namenode. Each 
+  Namenode decommissions its Block Pool. When all the Namenodes finish 
+  decommissioning a datanode, the datanode is considered to be decommissioned.
+
+  <<Step 1>>: To distributed an exclude file to all the Namenodes, use the 
+  following command:
+
+----
+"$HADOOP_PREFIX"/bin/distributed-exclude.sh <exclude_file>
+----
+
+  <<Step 2>>: Refresh all the Namenodes to pick up the new exclude file.
+
+----
+"$HADOOP_PREFIX"/bin/refresh-namenodes.sh
+----
+ 
+  The above command uses HDFS configuration to determine the Namenodes 
+  configured in the cluster and refreshes all the Namenodes to pick up 
+  the new exclude file.
+
+** Cluster Web Console
+
+  Similar to Namenode status web page, a Cluster Web Console is added in 
+  federation to monitor the federated cluster at 
+  <<<http://<any_nn_host:port>/dfsclusterhealth.jsp>>>.
+  Any Namenode in the cluster can be used to access this web page.
+
+  The web page provides the following information:
+
+  * Cluster summary that shows number of files, number of blocks and 
+    total configured storage capacity, available and used storage information 
+    for the entire cluster.
+
+  * Provides list of Namenodes and summary that includes number of files,
+    blocks, missing blocks, number of live and dead data nodes for each 
+    Namenode. It also provides a link to conveniently access Namenode web UI.
+
+  * It also provides decommissioning status of datanodes.
+
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm
new file mode 100644
index 0000000..efa3f93
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm
@@ -0,0 +1,880 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop Distributed File System-${project.version} - High Availability
+  ---
+  ---
+  ${maven.build.timestamp}
+
+HDFS High Availability
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+* {Purpose}
+
+  This guide provides an overview of the HDFS High Availability (HA) feature and
+  how to configure and manage an HA HDFS cluster, using NFS for the shared
+  storage required by the NameNodes.
+ 
+  This document assumes that the reader has a general understanding of
+  general components and node types in an HDFS cluster. Please refer to the
+  HDFS Architecture guide for details.
+
+* {Note: Using the Quorum Journal Manager or Conventional Shared Storage}
+
+  This guide discusses how to configure and use HDFS HA using a shared NFS
+  directory to share edit logs between the Active and Standby NameNodes. For
+  information on how to configure HDFS HA using the Quorum Journal Manager
+  instead of NFS, please see {{{./HDFSHighAvailabilityWithQJM.html}this
+  alternative guide.}}
+
+* {Background}
+
+  Prior to Hadoop 2.0.0, the NameNode was a single point of failure (SPOF) in
+  an HDFS cluster. Each cluster had a single NameNode, and if that machine or
+  process became unavailable, the cluster as a whole would be unavailable
+  until the NameNode was either restarted or brought up on a separate machine.
+  
+  This impacted the total availability of the HDFS cluster in two major ways:
+
+    * In the case of an unplanned event such as a machine crash, the cluster would
+      be unavailable until an operator restarted the NameNode.
+
+    * Planned maintenance events such as software or hardware upgrades on the
+      NameNode machine would result in windows of cluster downtime.
+  
+  The HDFS High Availability feature addresses the above problems by providing
+  the option of running two redundant NameNodes in the same cluster in an
+  Active/Passive configuration with a hot standby. This allows a fast failover to
+  a new NameNode in the case that a machine crashes, or a graceful
+  administrator-initiated failover for the purpose of planned maintenance.
+
+* {Architecture}
+
+  In a typical HA cluster, two separate machines are configured as NameNodes.
+  At any point in time, exactly one of the NameNodes is in an <Active> state,
+  and the other is in a <Standby> state. The Active NameNode is responsible
+  for all client operations in the cluster, while the Standby is simply acting
+  as a slave, maintaining enough state to provide a fast failover if
+  necessary.
+  
+  In order for the Standby node to keep its state synchronized with the Active
+  node, the current implementation requires that the two nodes both have access
+  to a directory on a shared storage device (eg an NFS mount from a NAS). This
+  restriction will likely be relaxed in future versions.
+
+  When any namespace modification is performed by the Active node, it durably
+  logs a record of the modification to an edit log file stored in the shared
+  directory.  The Standby node is constantly watching this directory for edits,
+  and as it sees the edits, it applies them to its own namespace. In the event of
+  a failover, the Standby will ensure that it has read all of the edits from the
+  shared storage before promoting itself to the Active state. This ensures that
+  the namespace state is fully synchronized before a failover occurs.
+  
+  In order to provide a fast failover, it is also necessary that the Standby node
+  have up-to-date information regarding the location of blocks in the cluster.
+  In order to achieve this, the DataNodes are configured with the location of
+  both NameNodes, and send block location information and heartbeats to both.
+  
+  It is vital for the correct operation of an HA cluster that only one of the
+  NameNodes be Active at a time. Otherwise, the namespace state would quickly
+  diverge between the two, risking data loss or other incorrect results.  In
+  order to ensure this property and prevent the so-called "split-brain scenario,"
+  the administrator must configure at least one <fencing method> for the shared
+  storage. During a failover, if it cannot be verified that the previous Active
+  node has relinquished its Active state, the fencing process is responsible for
+  cutting off the previous Active's access to the shared edits storage. This
+  prevents it from making any further edits to the namespace, allowing the new
+  Active to safely proceed with failover.
+
+* {Hardware resources}
+
+  In order to deploy an HA cluster, you should prepare the following:
+
+    * <<NameNode machines>> - the machines on which you run the Active and
+    Standby NameNodes should have equivalent hardware to each other, and
+    equivalent hardware to what would be used in a non-HA cluster.
+
+    * <<Shared storage>> - you will need to have a shared directory which both
+    NameNode machines can have read/write access to. Typically this is a remote
+    filer which supports NFS and is mounted on each of the NameNode machines.
+    Currently only a single shared edits directory is supported. Thus, the
+    availability of the system is limited by the availability of this shared edits
+    directory, and therefore in order to remove all single points of failure there
+    needs to be redundancy for the shared edits directory. Specifically, multiple
+    network paths to the storage, and redundancy in the storage itself (disk,
+    network, and power). Beacuse of this, it is recommended that the shared storage
+    server be a high-quality dedicated NAS appliance rather than a simple Linux
+    server.
+  
+  Note that, in an HA cluster, the Standby NameNode also performs checkpoints of
+  the namespace state, and thus it is not necessary to run a Secondary NameNode,
+  CheckpointNode, or BackupNode in an HA cluster. In fact, to do so would be an
+  error. This also allows one who is reconfiguring a non-HA-enabled HDFS cluster
+  to be HA-enabled to reuse the hardware which they had previously dedicated to
+  the Secondary NameNode.
+
+* {Deployment}
+
+** Configuration overview
+
+  Similar to Federation configuration, HA configuration is backward compatible
+  and allows existing single NameNode configurations to work without change.
+  The new configuration is designed such that all the nodes in the cluster may
+  have the same configuration without the need for deploying different
+  configuration files to different machines based on the type of the node.
+ 
+  Like HDFS Federation, HA clusters reuse the <<<nameservice ID>>> to identify a
+  single HDFS instance that may in fact consist of multiple HA NameNodes. In
+  addition, a new abstraction called <<<NameNode ID>>> is added with HA. Each
+  distinct NameNode in the cluster has a different NameNode ID to distinguish it.
+  To support a single configuration file for all of the NameNodes, the relevant
+  configuration parameters are suffixed with the <<nameservice ID>> as well as
+  the <<NameNode ID>>.
+
+** Configuration details
+
+  To configure HA NameNodes, you must add several configuration options to your
+  <<hdfs-site.xml>> configuration file.
+
+  The order in which you set these configurations is unimportant, but the values
+  you choose for <<dfs.nameservices>> and
+  <<dfs.ha.namenodes.[nameservice ID]>> will determine the keys of those that
+  follow. Thus, you should decide on these values before setting the rest of the
+  configuration options.
+
+  * <<dfs.nameservices>> - the logical name for this new nameservice
+
+    Choose a logical name for this nameservice, for example "mycluster", and use
+    this logical name for the value of this config option. The name you choose is
+    arbitrary. It will be used both for configuration and as the authority
+    component of absolute HDFS paths in the cluster.
+
+    <<Note:>> If you are also using HDFS Federation, this configuration setting
+    should also include the list of other nameservices, HA or otherwise, as a
+    comma-separated list.
+
+----
+<property>
+  <name>dfs.nameservices</name>
+  <value>mycluster</value>
+</property>
+----
+
+  * <<dfs.ha.namenodes.[nameservice ID]>> - unique identifiers for each NameNode in the nameservice
+
+    Configure with a list of comma-separated NameNode IDs. This will be used by
+    DataNodes to determine all the NameNodes in the cluster. For example, if you
+    used "mycluster" as the nameservice ID previously, and you wanted to use "nn1"
+    and "nn2" as the individual IDs of the NameNodes, you would configure this as
+    such:
+
+----
+<property>
+  <name>dfs.ha.namenodes.mycluster</name>
+  <value>nn1,nn2</value>
+</property>
+----
+
+    <<Note:>> Currently, only a maximum of two NameNodes may be configured per
+    nameservice.
+
+  * <<dfs.namenode.rpc-address.[nameservice ID].[name node ID]>> - the fully-qualified RPC address for each NameNode to listen on
+
+    For both of the previously-configured NameNode IDs, set the full address and
+    IPC port of the NameNode processs. Note that this results in two separate
+    configuration options. For example:
+
+----
+<property>
+  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
+  <value>machine1.example.com:8020</value>
+</property>
+<property>
+  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
+  <value>machine2.example.com:8020</value>
+</property>
+----
+
+    <<Note:>> You may similarly configure the "<<servicerpc-address>>" setting if
+    you so desire.
+
+  * <<dfs.namenode.http-address.[nameservice ID].[name node ID]>> - the fully-qualified HTTP address for each NameNode to listen on
+
+    Similarly to <rpc-address> above, set the addresses for both NameNodes' HTTP
+    servers to listen on. For example:
+
+----
+<property>
+  <name>dfs.namenode.http-address.mycluster.nn1</name>
+  <value>machine1.example.com:50070</value>
+</property>
+<property>
+  <name>dfs.namenode.http-address.mycluster.nn2</name>
+  <value>machine2.example.com:50070</value>
+</property>
+----
+
+    <<Note:>> If you have Hadoop's security features enabled, you should also set
+    the <https-address> similarly for each NameNode.
+
+  * <<dfs.namenode.shared.edits.dir>> - the location of the shared storage directory
+
+    This is where one configures the path to the remote shared edits directory
+    which the Standby NameNode uses to stay up-to-date with all the file system
+    changes the Active NameNode makes. <<You should only configure one of these
+    directories.>> This directory should be mounted r/w on both NameNode machines.
+    The value of this setting should be the absolute path to this directory on the
+    NameNode machines. For example:
+
+----
+<property>
+  <name>dfs.namenode.shared.edits.dir</name>
+  <value>file:///mnt/filer1/dfs/ha-name-dir-shared</value>
+</property>
+----
+
+  * <<dfs.client.failover.proxy.provider.[nameservice ID]>> - the Java class that HDFS clients use to contact the Active NameNode
+
+    Configure the name of the Java class which will be used by the DFS Client to
+    determine which NameNode is the current Active, and therefore which NameNode is
+    currently serving client requests. The only implementation which currently
+    ships with Hadoop is the <<ConfiguredFailoverProxyProvider>>, so use this
+    unless you are using a custom one. For example:
+
+----
+<property>
+  <name>dfs.client.failover.proxy.provider.mycluster</name>
+  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
+</property>
+----
+
+  * <<dfs.ha.fencing.methods>> - a list of scripts or Java classes which will be used to fence the Active NameNode during a failover
+
+    It is critical for correctness of the system that only one NameNode be in the
+    Active state at any given time. Thus, during a failover, we first ensure that
+    the Active NameNode is either in the Standby state, or the process has
+    terminated, before transitioning the other NameNode to the Active state. In
+    order to do this, you must configure at least one <<fencing method.>> These are
+    configured as a carriage-return-separated list, which will be attempted in order
+    until one indicates that fencing has succeeded. There are two methods which
+    ship with Hadoop: <shell> and <sshfence>. For information on implementing
+    your own custom fencing method, see the <org.apache.hadoop.ha.NodeFencer> class.
+
+    * <<sshfence>> - SSH to the Active NameNode and kill the process
+
+      The <sshfence> option SSHes to the target node and uses <fuser> to kill the
+      process listening on the service's TCP port. In order for this fencing option
+      to work, it must be able to SSH to the target node without providing a
+      passphrase. Thus, one must also configure the
+      <<dfs.ha.fencing.ssh.private-key-files>> option, which is a
+      comma-separated list of SSH private key files. For example:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>sshfence</value>
+</property>
+
+<property>
+  <name>dfs.ha.fencing.ssh.private-key-files</name>
+  <value>/home/exampleuser/.ssh/id_rsa</value>
+</property>
+---
+
+      Optionally, one may configure a non-standard username or port to perform the
+      SSH. One may also configure a timeout, in milliseconds, for the SSH, after
+      which this fencing method will be considered to have failed. It may be
+      configured like so:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>sshfence([[username][:port]])</value>
+</property>
+<property>
+  <name>dfs.ha.fencing.ssh.connect-timeout</name>
+  <value>30000</value>
+</property>
+---
+
+    * <<shell>> - run an arbitrary shell command to fence the Active NameNode
+
+      The <shell> fencing method runs an arbitrary shell command. It may be
+      configured like so:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>shell(/path/to/my/script.sh arg1 arg2 ...)</value>
+</property>
+---
+
+      The string between '(' and ')' is passed directly to a bash shell and may not
+      include any closing parentheses.
+
+      The shell command will be run with an environment set up to contain all of the
+      current Hadoop configuration variables, with the '_' character replacing any
+      '.' characters in the configuration keys. The configuration used has already had
+      any namenode-specific configurations promoted to their generic forms -- for example
+      <<dfs_namenode_rpc-address>> will contain the RPC address of the target node, even
+      though the configuration may specify that variable as
+      <<dfs.namenode.rpc-address.ns1.nn1>>.
+      
+      Additionally, the following variables referring to the target node to be fenced
+      are also available:
+
+*-----------------------:-----------------------------------+
+| $target_host          | hostname of the node to be fenced |
+*-----------------------:-----------------------------------+
+| $target_port          | IPC port of the node to be fenced |
+*-----------------------:-----------------------------------+
+| $target_address       | the above two, combined as host:port |
+*-----------------------:-----------------------------------+
+| $target_nameserviceid | the nameservice ID of the NN to be fenced |
+*-----------------------:-----------------------------------+
+| $target_namenodeid    | the namenode ID of the NN to be fenced |
+*-----------------------:-----------------------------------+
+      
+      These environment variables may also be used as substitutions in the shell
+      command itself. For example:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>shell(/path/to/my/script.sh --nameservice=$target_nameserviceid $target_host:$target_port)</value>
+</property>
+---
+      
+      If the shell command returns an exit
+      code of 0, the fencing is determined to be successful. If it returns any other
+      exit code, the fencing was not successful and the next fencing method in the
+      list will be attempted.
+
+      <<Note:>> This fencing method does not implement any timeout. If timeouts are
+      necessary, they should be implemented in the shell script itself (eg by forking
+      a subshell to kill its parent in some number of seconds).
+
+  * <<fs.defaultFS>> - the default path prefix used by the Hadoop FS client when none is given
+
+    Optionally, you may now configure the default path for Hadoop clients to use
+    the new HA-enabled logical URI. If you used "mycluster" as the nameservice ID
+    earlier, this will be the value of the authority portion of all of your HDFS
+    paths. This may be configured like so, in your <<core-site.xml>> file:
+
+---
+<property>
+  <name>fs.defaultFS</name>
+  <value>hdfs://mycluster</value>
+</property>
+---
+
+** Deployment details
+
+  After all of the necessary configuration options have been set, one must
+  initially synchronize the two HA NameNodes' on-disk metadata.
+
+    * If you are setting up a fresh HDFS cluster, you should first run the format
+    command (<hdfs namenode -format>) on one of NameNodes.
+  
+    * If you have already formatted the NameNode, or are converting a
+    non-HA-enabled cluster to be HA-enabled, you should now copy over the
+    contents of your NameNode metadata directories to the other, unformatted
+    NameNode by running the command "<hdfs namenode -bootstrapStandby>" on the
+    unformatted NameNode. Running this command will also ensure that the shared
+    edits directory (as configured by <<dfs.namenode.shared.edits.dir>>) contains
+    sufficient edits transactions to be able to start both NameNodes.
+  
+    * If you are converting a non-HA NameNode to be HA, you should run the
+    command "<hdfs -initializeSharedEdits>", which will initialize the shared
+    edits directory with the edits data from the local NameNode edits directories.
+
+  At this point you may start both of your HA NameNodes as you normally would
+  start a NameNode.
+
+  You can visit each of the NameNodes' web pages separately by browsing to their
+  configured HTTP addresses. You should notice that next to the configured
+  address will be the HA state of the NameNode (either "standby" or "active".)
+  Whenever an HA NameNode starts, it is initially in the Standby state.
+
+** Administrative commands
+
+  Now that your HA NameNodes are configured and started, you will have access
+  to some additional commands to administer your HA HDFS cluster. Specifically,
+  you should familiarize yourself with all of the subcommands of the "<hdfs
+  haadmin>" command. Running this command without any additional arguments will
+  display the following usage information:
+
+---
+Usage: DFSHAAdmin [-ns <nameserviceId>]
+    [-transitionToActive <serviceId>]
+    [-transitionToStandby <serviceId>]
+    [-failover [--forcefence] [--forceactive] <serviceId> <serviceId>]
+    [-getServiceState <serviceId>]
+    [-checkHealth <serviceId>]
+    [-help <command>]
+---
+
+  This guide describes high-level uses of each of these subcommands. For
+  specific usage information of each subcommand, you should run "<hdfs haadmin
+  -help <command>>".
+
+  * <<transitionToActive>> and <<transitionToStandby>> - transition the state of the given NameNode to Active or Standby
+
+    These subcommands cause a given NameNode to transition to the Active or Standby
+    state, respectively. <<These commands do not attempt to perform any fencing,
+    and thus should rarely be used.>> Instead, one should almost always prefer to
+    use the "<hdfs haadmin -failover>" subcommand.
+
+  * <<failover>> - initiate a failover between two NameNodes
+
+    This subcommand causes a failover from the first provided NameNode to the
+    second. If the first NameNode is in the Standby state, this command simply
+    transitions the second to the Active state without error. If the first NameNode
+    is in the Active state, an attempt will be made to gracefully transition it to
+    the Standby state. If this fails, the fencing methods (as configured by
+    <<dfs.ha.fencing.methods>>) will be attempted in order until one
+    succeeds. Only after this process will the second NameNode be transitioned to
+    the Active state. If no fencing method succeeds, the second NameNode will not
+    be transitioned to the Active state, and an error will be returned.
+
+  * <<getServiceState>> - determine whether the given NameNode is Active or Standby
+
+    Connect to the provided NameNode to determine its current state, printing
+    either "standby" or "active" to STDOUT appropriately. This subcommand might be
+    used by cron jobs or monitoring scripts which need to behave differently based
+    on whether the NameNode is currently Active or Standby.
+
+  * <<checkHealth>> - check the health of the given NameNode
+
+    Connect to the provided NameNode to check its health. The NameNode is capable
+    of performing some diagnostics on itself, including checking if internal
+    services are running as expected. This command will return 0 if the NameNode is
+    healthy, non-zero otherwise. One might use this command for monitoring
+    purposes.
+
+    <<Note:>> This is not yet implemented, and at present will always return
+    success, unless the given NameNode is completely down.
+
+* {Automatic Failover}
+
+** Introduction
+
+  The above sections describe how to configure manual failover. In that mode,
+  the system will not automatically trigger a failover from the active to the
+  standby NameNode, even if the active node has failed. This section describes
+  how to configure and deploy automatic failover.
+
+** Components
+
+  Automatic failover adds two new components to an HDFS deployment: a ZooKeeper
+  quorum, and the ZKFailoverController process (abbreviated as ZKFC).
+
+  Apache ZooKeeper is a highly available service for maintaining small amounts
+  of coordination data, notifying clients of changes in that data, and
+  monitoring clients for failures. The implementation of automatic HDFS failover
+  relies on ZooKeeper for the following things:
+  
+    * <<Failure detection>> - each of the NameNode machines in the cluster
+    maintains a persistent session in ZooKeeper. If the machine crashes, the
+    ZooKeeper session will expire, notifying the other NameNode that a failover
+    should be triggered.
+
+    * <<Active NameNode election>> - ZooKeeper provides a simple mechanism to
+    exclusively elect a node as active. If the current active NameNode crashes,
+    another node may take a special exclusive lock in ZooKeeper indicating that
+    it should become the next active.
+
+  The ZKFailoverController (ZKFC) is a new component which is a ZooKeeper client
+  which also monitors and manages the state of the NameNode.  Each of the
+  machines which runs a NameNode also runs a ZKFC, and that ZKFC is responsible
+  for:
+
+    * <<Health monitoring>> - the ZKFC pings its local NameNode on a periodic
+    basis with a health-check command. So long as the NameNode responds in a
+    timely fashion with a healthy status, the ZKFC considers the node
+    healthy. If the node has crashed, frozen, or otherwise entered an unhealthy
+    state, the health monitor will mark it as unhealthy.
+
+    * <<ZooKeeper session management>> - when the local NameNode is healthy, the
+    ZKFC holds a session open in ZooKeeper. If the local NameNode is active, it
+    also holds a special "lock" znode. This lock uses ZooKeeper's support for
+    "ephemeral" nodes; if the session expires, the lock node will be
+    automatically deleted.
+
+    * <<ZooKeeper-based election>> - if the local NameNode is healthy, and the
+    ZKFC sees that no other node currently holds the lock znode, it will itself
+    try to acquire the lock. If it succeeds, then it has "won the election", and
+    is responsible for running a failover to make its local NameNode active. The
+    failover process is similar to the manual failover described above: first,
+    the previous active is fenced if necessary, and then the local NameNode
+    transitions to active state.
+
+  For more details on the design of automatic failover, refer to the design
+  document attached to HDFS-2185 on the Apache HDFS JIRA.
+
+** Deploying ZooKeeper
+
+  In a typical deployment, ZooKeeper daemons are configured to run on three or
+  five nodes. Since ZooKeeper itself has light resource requirements, it is
+  acceptable to collocate the ZooKeeper nodes on the same hardware as the HDFS
+  NameNode and Standby Node. Many operators choose to deploy the third ZooKeeper
+  process on the same node as the YARN ResourceManager. It is advisable to
+  configure the ZooKeeper nodes to store their data on separate disk drives from
+  the HDFS metadata for best performance and isolation.
+
+  The setup of ZooKeeper is out of scope for this document. We will assume that
+  you have set up a ZooKeeper cluster running on three or more nodes, and have
+  verified its correct operation by connecting using the ZK CLI.
+
+** Before you begin
+
+  Before you begin configuring automatic failover, you should shut down your
+  cluster. It is not currently possible to transition from a manual failover
+  setup to an automatic failover setup while the cluster is running.
+
+** Configuring automatic failover
+
+  The configuration of automatic failover requires the addition of two new
+  parameters to your configuration. In your <<<hdfs-site.xml>>> file, add:
+
+----
+ <property>
+   <name>dfs.ha.automatic-failover.enabled</name>
+   <value>true</value>
+ </property>
+----
+
+  This specifies that the cluster should be set up for automatic failover.
+  In your <<<core-site.xml>>> file, add:
+
+----
+ <property>
+   <name>ha.zookeeper.quorum</name>
+   <value>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181</value>
+ </property>
+----
+
+  This lists the host-port pairs running the ZooKeeper service.
+
+  As with the parameters described earlier in the document, these settings may
+  be configured on a per-nameservice basis by suffixing the configuration key
+  with the nameservice ID. For example, in a cluster with federation enabled,
+  you can explicitly enable automatic failover for only one of the nameservices
+  by setting <<<dfs.ha.automatic-failover.enabled.my-nameservice-id>>>.
+
+  There are also several other configuration parameters which may be set to
+  control the behavior of automatic failover; however, they are not necessary
+  for most installations. Please refer to the configuration key specific
+  documentation for details.
+
+** Initializing HA state in ZooKeeper
+
+  After the configuration keys have been added, the next step is to initialize
+  required state in ZooKeeper. You can do so by running the following command
+  from one of the NameNode hosts.
+
+----
+$ hdfs zkfc -formatZK
+----
+
+  This will create a znode in ZooKeeper inside of which the automatic failover
+  system stores its data.
+
+** Starting the cluster with <<<start-dfs.sh>>>
+
+  Since automatic failover has been enabled in the configuration, the
+  <<<start-dfs.sh>>> script will now automatically start a ZKFC daemon on any
+  machine that runs a NameNode. When the ZKFCs start, they will automatically
+  select one of the NameNodes to become active.
+
+** Starting the cluster manually
+
+  If you manually manage the services on your cluster, you will need to manually
+  start the <<<zkfc>>> daemon on each of the machines that runs a NameNode. You
+  can start the daemon by running:
+
+----
+$ hadoop-daemon.sh start zkfc
+----
+
+** Securing access to ZooKeeper
+
+  If you are running a secure cluster, you will likely want to ensure that the
+  information stored in ZooKeeper is also secured. This prevents malicious
+  clients from modifying the metadata in ZooKeeper or potentially triggering a
+  false failover.
+
+  In order to secure the information in ZooKeeper, first add the following to
+  your <<<core-site.xml>>> file:
+
+----
+ <property>
+   <name>ha.zookeeper.auth</name>
+   <value>@/path/to/zk-auth.txt</value>
+ </property>
+ <property>
+   <name>ha.zookeeper.acl</name>
+   <value>@/path/to/zk-acl.txt</value>
+ </property>
+----
+
+  Please note the '@' character in these values -- this specifies that the
+  configurations are not inline, but rather point to a file on disk.
+
+  The first configured file specifies a list of ZooKeeper authentications, in
+  the same format as used by the ZK CLI. For example, you may specify something
+  like:
+
+----
+digest:hdfs-zkfcs:mypassword
+----
+  ...where <<<hdfs-zkfcs>>> is a unique username for ZooKeeper, and
+  <<<mypassword>>> is some unique string used as a password.
+
+  Next, generate a ZooKeeper ACL that corresponds to this authentication, using
+  a command like the following:
+
+----
+$ java -cp $ZK_HOME/lib/*:$ZK_HOME/zookeeper-3.4.2.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:mypassword
+output: hdfs-zkfcs:mypassword->hdfs-zkfcs:P/OQvnYyU/nF/mGYvB/xurX8dYs=
+----
+
+  Copy and paste the section of this output after the '->' string into the file
+  <<<zk-acls.txt>>>, prefixed by the string "<<<digest:>>>". For example:
+
+----
+digest:hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=:rwcda
+----
+
+  In order for these ACLs to take effect, you should then rerun the
+  <<<zkfc -formatZK>>> command as described above.
+
+  After doing so, you may verify the ACLs from the ZK CLI as follows:
+
+----
+[zk: localhost:2181(CONNECTED) 1] getAcl /hadoop-ha
+'digest,'hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=
+: cdrwa
+----
+
+** Verifying automatic failover
+
+  Once automatic failover has been set up, you should test its operation. To do
+  so, first locate the active NameNode. You can tell which node is active by
+  visiting the NameNode web interfaces -- each node reports its HA state at the
+  top of the page.
+
+  Once you have located your active NameNode, you may cause a failure on that
+  node.  For example, you can use <<<kill -9 <pid of NN>>>> to simulate a JVM
+  crash. Or, you could power cycle the machine or unplug its network interface
+  to simulate a different kind of outage.  After triggering the outage you wish
+  to test, the other NameNode should automatically become active within several
+  seconds. The amount of time required to detect a failure and trigger a
+  fail-over depends on the configuration of
+  <<<ha.zookeeper.session-timeout.ms>>>, but defaults to 5 seconds.
+
+  If the test does not succeed, you may have a misconfiguration. Check the logs
+  for the <<<zkfc>>> daemons as well as the NameNode daemons in order to further
+  diagnose the issue.
+
+
+* Automatic Failover FAQ
+
+  * <<Is it important that I start the ZKFC and NameNode daemons in any
+    particular order?>>
+
+  No. On any given node you may start the ZKFC before or after its corresponding
+  NameNode.
+
+  * <<What additional monitoring should I put in place?>>
+
+  You should add monitoring on each host that runs a NameNode to ensure that the
+  ZKFC remains running. In some types of ZooKeeper failures, for example, the
+  ZKFC may unexpectedly exit, and should be restarted to ensure that the system
+  is ready for automatic failover.
+
+  Additionally, you should monitor each of the servers in the ZooKeeper
+  quorum. If ZooKeeper crashes, then automatic failover will not function.
+
+  * <<What happens if ZooKeeper goes down?>>
+
+  If the ZooKeeper cluster crashes, no automatic failovers will be triggered.
+  However, HDFS will continue to run without any impact. When ZooKeeper is
+  restarted, HDFS will reconnect with no issues.
+
+  * <<Can I designate one of my NameNodes as primary/preferred?>>
+
+  No. Currently, this is not supported. Whichever NameNode is started first will
+  become active. You may choose to start the cluster in a specific order such
+  that your preferred node starts first.
+
+  * <<How can I initiate a manual failover when automatic failover is
+    configured?>>
+
+  Even if automatic failover is configured, you may initiate a manual failover
+  using the same <<<hdfs haadmin>>> command. It will perform a coordinated
+  failover.
+
+ 
+* BookKeeper as a Shared storage (EXPERIMENTAL)
+
+   One option for shared storage for the NameNode is BookKeeper. 
+  BookKeeper achieves high availability and strong durability guarantees by replicating
+  edit log entries across multiple storage nodes. The edit log can be striped across 
+  the storage nodes for high performance. Fencing is supported in the protocol, i.e, 
+  BookKeeper will not allow two writers to write the single edit log.
+
+  The meta data for BookKeeper is stored in ZooKeeper.
+  In current HA architecture, a Zookeeper cluster is required for ZKFC. The same cluster can be
+  for BookKeeper metadata.
+
+  For more details on building a BookKeeper cluster, please refer to the 
+   {{{http://zookeeper.apache.org/bookkeeper/docs/trunk/bookkeeperConfig.html }BookKeeper documentation}}
+
+ The BookKeeperJournalManager is an implementation of the HDFS JournalManager interface, which allows custom write ahead logging implementations to be plugged into the HDFS NameNode.
+ 
+ **<<BookKeeper Journal Manager>>
+
+   To use BookKeeperJournalManager, add the following to hdfs-site.xml.
+
+----
+    <property>
+      <name>dfs.namenode.shared.edits.dir</name>
+      <value>bookkeeper://zk1:2181;zk2:2181;zk3:2181/hdfsjournal</value>
+    </property>
+
+    <property>
+      <name>dfs.namenode.edits.journal-plugin.bookkeeper</name>
+      <value>org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager</value>
+    </property>
+----
+
+   The URI format for bookkeeper is <<<bookkeeper://[zkEnsemble]/[rootZnode]
+   [zookkeeper ensemble]>>> is a list of semi-colon separated, zookeeper host:port
+   pairs. In the example above there are 3 servers, in the ensemble,
+   zk1, zk2 & zk3, each one listening on port 2181.
+
+   <<<[root znode]>>> is the path of the zookeeper znode, under which the edit log
+   information will be stored.
+
+   The class specified for the journal-plugin must be available in the NameNode's
+   classpath. We explain how to generate a jar file with the journal manager and
+   its dependencies, and how to put it into the classpath below.
+
+ *** <<More configuration options>> 
+
+     * <<dfs.namenode.bookkeeperjournal.output-buffer-size>> - 
+       Number of bytes a bookkeeper journal stream will buffer before
+       forcing a flush. Default is 1024.
+     
+----
+       <property>
+         <name>dfs.namenode.bookkeeperjournal.output-buffer-size</name>
+         <value>1024</value>
+       </property>
+----
+
+     * <<dfs.namenode.bookkeeperjournal.ensemble-size>> - 
+       Number of bookkeeper servers in edit log ensembles. This
+       is the number of bookkeeper servers which need to be available
+       for the edit log to be writable. Default is 3.
+
+----
+       <property>
+         <name>dfs.namenode.bookkeeperjournal.ensemble-size</name>
+         <value>3</value>
+       </property>
+----
+
+     * <<dfs.namenode.bookkeeperjournal.quorum-size>> - 
+       Number of bookkeeper servers in the write quorum. This is the
+       number of bookkeeper servers which must have acknowledged the
+       write of an entry before it is considered written. Default is 2.
+
+----
+       <property>
+         <name>dfs.namenode.bookkeeperjournal.quorum-size</name>
+         <value>2</value>
+       </property>
+----
+
+     * <<dfs.namenode.bookkeeperjournal.digestPw>> - 
+       Password to use when creating edit log segments.
+
+----
+       <property>
+        <name>dfs.namenode.bookkeeperjournal.digestPw</name>
+        <value>myPassword</value>
+       </property>
+----
+
+     * <<dfs.namenode.bookkeeperjournal.zk.session.timeout>> - 
+       Session timeout for Zookeeper client from BookKeeper Journal Manager.
+       Hadoop recommends that this value should be less than the ZKFC 
+       session timeout value. Default value is 3000.
+
+----
+       <property>
+         <name>dfs.namenode.bookkeeperjournal.zk.session.timeout</name>
+         <value>3000</value>
+       </property>
+----
+
+ *** <<Building BookKeeper Journal Manager plugin jar>>
+
+     To generate the distribution packages for BK journal, do the
+     following.
+
+     $ mvn clean package -Pdist
+
+     This will generate a jar with the BookKeeperJournalManager, all the dependencies
+     needed by the journal manager,
+     hadoop-hdfs/src/contrib/bkjournal/target/hadoop-hdfs-bkjournal-<VERSION>.jar
+
+     Note that the -Pdist part of the build command is important, as otherwise
+     the dependencies would not be packaged in the jar. The dependencies included in
+     the jar are {{{http://maven.apache.org/plugins/maven-shade-plugin/}shaded}} to
+     avoid conflicts with other dependencies of the NameNode.
+
+ *** <<Putting the BookKeeperJournalManager in the NameNode classpath>>
+
+    To run a HDFS namenode using BookKeeper as a backend, copy the bkjournal
+    jar, generated above, into the lib directory of hdfs. In the standard 
+    distribution of HDFS, this is at $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/
+
+    cp hadoop-hdfs/src/contrib/bkjournal/target/hadoop-hdfs-bkjournal-<VERSION>.jar $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/
+
+ *** <<Current limitations>> 
+
+      1) NameNode format command will not format the BookKeeper data automatically. 
+         We have to clean the data manually from BookKeeper cluster 
+         and create the /ledgers/available path in Zookeeper. 
+----
+$ zkCli.sh create /ledgers 0
+$ zkCli.sh create /ledgers/available 0
+----
+         Note:
+          bookkeeper://zk1:2181;zk2:2181;zk3:2181/hdfsjournal
+          The final part /hdfsjournal specifies the znode in zookeeper where
+          ledger metadata will be stored. Administrators may set this to anything
+          they wish.
+
+      2) Security in BookKeeper. BookKeeper does not support SASL nor SSL for
+         connections between the NameNode and BookKeeper storage nodes.
+
+      3) Auto-Recovery of storage node failures. Work inprogress 
+      {{{https://issues.apache.org/jira/browse/BOOKKEEPER-237 }BOOKKEEPER-237}}.
+         Currently we have the tools to manually recover the data from failed storage nodes.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
new file mode 100644
index 0000000..2aefc35
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
@@ -0,0 +1,767 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop Distributed File System-${project.version} - High Availability
+  ---
+  ---
+  ${maven.build.timestamp}
+
+HDFS High Availability Using the Quorum Journal Manager
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+* {Purpose}
+
+  This guide provides an overview of the HDFS High Availability (HA) feature
+  and how to configure and manage an HA HDFS cluster, using the Quorum Journal
+  Manager (QJM) feature.
+ 
+  This document assumes that the reader has a general understanding of
+  general components and node types in an HDFS cluster. Please refer to the
+  HDFS Architecture guide for details.
+
+* {Note: Using the Quorum Journal Manager or Conventional Shared Storage}
+
+  This guide discusses how to configure and use HDFS HA using the Quorum
+  Journal Manager (QJM) to share edit logs between the Active and Standby
+  NameNodes. For information on how to configure HDFS HA using NFS for shared
+  storage instead of the QJM, please see
+  {{{./HDFSHighAvailabilityWithNFS.html}this alternative guide.}}
+
+* {Background}
+
+  Prior to Hadoop 2.0.0, the NameNode was a single point of failure (SPOF) in
+  an HDFS cluster. Each cluster had a single NameNode, and if that machine or
+  process became unavailable, the cluster as a whole would be unavailable
+  until the NameNode was either restarted or brought up on a separate machine.
+  
+  This impacted the total availability of the HDFS cluster in two major ways:
+
+    * In the case of an unplanned event such as a machine crash, the cluster would
+      be unavailable until an operator restarted the NameNode.
+
+    * Planned maintenance events such as software or hardware upgrades on the
+      NameNode machine would result in windows of cluster downtime.
+  
+  The HDFS High Availability feature addresses the above problems by providing
+  the option of running two redundant NameNodes in the same cluster in an
+  Active/Passive configuration with a hot standby. This allows a fast failover to
+  a new NameNode in the case that a machine crashes, or a graceful
+  administrator-initiated failover for the purpose of planned maintenance.
+
+* {Architecture}
+
+  In a typical HA cluster, two separate machines are configured as NameNodes.
+  At any point in time, exactly one of the NameNodes is in an <Active> state,
+  and the other is in a <Standby> state. The Active NameNode is responsible
+  for all client operations in the cluster, while the Standby is simply acting
+  as a slave, maintaining enough state to provide a fast failover if
+  necessary.
+  
+  In order for the Standby node to keep its state synchronized with the Active
+  node, both nodes communicate with a group of separate daemons called
+  "JournalNodes" (JNs). When any namespace modification is performed by the
+  Active node, it durably logs a record of the modification to a majority of
+  these JNs. The Standby node is capable of reading the edits from the JNs, and
+  is constantly watching them for changes to the edit log. As the Standby Node
+  sees the edits, it applies them to its own namespace. In the event of a
+  failover, the Standby will ensure that it has read all of the edits from the
+  JounalNodes before promoting itself to the Active state. This ensures that the
+  namespace state is fully synchronized before a failover occurs.
+  
+  In order to provide a fast failover, it is also necessary that the Standby node
+  have up-to-date information regarding the location of blocks in the cluster.
+  In order to achieve this, the DataNodes are configured with the location of
+  both NameNodes, and send block location information and heartbeats to both.
+  
+  It is vital for the correct operation of an HA cluster that only one of the
+  NameNodes be Active at a time. Otherwise, the namespace state would quickly
+  diverge between the two, risking data loss or other incorrect results.  In
+  order to ensure this property and prevent the so-called "split-brain scenario,"
+  the JournalNodes will only ever allow a single NameNode to be a writer at a
+  time. During a failover, the NameNode which is to become active will simply
+  take over the role of writing to the JournalNodes, which will effectively
+  prevent the other NameNode from continuing in the Active state, allowing the
+  new Active to safely proceed with failover.
+
+* {Hardware resources}
+
+  In order to deploy an HA cluster, you should prepare the following:
+
+    * <<NameNode machines>> - the machines on which you run the Active and
+    Standby NameNodes should have equivalent hardware to each other, and
+    equivalent hardware to what would be used in a non-HA cluster.
+
+    * <<JournalNode machines>> - the machines on which you run the JournalNodes.
+    The JournalNode daemon is relatively lightweight, so these daemons may
+    reasonably be collocated on machines with other Hadoop daemons, for example
+    NameNodes, the JobTracker, or the YARN ResourceManager. <<Note:>> There
+    must be at least 3 JournalNode daemons, since edit log modifications must be
+    written to a majority of JNs. This will allow the system to tolerate the
+    failure of a single machine. You may also run more than 3 JournalNodes, but
+    in order to actually increase the number of failures the system can tolerate,
+    you should run an odd number of JNs, (i.e. 3, 5, 7, etc.). Note that when
+    running with N JournalNodes, the system can tolerate at most (N - 1) / 2
+    failures and continue to function normally.
+  
+  Note that, in an HA cluster, the Standby NameNode also performs checkpoints of
+  the namespace state, and thus it is not necessary to run a Secondary NameNode,
+  CheckpointNode, or BackupNode in an HA cluster. In fact, to do so would be an
+  error. This also allows one who is reconfiguring a non-HA-enabled HDFS cluster
+  to be HA-enabled to reuse the hardware which they had previously dedicated to
+  the Secondary NameNode.
+
+* {Deployment}
+
+** Configuration overview
+
+  Similar to Federation configuration, HA configuration is backward compatible
+  and allows existing single NameNode configurations to work without change.
+  The new configuration is designed such that all the nodes in the cluster may
+  have the same configuration without the need for deploying different
+  configuration files to different machines based on the type of the node.
+ 
+  Like HDFS Federation, HA clusters reuse the <<<nameservice ID>>> to identify a
+  single HDFS instance that may in fact consist of multiple HA NameNodes. In
+  addition, a new abstraction called <<<NameNode ID>>> is added with HA. Each
+  distinct NameNode in the cluster has a different NameNode ID to distinguish it.
+  To support a single configuration file for all of the NameNodes, the relevant
+  configuration parameters are suffixed with the <<nameservice ID>> as well as
+  the <<NameNode ID>>.
+
+** Configuration details
+
+  To configure HA NameNodes, you must add several configuration options to your
+  <<hdfs-site.xml>> configuration file.
+
+  The order in which you set these configurations is unimportant, but the values
+  you choose for <<dfs.nameservices>> and
+  <<dfs.ha.namenodes.[nameservice ID]>> will determine the keys of those that
+  follow. Thus, you should decide on these values before setting the rest of the
+  configuration options.
+
+  * <<dfs.nameservices>> - the logical name for this new nameservice
+
+    Choose a logical name for this nameservice, for example "mycluster", and use
+    this logical name for the value of this config option. The name you choose is
+    arbitrary. It will be used both for configuration and as the authority
+    component of absolute HDFS paths in the cluster.
+
+    <<Note:>> If you are also using HDFS Federation, this configuration setting
+    should also include the list of other nameservices, HA or otherwise, as a
+    comma-separated list.
+
+----
+<property>
+  <name>dfs.nameservices</name>
+  <value>mycluster</value>
+</property>
+----
+
+  * <<dfs.ha.namenodes.[nameservice ID]>> - unique identifiers for each NameNode in the nameservice
+
+    Configure with a list of comma-separated NameNode IDs. This will be used by
+    DataNodes to determine all the NameNodes in the cluster. For example, if you
+    used "mycluster" as the nameservice ID previously, and you wanted to use "nn1"
+    and "nn2" as the individual IDs of the NameNodes, you would configure this as
+    such:
+
+----
+<property>
+  <name>dfs.ha.namenodes.mycluster</name>
+  <value>nn1,nn2</value>
+</property>
+----
+
+    <<Note:>> Currently, only a maximum of two NameNodes may be configured per
+    nameservice.
+
+  * <<dfs.namenode.rpc-address.[nameservice ID].[name node ID]>> - the fully-qualified RPC address for each NameNode to listen on
+
+    For both of the previously-configured NameNode IDs, set the full address and
+    IPC port of the NameNode processs. Note that this results in two separate
+    configuration options. For example:
+
+----
+<property>
+  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
+  <value>machine1.example.com:8020</value>
+</property>
+<property>
+  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
+  <value>machine2.example.com:8020</value>
+</property>
+----
+
+    <<Note:>> You may similarly configure the "<<servicerpc-address>>" setting if
+    you so desire.
+
+  * <<dfs.namenode.http-address.[nameservice ID].[name node ID]>> - the fully-qualified HTTP address for each NameNode to listen on
+
+    Similarly to <rpc-address> above, set the addresses for both NameNodes' HTTP
+    servers to listen on. For example:
+
+----
+<property>
+  <name>dfs.namenode.http-address.mycluster.nn1</name>
+  <value>machine1.example.com:50070</value>
+</property>
+<property>
+  <name>dfs.namenode.http-address.mycluster.nn2</name>
+  <value>machine2.example.com:50070</value>
+</property>
+----
+
+    <<Note:>> If you have Hadoop's security features enabled, you should also set
+    the <https-address> similarly for each NameNode.
+
+  * <<dfs.namenode.shared.edits.dir>> - the URI which identifies the group of JNs where the NameNodes will write/read edits
+
+    This is where one configures the addresses of the JournalNodes which provide
+    the shared edits storage, written to by the Active nameNode and read by the
+    Standby NameNode to stay up-to-date with all the file system changes the Active
+    NameNode makes. Though you must specify several JournalNode addresses,
+    <<you should only configure one of these URIs.>> The URI should be of the form:
+    "qjournal://<host1:port1>;<host2:port2>;<host3:port3>/<journalId>". The Journal
+    ID is a unique identifier for this nameservice, which allows a single set of
+    JournalNodes to provide storage for multiple federated namesystems. Though not
+    a requirement, it's a good idea to reuse the nameservice ID for the journal
+    identifier.
+
+    For example, if the JournalNodes for this cluster were running on the
+    machines "node1.example.com", "node2.example.com", and "node3.example.com" and
+    the nameservice ID were "mycluster", you would use the following as the value
+    for this setting (the default port for the JournalNode is 8485):
+
+----
+<property>
+  <name>dfs.namenode.shared.edits.dir</name>
+  <value>qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/mycluster</value>
+</property>
+----
+
+  * <<dfs.client.failover.proxy.provider.[nameservice ID]>> - the Java class that HDFS clients use to contact the Active NameNode
+
+    Configure the name of the Java class which will be used by the DFS Client to
+    determine which NameNode is the current Active, and therefore which NameNode is
+    currently serving client requests. The only implementation which currently
+    ships with Hadoop is the <<ConfiguredFailoverProxyProvider>>, so use this
+    unless you are using a custom one. For example:
+
+----
+<property>
+  <name>dfs.client.failover.proxy.provider.mycluster</name>
+  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
+</property>
+----
+
+  * <<dfs.ha.fencing.methods>> - a list of scripts or Java classes which will be used to fence the Active NameNode during a failover
+
+    It is desirable for correctness of the system that only one NameNode be in
+    the Active state at any given time. <<Importantly, when using the Quorum
+    Journal Manager, only one NameNode will ever be allowed to write to the
+    JournalNodes, so there is no potential for corrupting the file system metadata
+    from a split-brain scenario.>> However, when a failover occurs, it is still
+    possible that the previous Active NameNode could serve read requests to
+    clients, which may be out of date until that NameNode shuts down when trying to
+    write to the JournalNodes. For this reason, it is still desirable to configure
+    some fencing methods even when using the Quorum Journal Manager. However, to
+    improve the availability of the system in the event the fencing mechanisms
+    fail, it is advisable to configure a fencing method which is guaranteed to
+    return success as the last fencing method in the list. Note that if you choose
+    to use no actual fencing methods, you still must configure something for this
+    setting, for example "<<<shell(/bin/true)>>>".
+
+    The fencing methods used during a failover are configured as a
+    carriage-return-separated list, which will be attempted in order until one
+    indicates that fencing has succeeded. There are two methods which ship with
+    Hadoop: <shell> and <sshfence>. For information on implementing your own custom
+    fencing method, see the <org.apache.hadoop.ha.NodeFencer> class.
+
+    * <<sshfence>> - SSH to the Active NameNode and kill the process
+
+      The <sshfence> option SSHes to the target node and uses <fuser> to kill the
+      process listening on the service's TCP port. In order for this fencing option
+      to work, it must be able to SSH to the target node without providing a
+      passphrase. Thus, one must also configure the
+      <<dfs.ha.fencing.ssh.private-key-files>> option, which is a
+      comma-separated list of SSH private key files. For example:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>sshfence</value>
+</property>
+
+<property>
+  <name>dfs.ha.fencing.ssh.private-key-files</name>
+  <value>/home/exampleuser/.ssh/id_rsa</value>
+</property>
+---
+
+      Optionally, one may configure a non-standard username or port to perform the
+      SSH. One may also configure a timeout, in milliseconds, for the SSH, after
+      which this fencing method will be considered to have failed. It may be
+      configured like so:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>sshfence([[username][:port]])</value>
+</property>
+<property>
+  <name>dfs.ha.fencing.ssh.connect-timeout</name>
+  <value>30000</value>
+</property>
+---
+
+    * <<shell>> - run an arbitrary shell command to fence the Active NameNode
+
+      The <shell> fencing method runs an arbitrary shell command. It may be
+      configured like so:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>shell(/path/to/my/script.sh arg1 arg2 ...)</value>
+</property>
+---
+
+      The string between '(' and ')' is passed directly to a bash shell and may not
+      include any closing parentheses.
+
+      The shell command will be run with an environment set up to contain all of the
+      current Hadoop configuration variables, with the '_' character replacing any
+      '.' characters in the configuration keys. The configuration used has already had
+      any namenode-specific configurations promoted to their generic forms -- for example
+      <<dfs_namenode_rpc-address>> will contain the RPC address of the target node, even
+      though the configuration may specify that variable as
+      <<dfs.namenode.rpc-address.ns1.nn1>>.
+      
+      Additionally, the following variables referring to the target node to be fenced
+      are also available:
+
+*-----------------------:-----------------------------------+
+| $target_host          | hostname of the node to be fenced |
+*-----------------------:-----------------------------------+
+| $target_port          | IPC port of the node to be fenced |
+*-----------------------:-----------------------------------+
+| $target_address       | the above two, combined as host:port |
+*-----------------------:-----------------------------------+
+| $target_nameserviceid | the nameservice ID of the NN to be fenced |
+*-----------------------:-----------------------------------+
+| $target_namenodeid    | the namenode ID of the NN to be fenced |
+*-----------------------:-----------------------------------+
+      
+      These environment variables may also be used as substitutions in the shell
+      command itself. For example:
+
+---
+<property>
+  <name>dfs.ha.fencing.methods</name>
+  <value>shell(/path/to/my/script.sh --nameservice=$target_nameserviceid $target_host:$target_port)</value>
+</property>
+---
+      
+      If the shell command returns an exit
+      code of 0, the fencing is determined to be successful. If it returns any other
+      exit code, the fencing was not successful and the next fencing method in the
+      list will be attempted.
+
+      <<Note:>> This fencing method does not implement any timeout. If timeouts are
+      necessary, they should be implemented in the shell script itself (eg by forking
+      a subshell to kill its parent in some number of seconds).
+
+  * <<fs.defaultFS>> - the default path prefix used by the Hadoop FS client when none is given
+
+    Optionally, you may now configure the default path for Hadoop clients to use
+    the new HA-enabled logical URI. If you used "mycluster" as the nameservice ID
+    earlier, this will be the value of the authority portion of all of your HDFS
+    paths. This may be configured like so, in your <<core-site.xml>> file:
+
+---
+<property>
+  <name>fs.defaultFS</name>
+  <value>hdfs://mycluster</value>
+</property>
+---
+
+
+  * <<dfs.journalnode.edits.dir>> - the path where the JournalNode daemon will store its local state
+
+    This is the absolute path on the JournalNode machines where the edits and
+    other local state used by the JNs will be stored. You may only use a single
+    path for this configuration. Redundancy for this data is provided by running
+    multiple separate JournalNodes, or by configuring this directory on a
+    locally-attached RAID array. For example:
+
+---
+<property>
+  <name>dfs.journalnode.edits.dir</name>
+  <value>/path/to/journal/node/local/data</value>
+</property>
+---
+
+** Deployment details
+
+  After all of the necessary configuration options have been set, you must
+  start the JournalNode daemons on the set of machines where they will run. This
+  can be done by running the command "<hdfs-daemon.sh journalnode>" and waiting
+  for the daemon to start on each of the relevant machines.
+
+  Once the JournalNodes have been started, one must initially synchronize the
+  two HA NameNodes' on-disk metadata.
+
+    * If you are setting up a fresh HDFS cluster, you should first run the format
+    command (<hdfs namenode -format>) on one of NameNodes.
+  
+    * If you have already formatted the NameNode, or are converting a
+    non-HA-enabled cluster to be HA-enabled, you should now copy over the
+    contents of your NameNode metadata directories to the other, unformatted
+    NameNode by running the command "<hdfs namenode -bootstrapStandby>" on the
+    unformatted NameNode. Running this command will also ensure that the
+    JournalNodes (as configured by <<dfs.namenode.shared.edits.dir>>) contain
+    sufficient edits transactions to be able to start both NameNodes.
+  
+    * If you are converting a non-HA NameNode to be HA, you should run the
+    command "<hdfs -initializeSharedEdits>", which will initialize the
+    JournalNodes with the edits data from the local NameNode edits directories.
+
+  At this point you may start both of your HA NameNodes as you normally would
+  start a NameNode.
+
+  You can visit each of the NameNodes' web pages separately by browsing to their
+  configured HTTP addresses. You should notice that next to the configured
+  address will be the HA state of the NameNode (either "standby" or "active".)
+  Whenever an HA NameNode starts, it is initially in the Standby state.
+
+** Administrative commands
+
+  Now that your HA NameNodes are configured and started, you will have access
+  to some additional commands to administer your HA HDFS cluster. Specifically,
+  you should familiarize yourself with all of the subcommands of the "<hdfs
+  haadmin>" command. Running this command without any additional arguments will
+  display the following usage information:
+
+---
+Usage: DFSHAAdmin [-ns <nameserviceId>]
+    [-transitionToActive <serviceId>]
+    [-transitionToStandby <serviceId>]
+    [-failover [--forcefence] [--forceactive] <serviceId> <serviceId>]
+    [-getServiceState <serviceId>]
+    [-checkHealth <serviceId>]
+    [-help <command>]
+---
+
+  This guide describes high-level uses of each of these subcommands. For
+  specific usage information of each subcommand, you should run "<hdfs haadmin
+  -help <command>>".
+
+  * <<transitionToActive>> and <<transitionToStandby>> - transition the state of the given NameNode to Active or Standby
+
+    These subcommands cause a given NameNode to transition to the Active or Standby
+    state, respectively. <<These commands do not attempt to perform any fencing,
+    and thus should rarely be used.>> Instead, one should almost always prefer to
+    use the "<hdfs haadmin -failover>" subcommand.
+
+  * <<failover>> - initiate a failover between two NameNodes
+
+    This subcommand causes a failover from the first provided NameNode to the
+    second. If the first NameNode is in the Standby state, this command simply
+    transitions the second to the Active state without error. If the first NameNode
+    is in the Active state, an attempt will be made to gracefully transition it to
+    the Standby state. If this fails, the fencing methods (as configured by
+    <<dfs.ha.fencing.methods>>) will be attempted in order until one
+    succeeds. Only after this process will the second NameNode be transitioned to
+    the Active state. If no fencing method succeeds, the second NameNode will not
+    be transitioned to the Active state, and an error will be returned.
+
+  * <<getServiceState>> - determine whether the given NameNode is Active or Standby
+
+    Connect to the provided NameNode to determine its current state, printing
+    either "standby" or "active" to STDOUT appropriately. This subcommand might be
+    used by cron jobs or monitoring scripts which need to behave differently based
+    on whether the NameNode is currently Active or Standby.
+
+  * <<checkHealth>> - check the health of the given NameNode
+
+    Connect to the provided NameNode to check its health. The NameNode is capable
+    of performing some diagnostics on itself, including checking if internal
+    services are running as expected. This command will return 0 if the NameNode is
+    healthy, non-zero otherwise. One might use this command for monitoring
+    purposes.
+
+    <<Note:>> This is not yet implemented, and at present will always return
+    success, unless the given NameNode is completely down.
+
+* {Automatic Failover}
+
+** Introduction
+
+  The above sections describe how to configure manual failover. In that mode,
+  the system will not automatically trigger a failover from the active to the
+  standby NameNode, even if the active node has failed. This section describes
+  how to configure and deploy automatic failover.
+
+** Components
+
+  Automatic failover adds two new components to an HDFS deployment: a ZooKeeper
+  quorum, and the ZKFailoverController process (abbreviated as ZKFC).
+
+  Apache ZooKeeper is a highly available service for maintaining small amounts
+  of coordination data, notifying clients of changes in that data, and
+  monitoring clients for failures. The implementation of automatic HDFS failover
+  relies on ZooKeeper for the following things:
+  
+    * <<Failure detection>> - each of the NameNode machines in the cluster
+    maintains a persistent session in ZooKeeper. If the machine crashes, the
+    ZooKeeper session will expire, notifying the other NameNode that a failover
+    should be triggered.
+
+    * <<Active NameNode election>> - ZooKeeper provides a simple mechanism to
+    exclusively elect a node as active. If the current active NameNode crashes,
+    another node may take a special exclusive lock in ZooKeeper indicating that
+    it should become the next active.
+
+  The ZKFailoverController (ZKFC) is a new component which is a ZooKeeper client
+  which also monitors and manages the state of the NameNode.  Each of the
+  machines which runs a NameNode also runs a ZKFC, and that ZKFC is responsible
+  for:
+
+    * <<Health monitoring>> - the ZKFC pings its local NameNode on a periodic
+    basis with a health-check command. So long as the NameNode responds in a
+    timely fashion with a healthy status, the ZKFC considers the node
+    healthy. If the node has crashed, frozen, or otherwise entered an unhealthy
+    state, the health monitor will mark it as unhealthy.
+
+    * <<ZooKeeper session management>> - when the local NameNode is healthy, the
+    ZKFC holds a session open in ZooKeeper. If the local NameNode is active, it
+    also holds a special "lock" znode. This lock uses ZooKeeper's support for
+    "ephemeral" nodes; if the session expires, the lock node will be
+    automatically deleted.
+
+    * <<ZooKeeper-based election>> - if the local NameNode is healthy, and the
+    ZKFC sees that no other node currently holds the lock znode, it will itself
+    try to acquire the lock. If it succeeds, then it has "won the election", and
+    is responsible for running a failover to make its local NameNode active. The
+    failover process is similar to the manual failover described above: first,
+    the previous active is fenced if necessary, and then the local NameNode
+    transitions to active state.
+
+  For more details on the design of automatic failover, refer to the design
+  document attached to HDFS-2185 on the Apache HDFS JIRA.
+
+** Deploying ZooKeeper
+
+  In a typical deployment, ZooKeeper daemons are configured to run on three or
+  five nodes. Since ZooKeeper itself has light resource requirements, it is
+  acceptable to collocate the ZooKeeper nodes on the same hardware as the HDFS
+  NameNode and Standby Node. Many operators choose to deploy the third ZooKeeper
+  process on the same node as the YARN ResourceManager. It is advisable to
+  configure the ZooKeeper nodes to store their data on separate disk drives from
+  the HDFS metadata for best performance and isolation.
+
+  The setup of ZooKeeper is out of scope for this document. We will assume that
+  you have set up a ZooKeeper cluster running on three or more nodes, and have
+  verified its correct operation by connecting using the ZK CLI.
+
+** Before you begin
+
+  Before you begin configuring automatic failover, you should shut down your
+  cluster. It is not currently possible to transition from a manual failover
+  setup to an automatic failover setup while the cluster is running.
+
+** Configuring automatic failover
+
+  The configuration of automatic failover requires the addition of two new
+  parameters to your configuration. In your <<<hdfs-site.xml>>> file, add:
+
+----
+ <property>
+   <name>dfs.ha.automatic-failover.enabled</name>
+   <value>true</value>
+ </property>
+----
+
+  This specifies that the cluster should be set up for automatic failover.
+  In your <<<core-site.xml>>> file, add:
+
+----
+ <property>
+   <name>ha.zookeeper.quorum</name>
+   <value>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181</value>
+ </property>
+----
+
+  This lists the host-port pairs running the ZooKeeper service.
+
+  As with the parameters described earlier in the document, these settings may
+  be configured on a per-nameservice basis by suffixing the configuration key
+  with the nameservice ID. For example, in a cluster with federation enabled,
+  you can explicitly enable automatic failover for only one of the nameservices
+  by setting <<<dfs.ha.automatic-failover.enabled.my-nameservice-id>>>.
+
+  There are also several other configuration parameters which may be set to
+  control the behavior of automatic failover; however, they are not necessary
+  for most installations. Please refer to the configuration key specific
+  documentation for details.
+
+** Initializing HA state in ZooKeeper
+
+  After the configuration keys have been added, the next step is to initialize
+  required state in ZooKeeper. You can do so by running the following command
+  from one of the NameNode hosts.
+
+----
+$ hdfs zkfc -formatZK
+----
+
+  This will create a znode in ZooKeeper inside of which the automatic failover
+  system stores its data.
+
+** Starting the cluster with <<<start-dfs.sh>>>
+
+  Since automatic failover has been enabled in the configuration, the
+  <<<start-dfs.sh>>> script will now automatically start a ZKFC daemon on any
+  machine that runs a NameNode. When the ZKFCs start, they will automatically
+  select one of the NameNodes to become active.
+
+** Starting the cluster manually
+
+  If you manually manage the services on your cluster, you will need to manually
+  start the <<<zkfc>>> daemon on each of the machines that runs a NameNode. You
+  can start the daemon by running:
+
+----
+$ hadoop-daemon.sh start zkfc
+----
+
+** Securing access to ZooKeeper
+
+  If you are running a secure cluster, you will likely want to ensure that the
+  information stored in ZooKeeper is also secured. This prevents malicious
+  clients from modifying the metadata in ZooKeeper or potentially triggering a
+  false failover.
+
+  In order to secure the information in ZooKeeper, first add the following to
+  your <<<core-site.xml>>> file:
+
+----
+ <property>
+   <name>ha.zookeeper.auth</name>
+   <value>@/path/to/zk-auth.txt</value>
+ </property>
+ <property>
+   <name>ha.zookeeper.acl</name>
+   <value>@/path/to/zk-acl.txt</value>
+ </property>
+----
+
+  Please note the '@' character in these values -- this specifies that the
+  configurations are not inline, but rather point to a file on disk.
+
+  The first configured file specifies a list of ZooKeeper authentications, in
+  the same format as used by the ZK CLI. For example, you may specify something
+  like:
+
+----
+digest:hdfs-zkfcs:mypassword
+----
+  ...where <<<hdfs-zkfcs>>> is a unique username for ZooKeeper, and
+  <<<mypassword>>> is some unique string used as a password.
+
+  Next, generate a ZooKeeper ACL that corresponds to this authentication, using
+  a command like the following:
+
+----
+$ java -cp $ZK_HOME/lib/*:$ZK_HOME/zookeeper-3.4.2.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:mypassword
+output: hdfs-zkfcs:mypassword->hdfs-zkfcs:P/OQvnYyU/nF/mGYvB/xurX8dYs=
+----
+
+  Copy and paste the section of this output after the '->' string into the file
+  <<<zk-acls.txt>>>, prefixed by the string "<<<digest:>>>". For example:
+
+----
+digest:hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=:rwcda
+----
+
+  In order for these ACLs to take effect, you should then rerun the
+  <<<zkfc -formatZK>>> command as described above.
+
+  After doing so, you may verify the ACLs from the ZK CLI as follows:
+
+----
+[zk: localhost:2181(CONNECTED) 1] getAcl /hadoop-ha
+'digest,'hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=
+: cdrwa
+----
+
+** Verifying automatic failover
+
+  Once automatic failover has been set up, you should test its operation. To do
+  so, first locate the active NameNode. You can tell which node is active by
+  visiting the NameNode web interfaces -- each node reports its HA state at the
+  top of the page.
+
+  Once you have located your active NameNode, you may cause a failure on that
+  node.  For example, you can use <<<kill -9 <pid of NN>>>> to simulate a JVM
+  crash. Or, you could power cycle the machine or unplug its network interface
+  to simulate a different kind of outage.  After triggering the outage you wish
+  to test, the other NameNode should automatically become active within several
+  seconds. The amount of time required to detect a failure and trigger a
+  fail-over depends on the configuration of
+  <<<ha.zookeeper.session-timeout.ms>>>, but defaults to 5 seconds.
+
+  If the test does not succeed, you may have a misconfiguration. Check the logs
+  for the <<<zkfc>>> daemons as well as the NameNode daemons in order to further
+  diagnose the issue.
+
+
+* Automatic Failover FAQ
+
+  * <<Is it important that I start the ZKFC and NameNode daemons in any
+    particular order?>>
+
+  No. On any given node you may start the ZKFC before or after its corresponding
+  NameNode.
+
+  * <<What additional monitoring should I put in place?>>
+
+  You should add monitoring on each host that runs a NameNode to ensure that the
+  ZKFC remains running. In some types of ZooKeeper failures, for example, the
+  ZKFC may unexpectedly exit, and should be restarted to ensure that the system
+  is ready for automatic failover.
+
+  Additionally, you should monitor each of the servers in the ZooKeeper
+  quorum. If ZooKeeper crashes, then automatic failover will not function.
+
+  * <<What happens if ZooKeeper goes down?>>
+
+  If the ZooKeeper cluster crashes, no automatic failovers will be triggered.
+  However, HDFS will continue to run without any impact. When ZooKeeper is
+  restarted, HDFS will reconnect with no issues.
+
+  * <<Can I designate one of my NameNodes as primary/preferred?>>
+
+  No. Currently, this is not supported. Whichever NameNode is started first will
+  become active. You may choose to start the cluster in a specific order such
+  that your preferred node starts first.
+
+  * <<How can I initiate a manual failover when automatic failover is
+    configured?>>
+
+  Even if automatic failover is configured, you may initiate a manual failover
+  using the same <<<hdfs haadmin>>> command. It will perform a coordinated
+  failover.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/WebHDFS.apt.vm b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/WebHDFS.apt.vm
new file mode 100644
index 0000000..38b8dc8
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/WebHDFS.apt.vm
@@ -0,0 +1,1769 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop Distributed File System-${project.version} - WebHDFS REST API
+  ---
+  ---
+  ${maven.build.timestamp}
+
+WebHDFS REST API
+
+  \[ {{{./index.html}Go Back}} \]
+
+%{toc|section=1|fromDepth=0}
+
+* {Document Conventions}
+
+*----------------------+-------------------------------------------------------------------------------+
+| <<<Monospaced>>>     | Used for commands, HTTP request and responses and code blocks.                |
+*----------------------+-------------------------------------------------------------------------------+
+| <<<\<Monospaced\>>>> | User entered values.                                                          |
+*----------------------+-------------------------------------------------------------------------------+
+| <<<[Monospaced]>>>   | Optional values.  When the value is not specified, the default value is used. |
+*----------------------+-------------------------------------------------------------------------------+
+| <Italics>            | Important phrases and words.                                                  |
+*----------------------+-------------------------------------------------------------------------------+
+
+
+* {Introduction}
+
+  The HTTP REST API supports the complete
+  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}/{{{../../api/org/apache/hadoop/fs/FileContext.html}FileContext}}
+  interface for HDFS.
+  The operations and the corresponding FileSystem/FileContext methods are shown in the next section.
+  The Section {{HTTP Query Parameter Dictionary}} specifies the parameter details
+  such as the defaults and the valid values.
+
+** {Operations}
+
+  * HTTP GET
+
+    * {{{Open and Read a File}<<<OPEN>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.open)
+
+    * {{{Status of a File/Directory}<<<GETFILESTATUS>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileStatus)
+
+    * {{<<<LISTSTATUS>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.listStatus)
+
+    * {{{Get Content Summary of a Directory}<<<GETCONTENTSUMMARY>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getContentSummary)
+
+    * {{{Get File Checksum}<<<GETFILECHECKSUM>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileChecksum)
+
+    * {{{Get Home Directory}<<<GETHOMEDIRECTORY>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getHomeDirectory)
+
+    * {{{Get Delegation Token}<<<GETDELEGATIONTOKEN>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationToken)
+
+    * {{{Get Delegation Tokens}<<<GETDELEGATIONTOKENS>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationTokens)
+
+  * HTTP PUT
+
+    * {{{Create and Write to a File}<<<CREATE>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.create)
+
+    * {{{Make a Directory}<<<MKDIRS>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.mkdirs)
+
+    * {{{Create a Symbolic Link}<<<CREATESYMLINK>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileContext.html}FileContext}}.createSymlink)
+
+    * {{{Rename a File/Directory}<<<RENAME>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.rename)
+
+    * {{{Set Replication Factor}<<<SETREPLICATION>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setReplication)
+
+    * {{{Set Owner}<<<SETOWNER>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setOwner)
+
+    * {{{Set Permission}<<<SETPERMISSION>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setPermission)
+
+    * {{{Set Access or Modification Time}<<<SETTIMES>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setTimes)
+
+    * {{{Renew Delegation Token}<<<RENEWDELEGATIONTOKEN>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.renewDelegationToken)
+
+    * {{{Cancel Delegation Token}<<<CANCELDELEGATIONTOKEN>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.cancelDelegationToken)
+
+  * HTTP POST
+
+    * {{{Append to a File}<<<APPEND>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.append)
+
+  * HTTP DELETE
+
+    * {{{Delete a File/Directory}<<<DELETE>>>}}
+        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.delete)
+
+** {FileSystem URIs vs HTTP URLs}
+
+  The FileSystem scheme of WebHDFS is "<<<webhdfs://>>>".
+  A WebHDFS FileSystem URI has the following format.
+
++---------------------------------
+  webhdfs://<HOST>:<HTTP_PORT>/<PATH>
++---------------------------------
+
+  The above WebHDFS URI corresponds to the below HDFS URI.
+
++---------------------------------
+  hdfs://<HOST>:<RPC_PORT>/<PATH>
++---------------------------------
+
+  In the REST API, the prefix "<<</webhdfs/v1>>>" is inserted in the path and a query is appended at the end.
+  Therefore, the corresponding HTTP URL has the following format.
+
++---------------------------------
+  http://<HOST>:<HTTP_PORT>/webhdfs/v1/<PATH>?op=...
++---------------------------------
+
+** {HDFS Configuration Options}
+
+  Below are the HDFS configuration options for WebHDFS.
+
+*-------------------------------------------------+---------------------------------------------------+
+|| Property Name	                          || Description                                      |
+*-------------------------------------------------+---------------------------------------------------+
+| <<<dfs.webhdfs.enabled                      >>> | Enable/disable WebHDFS in Namenodes and Datanodes |
+*-------------------------------------------------+---------------------------------------------------+
+| <<<dfs.web.authentication.kerberos.principal>>> | The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. The HTTP Kerberos principal MUST start with 'HTTP/' per Kerberos HTTP SPNEGO specification. |
+*-------------------------------------------------+---------------------------------------------------+
+| <<<dfs.web.authentication.kerberos.keytab   >>> | The Kerberos keytab file with the credentials for the HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. |
+*-------------------------------------------------+---------------------------------------------------+
+
+* {Authentication}
+
+  When security is <off>, the authenticated user is the username specified in the <<<user.name>>> query parameter.
+  If the <<<user.name>>> parameter is not set,
+  the server may either set the authenticated user to a default web user, if there is any, or return an error response.
+
+
+  When security is <on>, authentication is performed by either Hadoop delegation token or Kerberos SPNEGO.
+  If a token is set in the <<<delegation>>> query parameter, the authenticated user is the user encoded in the token.
+  If the <<<delegation>>> parameter is not set, the user is authenticated by Kerberos SPNEGO.
+
+
+  Below are examples using the <<<curl>>> command tool.
+
+  [[1]] Authentication when security is off:
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?[user.name=<USER>&]op=..."
++---------------------------------
+ 
+  [[1]] Authentication using Kerberos SPNEGO when security is on:
+
++---------------------------------
+curl -i --negotiate -u : "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=..."
++---------------------------------
+ 
+  [[1]] Authentication using Hadoop delegation token when security is on:
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?delegation=<TOKEN>&op=..."
++---------------------------------
+
+* {Proxy Users}
+
+  When the proxy user feature is enabled, a proxy user <P> may submit a request on behalf of another user <U>.
+  The username of <U> must be specified in the <<<doas>>> query parameter unless a delegation token is presented in authentication.
+  In such case, the information of both users <P> and <U> must be encoded in the delegation token.
+
+  [[1]] A proxy request when security is off:
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?[user.name=<USER>&]doas=<USER>&op=..."
++---------------------------------
+
+  [[1]] A proxy request using Kerberos SPNEGO when security is on:
+
++---------------------------------
+curl -i --negotiate -u : "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?doas=<USER>&op=..."
++---------------------------------
+
+  [[1]] A proxy request using Hadoop delegation token when security is on:
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?delegation=<TOKEN>&op=..."
++---------------------------------
+
+
+* {File and Directory Operations}
+
+** {Create and Write to a File}
+
+  * Step 1: Submit a HTTP PUT request without automatically following redirects and without sending the file data.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=CREATE
+                    [&overwrite=<true|false>][&blocksize=<LONG>][&replication=<SHORT>]
+                    [&permission=<OCTAL>][&buffersize=<INT>]"
++---------------------------------
+
+  The request is redirected to a datanode where the file data is to be written:
+
++---------------------------------
+HTTP/1.1 307 TEMPORARY_REDIRECT
+Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=CREATE...
+Content-Length: 0
++---------------------------------
+
+  * Step 2: Submit another HTTP PUT request using the URL in the <<<Location>>> header with the file data to be written.
+
++---------------------------------
+curl -i -X PUT -T <LOCAL_FILE> "http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=CREATE..."
++---------------------------------
+
+  The client receives a <<<201 Created>>> response with zero content length
+  and the WebHDFS URI of the file in the <<<Location>>> header:
+
++---------------------------------
+HTTP/1.1 201 Created
+Location: webhdfs://<HOST>:<PORT>/<PATH>
+Content-Length: 0
++---------------------------------
+
+  []
+
+  <<Note>> that the reason of having two-step create/append is
+  for preventing clients to send out data before the redirect.
+  This issue is addressed by the "<<<Expect: 100-continue>>>" header in HTTP/1.1;
+  see {{{http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.2.3}RFC 2616, Section 8.2.3}}.
+  Unfortunately, there are software library bugs (e.g. Jetty 6 HTTP server and Java 6 HTTP client),
+  which do not correctly implement "<<<Expect: 100-continue>>>".
+  The two-step create/append is a temporary workaround for the software library bugs.
+
+  See also:
+  {{{Overwrite}<<<overwrite>>>}},
+  {{{Block Size}<<<blocksize>>>}},
+  {{{Replication}<<<replication>>>}},
+  {{{Permission}<<<permission>>>}},
+  {{{Buffer Size}<<<buffersize>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.create
+
+
+** {Append to a File}
+
+  * Step 1: Submit a HTTP POST request without automatically following redirects and without sending the file data.
+
++---------------------------------
+curl -i -X POST "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=APPEND[&buffersize=<INT>]"
++---------------------------------
+
+  The request is redirected to a datanode where the file data is to be appended:
+
++---------------------------------
+HTTP/1.1 307 TEMPORARY_REDIRECT
+Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=APPEND...
+Content-Length: 0
++---------------------------------
+
+  * Step 2: Submit another HTTP POST request using the URL in the <<<Location>>> header with the file data to be appended.
+
++---------------------------------
+curl -i -X POST -T <LOCAL_FILE> "http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=APPEND..."
++---------------------------------
+
+  The client receives a response with zero content length:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Length: 0
++---------------------------------
+
+  []
+
+  See the note in the previous section for the description of why this operation requires two steps.
+
+  See also:
+  {{{Buffer Size}<<<buffersize>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.append
+
+
+** {Open and Read a File}
+
+  * Submit a HTTP GET request with automatically following redirects.
+
++---------------------------------
+curl -i -L "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=OPEN
+                    [&offset=<LONG>][&length=<LONG>][&buffersize=<INT>]"
++---------------------------------
+
+  The request is redirected to a datanode where the file data can be read:
+
++---------------------------------
+HTTP/1.1 307 TEMPORARY_REDIRECT
+Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=OPEN...
+Content-Length: 0
++---------------------------------
+
+  The client follows the redirect to the datanode and receives the file data:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/octet-stream
+Content-Length: 22
+
+Hello, webhdfs user!
++---------------------------------
+
+  []
+
+  See also:
+  {{{Offset}<<<offset>>>}},
+  {{{Length}<<<length>>>}},
+  {{{Buffer Size}<<<buffersize>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.open
+
+
+** {Make a Directory}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/<PATH>?op=MKDIRS[&permission=<OCTAL>]"
++---------------------------------
+
+  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{"boolean": true}
++---------------------------------
+
+  []
+
+  See also:
+  {{{Permission}<<<permission>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.mkdirs
+
+
+** {Create a Symbolic Link}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/<PATH>?op=CREATESYMLINK
+                              &destination=<PATH>[&createParent=<true|false>]"
++---------------------------------
+
+  The client receives a response with zero content length:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Length: 0
++---------------------------------
+
+  []
+
+  See also:
+  {{{Destination}<<<destination>>>}},
+  {{{Create Parent}<<<createParent>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.createSymlink
+
+
+** {Rename a File/Directory}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "<HOST>:<PORT>/webhdfs/v1/<PATH>?op=RENAME&destination=<PATH>"
++---------------------------------
+
+  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{"boolean": true}
++---------------------------------
+
+  []
+
+  See also:
+  {{{Destination}<<<destination>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.rename
+
+
+** {Delete a File/Directory}
+
+  * Submit a HTTP DELETE request.
+
++---------------------------------
+curl -i -X DELETE "http://<host>:<port>/webhdfs/v1/<path>?op=DELETE
+                              [&recursive=<true|false>]"
++---------------------------------
+
+  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{"boolean": true}
++---------------------------------
+
+  []
+
+  See also:
+  {{{Recursive}<<<recursive>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.delete
+
+
+** {Status of a File/Directory}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i  "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=GETFILESTATUS"
++---------------------------------
+
+  The client receives a response with a {{{FileStatus JSON Schema}<<<FileStatus>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "FileStatus":
+  {
+    "accessTime"      : 0,
+    "blockSize"       : 0,
+    "group"           : "supergroup",
+    "length"          : 0,             //in bytes, zero for directories
+    "modificationTime": 1320173277227,
+    "owner"           : "webuser",
+    "pathSuffix"      : "",
+    "permission"      : "777",
+    "replication"     : 0,
+    "type"            : "DIRECTORY"    //enum {FILE, DIRECTORY, SYMLINK}
+  }
+}
++---------------------------------
+
+  []
+
+  See also:
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileStatus
+
+
+** {List a Directory}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i  "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=LISTSTATUS"
++---------------------------------
+
+  The client receives a response with a {{{FileStatuses JSON Schema}<<<FileStatuses>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Content-Length: 427
+
+{
+  "FileStatuses":
+  {
+    "FileStatus":
+    [
+      {
+        "accessTime"      : 1320171722771,
+        "blockSize"       : 33554432,
+        "group"           : "supergroup",
+        "length"          : 24930,
+        "modificationTime": 1320171722771,
+        "owner"           : "webuser",
+        "pathSuffix"      : "a.patch",
+        "permission"      : "644",
+        "replication"     : 1,
+        "type"            : "FILE"
+      },
+      {
+        "accessTime"      : 0,
+        "blockSize"       : 0,
+        "group"           : "supergroup",
+        "length"          : 0,
+        "modificationTime": 1320895981256,
+        "owner"           : "szetszwo",
+        "pathSuffix"      : "bar",
+        "permission"      : "711",
+        "replication"     : 0,
+        "type"            : "DIRECTORY"
+      },
+      ...
+    ]
+  }
+}
++---------------------------------
+
+  []
+
+  See also:
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.listStatus
+
+
+* {Other File System Operations}
+
+** {Get Content Summary of a Directory}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=GETCONTENTSUMMARY"
++---------------------------------
+
+  The client receives a response with a {{{ContentSummary JSON Schema}<<<ContentSummary>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "ContentSummary":
+  {
+    "directoryCount": 2,
+    "fileCount"     : 1,
+    "length"        : 24930,
+    "quota"         : -1,
+    "spaceConsumed" : 24930,
+    "spaceQuota"    : -1
+  }
+}
++---------------------------------
+
+  []
+
+  See also:
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getContentSummary
+
+
+** {Get File Checksum}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=GETFILECHECKSUM"
++---------------------------------
+
+  The request is redirected to a datanode:
+
++---------------------------------
+HTTP/1.1 307 TEMPORARY_REDIRECT
+Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=GETFILECHECKSUM...
+Content-Length: 0
++---------------------------------
+
+  The client follows the redirect to the datanode and receives a {{{FileChecksum JSON Schema}<<<FileChecksum>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "FileChecksum":
+  {
+    "algorithm": "MD5-of-1MD5-of-512CRC32",
+    "bytes"    : "eadb10de24aa315748930df6e185c0d ...",
+    "length"   : 28
+  }
+}
++---------------------------------
+
+  []
+
+  See also:
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileChecksum
+
+
+** {Get Home Directory}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/?op=GETHOMEDIRECTORY"
++---------------------------------
+
+  The client receives a response with a {{{Path JSON Schema}<<<Path>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{"Path": "/user/szetszwo"}
++---------------------------------
+
+  []
+
+  See also:
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getHomeDirectory
+
+
+** {Set Permission}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETPERMISSION
+                              [&permission=<OCTAL>]"
++---------------------------------
+
+  The client receives a response with zero content length:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Length: 0
++---------------------------------
+
+  []
+
+  See also:
+  {{{Permission}<<<permission>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setPermission
+
+
+** {Set Owner}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETOWNER
+                              [&owner=<USER>][&group=<GROUP>]"
++---------------------------------
+
+  The client receives a response with zero content length:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Length: 0
++---------------------------------
+
+  []
+
+  See also:
+  {{{Owner}<<<owner>>>}},
+  {{{Group}<<<group>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setOwner
+
+
+** {Set Replication Factor}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETREPLICATION
+                              [&replication=<SHORT>]"
++---------------------------------
+
+  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{"boolean": true}
++---------------------------------
+
+  []
+
+  See also:
+  {{{Replication}<<<replication>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setReplication
+
+
+** {Set Access or Modification Time}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETTIMES
+                              [&modificationtime=<TIME>][&accesstime=<TIME>]"
++---------------------------------
+
+  The client receives a response with zero content length:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Length: 0
++---------------------------------
+
+  []
+
+  See also:
+  {{{Modification Time}<<<modificationtime>>>}},
+  {{{Access Time}<<<accesstime>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setTimes
+
+
+* {Delegation Token Operations}
+
+** {Get Delegation Token}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/?op=GETDELEGATIONTOKEN&renewer=<USER>"
++---------------------------------
+
+  The client receives a response with a {{{Token JSON Schema}<<<Token>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "Token":
+  {
+    "urlString": "JQAIaG9y..."
+  }
+}
++---------------------------------
+
+  []
+
+  See also:
+  {{{Renewer}<<<renewer>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationToken
+
+
+** {Get Delegation Tokens}
+
+  * Submit a HTTP GET request.
+
++---------------------------------
+curl -i "http://<HOST>:<PORT>/webhdfs/v1/?op=GETDELEGATIONTOKENS&renewer=<USER>"
++---------------------------------
+
+  The client receives a response with a {{{Tokens JSON Schema}<<<Tokens>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "Tokens":
+  {
+    "Token":
+    [
+      {
+        "urlString":"KAAKSm9i ..."
+      }
+    ]
+  }
+}
++---------------------------------
+
+  []
+
+  See also:
+  {{{Renewer}<<<renewer>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationTokens
+
+
+** {Renew Delegation Token}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&token=<TOKEN>"
++---------------------------------
+
+  The client receives a response with a {{{Long JSON Schema}<<<long>>> JSON object}}:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{"long": 1320962673997}           //the new expiration time
++---------------------------------
+
+  []
+
+  See also:
+  {{{Token}<<<token>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.renewDelegationToken
+
+
+** {Cancel Delegation Token}
+
+  * Submit a HTTP PUT request.
+
++---------------------------------
+curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/?op=CANCELDELEGATIONTOKEN&token=<TOKEN>"
++---------------------------------
+
+  The client receives a response with zero content length:
+
++---------------------------------
+HTTP/1.1 200 OK
+Content-Length: 0
++---------------------------------
+
+  []
+
+  See also:
+  {{{Token}<<<token>>>}},
+   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.cancelDelegationToken
+
+
+* {Error Responses}
+
+  When an operation fails, the server may throw an exception.
+  The JSON schema of error responses is defined in {{<<<RemoteException>>> JSON schema}}.
+  The table below shows the mapping from exceptions to HTTP response codes.
+
+** {HTTP Response Codes}
+
+*-------------------------------------+---------------------------------+
+|| Exceptions                         || HTTP Response Codes            |
+*-------------------------------------+---------------------------------+
+| <<<IllegalArgumentException     >>> | <<<400 Bad Request          >>> |
+*-------------------------------------+---------------------------------+
+| <<<UnsupportedOperationException>>> | <<<400 Bad Request          >>> |
+*-------------------------------------+---------------------------------+
+| <<<SecurityException            >>> | <<<401 Unauthorized         >>> |
+*-------------------------------------+---------------------------------+
+| <<<IOException                  >>> | <<<403 Forbidden            >>> |
+*-------------------------------------+---------------------------------+
+| <<<FileNotFoundException        >>> | <<<404 Not Found            >>> |
+*-------------------------------------+---------------------------------+
+| <<<RumtimeException             >>> | <<<500 Internal Server Error>>> |
+*-------------------------------------+---------------------------------+
+
+  Below are examples of exception responses.
+
+*** {Illegal Argument Exception}
+
++---------------------------------
+HTTP/1.1 400 Bad Request
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "RemoteException":
+  {
+    "exception"    : "IllegalArgumentException",
+    "javaClassName": "java.lang.IllegalArgumentException",
+    "message"      : "Invalid value for webhdfs parameter \"permission\": ..."
+  }
+}
++---------------------------------
+
+
+*** {Security Exception}
+
++---------------------------------
+HTTP/1.1 401 Unauthorized
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "RemoteException":
+  {
+    "exception"    : "SecurityException",
+    "javaClassName": "java.lang.SecurityException",
+    "message"      : "Failed to obtain user group information: ..."
+  }
+}
++---------------------------------
+
+
+*** {Access Control Exception}
+
++---------------------------------
+HTTP/1.1 403 Forbidden
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "RemoteException":
+  {
+    "exception"    : "AccessControlException",
+    "javaClassName": "org.apache.hadoop.security.AccessControlException",
+    "message"      : "Permission denied: ..."
+  }
+}
++---------------------------------
+
+
+*** {File Not Found Exception}
+
++---------------------------------
+HTTP/1.1 404 Not Found
+Content-Type: application/json
+Transfer-Encoding: chunked
+
+{
+  "RemoteException":
+  {
+    "exception"    : "FileNotFoundException",
+    "javaClassName": "java.io.FileNotFoundException",
+    "message"      : "File does not exist: /foo/a.patch"
+  }
+}
++---------------------------------
+
+
+* {JSON Schemas}
+
+  All operations, except for {{{Open and Read a File}<<<OPEN>>>}},
+  either return a zero-length response or a JSON response. 
+  For {{{Open and Read a File}<<<OPEN>>>}}, the response is an octet-stream.
+  The JSON schemas are shown below.
+  See {{{http://tools.ietf.org/id/draft-zyp-json-schema-03.html}draft-zyp-json-schema-03}}
+  for the syntax definitions of the JSON schemas.
+
+
+** {Boolean JSON Schema}
+
++---------------------------------
+{
+  "name"      : "boolean",
+  "properties":
+  {
+    "boolean":
+    {
+      "description": "A boolean value",
+      "type"       : "boolean",
+      "required"   : true
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Make a Directory}<<<MKDIRS>>>}},
+  {{{Rename a File/Directory}<<<RENAME>>>}},
+  {{{Delete a File/Directory}<<<DELETE>>>}},
+  {{{Set Replication Factor}<<<SETREPLICATION>>>}}
+
+
+** {ContentSummary JSON Schema}
+
++---------------------------------
+{
+  "name"      : "ContentSummary",
+  "properties":
+  {
+    "ContentSummary":
+    {
+      "type"      : "object",
+      "properties":
+      {
+        "directoryCount":
+        {
+          "description": "The number of directories.",
+          "type"       : "integer",
+          "required"   : true
+        },
+        "fileCount":
+        {
+          "description": "The number of files.",
+          "type"       : "integer",
+          "required"   : true
+        },
+        "length":
+        {
+          "description": "The number of bytes used by the content.",
+          "type"       : "integer",
+          "required"   : true
+        },
+        "quota":
+        {
+          "description": "The namespace quota of this directory.",
+          "type"       : "integer",
+          "required"   : true
+        },
+        "spaceConsumed":
+        {
+          "description": "The disk space consumed by the content.",
+          "type"       : "integer",
+          "required"   : true
+        },
+        "spaceQuota":
+        {
+          "description": "The disk space quota.",
+          "type"       : "integer",
+          "required"   : true
+        }
+      }
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Get Content Summary of a Directory}<<<GETCONTENTSUMMARY>>>}}
+
+
+** {FileChecksum JSON Schema}
+
++---------------------------------
+{
+  "name"      : "FileChecksum",
+  "properties":
+  {
+    "FileChecksum":
+    {
+      "type"      : "object",
+      "properties":
+      {
+        "algorithm":
+        {
+          "description": "The name of the checksum algorithm.",
+          "type"       : "string",
+          "required"   : true
+        },
+        "bytes":
+        {
+          "description": "The byte sequence of the checksum in hexadecimal.",
+          "type"       : "string",
+          "required"   : true
+        },
+        "length":
+        {
+          "description": "The length of the bytes (not the length of the string).",
+          "type"       : "integer",
+          "required"   : true
+        }
+      }
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Get File Checksum}<<<GETFILECHECKSUM>>>}}
+
+
+** {FileStatus JSON Schema}
+
++---------------------------------
+{
+  "name"      : "FileStatus",
+  "properties":
+  {
+    "FileStatus": fileStatusProperties      //See FileStatus Properties
+  }
+}
++---------------------------------
+
+  See also:
+  {{{FileStatus Properties}<<<FileStatus>>> Properties}},
+  {{{Status of a File/Directory}<<<GETFILESTATUS>>>}},
+  {{{../../api/org/apache/hadoop/fs/FileStatus}FileStatus}}
+
+
+*** {FileStatus Properties}
+
+  JavaScript syntax is used to define <<<fileStatusProperties>>>
+  so that it can be referred in both <<<FileStatus>>> and <<<FileStatuses>>> JSON schemas.
+
++---------------------------------
+var fileStatusProperties =
+{
+  "type"      : "object",
+  "properties":
+  {
+    "accessTime":
+    {
+      "description": "The access time.",
+      "type"       : "integer",
+      "required"   : true
+    },
+    "blockSize":
+    {
+      "description": "The block size of a file.",
+      "type"       : "integer",
+      "required"   : true
+    },
+    "group":
+    {
+      "description": "The group owner.",
+      "type"       : "string",
+      "required"   : true
+    },
+    "length":
+    {
+      "description": "The number of bytes in a file.",
+      "type"       : "integer",
+      "required"   : true
+    },
+    "modificationTime":
+    {
+      "description": "The modification time.",
+      "type"       : "integer",
+      "required"   : true
+    },
+    "owner":
+    {
+      "description": "The user who is the owner.",
+      "type"       : "string",
+      "required"   : true
+    },
+    "pathSuffix":
+    {
+      "description": "The path suffix.",
+      "type"       : "string",
+      "required"   : true
+    },
+    "permission":
+    {
+      "description": "The permission represented as a octal string.",
+      "type"       : "string",
+      "required"   : true
+    },
+    "replication":
+    {
+      "description": "The number of replication of a file.",
+      "type"       : "integer",
+      "required"   : true
+    },
+   "symlink":                                         //an optional property
+    {
+      "description": "The link target of a symlink.",
+      "type"       : "string"
+    },
+   "type":
+    {
+      "description": "The type of the path object.",
+      "enum"       : ["FILE", "DIRECTORY", "SYMLINK"],
+      "required"   : true
+    }
+  }
+};
++---------------------------------
+
+
+** {FileStatuses JSON Schema}
+
+  A <<<FileStatuses>>> JSON object represents an array of <<<FileStatus>>> JSON objects.
+
++---------------------------------
+{
+  "name"      : "FileStatuses",
+  "properties":
+  {
+    "FileStatuses":
+    {
+      "type"      : "object",
+      "properties":
+      {
+        "FileStatus":
+        {
+          "description": "An array of FileStatus",
+          "type"       : "array",
+          "items"      : fileStatusProperties      //See FileStatus Properties
+        }
+      }
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{FileStatus Properties}<<<FileStatus>>> Properties}},
+  {{{List a Directory}<<<LISTSTATUS>>>}},
+  {{{../../api/org/apache/hadoop/fs/FileStatus}FileStatus}}
+
+
+** {Long JSON Schema}
+
++---------------------------------
+{
+  "name"      : "long",
+  "properties":
+  {
+    "long":
+    {
+      "description": "A long integer value",
+      "type"       : "integer",
+      "required"   : true
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Renew Delegation Token}<<<RENEWDELEGATIONTOKEN>>>}},
+
+
+** {Path JSON Schema}
+
++---------------------------------
+{
+  "name"      : "Path",
+  "properties":
+  {
+    "Path":
+    {
+      "description": "The string representation a Path.",
+      "type"       : "string",
+      "required"   : true
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Get Home Directory}<<<GETHOMEDIRECTORY>>>}},
+  {{{../../api/org/apache/hadoop/fs/Path}Path}}
+
+
+** {RemoteException JSON Schema}
+
++---------------------------------
+{
+  "name"      : "RemoteException",
+  "properties":
+  {
+    "RemoteException":
+    {
+      "type"      : "object",
+      "properties":
+      {
+        "exception":
+        {
+          "description": "Name of the exception",
+          "type"       : "string",
+          "required"   : true
+        },
+        "message":
+        {
+          "description": "Exception message",
+          "type"       : "string",
+          "required"   : true
+        },
+        "javaClassName":                                     //an optional property
+        {
+          "description": "Java class name of the exception",
+          "type"       : "string",
+        }
+      }
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{Error Responses}}
+
+
+** {Token JSON Schema}
+
++---------------------------------
+{
+  "name"      : "Token",
+  "properties":
+  {
+    "Token": tokenProperties      //See Token Properties
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Token Properties}<<<Token>>> Properties}},
+  {{{Get Delegation Token}<<<GETDELEGATIONTOKEN>>>}},
+  the note in {{Delegation}}.
+
+*** {Token Properties}
+
+  JavaScript syntax is used to define <<<tokenProperties>>>
+  so that it can be referred in both <<<Token>>> and <<<Tokens>>> JSON schemas.
+
++---------------------------------
+var tokenProperties =
+{
+  "type"      : "object",
+  "properties":
+  {
+    "urlString":
+    {
+      "description": "A delegation token encoded as a URL safe string.",
+      "type"       : "string",
+      "required"   : true
+    }
+  }
+}
++---------------------------------
+
+** {Tokens JSON Schema}
+
+  A <<<Tokens>>> JSON object represents an array of <<<Token>>> JSON objects.
+
++---------------------------------
+{
+  "name"      : "Tokens",
+  "properties":
+  {
+    "Tokens":
+    {
+      "type"      : "object",
+      "properties":
+      {
+        "Token":
+        {
+          "description": "An array of Token",
+          "type"       : "array",
+          "items"      : "Token": tokenProperties      //See Token Properties
+        }
+      }
+    }
+  }
+}
++---------------------------------
+
+  See also:
+  {{{Token Properties}<<<Token>>> Properties}},
+  {{{Get Delegation Tokens}<<<GETDELEGATIONTOKENS>>>}},
+  the note in {{Delegation}}.
+
+
+* {HTTP Query Parameter Dictionary}
+
+** {Access Time}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<accesstime>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The access time of a file/directory. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | long |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | -1 (means keeping it unchanged) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | -1 or a timestamp |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Set Access or Modification Time}<<<SETTIMES>>>}}
+
+
+** {Block Size}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<blocksize>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The block size of a file. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | long |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | Specified in the configuration. |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | \> 0 |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create and Write to a File}<<<CREATE>>>}}
+
+
+** {Buffer Size}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<buffersize>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The size of the buffer used in transferring data. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | int |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | Specified in the configuration. |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | \> 0 |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create and Write to a File}<<<CREATE>>>}},
+  {{{Append to a File}<<<APPEND>>>}},
+  {{{Open and Read a File}<<<OPEN>>>}}
+
+
+** {Create Parent}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<createparent>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | If the parent directories do not exist, should they be created?   |
+*----------------+-------------------------------------------------------------------+
+|| Type          | boolean |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | false |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | true | false |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | true | false |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create a Symbolic Link}<<<CREATESYMLINK>>>}}
+
+
+** {Delegation}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<delegation>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The delegation token used for authentication. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | \<empty\> |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | An encoded token. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | See the note below. |
+*----------------+-------------------------------------------------------------------+
+
+  <<Note>> that delegation tokens are encoded as a URL safe string;
+  see <<<encodeToUrlString()>>>
+  and <<<decodeFromUrlString(String)>>>
+  in <<<org.apache.hadoop.security.token.Token>>> for the details of the encoding.
+
+
+  See also:
+  {{Authentication}}
+
+
+** {Destination}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<destination>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The destination path. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | Path |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | \<empty\> (an invalid path) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | An absolute FileSystem path without scheme and authority. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any path. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create a Symbolic Link}<<<CREATESYMLINK>>>}},
+  {{{Rename a File/Directory}<<<RENAME>>>}}
+
+
+** {Do As}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<doas>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | Allowing a proxy user to do as another user. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | null |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | Any valid username. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any string. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{Proxy Users}}
+
+
+** {Group}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<group>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The name of a group. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | \<empty\> (means keeping it unchanged) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | Any valid group name. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any string. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Set Owner}<<<SETOWNER>>>}}
+
+
+** {Length}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<length>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The number of bytes to be processed. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | long |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | null (means the entire file) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | \>= 0 or null |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Open and Read a File}<<<OPEN>>>}}
+
+
+** {Modification Time}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<modificationtime>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The modification time of a file/directory. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | long |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | -1 (means keeping it unchanged) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | -1 or a timestamp |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Set Access or Modification Time}<<<SETTIMES>>>}}
+
+
+** {Offset}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<offset>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The starting byte position. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | long |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | 0 |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | \>= 0 |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Open and Read a File}<<<OPEN>>>}}
+
+
+** {Op}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<op>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The name of the operation to be executed. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | enum |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | null (an invalid value) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | Any valid operation name. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any string. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{Operations}}
+
+
+** {Overwrite}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<overwrite>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | If a file already exists, should it be overwritten? |
+*----------------+-------------------------------------------------------------------+
+|| Type          | boolean |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | false |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | true | false |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | true | false |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create and Write to a File}<<<CREATE>>>}}
+
+
+** {Owner}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<owner>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The username who is the owner of a file/directory. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | \<empty\> (means keeping it unchanged) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | Any valid username. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any string. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Set Owner}<<<SETOWNER>>>}}
+
+
+** {Permission}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<permission>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The permission of a file/directory. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | Octal |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | 755 |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | 0 - 1777 |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any radix-8 integer (leading zeros may be omitted.) |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create and Write to a File}<<<CREATE>>>}},
+  {{{Make a Directory}<<<MKDIRS>>>}},
+  {{{Set Permission}<<<SETPERMISSION>>>}}
+
+
+** {Recursive}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<recursive>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | Should the operation act on the content in the subdirectories? |
+*----------------+-------------------------------------------------------------------+
+|| Type          | boolean |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | false |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | true | false |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | true | false |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Rename a File/Directory}<<<RENAME>>>}}
+
+
+** {Renewer}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<renewer>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The username of the renewer of a delegation token. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | \<empty\> (means the current user) |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | Any valid username. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any string. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Get Delegation Token}<<<GETDELEGATIONTOKEN>>>}},
+  {{{Get Delegation Tokens}<<<GETDELEGATIONTOKENS>>>}}
+
+
+** {Replication}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<replication>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The number of replications of a file. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | short |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | Specified in the configuration. |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | \> 0 |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any integer. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Create and Write to a File}<<<CREATE>>>}},
+  {{{Set Replication Factor}<<<SETREPLICATION>>>}}
+
+
+** {Token}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<token>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The delegation token used for the operation. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | \<empty\> |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | An encoded token. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | See the note in {{Delegation}}. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{{Renew Delegation Token}<<<RENEWDELEGATIONTOKEN>>>}},
+  {{{Cancel Delegation Token}<<<CANCELDELEGATIONTOKEN>>>}}
+
+
+** {Username}
+
+*----------------+-------------------------------------------------------------------+
+|| Name          | <<<user.name>>> |
+*----------------+-------------------------------------------------------------------+
+|| Description   | The authenticated user; see {{Authentication}}. |
+*----------------+-------------------------------------------------------------------+
+|| Type          | String |
+*----------------+-------------------------------------------------------------------+
+|| Default Value | null |
+*----------------+-------------------------------------------------------------------+
+|| Valid Values  | Any valid username. |
+*----------------+-------------------------------------------------------------------+
+|| Syntax        | Any string. |
+*----------------+-------------------------------------------------------------------+
+
+  See also:
+  {{Authentication}}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/resources/css/site.css b/hadoop-hdfs-project/hadoop-hdfs/src/site/resources/css/site.css
new file mode 100644
index 0000000..f830baa
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/resources/css/site.css
@@ -0,0 +1,30 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the "License"); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+#banner {
+  height: 93px;
+  background: none;
+}
+
+#bannerLeft img {
+  margin-left: 30px;
+  margin-top: 10px;
+}
+
+#bannerRight img {
+  margin: 17px;
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/site.xml b/hadoop-hdfs-project/hadoop-hdfs/src/site/site.xml
new file mode 100644
index 0000000..1296cea
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/site.xml
@@ -0,0 +1,28 @@
+<!--
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+
+   http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License. See accompanying LICENSE file.
+-->
+<project name="Apache Hadoop ${project.version}">
+
+  <skin>
+    <groupId>org.apache.maven.skins</groupId>
+    <artifactId>maven-stylus-skin</artifactId>
+    <version>1.2</version>
+  </skin>
+
+  <body>
+    <links>
+      <item name="Apache Hadoop" href="http://hadoop.apache.org/"/>
+    </links>
+  </body>
+
+</project>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/EncryptedShuffle.apt.vm b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/EncryptedShuffle.apt.vm
new file mode 100644
index 0000000..e05951c
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/EncryptedShuffle.apt.vm
@@ -0,0 +1,320 @@
+~~ Licensed under the Apache License, Version 2.0 (the "License");
+~~ you may not use this file except in compliance with the License.
+~~ You may obtain a copy of the License at
+~~
+~~   http://www.apache.org/licenses/LICENSE-2.0
+~~
+~~ Unless required by applicable law or agreed to in writing, software
+~~ distributed under the License is distributed on an "AS IS" BASIS,
+~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+~~ See the License for the specific language governing permissions and
+~~ limitations under the License. See accompanying LICENSE file.
+
+  ---
+  Hadoop Map Reduce Next Generation-${project.version} - Encrypted Shuffle
+  ---
+  ---
+  ${maven.build.timestamp}
+
+Hadoop MapReduce Next Generation - Encrypted Shuffle
+
+  \[ {{{./index.html}Go Back}} \]
+
+* {Introduction}
+
+  The Encrypted Shuffle capability allows encryption of the MapReduce shuffle
+  using HTTPS and with optional client authentication (also known as
+  bi-directional HTTPS, or HTTPS with client certificates). It comprises:
+
+  * A Hadoop configuration setting for toggling the shuffle between HTTP and
+    HTTPS.
+
+  * A Hadoop configuration settings for specifying the keystore and truststore
+   properties (location, type, passwords) used by the shuffle service and the
+   reducers tasks fetching shuffle data.
+
+  * A way to re-load truststores across the cluster (when a node is added or
+    removed).
+
+* {Configuration}
+
+**  <<core-site.xml>> Properties
+
+  To enable encrypted shuffle, set the following properties in core-site.xml of
+  all nodes in the cluster:
+
+*--------------------------------------+---------------------+-----------------+
+| <<Property>>                         | <<Default Value>>   | <<Explanation>> |
+*--------------------------------------+---------------------+-----------------+
+| <<<hadoop.ssl.require.client.cert>>> | <<<false>>>         | Whether client certificates are required |
+*--------------------------------------+---------------------+-----------------+
+| <<<hadoop.ssl.hostname.verifier>>>   | <<<DEFAULT>>>       | The hostname verifier to provide for HttpsURLConnections. Valid values are: <<DEFAULT>>, <<STRICT>>, <<STRICT_I6>>, <<DEFAULT_AND_LOCALHOST>> and <<ALLOW_ALL>> |
+*--------------------------------------+---------------------+-----------------+
+| <<<hadoop.ssl.keystores.factory.class>>> | <<<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory>>> | The KeyStoresFactory implementation to use |
+*--------------------------------------+---------------------+-----------------+
+| <<<hadoop.ssl.server.conf>>>         | <<<ss-server.xml>>> | Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory |
+*--------------------------------------+---------------------+-----------------+
+| <<<hadoop.ssl.client.conf>>>         | <<<ss-client.xml>>> | Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory |
+*--------------------------------------+---------------------+-----------------+
+
+  <<IMPORTANT:>> Currently requiring client certificates should be set to false.
+  Refer the {{{ClientCertificates}Client Certificates}} section for details.
+
+  <<IMPORTANT:>> All these properties should be marked as final in the cluster
+  configuration files.
+
+*** Example:
+
+------
+    ...
+    <property>
+      <name>hadoop.ssl.require.client.cert</name>
+      <value>false</value>
+      <final>true</final>
+    </property>
+
+    <property>
+      <name>hadoop.ssl.hostname.verifier</name>
+      <value>DEFAULT</value>
+      <final>true</final>
+    </property>
+
+    <property>
+      <name>hadoop.ssl.keystores.factory.class</name>
+      <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
+      <final>true</final>
+    </property>
+
+    <property>
+      <name>hadoop.ssl.server.conf</name>
+      <value>ssl-server.xml</value>
+      <final>true</final>
+    </property>
+
+    <property>
+      <name>hadoop.ssl.client.conf</name>
+      <value>ssl-client.xml</value>
+      <final>true</final>
+    </property>
+    ...
+------
+
+**  <<<mapred-site.xml>>> Properties
+
+  To enable encrypted shuffle, set the following property in mapred-site.xml
+  of all nodes in the cluster:
+
+*--------------------------------------+---------------------+-----------------+
+| <<Property>>                         | <<Default Value>>   | <<Explanation>> |
+*--------------------------------------+---------------------+-----------------+
+| <<<mapreduce.shuffle.ssl.enabled>>>  | <<<false>>>         | Whether encrypted shuffle is enabled |
+*--------------------------------------+---------------------+-----------------+
+
+  <<IMPORTANT:>> This property should be marked as final in the cluster
+  configuration files.
+
+*** Example:
+
+------
+    ...
+    <property>
+      <name>mapreduce.shuffle.ssl.enabled</name>
+      <value>true</value>
+      <final>true</final>
+    </property>
+    ...
+------
+
+  The Linux container executor should be set to prevent job tasks from
+  reading the server keystore information and gaining access to the shuffle
+  server certificates.
+
+  Refer to Hadoop Kerberos configuration for details on how to do this.
+
+* {Keystore and Truststore Settings}
+
+  Currently <<<FileBasedKeyStoresFactory>>> is the only <<<KeyStoresFactory>>>
+  implementation. The <<<FileBasedKeyStoresFactory>>> implementation uses the
+  following properties, in the <<ssl-server.xml>> and <<ssl-client.xml>> files,
+  to configure the keystores and truststores.
+
+** <<<ssl-server.xml>>> (Shuffle server) Configuration:
+
+  The mapred user should own the <<ssl-server.xml>> file and have exclusive
+  read access to it.
+
+*---------------------------------------------+---------------------+-----------------+
+| <<Property>>                                | <<Default Value>>   | <<Explanation>> |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.keystore.type>>>              | <<<jks>>>           | Keystore file type |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.keystore.location>>>          | NONE                | Keystore file location. The mapred user should own this file and have exclusive read access to it. |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.keystore.password>>>          | NONE                | Keystore file password |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.truststore.type>>>            | <<<jks>>>           | Truststore file type |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.truststore.location>>>        | NONE                | Truststore file location. The mapred user should own this file and have exclusive read access to it. |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.truststore.password>>>        | NONE                | Truststore file password |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.server.truststore.reload.interval>>> | 10000               | Truststore reload interval, in milliseconds |
+*--------------------------------------+----------------------------+-----------------+
+
+*** Example:
+
+------
+<configuration>
+
+  <!-- Server Certificate Store -->
+  <property>
+    <name>ssl.server.keystore.type</name>
+    <value>jks</value>
+  </property>
+  <property>
+    <name>ssl.server.keystore.location</name>
+    <value>${user.home}/keystores/server-keystore.jks</value>
+  </property>
+  <property>
+    <name>ssl.server.keystore.password</name>
+    <value>serverfoo</value>
+  </property>
+
+  <!-- Server Trust Store -->
+  <property>
+    <name>ssl.server.truststore.type</name>
+    <value>jks</value>
+  </property>
+  <property>
+    <name>ssl.server.truststore.location</name>
+    <value>${user.home}/keystores/truststore.jks</value>
+  </property>
+  <property>
+    <name>ssl.server.truststore.password</name>
+    <value>clientserverbar</value>
+  </property>
+  <property>
+    <name>ssl.server.truststore.reload.interval</name>
+    <value>10000</value>
+  </property>
+</configuration>
+------
+
+** <<<ssl-client.xml>>> (Reducer/Fetcher) Configuration:
+
+  The mapred user should own the <<ssl-server.xml>> file and it should have
+  default permissions.
+
+*---------------------------------------------+---------------------+-----------------+
+| <<Property>>                                | <<Default Value>>   | <<Explanation>> |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.keystore.type>>>              | <<<jks>>>           | Keystore file type |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.keystore.location>>>          | NONE                | Keystore file location. The mapred user should own this file and it should have default permissions. |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.keystore.password>>>          | NONE                | Keystore file password |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.truststore.type>>>            | <<<jks>>>           | Truststore file type |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.truststore.location>>>        | NONE                | Truststore file location. The mapred user should own this file and it should have default permissions. |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.truststore.password>>>        | NONE                | Truststore file password |
+*---------------------------------------------+---------------------+-----------------+
+| <<<ssl.client.truststore.reload.interval>>> | 10000                | Truststore reload interval, in milliseconds |
+*--------------------------------------+----------------------------+-----------------+
+
+*** Example:
+
+------
+<configuration>
+
+  <!-- Client certificate Store -->
+  <property>
+    <name>ssl.client.keystore.type</name>
+    <value>jks</value>
+  </property>
+  <property>
+    <name>ssl.client.keystore.location</name>
+    <value>${user.home}/keystores/client-keystore.jks</value>
+  </property>
+  <property>
+    <name>ssl.client.keystore.password</name>
+    <value>clientfoo</value>
+  </property>
+
+  <!-- Client Trust Store -->
+  <property>
+    <name>ssl.client.truststore.type</name>
+    <value>jks</value>
+  </property>
+  <property>
+    <name>ssl.client.truststore.location</name>
+    <value>${user.home}/keystores/truststore.jks</value>
+  </property>
+  <property>
+    <name>ssl.client.truststore.password</name>
+    <value>clientserverbar</value>
+  </property>
+  <property>
+    <name>ssl.client.truststore.reload.interval</name>
+    <value>10000</value>
+  </property>
+</configuration>
+------
+
+* Activating Encrypted Shuffle
+
+  When you have made the above configuration changes, activate Encrypted
+  Shuffle by re-starting all NodeManagers.
+
+  <<IMPORTANT:>> Using encrypted shuffle will incur in a significant
+  performance impact. Users should profile this and potentially reserve
+  1 or more cores for encrypted shuffle.
+
+* {ClientCertificates} Client Certificates
+
+  Using Client Certificates does not fully ensure that the client is a
+  reducer task for the job. Currently, Client Certificates (their private key)
+  keystore files must be readable by all users submitting jobs to the cluster.
+  This means that a rogue job could read such those keystore files and use
+  the client certificates in them to establish a secure connection with a
+  Shuffle server. However, unless the rogue job has a proper JobToken, it won't
+  be able to retrieve shuffle data from the Shuffle server. A job, using its
+  own JobToken, can only retrieve shuffle data that belongs to itself.
+
+* Reloading Truststores
+
+  By default the truststores will reload their configuration every 10 seconds.
+  If a new truststore file is copied over the old one, it will be re-read,
+  and its certificates will replace the old ones. This mechanism is useful for
+  adding or removing nodes from the cluster, or for adding or removing trusted
+  clients. In these cases, the client or NodeManager certificate is added to
+  (or removed from) all the truststore files in the system, and the new
+  configuration will be picked up without you having to restart the NodeManager
+  daemons.
+
+* Debugging
+
+  <<NOTE:>> Enable debugging only for troubleshooting, and then only for jobs
+  running on small amounts of data. It is very verbose and slows down jobs by
+  several orders of magnitude. (You might need to increase mapred.task.timeout
+  to prevent jobs from failing because tasks run so slowly.)
+
+  To enable SSL debugging in the reducers, set <<<-Djavax.net.debug=all>>> in
+  the <<<mapreduce.reduce.child.java.opts>>> property; for example:
+
+------
+  <property>
+    <name>mapred.reduce.child.java.opts</name>
+    <value>-Xmx-200m -Djavax.net.debug=all</value>
+  </property>
+------
+
+  You can do this on a per-job basis, or by means of a cluster-wide setting in
+  the <<<mapred-site.xml>>> file.
+
+  To set this property in NodeManager, set it in the <<<yarn-env.sh>>> file:
+
+------
+  YARN_NODEMANAGER_OPTS="-Djavax.net.debug=all $YARN_NODEMANAGER_OPTS"
+------
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/resources/css/site.css b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/resources/css/site.css
new file mode 100644
index 0000000..f830baa
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/resources/css/site.css
@@ -0,0 +1,30 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the "License"); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+#banner {
+  height: 93px;
+  background: none;
+}
+
+#bannerLeft img {
+  margin-left: 30px;
+  margin-top: 10px;
+}
+
+#bannerRight img {
+  margin: 17px;
+}
+
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/site.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/site.xml
new file mode 100644
index 0000000..1296cea
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/site.xml
@@ -0,0 +1,28 @@
+<!--
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+
+   http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License. See accompanying LICENSE file.
+-->
+<project name="Apache Hadoop ${project.version}">
+
+  <skin>
+    <groupId>org.apache.maven.skins</groupId>
+    <artifactId>maven-stylus-skin</artifactId>
+    <version>1.2</version>
+  </skin>
+
+  <body>
+    <links>
+      <item name="Apache Hadoop" href="http://hadoop.apache.org/"/>
+    </links>
+  </body>
+
+</project>
diff --git a/hadoop-project/src/site/apt/index.apt.vm b/hadoop-project/src/site/apt/index.apt.vm
index 32e708e..aacae46 100644
--- a/hadoop-project/src/site/apt/index.apt.vm
+++ b/hadoop-project/src/site/apt/index.apt.vm
@@ -34,7 +34,7 @@ Apache Hadoop ${project.version}
   Namenodes.
 
   More details are available in the 
-  {{{./hadoop-yarn/hadoop-yarn-site/Federation.html}HDFS Federation}}
+  {{{./hadoop-project-dist/hadoop-hdfs/Federation.html}HDFS Federation}}
   document.
 
   * {MapReduce NextGen aka YARN aka MRv2}
@@ -65,9 +65,9 @@ Getting Started
 
   The Hadoop documentation includes the information you need to get started using
   Hadoop. Begin with the
-  {{{./hadoop-yarn/hadoop-yarn-site/SingleCluster.html}Single Node Setup}} which
+  {{{./hadoop-project-dist/hadoop-common/SingleCluster.html}Single Node Setup}} which
   shows you how to set up a single-node Hadoop installation. Then move on to the
-  {{{./hadoop-yarn/hadoop-yarn-site/ClusterSetup.html}Cluster Setup}} to learn how
+  {{{./hadoop-project-dist/hadoop-common/ClusterSetup.html}Cluster Setup}} to learn how
   to set up a multi-node Hadoop installation.
   
 
diff --git a/hadoop-project/src/site/site.xml b/hadoop-project/src/site/site.xml
index aa849b6..0cbe34c 100644
--- a/hadoop-project/src/site/site.xml
+++ b/hadoop-project/src/site/site.xml
@@ -48,35 +48,37 @@
 
     <menu name="Common" inherit="top">
       <item name="Overview" href="index.html"/>
-      <item name="Single Node Setup" href="hadoop-yarn/hadoop-yarn-site/SingleCluster.html"/>
-      <item name="Cluster Setup" href="hadoop-yarn/hadoop-yarn-site/ClusterSetup.html"/>
-      <item name="Hadoop Commands" href="hadoop-project-dist/hadoop-common/commands_manual.html"/>
-    </menu>
+      <item name="Single Node Setup" href="hadoop-project-dist/hadoop-common/SingleCluster.html"/>
+      <item name="Cluster Setup" href="hadoop-project-dist/hadoop-common/ClusterSetup.html"/>
+      <item name="CLI Mini Cluster" href="hadoop-project-dist/hadoop-common/CLIMiniCluster.html"/>
+      </menu>
     
     <menu name="HDFS" inherit="top">
-      <item name="High Availability With QJM" href="hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html"/>
-      <item name="High Availability With NFS" href="hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithNFS.html"/>
-      <item name="Federation" href="hadoop-yarn/hadoop-yarn-site/Federation.html"/>
-      <item name="WebHDFS REST API" href="hadoop-yarn/hadoop-yarn-site/WebHDFS.html"/>
+      <item name="High Availability With QJM" href="hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html"/>
+      <item name="High Availability With NFS" href="hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html"/>
+      <item name="Federation" href="hadoop-project-dist/hadoop-hdfs/Federation.html"/>
+      <item name="WebHDFS REST API" href="hadoop-project-dist/hadoop-hdfs/WebHDFS.html"/>
       <item name="HttpFS Gateway" href="hadoop-hdfs-httpfs/index.html"/>
-      <item name="HDFS User Guide" href="hadoop-project-dist/hadoop-hdfs/hdfs_user_guide.html"/>
     </menu>
 
-    <menu name="Yarn/MapReduce" inherit="top">
+    <menu name="MapReduce" inherit="top">
+      <item name="Encrypted Shuffle" href="hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html"/>
+      <item name="Pluggable Shuffle/Sort" href="hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html"/>
+    </menu>
+    
+    <menu name="YARN" inherit="top">
       <item name="YARN Architecture" href="hadoop-yarn/hadoop-yarn-site/YARN.html"/>
-      <item name="Writing Yarn Applications" href="hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html"/>
+      <item name="Writing YARN Applications" href="hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html"/>
       <item name="Capacity Scheduler" href="hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html"/>
       <item name="Web Application Proxy" href="hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html"/>
-      <item name="Encrypted Shuffle" href="hadoop-yarn/hadoop-yarn-site/EncryptedShuffle.html"/>
-      <item name="Pluggable Shuffle/Sort" href="hadoop-yarn/hadoop-yarn-site/PluggableShuffleAndPluggableSort.html"/>
-      <item name="Yarn Commands" href="hadoop-yarn/hadoop-yarn-site/YarnCommands.html"/>
+      <item name="YARN Commands" href="hadoop-yarn/hadoop-yarn-site/YarnCommands.html"/>
     </menu>
 
-    <menu name="YARN REST API's" inherit="top">
+    <menu name="YARN REST APIs" inherit="top">
       <item name="Introduction" href="hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html"/>
       <item name="Resource Manager" href="hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html"/>
       <item name="Node Manager" href="hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html"/>
-      <item name="Mapreduce Application Master" href="hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html"/>
+      <item name="MR Application Master" href="hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html"/>
       <item name="History Server" href="hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html"/>
     </menu>
     
@@ -98,8 +100,8 @@
     <menu name="Configuration" inherit="top">
       <item name="core-default.xml" href="hadoop-project-dist/hadoop-common/core-default.xml"/>
       <item name="hdfs-default.xml" href="hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"/>
-      <item name="yarn-default.xml" href="hadoop-yarn/hadoop-yarn-common/yarn-default.xml"/>
       <item name="mapred-default.xml" href="hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml"/>
+      <item name="yarn-default.xml" href="hadoop-yarn/hadoop-yarn-common/yarn-default.xml"/>
       <item name="Deprecated Properties" href="hadoop-project-dist/hadoop-common/DeprecatedProperties.html"/>
     </menu>
 
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
deleted file mode 100644
index 957b994..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/CLIMiniCluster.apt.vm
+++ /dev/null
@@ -1,84 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop MapReduce Next Generation ${project.version} - CLI MiniCluster.
-  ---
-  ---
-  ${maven.build.timestamp}
-
-Hadoop MapReduce Next Generation - CLI MiniCluster.
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-* {Purpose}
-
-  Using the CLI MiniCluster, users can simply start and stop a single-node
-  Hadoop cluster with a single command, and without the need to set any
-  environment variables or manage configuration files. The CLI MiniCluster
-  starts both a <<<YARN>>>/<<<MapReduce>>> & <<<HDFS>>> clusters.
-
-  This is useful for cases where users want to quickly experiment with a real
-  Hadoop cluster or test non-Java programs that rely on significant Hadoop
-  functionality.
-
-* {Hadoop Tarball}
-
-  You should be able to obtain the Hadoop tarball from the release. Also, you
-  can directly create a tarball from the source:
-
-+---+
-$ mvn clean install -DskipTests
-$ mvn package -Pdist -Dtar -DskipTests -Dmaven.javadoc.skip
-+---+
-  <<NOTE:>> You will need protoc installed of version 2.4.1 or greater.
-
-  The tarball should be available in <<<hadoop-dist/target/>>> directory. 
-
-* {Running the MiniCluster}
-
-  From inside the root directory of the extracted tarball, you can start the CLI
-  MiniCluster using the following command:
-
-+---+
-$ bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${project.version}-tests.jar minicluster -rmport RM_PORT -jhsport JHS_PORT
-+---+
-
-  In the example command above, <<<RM_PORT>>> and <<<JHS_PORT>>> should be
-  replaced by the user's choice of these port numbers. If not specified, random
-  free ports will be used.
-
-  There are a number of command line arguments that the users can use to control
-  which services to start, and to pass other configuration properties.
-  The available command line arguments:
-
-+---+
-$ -D <property=value>    Options to pass into configuration object
-$ -datanodes <arg>       How many datanodes to start (default 1)
-$ -format                Format the DFS (default false)
-$ -help                  Prints option help.
-$ -jhsport <arg>         JobHistoryServer port (default 0--we choose)
-$ -namenode <arg>        URL of the namenode (default is either the DFS
-$                        cluster or a temporary dir)
-$ -nnport <arg>          NameNode port (default 0--we choose)
-$ -nodemanagers <arg>    How many nodemanagers to start (default 1)
-$ -nodfs                 Don't start a mini DFS cluster
-$ -nomr                  Don't start a mini MR cluster
-$ -rmport <arg>          ResourceManager port (default 0--we choose)
-$ -writeConfig <path>    Save configuration to this XML file.
-$ -writeDetails <path>   Write basic information to this JSON file.
-+---+
-
-  To display this full list of available arguments, the user can pass the
-  <<<-help>>> argument to the above command.
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/ClusterSetup.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/ClusterSetup.apt.vm
deleted file mode 100644
index b0b3831..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/ClusterSetup.apt.vm
+++ /dev/null
@@ -1,1126 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop Map Reduce Next Generation-${project.version} - Cluster Setup
-  ---
-  ---
-  ${maven.build.timestamp}
-
-Hadoop MapReduce Next Generation - Cluster Setup
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-* {Purpose}
-
-  This document describes how to install, configure and manage non-trivial 
-  Hadoop clusters ranging from a few nodes to extremely large clusters 
-  with thousands of nodes.
-
-  To play with Hadoop, you may first want to install it on a single 
-  machine (see {{{SingleCluster}Single Node Setup}}).
-  
-* {Prerequisites}
-
-  Download a stable version of Hadoop from Apache mirrors.
-  
-* {Installation}
-
-  Installing a Hadoop cluster typically involves unpacking the software on all 
-  the machines in the cluster or installing RPMs.
-
-  Typically one machine in the cluster is designated as the NameNode and 
-  another machine the as ResourceManager, exclusively. These are the masters. 
-  
-  The rest of the machines in the cluster act as both DataNode and NodeManager. 
-  These are the slaves.
-
-* {Running Hadoop in Non-Secure Mode}
-
-  The following sections describe how to configure a Hadoop cluster.
-
-  * {Configuration Files}
-  
-    Hadoop configuration is driven by two types of important configuration files:
-
-      * Read-only default configuration - <<<core-default.xml>>>, 
-        <<<hdfs-default.xml>>>, <<<yarn-default.xml>>> and 
-        <<<mapred-default.xml>>>.
-        
-      * Site-specific configuration - <<conf/core-site.xml>>, 
-        <<conf/hdfs-site.xml>>, <<conf/yarn-site.xml>> and 
-        <<conf/mapred-site.xml>>.
-
-
-    Additionally, you can control the Hadoop scripts found in the bin/ 
-    directory of the distribution, by setting site-specific values via the 
-    <<conf/hadoop-env.sh>> and <<yarn-env.sh>>.
-
-  * {Site Configuration}
-  
-  To configure the Hadoop cluster you will need to configure the 
-  <<<environment>>> in which the Hadoop daemons execute as well as the 
-  <<<configuration parameters>>> for the Hadoop daemons.
-
-  The Hadoop daemons are NameNode/DataNode and ResourceManager/NodeManager.
-
-
-    * {Configuring Environment of Hadoop Daemons}
-    
-    Administrators should use the <<conf/hadoop-env.sh>> and 
-    <<conf/yarn-env.sh>> script to do site-specific customization of the 
-    Hadoop daemons' process environment.
-
-    At the very least you should specify the <<<JAVA_HOME>>> so that it is 
-    correctly defined on each remote node.
-
-    In most cases you should also specify <<<HADOOP_PID_DIR>>> and 
-    <<<HADOOP_SECURE_DN_PID_DIR>>> to point to directories that can only be
-    written to by the users that are going to run the hadoop daemons.  
-    Otherwise there is the potential for a symlink attack.
-
-    Administrators can configure individual daemons using the configuration 
-    options shown below in the table:
-
-*--------------------------------------+--------------------------------------+
-|| Daemon                              || Environment Variable                |
-*--------------------------------------+--------------------------------------+
-| NameNode                             | HADOOP_NAMENODE_OPTS                 |
-*--------------------------------------+--------------------------------------+
-| DataNode                             | HADOOP_DATANODE_OPTS                 |
-*--------------------------------------+--------------------------------------+
-| Secondary NameNode                   | HADOOP_SECONDARYNAMENODE_OPTS        |
-*--------------------------------------+--------------------------------------+
-| ResourceManager                      | YARN_RESOURCEMANAGER_OPTS            |
-*--------------------------------------+--------------------------------------+
-| NodeManager                          | YARN_NODEMANAGER_OPTS                |
-*--------------------------------------+--------------------------------------+
-| WebAppProxy                          | YARN_PROXYSERVER_OPTS                |
-*--------------------------------------+--------------------------------------+
-| Map Reduce Job History Server        | HADOOP_JOB_HISTORYSERVER_OPTS        |
-*--------------------------------------+--------------------------------------+
-
-
-    For example, To configure Namenode to use parallelGC, the following 
-    statement should be added in hadoop-env.sh :
-     
-----
-    export HADOOP_NAMENODE_OPTS="-XX:+UseParallelGC ${HADOOP_NAMENODE_OPTS}" 
-----
-    
-    Other useful configuration parameters that you can customize include:
-
-      * <<<HADOOP_LOG_DIR>>> / <<<YARN_LOG_DIR>>> - The directory where the 
-        daemons' log files are stored. They are automatically created if they 
-        don't exist.
-        
-      * <<<HADOOP_HEAPSIZE>>> / <<<YARN_HEAPSIZE>>> - The maximum amount of 
-        heapsize to use, in MB e.g. if the varibale is set to 1000 the heap
-        will be set to 1000MB.  This is used to configure the heap 
-        size for the daemon. By default, the value is 1000.  If you want to
-        configure the values separately for each deamon you can use.
-*--------------------------------------+--------------------------------------+
-|| Daemon                              || Environment Variable                |
-*--------------------------------------+--------------------------------------+
-| ResourceManager                      | YARN_RESOURCEMANAGER_HEAPSIZE        |
-*--------------------------------------+--------------------------------------+
-| NodeManager                          | YARN_NODEMANAGER_HEAPSIZE            |
-*--------------------------------------+--------------------------------------+
-| WebAppProxy                          | YARN_PROXYSERVER_HEAPSIZE            |
-*--------------------------------------+--------------------------------------+
-| Map Reduce Job History Server        | HADOOP_JOB_HISTORYSERVER_HEAPSIZE    |
-*--------------------------------------+--------------------------------------+
- 
-    * {Configuring the Hadoop Daemons in Non-Secure Mode}
-
-      This section deals with important parameters to be specified in 
-      the given configuration files:
-       
-      * <<<conf/core-site.xml>>>
-      
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<fs.defaultFS>>>      | NameNode URI            | <hdfs://host:port/>    |
-*-------------------------+-------------------------+------------------------+
-| <<<io.file.buffer.size>>> | 131072 |  |
-| | | Size of read/write buffer used in SequenceFiles. |
-*-------------------------+-------------------------+------------------------+
-
-      * <<<conf/hdfs-site.xml>>>
-      
-        * Configurations for NameNode:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.name.dir>>> | | | 
-| | Path on the local filesystem where the NameNode stores the namespace | | 
-| | and transactions logs persistently. | | 
-| | | If this is a comma-delimited list of directories then the name table is  |
-| | | replicated in all of the directories, for redundancy. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.hosts>>> / <<<dfs.namenode.hosts.exclude>>> | | |
-| | List of permitted/excluded DataNodes. | |
-| | | If necessary, use these files to control the list of allowable |
-| | | datanodes. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.blocksize>>> | 268435456 | |
-| | | HDFS blocksize of 256MB for large file-systems. |      
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.handler.count>>> | 100 | |
-| | | More NameNode server threads to handle RPCs from large number of |
-| | | DataNodes. |
-*-------------------------+-------------------------+------------------------+
-        
-        * Configurations for DataNode:
-      
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.data.dir>>> | | |
-| | Comma separated list of paths on the local filesystem of a | | 
-| | <<<DataNode>>> where it should store its blocks. | |
-| | | If this is a comma-delimited list of directories, then data will be | 
-| | | stored in all named directories, typically on different devices. |
-*-------------------------+-------------------------+------------------------+
-      
-      * <<<conf/yarn-site.xml>>>
-
-        * Configurations for ResourceManager and NodeManager:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.acl.enable>>> | | |
-| | <<<true>>> / <<<false>>> | |
-| | | Enable ACLs? Defaults to <false>. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.admin.acl>>> | | |
-| | Admin ACL | |
-| | | ACL to set admins on the cluster. |
-| | | ACLs are of for <comma-separated-users><space><comma-separated-groups>. |
-| | | Defaults to special value of <<*>> which means <anyone>. |
-| | | Special value of just <space> means no one has access. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.log-aggregation-enable>>> | | |
-| | <false> | |
-| | | Configuration to enable or disable log aggregation |
-*-------------------------+-------------------------+------------------------+
-
-
-        * Configurations for ResourceManager:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.address>>> | | | 
-| | <<<ResourceManager>>> host:port for clients to submit jobs. | |
-| | | <host:port> |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.scheduler.address>>> | | | 
-| | <<<ResourceManager>>> host:port for ApplicationMasters to talk to | |
-| | Scheduler to obtain resources. | |
-| | | <host:port> |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.resource-tracker.address>>> | | | 
-| | <<<ResourceManager>>> host:port for NodeManagers. | |
-| | | <host:port> |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.admin.address>>> | | | 
-| | <<<ResourceManager>>> host:port for administrative commands. | |
-| | | <host:port> |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.webapp.address>>> | | | 
-| | <<<ResourceManager>>> web-ui host:port. | |
-| | | <host:port> |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.scheduler.class>>> | | |
-| | <<<ResourceManager>>> Scheduler class. | |
-| | | <<<CapacityScheduler>>> (recommended) or <<<FifoScheduler>>> |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.scheduler.minimum-allocation-mb>>> | | |
-| | Minimum limit of memory to allocate to each container request at the <<<Resource Manager>>>. | |
-| | | In MBs |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.scheduler.maximum-allocation-mb>>> | | |
-| | Maximum limit of memory to allocate to each container request at the <<<Resource Manager>>>. | |
-| | | In MBs |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.nodes.include-path>>> / | | | 
-| <<<yarn.resourcemanager.nodes.exclude-path>>> | | |  
-| | List of permitted/excluded NodeManagers. | |
-| | | If necessary, use these files to control the list of allowable | 
-| | | NodeManagers. |
-*-------------------------+-------------------------+------------------------+
- 
-        * Configurations for NodeManager:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.resource.memory-mb>>> | | |
-| | Resource i.e. available physical memory, in MB, for given <<<NodeManager>>> | |
-| | | Defines total available resources on the <<<NodeManager>>> to be made |
-| | | available to running containers |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.vmem-pmem-ratio>>> | | |
-| | Maximum ratio by which virtual memory usage of tasks may exceed |
-| | physical memory | |
-| | | The virtual memory usage of each task may exceed its physical memory |
-| | | limit by this ratio. The total amount of virtual memory used by tasks |
-| | | on the NodeManager may exceed its physical memory usage by this ratio. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.local-dirs>>> | | |
-| | Comma-separated list of paths on the local filesystem where | |
-| | intermediate data is written. ||
-| | | Multiple paths help spread disk i/o. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.log-dirs>>> | | |
-| | Comma-separated list of paths on the local filesystem where logs  | |
-| | are written. | |
-| | | Multiple paths help spread disk i/o. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.log.retain-seconds>>> | | |
-| | <10800> | |
-| | | Default time (in seconds) to retain log files on the NodeManager |
-| | | Only applicable if log-aggregation is disabled. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.remote-app-log-dir>>> | | |
-| | </logs> | |
-| | | HDFS directory where the application logs are moved on application |
-| | | completion. Need to set appropriate permissions. |
-| | | Only applicable if log-aggregation is enabled. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.remote-app-log-dir-suffix>>> | | |
-| | <logs> | |
-| | | Suffix appended to the remote log dir. Logs will be aggregated to  |
-| | | $\{yarn.nodemanager.remote-app-log-dir\}/$\{user\}/$\{thisParam\} |
-| | | Only applicable if log-aggregation is enabled. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.aux-services>>> | | |
-| | mapreduce.shuffle  | |
-| | | Shuffle service that needs to be set for Map Reduce applications. |
-*-------------------------+-------------------------+------------------------+
-
-        * Configurations for History Server (Needs to be moved elsewhere):
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.log-aggregation.retain-seconds>>> | | |
-| | <-1> | |
-| | | How long to keep aggregation logs before deleting them. -1 disables. |
-| | | Be careful, set this too small and you will spam the name node. |
-*-------------------------+-------------------------+------------------------+
-
-
-
-      * <<<conf/mapred-site.xml>>>
-
-        * Configurations for MapReduce Applications:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.framework.name>>> | | |
-| | yarn | |
-| | | Execution framework set to Hadoop YARN. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.map.memory.mb>>> | 1536 | |
-| | | Larger resource limit for maps. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.map.java.opts>>> | -Xmx1024M | |
-| | | Larger heap-size for child jvms of maps. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.reduce.memory.mb>>> | 3072 | |
-| | | Larger resource limit for reduces. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.reduce.java.opts>>> | -Xmx2560M | |
-| | | Larger heap-size for child jvms of reduces. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.task.io.sort.mb>>> | 512 | |
-| | | Higher memory-limit while sorting data for efficiency. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.task.io.sort.factor>>> | 100 | |
-| | | More streams merged at once while sorting files. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.reduce.shuffle.parallelcopies>>> | 50 | |
-| | | Higher number of parallel copies run by reduces to fetch outputs |
-| | | from very large number of maps. |
-*-------------------------+-------------------------+------------------------+
-
-        * Configurations for MapReduce JobHistory Server:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.address>>> | | |
-| | MapReduce JobHistory Server <host:port> | Default port is 10020. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.webapp.address>>> | | |
-| | MapReduce JobHistory Server Web UI <host:port> | Default port is 19888. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.intermediate-done-dir>>> | /mr-history/tmp | |
-|  | | Directory where history files are written by MapReduce jobs. | 
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.done-dir>>> | /mr-history/done| |
-| | | Directory where history files are managed by the MR JobHistory Server. | 
-*-------------------------+-------------------------+------------------------+
-
-      * Hadoop Rack Awareness
-      
-      The HDFS and the YARN components are rack-aware.
-
-      The NameNode and the ResourceManager obtains the rack information of the 
-      slaves in the cluster by invoking an API <resolve> in an administrator 
-      configured module. 
-      
-      The API resolves the DNS name (also IP address) to a rack id. 
-      
-      The site-specific module to use can be configured using the configuration 
-      item <<<topology.node.switch.mapping.impl>>>. The default implementation 
-      of the same runs a script/command configured using 
-      <<<topology.script.file.name>>>. If <<<topology.script.file.name>>> is 
-      not set, the rack id </default-rack> is returned for any passed IP address. 
-
-      * Monitoring Health of NodeManagers
-      
-      Hadoop provides a mechanism by which administrators can configure the 
-      NodeManager to run an administrator supplied script periodically to 
-      determine if a node is healthy or not. 
-      
-      Administrators can determine if the node is in a healthy state by 
-      performing any checks of their choice in the script. If the script 
-      detects the node to be in an unhealthy state, it must print a line to 
-      standard output beginning with the string ERROR. The NodeManager spawns 
-      the script periodically and checks its output. If the script's output 
-      contains the string ERROR, as described above, the node's status is 
-      reported as <<<unhealthy>>> and the node is black-listed by the 
-      ResourceManager. No further tasks will be assigned to this node. 
-      However, the NodeManager continues to run the script, so that if the 
-      node becomes healthy again, it will be removed from the blacklisted nodes
-      on the ResourceManager automatically. The node's health along with the 
-      output of the script, if it is unhealthy, is available to the 
-      administrator in the ResourceManager web interface. The time since the 
-      node was healthy is also displayed on the web interface.
-
-      The following parameters can be used to control the node health 
-      monitoring script in <<<conf/yarn-site.xml>>>.
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.health-checker.script.path>>> | | |
-| | Node health script  | |
-| | | Script to check for node's health status. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.health-checker.script.opts>>> | | |
-| | Node health script options  | |
-| | | Options for script to check for node's health status. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.health-checker.script.interval-ms>>> | | |
-| | Node health script interval  | |
-| | | Time interval for running health script. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.health-checker.script.timeout-ms>>> | | |
-| | Node health script timeout interval  | |
-| | | Timeout for health script execution. |
-*-------------------------+-------------------------+------------------------+
-
-    The health checker script is not supposed to give ERROR if only some of the
-    local disks become bad. NodeManager has the ability to periodically check
-    the health of the local disks (specifically checks nodemanager-local-dirs
-    and nodemanager-log-dirs) and after reaching the threshold of number of
-    bad directories based on the value set for the config property
-    yarn.nodemanager.disk-health-checker.min-healthy-disks, the whole node is
-    marked unhealthy and this info is sent to resource manager also. The boot
-    disk is either raided or a failure in the boot disk is identified by the
-    health checker script.
-
-    * {Slaves file}
-      
-    Typically you choose one machine in the cluster to act as the NameNode and 
-    one machine as to act as the ResourceManager, exclusively. The rest of the 
-    machines act as both a DataNode and NodeManager and are referred to as 
-    <slaves>.
-
-    List all slave hostnames or IP addresses in your <<<conf/slaves>>> file, 
-    one per line.
-
-    * {Logging}
-    
-    Hadoop uses the Apache log4j via the Apache Commons Logging framework for 
-    logging. Edit the <<<conf/log4j.properties>>> file to customize the 
-    Hadoop daemons' logging configuration (log-formats and so on).
-    
-  * {Operating the Hadoop Cluster}
-
-  Once all the necessary configuration is complete, distribute the files to the 
-  <<<HADOOP_CONF_DIR>>> directory on all the machines.
-
-    * Hadoop Startup
-  
-    To start a Hadoop cluster you will need to start both the HDFS and YARN 
-    cluster.
-
-    Format a new distributed filesystem:
-  
-----
-  $ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>
-----
-
-    Start the HDFS with the following command, run on the designated NameNode:
-  
-----
-  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
-----    	  
-
-    Run a script to start DataNodes on all slaves:
-
-----
-  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
-----    	  
-  
-    Start the YARN with the following command, run on the designated 
-    ResourceManager:
-  
-----
-  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager 
-----    	  
-
-    Run a script to start NodeManagers on all slaves:
-
-----
-  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager 
-----    	  
-
-    Start a standalone WebAppProxy server.  If multiple servers
-    are used with load balancing it should be run on each of them:
-
-----
-  $ $YARN_HOME/bin/yarn start proxyserver --config $HADOOP_CONF_DIR  
-----
-
-    Start the MapReduce JobHistory Server with the following command, run on the  
-    designated server:
-  
-----
-  $ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh start historyserver --config $HADOOP_CONF_DIR  
-----    	  
-
-    * Hadoop Shutdown      
-
-    Stop the NameNode with the following command, run on the designated 
-    NameNode:
-  
-----
-  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
-----    	  
-
-    Run a script to stop DataNodes on all slaves:
-
-----
-  $ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
-----    	  
-  
-    Stop the ResourceManager with the following command, run on the designated 
-    ResourceManager:
-  
-----
-  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager 
-----    	  
-
-    Run a script to stop NodeManagers on all slaves:
-
-----
-  $ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager 
-----    	  
-
-    Stop the WebAppProxy server. If multiple servers are used with load
-    balancing it should be run on each of them:
-
-----
-  $ $YARN_HOME/bin/yarn stop proxyserver --config $HADOOP_CONF_DIR  
-----
-
-
-    Stop the MapReduce JobHistory Server with the following command, run on the  
-    designated server:
-  
-----
-  $ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh stop historyserver --config $HADOOP_CONF_DIR  
-----    	  
-
-    
-* {Running Hadoop in Secure Mode}
-
-  This section deals with important parameters to be specified in 
-  to run Hadoop in <<secure mode>> with strong, Kerberos-based
-  authentication.
-      
-  * <<<User Accounts for Hadoop Daemons>>>
-      
-  Ensure that HDFS and YARN daemons run as different Unix users, for e.g.
-  <<<hdfs>>> and <<<yarn>>>. Also, ensure that the MapReduce JobHistory
-  server runs as user <<<mapred>>>. 
-      
-  It's recommended to have them share a Unix group, for e.g. <<<hadoop>>>.
-      
-*--------------------------------------+----------------------------------------------------------------------+
-|| User:Group                          || Daemons                                                             |
-*--------------------------------------+----------------------------------------------------------------------+
-| hdfs:hadoop                          | NameNode, Secondary NameNode, Checkpoint Node, Backup Node, DataNode |
-*--------------------------------------+----------------------------------------------------------------------+
-| yarn:hadoop                          | ResourceManager, NodeManager                                         |
-*--------------------------------------+----------------------------------------------------------------------+
-| mapred:hadoop                        | MapReduce JobHistory Server                                          |
-*--------------------------------------+----------------------------------------------------------------------+
-      
-  * <<<Permissions for both HDFS and local fileSystem paths>>>
-     
-  The following table lists various paths on HDFS and local filesystems (on
-  all nodes) and recommended permissions:
-   
-*-------------------+-------------------+------------------+------------------+
-|| Filesystem       || Path             || User:Group      || Permissions     |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<dfs.namenode.name.dir>>> | hdfs:hadoop | drwx------ | 
-*-------------------+-------------------+------------------+------------------+
-| local | <<<dfs.datanode.data.dir>>> | hdfs:hadoop | drwx------ |
-*-------------------+-------------------+------------------+------------------+
-| local | $HADOOP_LOG_DIR | hdfs:hadoop | drwxrwxr-x |
-*-------------------+-------------------+------------------+------------------+
-| local | $YARN_LOG_DIR | yarn:hadoop | drwxrwxr-x |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<yarn.nodemanager.local-dirs>>> | yarn:hadoop | drwxr-xr-x |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<yarn.nodemanager.log-dirs>>> | yarn:hadoop | drwxr-xr-x |
-*-------------------+-------------------+------------------+------------------+
-| local | container-executor | root:hadoop | --Sr-s--- |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<conf/container-executor.cfg>>> | root:hadoop | r-------- |
-*-------------------+-------------------+------------------+------------------+
-| hdfs | / | hdfs:hadoop | drwxr-xr-x |
-*-------------------+-------------------+------------------+------------------+
-| hdfs | /tmp | hdfs:hadoop | drwxrwxrwxt |
-*-------------------+-------------------+------------------+------------------+
-| hdfs | /user | hdfs:hadoop | drwxr-xr-x |
-*-------------------+-------------------+------------------+------------------+
-| hdfs | <<<yarn.nodemanager.remote-app-log-dir>>> | yarn:hadoop | drwxrwxrwxt |
-*-------------------+-------------------+------------------+------------------+
-| hdfs | <<<mapreduce.jobhistory.intermediate-done-dir>>> | mapred:hadoop | |
-| | | | drwxrwxrwxt |      
-*-------------------+-------------------+------------------+------------------+
-| hdfs | <<<mapreduce.jobhistory.done-dir>>> | mapred:hadoop | |
-| | | | drwxr-x--- |      
-*-------------------+-------------------+------------------+------------------+
-
-  * Kerberos Keytab files
-  
-    * HDFS
-    
-    The NameNode keytab file, on the NameNode host, should look like the 
-    following:
-    
-----
-
-$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/nn.service.keytab 
-Keytab name: FILE:/etc/security/keytab/nn.service.keytab
-KVNO Timestamp         Principal
-   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-
-----
-
-    The Secondary NameNode keytab file, on that host, should look like the 
-    following:
-    
-----
-
-$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/sn.service.keytab 
-Keytab name: FILE:/etc/security/keytab/sn.service.keytab
-KVNO Timestamp         Principal
-   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-
-----
-
-    The DataNode keytab file, on each host, should look like the following:
-    
-----
-
-$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/dn.service.keytab 
-Keytab name: FILE:/etc/security/keytab/dn.service.keytab
-KVNO Timestamp         Principal
-   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-
-----
-    
-    * YARN
-    
-    The ResourceManager keytab file, on the ResourceManager host, should look  
-    like the following:
-    
-----
-
-$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/rm.service.keytab 
-Keytab name: FILE:/etc/security/keytab/rm.service.keytab
-KVNO Timestamp         Principal
-   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-
-----
-
-    The NodeManager keytab file, on each host, should look like the following:
-    
-----
-
-$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/nm.service.keytab 
-Keytab name: FILE:/etc/security/keytab/nm.service.keytab
-KVNO Timestamp         Principal
-   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-
-----
-    
-    * MapReduce JobHistory Server
-
-    The MapReduce JobHistory Server keytab file, on that host, should look  
-    like the following:
-    
-----
-
-$ /usr/kerberos/bin/klist -e -k -t /etc/security/keytab/jhs.service.keytab 
-Keytab name: FILE:/etc/security/keytab/jhs.service.keytab
-KVNO Timestamp         Principal
-   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC) 
-   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5) 
-
-----
-    
-  * Configuration in Secure Mode
-  
-    * <<<conf/core-site.xml>>>
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<hadoop.security.authentication>>> | <kerberos> | <simple> is non-secure. |
-*-------------------------+-------------------------+------------------------+
-| <<<hadoop.security.authorization>>> | <true> | |
-| | | Enable RPC service-level authorization. |
-*-------------------------+-------------------------+------------------------+
-
-    * <<<conf/hdfs-site.xml>>>
-    
-      * Configurations for NameNode:
-    
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.block.access.token.enable>>> | <true> |  |
-| | | Enable HDFS block access tokens for secure operations. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.https.enable>>> | <true> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.https-address>>> | <nn_host_fqdn:50470> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.https.port>>> | <50470> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.keytab.file>>> | </etc/security/keytab/nn.service.keytab> | |
-| | | Kerberos keytab file for the NameNode. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.kerberos.principal>>> | nn/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the NameNode. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.kerberos.https.principal>>> | host/_HOST@REALM.TLD | |
-| | | HTTPS Kerberos principal name for the NameNode. |
-*-------------------------+-------------------------+------------------------+
-
-      * Configurations for Secondary NameNode:
-    
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.secondary.http-address>>> | <c_nn_host_fqdn:50090> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.secondary.https-port>>> | <50470> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.secondary.keytab.file>>> | | | 
-| | </etc/security/keytab/sn.service.keytab> | |
-| | | Kerberos keytab file for the NameNode. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.secondary.kerberos.principal>>> | sn/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the Secondary NameNode. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.namenode.secondary.kerberos.https.principal>>> | | |
-| | host/_HOST@REALM.TLD | |
-| | | HTTPS Kerberos principal name for the Secondary NameNode. |
-*-------------------------+-------------------------+------------------------+
-
-      * Configurations for DataNode:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.data.dir.perm>>> | 700 | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.address>>> | <0.0.0.0:2003> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.https.address>>> | <0.0.0.0:2005> | |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.keytab.file>>> | </etc/security/keytab/dn.service.keytab> | |
-| | | Kerberos keytab file for the DataNode. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.kerberos.principal>>> | dn/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the DataNode. |
-*-------------------------+-------------------------+------------------------+
-| <<<dfs.datanode.kerberos.https.principal>>> | | |
-| | host/_HOST@REALM.TLD | |
-| | | HTTPS Kerberos principal name for the DataNode. |
-*-------------------------+-------------------------+------------------------+
-
-    * <<<conf/yarn-site.xml>>>
-    
-      * WebAppProxy
-
-      The <<<WebAppProxy>>> provides a proxy between the web applications
-      exported by an application and an end user.  If security is enabled
-      it will warn users before accessing a potentially unsafe web application.
-      Authentication and authorization using the proxy is handled just like
-      any other privileged web application.
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.web-proxy.address>>> | | |
-| | <<<WebAppProxy>>> host:port for proxy to AM web apps. | |
-| | | <host:port> if this is the same as <<<yarn.resourcemanager.webapp.address>>>|
-| | | or it is not defined then the <<<ResourceManager>>> will run the proxy|
-| | | otherwise a standalone proxy server will need to be launched.|
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.web-proxy.keytab>>> | | |
-| | </etc/security/keytab/web-app.service.keytab> | |
-| | | Kerberos keytab file for the WebAppProxy. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.web-proxy.principal>>> | wap/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the WebAppProxy. |
-*-------------------------+-------------------------+------------------------+
-
-      * LinuxContainerExecutor
-      
-      A <<<ContainerExecutor>>> used by YARN framework which define how any
-      <container> launched and controlled. 
-      
-      The following are the available in Hadoop YARN:
-
-*--------------------------------------+--------------------------------------+
-|| ContainerExecutor                   || Description                         |
-*--------------------------------------+--------------------------------------+
-| <<<DefaultContainerExecutor>>>             | |
-| | The default executor which YARN uses to manage container execution. |
-| | The container process has the same Unix user as the NodeManager.  |
-*--------------------------------------+--------------------------------------+
-| <<<LinuxContainerExecutor>>>               | |
-| | Supported only on GNU/Linux, this executor runs the containers as the | 
-| | user who submitted the application. It requires all user accounts to be |
-| | created on the cluster nodes where the containers are launched. It uses |
-| | a <setuid> executable that is included in the Hadoop distribution. |
-| | The NodeManager uses this executable to launch and kill containers. |
-| | The setuid executable switches to the user who has submitted the |
-| | application and launches or kills the containers. For maximum security, |
-| | this executor sets up restricted permissions and user/group ownership of |
-| | local files and directories used by the containers such as the shared |
-| | objects, jars, intermediate files, log files etc. Particularly note that, |
-| | because of this, except the application owner and NodeManager, no other |
-| | user can access any of the local files/directories including those |
-| | localized as part of the distributed cache. |
-*--------------------------------------+--------------------------------------+
-
-      To build the LinuxContainerExecutor executable run:
-        
-----
- $ mvn package -Dcontainer-executor.conf.dir=/etc/hadoop/
-----
-        
-      The path passed in <<<-Dcontainer-executor.conf.dir>>> should be the 
-      path on the cluster nodes where a configuration file for the setuid 
-      executable should be located. The executable should be installed in
-      $YARN_HOME/bin.
-
-      The executable must have specific permissions: 6050 or --Sr-s--- 
-      permissions user-owned by <root> (super-user) and group-owned by a 
-      special group (e.g. <<<hadoop>>>) of which the NodeManager Unix user is 
-      the group member and no ordinary application user is. If any application 
-      user belongs to this special group, security will be compromised. This 
-      special group name should be specified for the configuration property 
-      <<<yarn.nodemanager.linux-container-executor.group>>> in both 
-      <<<conf/yarn-site.xml>>> and <<<conf/container-executor.cfg>>>. 
-      
-      For example, let's say that the NodeManager is run as user <yarn> who is 
-      part of the groups users and <hadoop>, any of them being the primary group.
-      Let also be that <users> has both <yarn> and another user 
-      (application submitter) <alice> as its members, and <alice> does not 
-      belong to <hadoop>. Going by the above description, the setuid/setgid 
-      executable should be set 6050 or --Sr-s--- with user-owner as <yarn> and 
-      group-owner as <hadoop> which has <yarn> as its member (and not <users> 
-      which has <alice> also as its member besides <yarn>).
-
-      The LinuxTaskController requires that paths including and leading up to 
-      the directories specified in <<<yarn.nodemanager.local-dirs>>> and 
-      <<<yarn.nodemanager.log-dirs>>> to be set 755 permissions as described 
-      above in the table on permissions on directories.
-
-        * <<<conf/container-executor.cfg>>>
-        
-        The executable requires a configuration file called 
-        <<<container-executor.cfg>>> to be present in the configuration 
-        directory passed to the mvn target mentioned above. 
-
-        The configuration file must be owned by the user running NodeManager 
-        (user <<<yarn>>> in the above example), group-owned by anyone and 
-        should have the permissions 0400 or r--------.
-
-        The executable requires following configuration items to be present 
-        in the <<<conf/container-executor.cfg>>> file. The items should be 
-        mentioned as simple key=value pairs, one per-line:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.linux-container-executor.group>>> | <hadoop> | |
-| | | Unix group of the NodeManager. The group owner of the |
-| | |<container-executor> binary should be this group. Should be same as the |
-| | | value with which the NodeManager is configured. This configuration is |
-| | | required for validating the secure access of the <container-executor> |
-| | | binary. |        
-*-------------------------+-------------------------+------------------------+
-| <<<banned.users>>> | hfds,yarn,mapred,bin | Banned users. |
-*-------------------------+-------------------------+------------------------+
-| <<<min.user.id>>> | 1000 | Prevent other super-users. |      
-*-------------------------+-------------------------+------------------------+
-
-      To re-cap, here are the local file-ssytem permissions required for the 
-      various paths related to the <<<LinuxContainerExecutor>>>:
-      
-*-------------------+-------------------+------------------+------------------+
-|| Filesystem       || Path             || User:Group      || Permissions     |
-*-------------------+-------------------+------------------+------------------+
-| local | container-executor | root:hadoop | --Sr-s--- |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<conf/container-executor.cfg>>> | root:hadoop | r-------- |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<yarn.nodemanager.local-dirs>>> | yarn:hadoop | drwxr-xr-x |
-*-------------------+-------------------+------------------+------------------+
-| local | <<<yarn.nodemanager.log-dirs>>> | yarn:hadoop | drwxr-xr-x |
-*-------------------+-------------------+------------------+------------------+
-      
-      * Configurations for ResourceManager:
-    
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.keytab>>> | | |
-| | </etc/security/keytab/rm.service.keytab> | |
-| | | Kerberos keytab file for the ResourceManager. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.resourcemanager.principal>>> | rm/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the ResourceManager. |
-*-------------------------+-------------------------+------------------------+
-      
-      * Configurations for NodeManager:
-      
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.keytab>>> | </etc/security/keytab/nm.service.keytab> | |
-| | | Kerberos keytab file for the NodeManager. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.principal>>> | nm/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the NodeManager. |
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.container-executor.class>>> | | |
-| | <<<org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor>>> | 
-| | | Use LinuxContainerExecutor. | 
-*-------------------------+-------------------------+------------------------+
-| <<<yarn.nodemanager.linux-container-executor.group>>> | <hadoop> | |
-| | | Unix group of the NodeManager. |
-*-------------------------+-------------------------+------------------------+
-
-    * <<<conf/mapred-site.xml>>>
-    
-      * Configurations for MapReduce JobHistory Server:
-
-*-------------------------+-------------------------+------------------------+
-|| Parameter              || Value                  || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.address>>> | | |
-| | MapReduce JobHistory Server <host:port> | Default port is 10020. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.keytab>>> | |
-| | </etc/security/keytab/jhs.service.keytab> | |
-| | | Kerberos keytab file for the MapReduce JobHistory Server. |
-*-------------------------+-------------------------+------------------------+
-| <<<mapreduce.jobhistory.principal>>> | mapred/_HOST@REALM.TLD | |
-| | | Kerberos principal name for the MapReduce JobHistory Server. |
-*-------------------------+-------------------------+------------------------+
-
-        
-  * {Operating the Hadoop Cluster}
-
-  Once all the necessary configuration is complete, distribute the files to the 
-  <<<HADOOP_CONF_DIR>>> directory on all the machines.
-
-  This section also describes the various Unix users who should be starting the
-  various components and uses the same Unix accounts and groups used previously:
-  
-    * Hadoop Startup
-  
-    To start a Hadoop cluster you will need to start both the HDFS and YARN 
-    cluster.
-
-    Format a new distributed filesystem as <hdfs>:
-  
-----
-[hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>
-----
-
-    Start the HDFS with the following command, run on the designated NameNode
-    as <hdfs>:
-  
-----
-[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
-----    	  
-
-    Run a script to start DataNodes on all slaves as <root> with a special
-    environment variable <<<HADOOP_SECURE_DN_USER>>> set to <hdfs>:
-
-----
-[root]$ HADOOP_SECURE_DN_USER=hdfs $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
-----    	  
-  
-    Start the YARN with the following command, run on the designated 
-    ResourceManager as <yarn>:
-  
-----
-[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager 
-----    	  
-
-    Run a script to start NodeManagers on all slaves as <yarn>:
-
-----
-[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager 
-----    	  
-
-    Start a standalone WebAppProxy server. Run on the WebAppProxy 
-    server as <yarn>.  If multiple servers are used with load balancing
-    it should be run on each of them:
-
-----
-[yarn]$ $YARN_HOME/bin/yarn start proxyserver --config $HADOOP_CONF_DIR  
-----    	  
-
-    Start the MapReduce JobHistory Server with the following command, run on the  
-    designated server as <mapred>:
-  
-----
-[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh start historyserver --config $HADOOP_CONF_DIR  
-----    	  
-
-    * Hadoop Shutdown      
-
-    Stop the NameNode with the following command, run on the designated NameNode
-    as <hdfs>:
-  
-----
-[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
-----    	  
-
-    Run a script to stop DataNodes on all slaves as <root>:
-
-----
-[root]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
-----    	  
-  
-    Stop the ResourceManager with the following command, run on the designated 
-    ResourceManager as <yarn>:
-  
-----
-[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager 
-----    	  
-
-    Run a script to stop NodeManagers on all slaves as <yarn>:
-
-----
-[yarn]$ $YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager 
-----    	  
-
-    Stop the WebAppProxy server. Run on the WebAppProxy  server as
-    <yarn>.  If multiple servers are used with load balancing it
-    should be run on each of them:
-
-----
-[yarn]$ $YARN_HOME/bin/yarn stop proxyserver --config $HADOOP_CONF_DIR  
-----
-
-    Stop the MapReduce JobHistory Server with the following command, run on the  
-    designated server as <mapred>:
-
-----
-[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh stop historyserver --config $HADOOP_CONF_DIR  
-----    	  
-    
-* {Web Interfaces}      
-
-    Once the Hadoop cluster is up and running check the web-ui of the 
-    components as described below:
-    
-*-------------------------+-------------------------+------------------------+
-|| Daemon                 || Web Interface          || Notes                 |
-*-------------------------+-------------------------+------------------------+
-| NameNode | http://<nn_host:port>/ | Default HTTP port is 50070. |
-*-------------------------+-------------------------+------------------------+
-| ResourceManager | http://<rm_host:port>/ | Default HTTP port is 8088. |
-*-------------------------+-------------------------+------------------------+
-| MapReduce JobHistory Server | http://<jhs_host:port>/ | |
-| | | Default HTTP port is 19888. |
-*-------------------------+-------------------------+------------------------+
-    
-    
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/EncryptedShuffle.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/EncryptedShuffle.apt.vm
deleted file mode 100644
index e05951c..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/EncryptedShuffle.apt.vm
+++ /dev/null
@@ -1,320 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop Map Reduce Next Generation-${project.version} - Encrypted Shuffle
-  ---
-  ---
-  ${maven.build.timestamp}
-
-Hadoop MapReduce Next Generation - Encrypted Shuffle
-
-  \[ {{{./index.html}Go Back}} \]
-
-* {Introduction}
-
-  The Encrypted Shuffle capability allows encryption of the MapReduce shuffle
-  using HTTPS and with optional client authentication (also known as
-  bi-directional HTTPS, or HTTPS with client certificates). It comprises:
-
-  * A Hadoop configuration setting for toggling the shuffle between HTTP and
-    HTTPS.
-
-  * A Hadoop configuration settings for specifying the keystore and truststore
-   properties (location, type, passwords) used by the shuffle service and the
-   reducers tasks fetching shuffle data.
-
-  * A way to re-load truststores across the cluster (when a node is added or
-    removed).
-
-* {Configuration}
-
-**  <<core-site.xml>> Properties
-
-  To enable encrypted shuffle, set the following properties in core-site.xml of
-  all nodes in the cluster:
-
-*--------------------------------------+---------------------+-----------------+
-| <<Property>>                         | <<Default Value>>   | <<Explanation>> |
-*--------------------------------------+---------------------+-----------------+
-| <<<hadoop.ssl.require.client.cert>>> | <<<false>>>         | Whether client certificates are required |
-*--------------------------------------+---------------------+-----------------+
-| <<<hadoop.ssl.hostname.verifier>>>   | <<<DEFAULT>>>       | The hostname verifier to provide for HttpsURLConnections. Valid values are: <<DEFAULT>>, <<STRICT>>, <<STRICT_I6>>, <<DEFAULT_AND_LOCALHOST>> and <<ALLOW_ALL>> |
-*--------------------------------------+---------------------+-----------------+
-| <<<hadoop.ssl.keystores.factory.class>>> | <<<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory>>> | The KeyStoresFactory implementation to use |
-*--------------------------------------+---------------------+-----------------+
-| <<<hadoop.ssl.server.conf>>>         | <<<ss-server.xml>>> | Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory |
-*--------------------------------------+---------------------+-----------------+
-| <<<hadoop.ssl.client.conf>>>         | <<<ss-client.xml>>> | Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory |
-*--------------------------------------+---------------------+-----------------+
-
-  <<IMPORTANT:>> Currently requiring client certificates should be set to false.
-  Refer the {{{ClientCertificates}Client Certificates}} section for details.
-
-  <<IMPORTANT:>> All these properties should be marked as final in the cluster
-  configuration files.
-
-*** Example:
-
-------
-    ...
-    <property>
-      <name>hadoop.ssl.require.client.cert</name>
-      <value>false</value>
-      <final>true</final>
-    </property>
-
-    <property>
-      <name>hadoop.ssl.hostname.verifier</name>
-      <value>DEFAULT</value>
-      <final>true</final>
-    </property>
-
-    <property>
-      <name>hadoop.ssl.keystores.factory.class</name>
-      <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
-      <final>true</final>
-    </property>
-
-    <property>
-      <name>hadoop.ssl.server.conf</name>
-      <value>ssl-server.xml</value>
-      <final>true</final>
-    </property>
-
-    <property>
-      <name>hadoop.ssl.client.conf</name>
-      <value>ssl-client.xml</value>
-      <final>true</final>
-    </property>
-    ...
-------
-
-**  <<<mapred-site.xml>>> Properties
-
-  To enable encrypted shuffle, set the following property in mapred-site.xml
-  of all nodes in the cluster:
-
-*--------------------------------------+---------------------+-----------------+
-| <<Property>>                         | <<Default Value>>   | <<Explanation>> |
-*--------------------------------------+---------------------+-----------------+
-| <<<mapreduce.shuffle.ssl.enabled>>>  | <<<false>>>         | Whether encrypted shuffle is enabled |
-*--------------------------------------+---------------------+-----------------+
-
-  <<IMPORTANT:>> This property should be marked as final in the cluster
-  configuration files.
-
-*** Example:
-
-------
-    ...
-    <property>
-      <name>mapreduce.shuffle.ssl.enabled</name>
-      <value>true</value>
-      <final>true</final>
-    </property>
-    ...
-------
-
-  The Linux container executor should be set to prevent job tasks from
-  reading the server keystore information and gaining access to the shuffle
-  server certificates.
-
-  Refer to Hadoop Kerberos configuration for details on how to do this.
-
-* {Keystore and Truststore Settings}
-
-  Currently <<<FileBasedKeyStoresFactory>>> is the only <<<KeyStoresFactory>>>
-  implementation. The <<<FileBasedKeyStoresFactory>>> implementation uses the
-  following properties, in the <<ssl-server.xml>> and <<ssl-client.xml>> files,
-  to configure the keystores and truststores.
-
-** <<<ssl-server.xml>>> (Shuffle server) Configuration:
-
-  The mapred user should own the <<ssl-server.xml>> file and have exclusive
-  read access to it.
-
-*---------------------------------------------+---------------------+-----------------+
-| <<Property>>                                | <<Default Value>>   | <<Explanation>> |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.keystore.type>>>              | <<<jks>>>           | Keystore file type |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.keystore.location>>>          | NONE                | Keystore file location. The mapred user should own this file and have exclusive read access to it. |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.keystore.password>>>          | NONE                | Keystore file password |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.truststore.type>>>            | <<<jks>>>           | Truststore file type |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.truststore.location>>>        | NONE                | Truststore file location. The mapred user should own this file and have exclusive read access to it. |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.truststore.password>>>        | NONE                | Truststore file password |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.server.truststore.reload.interval>>> | 10000               | Truststore reload interval, in milliseconds |
-*--------------------------------------+----------------------------+-----------------+
-
-*** Example:
-
-------
-<configuration>
-
-  <!-- Server Certificate Store -->
-  <property>
-    <name>ssl.server.keystore.type</name>
-    <value>jks</value>
-  </property>
-  <property>
-    <name>ssl.server.keystore.location</name>
-    <value>${user.home}/keystores/server-keystore.jks</value>
-  </property>
-  <property>
-    <name>ssl.server.keystore.password</name>
-    <value>serverfoo</value>
-  </property>
-
-  <!-- Server Trust Store -->
-  <property>
-    <name>ssl.server.truststore.type</name>
-    <value>jks</value>
-  </property>
-  <property>
-    <name>ssl.server.truststore.location</name>
-    <value>${user.home}/keystores/truststore.jks</value>
-  </property>
-  <property>
-    <name>ssl.server.truststore.password</name>
-    <value>clientserverbar</value>
-  </property>
-  <property>
-    <name>ssl.server.truststore.reload.interval</name>
-    <value>10000</value>
-  </property>
-</configuration>
-------
-
-** <<<ssl-client.xml>>> (Reducer/Fetcher) Configuration:
-
-  The mapred user should own the <<ssl-server.xml>> file and it should have
-  default permissions.
-
-*---------------------------------------------+---------------------+-----------------+
-| <<Property>>                                | <<Default Value>>   | <<Explanation>> |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.keystore.type>>>              | <<<jks>>>           | Keystore file type |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.keystore.location>>>          | NONE                | Keystore file location. The mapred user should own this file and it should have default permissions. |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.keystore.password>>>          | NONE                | Keystore file password |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.truststore.type>>>            | <<<jks>>>           | Truststore file type |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.truststore.location>>>        | NONE                | Truststore file location. The mapred user should own this file and it should have default permissions. |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.truststore.password>>>        | NONE                | Truststore file password |
-*---------------------------------------------+---------------------+-----------------+
-| <<<ssl.client.truststore.reload.interval>>> | 10000                | Truststore reload interval, in milliseconds |
-*--------------------------------------+----------------------------+-----------------+
-
-*** Example:
-
-------
-<configuration>
-
-  <!-- Client certificate Store -->
-  <property>
-    <name>ssl.client.keystore.type</name>
-    <value>jks</value>
-  </property>
-  <property>
-    <name>ssl.client.keystore.location</name>
-    <value>${user.home}/keystores/client-keystore.jks</value>
-  </property>
-  <property>
-    <name>ssl.client.keystore.password</name>
-    <value>clientfoo</value>
-  </property>
-
-  <!-- Client Trust Store -->
-  <property>
-    <name>ssl.client.truststore.type</name>
-    <value>jks</value>
-  </property>
-  <property>
-    <name>ssl.client.truststore.location</name>
-    <value>${user.home}/keystores/truststore.jks</value>
-  </property>
-  <property>
-    <name>ssl.client.truststore.password</name>
-    <value>clientserverbar</value>
-  </property>
-  <property>
-    <name>ssl.client.truststore.reload.interval</name>
-    <value>10000</value>
-  </property>
-</configuration>
-------
-
-* Activating Encrypted Shuffle
-
-  When you have made the above configuration changes, activate Encrypted
-  Shuffle by re-starting all NodeManagers.
-
-  <<IMPORTANT:>> Using encrypted shuffle will incur in a significant
-  performance impact. Users should profile this and potentially reserve
-  1 or more cores for encrypted shuffle.
-
-* {ClientCertificates} Client Certificates
-
-  Using Client Certificates does not fully ensure that the client is a
-  reducer task for the job. Currently, Client Certificates (their private key)
-  keystore files must be readable by all users submitting jobs to the cluster.
-  This means that a rogue job could read such those keystore files and use
-  the client certificates in them to establish a secure connection with a
-  Shuffle server. However, unless the rogue job has a proper JobToken, it won't
-  be able to retrieve shuffle data from the Shuffle server. A job, using its
-  own JobToken, can only retrieve shuffle data that belongs to itself.
-
-* Reloading Truststores
-
-  By default the truststores will reload their configuration every 10 seconds.
-  If a new truststore file is copied over the old one, it will be re-read,
-  and its certificates will replace the old ones. This mechanism is useful for
-  adding or removing nodes from the cluster, or for adding or removing trusted
-  clients. In these cases, the client or NodeManager certificate is added to
-  (or removed from) all the truststore files in the system, and the new
-  configuration will be picked up without you having to restart the NodeManager
-  daemons.
-
-* Debugging
-
-  <<NOTE:>> Enable debugging only for troubleshooting, and then only for jobs
-  running on small amounts of data. It is very verbose and slows down jobs by
-  several orders of magnitude. (You might need to increase mapred.task.timeout
-  to prevent jobs from failing because tasks run so slowly.)
-
-  To enable SSL debugging in the reducers, set <<<-Djavax.net.debug=all>>> in
-  the <<<mapreduce.reduce.child.java.opts>>> property; for example:
-
-------
-  <property>
-    <name>mapred.reduce.child.java.opts</name>
-    <value>-Xmx-200m -Djavax.net.debug=all</value>
-  </property>
-------
-
-  You can do this on a per-job basis, or by means of a cluster-wide setting in
-  the <<<mapred-site.xml>>> file.
-
-  To set this property in NodeManager, set it in the <<<yarn-env.sh>>> file:
-
-------
-  YARN_NODEMANAGER_OPTS="-Djavax.net.debug=all $YARN_NODEMANAGER_OPTS"
-------
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/Federation.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/Federation.apt.vm
deleted file mode 100644
index c7c8770..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/Federation.apt.vm
+++ /dev/null
@@ -1,342 +0,0 @@
-
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop Distributed File System-${project.version} - Federation
-  ---
-  ---
-  ${maven.build.timestamp}
-
-HDFS Federation
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-  This guide provides an overview of the HDFS Federation feature and
-  how to configure and manage the federated cluster.
-
-* {Background}
-
-[./federation-background.gif] HDFS Layers
-
-  HDFS has two main layers:
-
-  * <<Namespace>>
-
-    * Consists of directories, files and blocks
-
-    * It supports all the namespace related file system operations such as 
-      create, delete, modify and list files and directories.
-
-  * <<Block Storage Service>> has two parts
-
-    * Block Management (which is done in Namenode)
-
-      * Provides datanode cluster membership by handling registrations, and 
-        periodic heart beats.
-
-      * Processes block reports and maintains location of blocks.
-
-      * Supports block related operations such as create, delete, modify and 
-        get block location.
-
-      * Manages replica placement and replication of a block for under 
-        replicated blocks and deletes blocks that are over replicated.
-
-    * Storage - is provided by datanodes by storing blocks on the local file 
-      system and allows read/write access.
-
-  The prior HDFS architecture allows only a single namespace for the 
-  entire cluster. A single Namenode manages this namespace. HDFS 
-  Federation addresses limitation of the prior architecture by adding 
-  support multiple Namenodes/namespaces to HDFS file system.
-    
-* {Multiple Namenodes/Namespaces}
-
-  In order to scale the name service horizontally, federation uses multiple 
-  independent Namenodes/namespaces. The Namenodes are federated, that is, the 
-  Namenodes are independent and don’t require coordination with each other. 
-  The datanodes are used as common storage for blocks by all the Namenodes. 
-  Each datanode registers with all the Namenodes in the cluster. Datanodes 
-  send periodic heartbeats and block reports and handles commands from the 
-  Namenodes.
-
-[./federation.gif] HDFS Federation Architecture
-
-
-  <<Block Pool>>
-
-  A Block Pool is a set of blocks that belong to a single namespace. 
-  Datanodes store blocks for all the block pools in the cluster.
-  It is managed independently of other block pools. This allows a namespace 
-  to generate Block IDs for new blocks without the need for coordination 
-  with the other namespaces. The failure of a Namenode does not prevent 
-  the datanode from serving other Namenodes in the cluster.
-
-  A Namespace and its block pool together are called Namespace Volume. 
-  It is a self-contained unit of management. When a Namenode/namespace 
-  is deleted, the corresponding block pool at the datanodes is deleted.
-  Each namespace volume is upgraded as a unit, during cluster upgrade.
-
-  <<ClusterID>>
-
-  A new identifier <<ClusterID>> is added to identify all the nodes in 
-  the cluster.  When a Namenode is formatted, this identifier is provided 
-  or auto generated. This ID should be used for formatting the other 
-  Namenodes into the cluster.
-
-** Key Benefits
-
-  * Namespace Scalability - HDFS cluster storage scales horizontally but 
-    the namespace does not. Large deployments or deployments using lot 
-    of small files benefit from scaling the namespace by adding more 
-    Namenodes to the cluster
-
-  * Performance - File system operation throughput is limited by a single
-    Namenode in the prior architecture. Adding more Namenodes to the cluster
-    scales the file system read/write operations throughput.
-
-  * Isolation - A single Namenode offers no isolation in multi user 
-    environment. An experimental application can overload the Namenode 
-    and slow down production critical applications. With multiple Namenodes, 
-    different categories of applications and users can be isolated to 
-    different namespaces.
-
-* {Federation Configuration}
-
-  Federation configuration is <<backward compatible>> and allows existing 
-  single Namenode configuration to work without any change. The new 
-  configuration is designed such that all the nodes in the cluster have 
-  same configuration without the need for deploying different configuration 
-  based on the type of the node in the cluster.
-
-  A new abstraction called <<<NameServiceID>>> is added with
-  federation. The Namenode and its corresponding secondary/backup/checkpointer
-  nodes belong to this. To support single configuration file, the Namenode and
-  secondary/backup/checkpointer configuration parameters are suffixed with
-  <<<NameServiceID>>> and are added to the same configuration file.
-
-
-** Configuration:
-
-  <<Step 1>>: Add the following parameters to your configuration:
-  <<<dfs.nameservices>>>: Configure with list of comma separated 
-  NameServiceIDs. This will be used by Datanodes to determine all the 
-  Namenodes in the cluster.
-  
-  <<Step 2>>: For each Namenode and Secondary Namenode/BackupNode/Checkpointer 
-  add the following configuration suffixed with the corresponding 
-  <<<NameServiceID>>> into the common configuration file.
-
-*---------------------+--------------------------------------------+
-|| Daemon             || Configuration Parameter                   |
-*---------------------+--------------------------------------------+
-| Namenode            | <<<dfs.namenode.rpc-address>>>             |
-|                     | <<<dfs.namenode.servicerpc-address>>>      |
-|                     | <<<dfs.namenode.http-address>>>            |
-|                     | <<<dfs.namenode.https-address>>>           |
-|                     | <<<dfs.namenode.keytab.file>>>             |
-|                     | <<<dfs.namenode.name.dir>>>                |
-|                     | <<<dfs.namenode.edits.dir>>>               |
-|                     | <<<dfs.namenode.checkpoint.dir>>>          |
-|                     | <<<dfs.namenode.checkpoint.edits.dir>>>    |
-*---------------------+--------------------------------------------+
-| Secondary Namenode  | <<<dfs.namenode.secondary.http-address>>>  |
-|                     | <<<dfs.secondary.namenode.keytab.file>>>   |
-*---------------------+--------------------------------------------+
-| BackupNode          | <<<dfs.namenode.backup.address>>>          |
-|                     | <<<dfs.secondary.namenode.keytab.file>>>   |
-*---------------------+--------------------------------------------+
-    
-  Here is an example configuration with two namenodes:
-
-----
-<configuration>
-  <property>
-    <name>dfs.nameservices</name>
-    <value>ns1,ns2</value>
-  </property>
-  <property>
-    <name>dfs.namenode.rpc-address.ns1</name>
-    <value>nn-host1:rpc-port</value>
-  </property>
-  <property>
-    <name>dfs.namenode.http-address.ns1</name>
-    <value>nn-host1:http-port</value>
-  </property>
-  <property>
-    <name>dfs.namenode.secondaryhttp-address.ns1</name>
-    <value>snn-host1:http-port</value>
-  </property>
-  <property>
-    <name>dfs.namenode.rpc-address.ns2</name>
-    <value>nn-host2:rpc-port</value>
-  </property>
-  <property>
-    <name>dfs.namenode.http-address.ns2</name>
-    <value>nn-host2:http-port</value>
-  </property>
-  <property>
-    <name>dfs.namenode.secondaryhttp-address.ns2</name>
-    <value>snn-host2:http-port</value>
-  </property>
-
-  .... Other common configuration ...
-</configuration>
-----
-
-** Formatting Namenodes
-
-  <<Step 1>>: Format a namenode using the following command:
-  
-----
-> $HADOOP_PREFIX_HOME/bin/hdfs namenode -format [-clusterId <cluster_id>]
-----
-  Choose a unique cluster_id, which will not conflict other clusters in 
-  your environment. If it is not provided, then a unique ClusterID is 
-  auto generated.
-
-  <<Step 2>>: Format additional namenode using the following command:
-
-----
-> $HADOOP_PREFIX_HOME/bin/hdfs namenode -format -clusterId <cluster_id>
-----
-  Note that the cluster_id in step 2 must be same as that of the 
-  cluster_id in step 1. If they are different, the additional Namenodes 
-  will not be part of the federated cluster.
-
-** Upgrading from older release to 0.23 and configuring federation
-
-  Older releases supported a single Namenode. Here are the steps enable 
-  federation:
-
-  Step 1: Upgrade the cluster to newer release. During upgrade you can 
-  provide a ClusterID as follows:
-
-----
-> $HADOOP_PREFIX_HOME/bin/hdfs start namenode --config $HADOOP_CONF_DIR  -upgrade -clusterId <cluster_ID>
-----
-  If ClusterID is not provided, it is auto generated.
-
-** Adding a new Namenode to an existing HDFS cluster
-
-  Follow the following steps:
-
-  * Add configuration parameter <<<dfs.nameservices>>> to the configuration.
-
-  * Update the configuration with NameServiceID suffix. Configuration 
-    key names have changed post release 0.20. You must use new configuration 
-    parameter names, for federation.
-
-  * Add new Namenode related config to the configuration files.
-
-  * Propagate the configuration file to the all the nodes in the cluster.
-
-  * Start the new Namenode, Secondary/Backup.
-
-  * Refresh the datanodes to pickup the newly added Namenode by running 
-    the following command:
-
-----
-> $HADOOP_PREFIX_HOME/bin/hdfs dfadmin -refreshNameNode <datanode_host_name>:<datanode_rpc_port>
-----
-
-  * The above command must be run against all the datanodes in the cluster.
-
-* {Managing the cluster}
-
-**  Starting and stopping cluster
-
-  To start the cluster run the following command:
-
-----
-> $HADOOP_PREFIX_HOME/bin/start-dfs.sh
-----
-
-  To stop the cluster run the following command:
-
-----
-> $HADOOP_PREFIX_HOME/bin/stop-dfs.sh
-----
-
-  These commands can be run from any node where the HDFS configuration is 
-  available.  The command uses configuration to determine the Namenodes 
-  in the cluster and starts the Namenode process on those nodes. The 
-  datanodes are started on nodes specified in the <<<slaves>>> file. The 
-  script can be used as reference for building your own scripts for 
-  starting and stopping the cluster.
-
-**  Balancer
-
-  Balancer has been changed to work with multiple Namenodes in the cluster to 
-  balance the cluster. Balancer can be run using the command:
-
-----
-"$HADOOP_PREFIX"/bin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script "$bin"/hdfs start balancer [-policy <policy>]
-----
-
-  Policy could be:
-
-  * <<<node>>> - this is the <default> policy. This balances the storage at 
-    the datanode level. This is similar to balancing policy from prior releases.
-
-  * <<<blockpool>>> - this balances the storage at the block pool level. 
-    Balancing at block pool level balances storage at the datanode level also.
-
-  Note that Balander only balances the data and does not balance the namespace.
-
-** Decommissioning
-
-  Decommissioning is similar to prior releases. The nodes that need to be 
-  decomissioned are added to the exclude file at all the Namenode. Each 
-  Namenode decommissions its Block Pool. When all the Namenodes finish 
-  decommissioning a datanode, the datanode is considered to be decommissioned.
-
-  <<Step 1>>: To distributed an exclude file to all the Namenodes, use the 
-  following command:
-
-----
-"$HADOOP_PREFIX"/bin/distributed-exclude.sh <exclude_file>
-----
-
-  <<Step 2>>: Refresh all the Namenodes to pick up the new exclude file.
-
-----
-"$HADOOP_PREFIX"/bin/refresh-namenodes.sh
-----
- 
-  The above command uses HDFS configuration to determine the Namenodes 
-  configured in the cluster and refreshes all the Namenodes to pick up 
-  the new exclude file.
-
-** Cluster Web Console
-
-  Similar to Namenode status web page, a Cluster Web Console is added in 
-  federation to monitor the federated cluster at 
-  <<<http://<any_nn_host:port>/dfsclusterhealth.jsp>>>.
-  Any Namenode in the cluster can be used to access this web page.
-
-  The web page provides the following information:
-
-  * Cluster summary that shows number of files, number of blocks and 
-    total configured storage capacity, available and used storage information 
-    for the entire cluster.
-
-  * Provides list of Namenodes and summary that includes number of files,
-    blocks, missing blocks, number of live and dead data nodes for each 
-    Namenode. It also provides a link to conveniently access Namenode web UI.
-
-  * It also provides decommissioning status of datanodes.
-
-
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm
deleted file mode 100644
index efa3f93..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithNFS.apt.vm
+++ /dev/null
@@ -1,880 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop Distributed File System-${project.version} - High Availability
-  ---
-  ---
-  ${maven.build.timestamp}
-
-HDFS High Availability
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-* {Purpose}
-
-  This guide provides an overview of the HDFS High Availability (HA) feature and
-  how to configure and manage an HA HDFS cluster, using NFS for the shared
-  storage required by the NameNodes.
- 
-  This document assumes that the reader has a general understanding of
-  general components and node types in an HDFS cluster. Please refer to the
-  HDFS Architecture guide for details.
-
-* {Note: Using the Quorum Journal Manager or Conventional Shared Storage}
-
-  This guide discusses how to configure and use HDFS HA using a shared NFS
-  directory to share edit logs between the Active and Standby NameNodes. For
-  information on how to configure HDFS HA using the Quorum Journal Manager
-  instead of NFS, please see {{{./HDFSHighAvailabilityWithQJM.html}this
-  alternative guide.}}
-
-* {Background}
-
-  Prior to Hadoop 2.0.0, the NameNode was a single point of failure (SPOF) in
-  an HDFS cluster. Each cluster had a single NameNode, and if that machine or
-  process became unavailable, the cluster as a whole would be unavailable
-  until the NameNode was either restarted or brought up on a separate machine.
-  
-  This impacted the total availability of the HDFS cluster in two major ways:
-
-    * In the case of an unplanned event such as a machine crash, the cluster would
-      be unavailable until an operator restarted the NameNode.
-
-    * Planned maintenance events such as software or hardware upgrades on the
-      NameNode machine would result in windows of cluster downtime.
-  
-  The HDFS High Availability feature addresses the above problems by providing
-  the option of running two redundant NameNodes in the same cluster in an
-  Active/Passive configuration with a hot standby. This allows a fast failover to
-  a new NameNode in the case that a machine crashes, or a graceful
-  administrator-initiated failover for the purpose of planned maintenance.
-
-* {Architecture}
-
-  In a typical HA cluster, two separate machines are configured as NameNodes.
-  At any point in time, exactly one of the NameNodes is in an <Active> state,
-  and the other is in a <Standby> state. The Active NameNode is responsible
-  for all client operations in the cluster, while the Standby is simply acting
-  as a slave, maintaining enough state to provide a fast failover if
-  necessary.
-  
-  In order for the Standby node to keep its state synchronized with the Active
-  node, the current implementation requires that the two nodes both have access
-  to a directory on a shared storage device (eg an NFS mount from a NAS). This
-  restriction will likely be relaxed in future versions.
-
-  When any namespace modification is performed by the Active node, it durably
-  logs a record of the modification to an edit log file stored in the shared
-  directory.  The Standby node is constantly watching this directory for edits,
-  and as it sees the edits, it applies them to its own namespace. In the event of
-  a failover, the Standby will ensure that it has read all of the edits from the
-  shared storage before promoting itself to the Active state. This ensures that
-  the namespace state is fully synchronized before a failover occurs.
-  
-  In order to provide a fast failover, it is also necessary that the Standby node
-  have up-to-date information regarding the location of blocks in the cluster.
-  In order to achieve this, the DataNodes are configured with the location of
-  both NameNodes, and send block location information and heartbeats to both.
-  
-  It is vital for the correct operation of an HA cluster that only one of the
-  NameNodes be Active at a time. Otherwise, the namespace state would quickly
-  diverge between the two, risking data loss or other incorrect results.  In
-  order to ensure this property and prevent the so-called "split-brain scenario,"
-  the administrator must configure at least one <fencing method> for the shared
-  storage. During a failover, if it cannot be verified that the previous Active
-  node has relinquished its Active state, the fencing process is responsible for
-  cutting off the previous Active's access to the shared edits storage. This
-  prevents it from making any further edits to the namespace, allowing the new
-  Active to safely proceed with failover.
-
-* {Hardware resources}
-
-  In order to deploy an HA cluster, you should prepare the following:
-
-    * <<NameNode machines>> - the machines on which you run the Active and
-    Standby NameNodes should have equivalent hardware to each other, and
-    equivalent hardware to what would be used in a non-HA cluster.
-
-    * <<Shared storage>> - you will need to have a shared directory which both
-    NameNode machines can have read/write access to. Typically this is a remote
-    filer which supports NFS and is mounted on each of the NameNode machines.
-    Currently only a single shared edits directory is supported. Thus, the
-    availability of the system is limited by the availability of this shared edits
-    directory, and therefore in order to remove all single points of failure there
-    needs to be redundancy for the shared edits directory. Specifically, multiple
-    network paths to the storage, and redundancy in the storage itself (disk,
-    network, and power). Beacuse of this, it is recommended that the shared storage
-    server be a high-quality dedicated NAS appliance rather than a simple Linux
-    server.
-  
-  Note that, in an HA cluster, the Standby NameNode also performs checkpoints of
-  the namespace state, and thus it is not necessary to run a Secondary NameNode,
-  CheckpointNode, or BackupNode in an HA cluster. In fact, to do so would be an
-  error. This also allows one who is reconfiguring a non-HA-enabled HDFS cluster
-  to be HA-enabled to reuse the hardware which they had previously dedicated to
-  the Secondary NameNode.
-
-* {Deployment}
-
-** Configuration overview
-
-  Similar to Federation configuration, HA configuration is backward compatible
-  and allows existing single NameNode configurations to work without change.
-  The new configuration is designed such that all the nodes in the cluster may
-  have the same configuration without the need for deploying different
-  configuration files to different machines based on the type of the node.
- 
-  Like HDFS Federation, HA clusters reuse the <<<nameservice ID>>> to identify a
-  single HDFS instance that may in fact consist of multiple HA NameNodes. In
-  addition, a new abstraction called <<<NameNode ID>>> is added with HA. Each
-  distinct NameNode in the cluster has a different NameNode ID to distinguish it.
-  To support a single configuration file for all of the NameNodes, the relevant
-  configuration parameters are suffixed with the <<nameservice ID>> as well as
-  the <<NameNode ID>>.
-
-** Configuration details
-
-  To configure HA NameNodes, you must add several configuration options to your
-  <<hdfs-site.xml>> configuration file.
-
-  The order in which you set these configurations is unimportant, but the values
-  you choose for <<dfs.nameservices>> and
-  <<dfs.ha.namenodes.[nameservice ID]>> will determine the keys of those that
-  follow. Thus, you should decide on these values before setting the rest of the
-  configuration options.
-
-  * <<dfs.nameservices>> - the logical name for this new nameservice
-
-    Choose a logical name for this nameservice, for example "mycluster", and use
-    this logical name for the value of this config option. The name you choose is
-    arbitrary. It will be used both for configuration and as the authority
-    component of absolute HDFS paths in the cluster.
-
-    <<Note:>> If you are also using HDFS Federation, this configuration setting
-    should also include the list of other nameservices, HA or otherwise, as a
-    comma-separated list.
-
-----
-<property>
-  <name>dfs.nameservices</name>
-  <value>mycluster</value>
-</property>
-----
-
-  * <<dfs.ha.namenodes.[nameservice ID]>> - unique identifiers for each NameNode in the nameservice
-
-    Configure with a list of comma-separated NameNode IDs. This will be used by
-    DataNodes to determine all the NameNodes in the cluster. For example, if you
-    used "mycluster" as the nameservice ID previously, and you wanted to use "nn1"
-    and "nn2" as the individual IDs of the NameNodes, you would configure this as
-    such:
-
-----
-<property>
-  <name>dfs.ha.namenodes.mycluster</name>
-  <value>nn1,nn2</value>
-</property>
-----
-
-    <<Note:>> Currently, only a maximum of two NameNodes may be configured per
-    nameservice.
-
-  * <<dfs.namenode.rpc-address.[nameservice ID].[name node ID]>> - the fully-qualified RPC address for each NameNode to listen on
-
-    For both of the previously-configured NameNode IDs, set the full address and
-    IPC port of the NameNode processs. Note that this results in two separate
-    configuration options. For example:
-
-----
-<property>
-  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
-  <value>machine1.example.com:8020</value>
-</property>
-<property>
-  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
-  <value>machine2.example.com:8020</value>
-</property>
-----
-
-    <<Note:>> You may similarly configure the "<<servicerpc-address>>" setting if
-    you so desire.
-
-  * <<dfs.namenode.http-address.[nameservice ID].[name node ID]>> - the fully-qualified HTTP address for each NameNode to listen on
-
-    Similarly to <rpc-address> above, set the addresses for both NameNodes' HTTP
-    servers to listen on. For example:
-
-----
-<property>
-  <name>dfs.namenode.http-address.mycluster.nn1</name>
-  <value>machine1.example.com:50070</value>
-</property>
-<property>
-  <name>dfs.namenode.http-address.mycluster.nn2</name>
-  <value>machine2.example.com:50070</value>
-</property>
-----
-
-    <<Note:>> If you have Hadoop's security features enabled, you should also set
-    the <https-address> similarly for each NameNode.
-
-  * <<dfs.namenode.shared.edits.dir>> - the location of the shared storage directory
-
-    This is where one configures the path to the remote shared edits directory
-    which the Standby NameNode uses to stay up-to-date with all the file system
-    changes the Active NameNode makes. <<You should only configure one of these
-    directories.>> This directory should be mounted r/w on both NameNode machines.
-    The value of this setting should be the absolute path to this directory on the
-    NameNode machines. For example:
-
-----
-<property>
-  <name>dfs.namenode.shared.edits.dir</name>
-  <value>file:///mnt/filer1/dfs/ha-name-dir-shared</value>
-</property>
-----
-
-  * <<dfs.client.failover.proxy.provider.[nameservice ID]>> - the Java class that HDFS clients use to contact the Active NameNode
-
-    Configure the name of the Java class which will be used by the DFS Client to
-    determine which NameNode is the current Active, and therefore which NameNode is
-    currently serving client requests. The only implementation which currently
-    ships with Hadoop is the <<ConfiguredFailoverProxyProvider>>, so use this
-    unless you are using a custom one. For example:
-
-----
-<property>
-  <name>dfs.client.failover.proxy.provider.mycluster</name>
-  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
-</property>
-----
-
-  * <<dfs.ha.fencing.methods>> - a list of scripts or Java classes which will be used to fence the Active NameNode during a failover
-
-    It is critical for correctness of the system that only one NameNode be in the
-    Active state at any given time. Thus, during a failover, we first ensure that
-    the Active NameNode is either in the Standby state, or the process has
-    terminated, before transitioning the other NameNode to the Active state. In
-    order to do this, you must configure at least one <<fencing method.>> These are
-    configured as a carriage-return-separated list, which will be attempted in order
-    until one indicates that fencing has succeeded. There are two methods which
-    ship with Hadoop: <shell> and <sshfence>. For information on implementing
-    your own custom fencing method, see the <org.apache.hadoop.ha.NodeFencer> class.
-
-    * <<sshfence>> - SSH to the Active NameNode and kill the process
-
-      The <sshfence> option SSHes to the target node and uses <fuser> to kill the
-      process listening on the service's TCP port. In order for this fencing option
-      to work, it must be able to SSH to the target node without providing a
-      passphrase. Thus, one must also configure the
-      <<dfs.ha.fencing.ssh.private-key-files>> option, which is a
-      comma-separated list of SSH private key files. For example:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>sshfence</value>
-</property>
-
-<property>
-  <name>dfs.ha.fencing.ssh.private-key-files</name>
-  <value>/home/exampleuser/.ssh/id_rsa</value>
-</property>
----
-
-      Optionally, one may configure a non-standard username or port to perform the
-      SSH. One may also configure a timeout, in milliseconds, for the SSH, after
-      which this fencing method will be considered to have failed. It may be
-      configured like so:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>sshfence([[username][:port]])</value>
-</property>
-<property>
-  <name>dfs.ha.fencing.ssh.connect-timeout</name>
-  <value>30000</value>
-</property>
----
-
-    * <<shell>> - run an arbitrary shell command to fence the Active NameNode
-
-      The <shell> fencing method runs an arbitrary shell command. It may be
-      configured like so:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>shell(/path/to/my/script.sh arg1 arg2 ...)</value>
-</property>
----
-
-      The string between '(' and ')' is passed directly to a bash shell and may not
-      include any closing parentheses.
-
-      The shell command will be run with an environment set up to contain all of the
-      current Hadoop configuration variables, with the '_' character replacing any
-      '.' characters in the configuration keys. The configuration used has already had
-      any namenode-specific configurations promoted to their generic forms -- for example
-      <<dfs_namenode_rpc-address>> will contain the RPC address of the target node, even
-      though the configuration may specify that variable as
-      <<dfs.namenode.rpc-address.ns1.nn1>>.
-      
-      Additionally, the following variables referring to the target node to be fenced
-      are also available:
-
-*-----------------------:-----------------------------------+
-| $target_host          | hostname of the node to be fenced |
-*-----------------------:-----------------------------------+
-| $target_port          | IPC port of the node to be fenced |
-*-----------------------:-----------------------------------+
-| $target_address       | the above two, combined as host:port |
-*-----------------------:-----------------------------------+
-| $target_nameserviceid | the nameservice ID of the NN to be fenced |
-*-----------------------:-----------------------------------+
-| $target_namenodeid    | the namenode ID of the NN to be fenced |
-*-----------------------:-----------------------------------+
-      
-      These environment variables may also be used as substitutions in the shell
-      command itself. For example:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>shell(/path/to/my/script.sh --nameservice=$target_nameserviceid $target_host:$target_port)</value>
-</property>
----
-      
-      If the shell command returns an exit
-      code of 0, the fencing is determined to be successful. If it returns any other
-      exit code, the fencing was not successful and the next fencing method in the
-      list will be attempted.
-
-      <<Note:>> This fencing method does not implement any timeout. If timeouts are
-      necessary, they should be implemented in the shell script itself (eg by forking
-      a subshell to kill its parent in some number of seconds).
-
-  * <<fs.defaultFS>> - the default path prefix used by the Hadoop FS client when none is given
-
-    Optionally, you may now configure the default path for Hadoop clients to use
-    the new HA-enabled logical URI. If you used "mycluster" as the nameservice ID
-    earlier, this will be the value of the authority portion of all of your HDFS
-    paths. This may be configured like so, in your <<core-site.xml>> file:
-
----
-<property>
-  <name>fs.defaultFS</name>
-  <value>hdfs://mycluster</value>
-</property>
----
-
-** Deployment details
-
-  After all of the necessary configuration options have been set, one must
-  initially synchronize the two HA NameNodes' on-disk metadata.
-
-    * If you are setting up a fresh HDFS cluster, you should first run the format
-    command (<hdfs namenode -format>) on one of NameNodes.
-  
-    * If you have already formatted the NameNode, or are converting a
-    non-HA-enabled cluster to be HA-enabled, you should now copy over the
-    contents of your NameNode metadata directories to the other, unformatted
-    NameNode by running the command "<hdfs namenode -bootstrapStandby>" on the
-    unformatted NameNode. Running this command will also ensure that the shared
-    edits directory (as configured by <<dfs.namenode.shared.edits.dir>>) contains
-    sufficient edits transactions to be able to start both NameNodes.
-  
-    * If you are converting a non-HA NameNode to be HA, you should run the
-    command "<hdfs -initializeSharedEdits>", which will initialize the shared
-    edits directory with the edits data from the local NameNode edits directories.
-
-  At this point you may start both of your HA NameNodes as you normally would
-  start a NameNode.
-
-  You can visit each of the NameNodes' web pages separately by browsing to their
-  configured HTTP addresses. You should notice that next to the configured
-  address will be the HA state of the NameNode (either "standby" or "active".)
-  Whenever an HA NameNode starts, it is initially in the Standby state.
-
-** Administrative commands
-
-  Now that your HA NameNodes are configured and started, you will have access
-  to some additional commands to administer your HA HDFS cluster. Specifically,
-  you should familiarize yourself with all of the subcommands of the "<hdfs
-  haadmin>" command. Running this command without any additional arguments will
-  display the following usage information:
-
----
-Usage: DFSHAAdmin [-ns <nameserviceId>]
-    [-transitionToActive <serviceId>]
-    [-transitionToStandby <serviceId>]
-    [-failover [--forcefence] [--forceactive] <serviceId> <serviceId>]
-    [-getServiceState <serviceId>]
-    [-checkHealth <serviceId>]
-    [-help <command>]
----
-
-  This guide describes high-level uses of each of these subcommands. For
-  specific usage information of each subcommand, you should run "<hdfs haadmin
-  -help <command>>".
-
-  * <<transitionToActive>> and <<transitionToStandby>> - transition the state of the given NameNode to Active or Standby
-
-    These subcommands cause a given NameNode to transition to the Active or Standby
-    state, respectively. <<These commands do not attempt to perform any fencing,
-    and thus should rarely be used.>> Instead, one should almost always prefer to
-    use the "<hdfs haadmin -failover>" subcommand.
-
-  * <<failover>> - initiate a failover between two NameNodes
-
-    This subcommand causes a failover from the first provided NameNode to the
-    second. If the first NameNode is in the Standby state, this command simply
-    transitions the second to the Active state without error. If the first NameNode
-    is in the Active state, an attempt will be made to gracefully transition it to
-    the Standby state. If this fails, the fencing methods (as configured by
-    <<dfs.ha.fencing.methods>>) will be attempted in order until one
-    succeeds. Only after this process will the second NameNode be transitioned to
-    the Active state. If no fencing method succeeds, the second NameNode will not
-    be transitioned to the Active state, and an error will be returned.
-
-  * <<getServiceState>> - determine whether the given NameNode is Active or Standby
-
-    Connect to the provided NameNode to determine its current state, printing
-    either "standby" or "active" to STDOUT appropriately. This subcommand might be
-    used by cron jobs or monitoring scripts which need to behave differently based
-    on whether the NameNode is currently Active or Standby.
-
-  * <<checkHealth>> - check the health of the given NameNode
-
-    Connect to the provided NameNode to check its health. The NameNode is capable
-    of performing some diagnostics on itself, including checking if internal
-    services are running as expected. This command will return 0 if the NameNode is
-    healthy, non-zero otherwise. One might use this command for monitoring
-    purposes.
-
-    <<Note:>> This is not yet implemented, and at present will always return
-    success, unless the given NameNode is completely down.
-
-* {Automatic Failover}
-
-** Introduction
-
-  The above sections describe how to configure manual failover. In that mode,
-  the system will not automatically trigger a failover from the active to the
-  standby NameNode, even if the active node has failed. This section describes
-  how to configure and deploy automatic failover.
-
-** Components
-
-  Automatic failover adds two new components to an HDFS deployment: a ZooKeeper
-  quorum, and the ZKFailoverController process (abbreviated as ZKFC).
-
-  Apache ZooKeeper is a highly available service for maintaining small amounts
-  of coordination data, notifying clients of changes in that data, and
-  monitoring clients for failures. The implementation of automatic HDFS failover
-  relies on ZooKeeper for the following things:
-  
-    * <<Failure detection>> - each of the NameNode machines in the cluster
-    maintains a persistent session in ZooKeeper. If the machine crashes, the
-    ZooKeeper session will expire, notifying the other NameNode that a failover
-    should be triggered.
-
-    * <<Active NameNode election>> - ZooKeeper provides a simple mechanism to
-    exclusively elect a node as active. If the current active NameNode crashes,
-    another node may take a special exclusive lock in ZooKeeper indicating that
-    it should become the next active.
-
-  The ZKFailoverController (ZKFC) is a new component which is a ZooKeeper client
-  which also monitors and manages the state of the NameNode.  Each of the
-  machines which runs a NameNode also runs a ZKFC, and that ZKFC is responsible
-  for:
-
-    * <<Health monitoring>> - the ZKFC pings its local NameNode on a periodic
-    basis with a health-check command. So long as the NameNode responds in a
-    timely fashion with a healthy status, the ZKFC considers the node
-    healthy. If the node has crashed, frozen, or otherwise entered an unhealthy
-    state, the health monitor will mark it as unhealthy.
-
-    * <<ZooKeeper session management>> - when the local NameNode is healthy, the
-    ZKFC holds a session open in ZooKeeper. If the local NameNode is active, it
-    also holds a special "lock" znode. This lock uses ZooKeeper's support for
-    "ephemeral" nodes; if the session expires, the lock node will be
-    automatically deleted.
-
-    * <<ZooKeeper-based election>> - if the local NameNode is healthy, and the
-    ZKFC sees that no other node currently holds the lock znode, it will itself
-    try to acquire the lock. If it succeeds, then it has "won the election", and
-    is responsible for running a failover to make its local NameNode active. The
-    failover process is similar to the manual failover described above: first,
-    the previous active is fenced if necessary, and then the local NameNode
-    transitions to active state.
-
-  For more details on the design of automatic failover, refer to the design
-  document attached to HDFS-2185 on the Apache HDFS JIRA.
-
-** Deploying ZooKeeper
-
-  In a typical deployment, ZooKeeper daemons are configured to run on three or
-  five nodes. Since ZooKeeper itself has light resource requirements, it is
-  acceptable to collocate the ZooKeeper nodes on the same hardware as the HDFS
-  NameNode and Standby Node. Many operators choose to deploy the third ZooKeeper
-  process on the same node as the YARN ResourceManager. It is advisable to
-  configure the ZooKeeper nodes to store their data on separate disk drives from
-  the HDFS metadata for best performance and isolation.
-
-  The setup of ZooKeeper is out of scope for this document. We will assume that
-  you have set up a ZooKeeper cluster running on three or more nodes, and have
-  verified its correct operation by connecting using the ZK CLI.
-
-** Before you begin
-
-  Before you begin configuring automatic failover, you should shut down your
-  cluster. It is not currently possible to transition from a manual failover
-  setup to an automatic failover setup while the cluster is running.
-
-** Configuring automatic failover
-
-  The configuration of automatic failover requires the addition of two new
-  parameters to your configuration. In your <<<hdfs-site.xml>>> file, add:
-
-----
- <property>
-   <name>dfs.ha.automatic-failover.enabled</name>
-   <value>true</value>
- </property>
-----
-
-  This specifies that the cluster should be set up for automatic failover.
-  In your <<<core-site.xml>>> file, add:
-
-----
- <property>
-   <name>ha.zookeeper.quorum</name>
-   <value>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181</value>
- </property>
-----
-
-  This lists the host-port pairs running the ZooKeeper service.
-
-  As with the parameters described earlier in the document, these settings may
-  be configured on a per-nameservice basis by suffixing the configuration key
-  with the nameservice ID. For example, in a cluster with federation enabled,
-  you can explicitly enable automatic failover for only one of the nameservices
-  by setting <<<dfs.ha.automatic-failover.enabled.my-nameservice-id>>>.
-
-  There are also several other configuration parameters which may be set to
-  control the behavior of automatic failover; however, they are not necessary
-  for most installations. Please refer to the configuration key specific
-  documentation for details.
-
-** Initializing HA state in ZooKeeper
-
-  After the configuration keys have been added, the next step is to initialize
-  required state in ZooKeeper. You can do so by running the following command
-  from one of the NameNode hosts.
-
-----
-$ hdfs zkfc -formatZK
-----
-
-  This will create a znode in ZooKeeper inside of which the automatic failover
-  system stores its data.
-
-** Starting the cluster with <<<start-dfs.sh>>>
-
-  Since automatic failover has been enabled in the configuration, the
-  <<<start-dfs.sh>>> script will now automatically start a ZKFC daemon on any
-  machine that runs a NameNode. When the ZKFCs start, they will automatically
-  select one of the NameNodes to become active.
-
-** Starting the cluster manually
-
-  If you manually manage the services on your cluster, you will need to manually
-  start the <<<zkfc>>> daemon on each of the machines that runs a NameNode. You
-  can start the daemon by running:
-
-----
-$ hadoop-daemon.sh start zkfc
-----
-
-** Securing access to ZooKeeper
-
-  If you are running a secure cluster, you will likely want to ensure that the
-  information stored in ZooKeeper is also secured. This prevents malicious
-  clients from modifying the metadata in ZooKeeper or potentially triggering a
-  false failover.
-
-  In order to secure the information in ZooKeeper, first add the following to
-  your <<<core-site.xml>>> file:
-
-----
- <property>
-   <name>ha.zookeeper.auth</name>
-   <value>@/path/to/zk-auth.txt</value>
- </property>
- <property>
-   <name>ha.zookeeper.acl</name>
-   <value>@/path/to/zk-acl.txt</value>
- </property>
-----
-
-  Please note the '@' character in these values -- this specifies that the
-  configurations are not inline, but rather point to a file on disk.
-
-  The first configured file specifies a list of ZooKeeper authentications, in
-  the same format as used by the ZK CLI. For example, you may specify something
-  like:
-
-----
-digest:hdfs-zkfcs:mypassword
-----
-  ...where <<<hdfs-zkfcs>>> is a unique username for ZooKeeper, and
-  <<<mypassword>>> is some unique string used as a password.
-
-  Next, generate a ZooKeeper ACL that corresponds to this authentication, using
-  a command like the following:
-
-----
-$ java -cp $ZK_HOME/lib/*:$ZK_HOME/zookeeper-3.4.2.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:mypassword
-output: hdfs-zkfcs:mypassword->hdfs-zkfcs:P/OQvnYyU/nF/mGYvB/xurX8dYs=
-----
-
-  Copy and paste the section of this output after the '->' string into the file
-  <<<zk-acls.txt>>>, prefixed by the string "<<<digest:>>>". For example:
-
-----
-digest:hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=:rwcda
-----
-
-  In order for these ACLs to take effect, you should then rerun the
-  <<<zkfc -formatZK>>> command as described above.
-
-  After doing so, you may verify the ACLs from the ZK CLI as follows:
-
-----
-[zk: localhost:2181(CONNECTED) 1] getAcl /hadoop-ha
-'digest,'hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=
-: cdrwa
-----
-
-** Verifying automatic failover
-
-  Once automatic failover has been set up, you should test its operation. To do
-  so, first locate the active NameNode. You can tell which node is active by
-  visiting the NameNode web interfaces -- each node reports its HA state at the
-  top of the page.
-
-  Once you have located your active NameNode, you may cause a failure on that
-  node.  For example, you can use <<<kill -9 <pid of NN>>>> to simulate a JVM
-  crash. Or, you could power cycle the machine or unplug its network interface
-  to simulate a different kind of outage.  After triggering the outage you wish
-  to test, the other NameNode should automatically become active within several
-  seconds. The amount of time required to detect a failure and trigger a
-  fail-over depends on the configuration of
-  <<<ha.zookeeper.session-timeout.ms>>>, but defaults to 5 seconds.
-
-  If the test does not succeed, you may have a misconfiguration. Check the logs
-  for the <<<zkfc>>> daemons as well as the NameNode daemons in order to further
-  diagnose the issue.
-
-
-* Automatic Failover FAQ
-
-  * <<Is it important that I start the ZKFC and NameNode daemons in any
-    particular order?>>
-
-  No. On any given node you may start the ZKFC before or after its corresponding
-  NameNode.
-
-  * <<What additional monitoring should I put in place?>>
-
-  You should add monitoring on each host that runs a NameNode to ensure that the
-  ZKFC remains running. In some types of ZooKeeper failures, for example, the
-  ZKFC may unexpectedly exit, and should be restarted to ensure that the system
-  is ready for automatic failover.
-
-  Additionally, you should monitor each of the servers in the ZooKeeper
-  quorum. If ZooKeeper crashes, then automatic failover will not function.
-
-  * <<What happens if ZooKeeper goes down?>>
-
-  If the ZooKeeper cluster crashes, no automatic failovers will be triggered.
-  However, HDFS will continue to run without any impact. When ZooKeeper is
-  restarted, HDFS will reconnect with no issues.
-
-  * <<Can I designate one of my NameNodes as primary/preferred?>>
-
-  No. Currently, this is not supported. Whichever NameNode is started first will
-  become active. You may choose to start the cluster in a specific order such
-  that your preferred node starts first.
-
-  * <<How can I initiate a manual failover when automatic failover is
-    configured?>>
-
-  Even if automatic failover is configured, you may initiate a manual failover
-  using the same <<<hdfs haadmin>>> command. It will perform a coordinated
-  failover.
-
- 
-* BookKeeper as a Shared storage (EXPERIMENTAL)
-
-   One option for shared storage for the NameNode is BookKeeper. 
-  BookKeeper achieves high availability and strong durability guarantees by replicating
-  edit log entries across multiple storage nodes. The edit log can be striped across 
-  the storage nodes for high performance. Fencing is supported in the protocol, i.e, 
-  BookKeeper will not allow two writers to write the single edit log.
-
-  The meta data for BookKeeper is stored in ZooKeeper.
-  In current HA architecture, a Zookeeper cluster is required for ZKFC. The same cluster can be
-  for BookKeeper metadata.
-
-  For more details on building a BookKeeper cluster, please refer to the 
-   {{{http://zookeeper.apache.org/bookkeeper/docs/trunk/bookkeeperConfig.html }BookKeeper documentation}}
-
- The BookKeeperJournalManager is an implementation of the HDFS JournalManager interface, which allows custom write ahead logging implementations to be plugged into the HDFS NameNode.
- 
- **<<BookKeeper Journal Manager>>
-
-   To use BookKeeperJournalManager, add the following to hdfs-site.xml.
-
-----
-    <property>
-      <name>dfs.namenode.shared.edits.dir</name>
-      <value>bookkeeper://zk1:2181;zk2:2181;zk3:2181/hdfsjournal</value>
-    </property>
-
-    <property>
-      <name>dfs.namenode.edits.journal-plugin.bookkeeper</name>
-      <value>org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager</value>
-    </property>
-----
-
-   The URI format for bookkeeper is <<<bookkeeper://[zkEnsemble]/[rootZnode]
-   [zookkeeper ensemble]>>> is a list of semi-colon separated, zookeeper host:port
-   pairs. In the example above there are 3 servers, in the ensemble,
-   zk1, zk2 & zk3, each one listening on port 2181.
-
-   <<<[root znode]>>> is the path of the zookeeper znode, under which the edit log
-   information will be stored.
-
-   The class specified for the journal-plugin must be available in the NameNode's
-   classpath. We explain how to generate a jar file with the journal manager and
-   its dependencies, and how to put it into the classpath below.
-
- *** <<More configuration options>> 
-
-     * <<dfs.namenode.bookkeeperjournal.output-buffer-size>> - 
-       Number of bytes a bookkeeper journal stream will buffer before
-       forcing a flush. Default is 1024.
-     
-----
-       <property>
-         <name>dfs.namenode.bookkeeperjournal.output-buffer-size</name>
-         <value>1024</value>
-       </property>
-----
-
-     * <<dfs.namenode.bookkeeperjournal.ensemble-size>> - 
-       Number of bookkeeper servers in edit log ensembles. This
-       is the number of bookkeeper servers which need to be available
-       for the edit log to be writable. Default is 3.
-
-----
-       <property>
-         <name>dfs.namenode.bookkeeperjournal.ensemble-size</name>
-         <value>3</value>
-       </property>
-----
-
-     * <<dfs.namenode.bookkeeperjournal.quorum-size>> - 
-       Number of bookkeeper servers in the write quorum. This is the
-       number of bookkeeper servers which must have acknowledged the
-       write of an entry before it is considered written. Default is 2.
-
-----
-       <property>
-         <name>dfs.namenode.bookkeeperjournal.quorum-size</name>
-         <value>2</value>
-       </property>
-----
-
-     * <<dfs.namenode.bookkeeperjournal.digestPw>> - 
-       Password to use when creating edit log segments.
-
-----
-       <property>
-        <name>dfs.namenode.bookkeeperjournal.digestPw</name>
-        <value>myPassword</value>
-       </property>
-----
-
-     * <<dfs.namenode.bookkeeperjournal.zk.session.timeout>> - 
-       Session timeout for Zookeeper client from BookKeeper Journal Manager.
-       Hadoop recommends that this value should be less than the ZKFC 
-       session timeout value. Default value is 3000.
-
-----
-       <property>
-         <name>dfs.namenode.bookkeeperjournal.zk.session.timeout</name>
-         <value>3000</value>
-       </property>
-----
-
- *** <<Building BookKeeper Journal Manager plugin jar>>
-
-     To generate the distribution packages for BK journal, do the
-     following.
-
-     $ mvn clean package -Pdist
-
-     This will generate a jar with the BookKeeperJournalManager, all the dependencies
-     needed by the journal manager,
-     hadoop-hdfs/src/contrib/bkjournal/target/hadoop-hdfs-bkjournal-<VERSION>.jar
-
-     Note that the -Pdist part of the build command is important, as otherwise
-     the dependencies would not be packaged in the jar. The dependencies included in
-     the jar are {{{http://maven.apache.org/plugins/maven-shade-plugin/}shaded}} to
-     avoid conflicts with other dependencies of the NameNode.
-
- *** <<Putting the BookKeeperJournalManager in the NameNode classpath>>
-
-    To run a HDFS namenode using BookKeeper as a backend, copy the bkjournal
-    jar, generated above, into the lib directory of hdfs. In the standard 
-    distribution of HDFS, this is at $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/
-
-    cp hadoop-hdfs/src/contrib/bkjournal/target/hadoop-hdfs-bkjournal-<VERSION>.jar $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/
-
- *** <<Current limitations>> 
-
-      1) NameNode format command will not format the BookKeeper data automatically. 
-         We have to clean the data manually from BookKeeper cluster 
-         and create the /ledgers/available path in Zookeeper. 
-----
-$ zkCli.sh create /ledgers 0
-$ zkCli.sh create /ledgers/available 0
-----
-         Note:
-          bookkeeper://zk1:2181;zk2:2181;zk3:2181/hdfsjournal
-          The final part /hdfsjournal specifies the znode in zookeeper where
-          ledger metadata will be stored. Administrators may set this to anything
-          they wish.
-
-      2) Security in BookKeeper. BookKeeper does not support SASL nor SSL for
-         connections between the NameNode and BookKeeper storage nodes.
-
-      3) Auto-Recovery of storage node failures. Work inprogress 
-      {{{https://issues.apache.org/jira/browse/BOOKKEEPER-237 }BOOKKEEPER-237}}.
-         Currently we have the tools to manually recover the data from failed storage nodes.
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
deleted file mode 100644
index 2aefc35..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
+++ /dev/null
@@ -1,767 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop Distributed File System-${project.version} - High Availability
-  ---
-  ---
-  ${maven.build.timestamp}
-
-HDFS High Availability Using the Quorum Journal Manager
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-* {Purpose}
-
-  This guide provides an overview of the HDFS High Availability (HA) feature
-  and how to configure and manage an HA HDFS cluster, using the Quorum Journal
-  Manager (QJM) feature.
- 
-  This document assumes that the reader has a general understanding of
-  general components and node types in an HDFS cluster. Please refer to the
-  HDFS Architecture guide for details.
-
-* {Note: Using the Quorum Journal Manager or Conventional Shared Storage}
-
-  This guide discusses how to configure and use HDFS HA using the Quorum
-  Journal Manager (QJM) to share edit logs between the Active and Standby
-  NameNodes. For information on how to configure HDFS HA using NFS for shared
-  storage instead of the QJM, please see
-  {{{./HDFSHighAvailabilityWithNFS.html}this alternative guide.}}
-
-* {Background}
-
-  Prior to Hadoop 2.0.0, the NameNode was a single point of failure (SPOF) in
-  an HDFS cluster. Each cluster had a single NameNode, and if that machine or
-  process became unavailable, the cluster as a whole would be unavailable
-  until the NameNode was either restarted or brought up on a separate machine.
-  
-  This impacted the total availability of the HDFS cluster in two major ways:
-
-    * In the case of an unplanned event such as a machine crash, the cluster would
-      be unavailable until an operator restarted the NameNode.
-
-    * Planned maintenance events such as software or hardware upgrades on the
-      NameNode machine would result in windows of cluster downtime.
-  
-  The HDFS High Availability feature addresses the above problems by providing
-  the option of running two redundant NameNodes in the same cluster in an
-  Active/Passive configuration with a hot standby. This allows a fast failover to
-  a new NameNode in the case that a machine crashes, or a graceful
-  administrator-initiated failover for the purpose of planned maintenance.
-
-* {Architecture}
-
-  In a typical HA cluster, two separate machines are configured as NameNodes.
-  At any point in time, exactly one of the NameNodes is in an <Active> state,
-  and the other is in a <Standby> state. The Active NameNode is responsible
-  for all client operations in the cluster, while the Standby is simply acting
-  as a slave, maintaining enough state to provide a fast failover if
-  necessary.
-  
-  In order for the Standby node to keep its state synchronized with the Active
-  node, both nodes communicate with a group of separate daemons called
-  "JournalNodes" (JNs). When any namespace modification is performed by the
-  Active node, it durably logs a record of the modification to a majority of
-  these JNs. The Standby node is capable of reading the edits from the JNs, and
-  is constantly watching them for changes to the edit log. As the Standby Node
-  sees the edits, it applies them to its own namespace. In the event of a
-  failover, the Standby will ensure that it has read all of the edits from the
-  JounalNodes before promoting itself to the Active state. This ensures that the
-  namespace state is fully synchronized before a failover occurs.
-  
-  In order to provide a fast failover, it is also necessary that the Standby node
-  have up-to-date information regarding the location of blocks in the cluster.
-  In order to achieve this, the DataNodes are configured with the location of
-  both NameNodes, and send block location information and heartbeats to both.
-  
-  It is vital for the correct operation of an HA cluster that only one of the
-  NameNodes be Active at a time. Otherwise, the namespace state would quickly
-  diverge between the two, risking data loss or other incorrect results.  In
-  order to ensure this property and prevent the so-called "split-brain scenario,"
-  the JournalNodes will only ever allow a single NameNode to be a writer at a
-  time. During a failover, the NameNode which is to become active will simply
-  take over the role of writing to the JournalNodes, which will effectively
-  prevent the other NameNode from continuing in the Active state, allowing the
-  new Active to safely proceed with failover.
-
-* {Hardware resources}
-
-  In order to deploy an HA cluster, you should prepare the following:
-
-    * <<NameNode machines>> - the machines on which you run the Active and
-    Standby NameNodes should have equivalent hardware to each other, and
-    equivalent hardware to what would be used in a non-HA cluster.
-
-    * <<JournalNode machines>> - the machines on which you run the JournalNodes.
-    The JournalNode daemon is relatively lightweight, so these daemons may
-    reasonably be collocated on machines with other Hadoop daemons, for example
-    NameNodes, the JobTracker, or the YARN ResourceManager. <<Note:>> There
-    must be at least 3 JournalNode daemons, since edit log modifications must be
-    written to a majority of JNs. This will allow the system to tolerate the
-    failure of a single machine. You may also run more than 3 JournalNodes, but
-    in order to actually increase the number of failures the system can tolerate,
-    you should run an odd number of JNs, (i.e. 3, 5, 7, etc.). Note that when
-    running with N JournalNodes, the system can tolerate at most (N - 1) / 2
-    failures and continue to function normally.
-  
-  Note that, in an HA cluster, the Standby NameNode also performs checkpoints of
-  the namespace state, and thus it is not necessary to run a Secondary NameNode,
-  CheckpointNode, or BackupNode in an HA cluster. In fact, to do so would be an
-  error. This also allows one who is reconfiguring a non-HA-enabled HDFS cluster
-  to be HA-enabled to reuse the hardware which they had previously dedicated to
-  the Secondary NameNode.
-
-* {Deployment}
-
-** Configuration overview
-
-  Similar to Federation configuration, HA configuration is backward compatible
-  and allows existing single NameNode configurations to work without change.
-  The new configuration is designed such that all the nodes in the cluster may
-  have the same configuration without the need for deploying different
-  configuration files to different machines based on the type of the node.
- 
-  Like HDFS Federation, HA clusters reuse the <<<nameservice ID>>> to identify a
-  single HDFS instance that may in fact consist of multiple HA NameNodes. In
-  addition, a new abstraction called <<<NameNode ID>>> is added with HA. Each
-  distinct NameNode in the cluster has a different NameNode ID to distinguish it.
-  To support a single configuration file for all of the NameNodes, the relevant
-  configuration parameters are suffixed with the <<nameservice ID>> as well as
-  the <<NameNode ID>>.
-
-** Configuration details
-
-  To configure HA NameNodes, you must add several configuration options to your
-  <<hdfs-site.xml>> configuration file.
-
-  The order in which you set these configurations is unimportant, but the values
-  you choose for <<dfs.nameservices>> and
-  <<dfs.ha.namenodes.[nameservice ID]>> will determine the keys of those that
-  follow. Thus, you should decide on these values before setting the rest of the
-  configuration options.
-
-  * <<dfs.nameservices>> - the logical name for this new nameservice
-
-    Choose a logical name for this nameservice, for example "mycluster", and use
-    this logical name for the value of this config option. The name you choose is
-    arbitrary. It will be used both for configuration and as the authority
-    component of absolute HDFS paths in the cluster.
-
-    <<Note:>> If you are also using HDFS Federation, this configuration setting
-    should also include the list of other nameservices, HA or otherwise, as a
-    comma-separated list.
-
-----
-<property>
-  <name>dfs.nameservices</name>
-  <value>mycluster</value>
-</property>
-----
-
-  * <<dfs.ha.namenodes.[nameservice ID]>> - unique identifiers for each NameNode in the nameservice
-
-    Configure with a list of comma-separated NameNode IDs. This will be used by
-    DataNodes to determine all the NameNodes in the cluster. For example, if you
-    used "mycluster" as the nameservice ID previously, and you wanted to use "nn1"
-    and "nn2" as the individual IDs of the NameNodes, you would configure this as
-    such:
-
-----
-<property>
-  <name>dfs.ha.namenodes.mycluster</name>
-  <value>nn1,nn2</value>
-</property>
-----
-
-    <<Note:>> Currently, only a maximum of two NameNodes may be configured per
-    nameservice.
-
-  * <<dfs.namenode.rpc-address.[nameservice ID].[name node ID]>> - the fully-qualified RPC address for each NameNode to listen on
-
-    For both of the previously-configured NameNode IDs, set the full address and
-    IPC port of the NameNode processs. Note that this results in two separate
-    configuration options. For example:
-
-----
-<property>
-  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
-  <value>machine1.example.com:8020</value>
-</property>
-<property>
-  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
-  <value>machine2.example.com:8020</value>
-</property>
-----
-
-    <<Note:>> You may similarly configure the "<<servicerpc-address>>" setting if
-    you so desire.
-
-  * <<dfs.namenode.http-address.[nameservice ID].[name node ID]>> - the fully-qualified HTTP address for each NameNode to listen on
-
-    Similarly to <rpc-address> above, set the addresses for both NameNodes' HTTP
-    servers to listen on. For example:
-
-----
-<property>
-  <name>dfs.namenode.http-address.mycluster.nn1</name>
-  <value>machine1.example.com:50070</value>
-</property>
-<property>
-  <name>dfs.namenode.http-address.mycluster.nn2</name>
-  <value>machine2.example.com:50070</value>
-</property>
-----
-
-    <<Note:>> If you have Hadoop's security features enabled, you should also set
-    the <https-address> similarly for each NameNode.
-
-  * <<dfs.namenode.shared.edits.dir>> - the URI which identifies the group of JNs where the NameNodes will write/read edits
-
-    This is where one configures the addresses of the JournalNodes which provide
-    the shared edits storage, written to by the Active nameNode and read by the
-    Standby NameNode to stay up-to-date with all the file system changes the Active
-    NameNode makes. Though you must specify several JournalNode addresses,
-    <<you should only configure one of these URIs.>> The URI should be of the form:
-    "qjournal://<host1:port1>;<host2:port2>;<host3:port3>/<journalId>". The Journal
-    ID is a unique identifier for this nameservice, which allows a single set of
-    JournalNodes to provide storage for multiple federated namesystems. Though not
-    a requirement, it's a good idea to reuse the nameservice ID for the journal
-    identifier.
-
-    For example, if the JournalNodes for this cluster were running on the
-    machines "node1.example.com", "node2.example.com", and "node3.example.com" and
-    the nameservice ID were "mycluster", you would use the following as the value
-    for this setting (the default port for the JournalNode is 8485):
-
-----
-<property>
-  <name>dfs.namenode.shared.edits.dir</name>
-  <value>qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/mycluster</value>
-</property>
-----
-
-  * <<dfs.client.failover.proxy.provider.[nameservice ID]>> - the Java class that HDFS clients use to contact the Active NameNode
-
-    Configure the name of the Java class which will be used by the DFS Client to
-    determine which NameNode is the current Active, and therefore which NameNode is
-    currently serving client requests. The only implementation which currently
-    ships with Hadoop is the <<ConfiguredFailoverProxyProvider>>, so use this
-    unless you are using a custom one. For example:
-
-----
-<property>
-  <name>dfs.client.failover.proxy.provider.mycluster</name>
-  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
-</property>
-----
-
-  * <<dfs.ha.fencing.methods>> - a list of scripts or Java classes which will be used to fence the Active NameNode during a failover
-
-    It is desirable for correctness of the system that only one NameNode be in
-    the Active state at any given time. <<Importantly, when using the Quorum
-    Journal Manager, only one NameNode will ever be allowed to write to the
-    JournalNodes, so there is no potential for corrupting the file system metadata
-    from a split-brain scenario.>> However, when a failover occurs, it is still
-    possible that the previous Active NameNode could serve read requests to
-    clients, which may be out of date until that NameNode shuts down when trying to
-    write to the JournalNodes. For this reason, it is still desirable to configure
-    some fencing methods even when using the Quorum Journal Manager. However, to
-    improve the availability of the system in the event the fencing mechanisms
-    fail, it is advisable to configure a fencing method which is guaranteed to
-    return success as the last fencing method in the list. Note that if you choose
-    to use no actual fencing methods, you still must configure something for this
-    setting, for example "<<<shell(/bin/true)>>>".
-
-    The fencing methods used during a failover are configured as a
-    carriage-return-separated list, which will be attempted in order until one
-    indicates that fencing has succeeded. There are two methods which ship with
-    Hadoop: <shell> and <sshfence>. For information on implementing your own custom
-    fencing method, see the <org.apache.hadoop.ha.NodeFencer> class.
-
-    * <<sshfence>> - SSH to the Active NameNode and kill the process
-
-      The <sshfence> option SSHes to the target node and uses <fuser> to kill the
-      process listening on the service's TCP port. In order for this fencing option
-      to work, it must be able to SSH to the target node without providing a
-      passphrase. Thus, one must also configure the
-      <<dfs.ha.fencing.ssh.private-key-files>> option, which is a
-      comma-separated list of SSH private key files. For example:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>sshfence</value>
-</property>
-
-<property>
-  <name>dfs.ha.fencing.ssh.private-key-files</name>
-  <value>/home/exampleuser/.ssh/id_rsa</value>
-</property>
----
-
-      Optionally, one may configure a non-standard username or port to perform the
-      SSH. One may also configure a timeout, in milliseconds, for the SSH, after
-      which this fencing method will be considered to have failed. It may be
-      configured like so:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>sshfence([[username][:port]])</value>
-</property>
-<property>
-  <name>dfs.ha.fencing.ssh.connect-timeout</name>
-  <value>30000</value>
-</property>
----
-
-    * <<shell>> - run an arbitrary shell command to fence the Active NameNode
-
-      The <shell> fencing method runs an arbitrary shell command. It may be
-      configured like so:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>shell(/path/to/my/script.sh arg1 arg2 ...)</value>
-</property>
----
-
-      The string between '(' and ')' is passed directly to a bash shell and may not
-      include any closing parentheses.
-
-      The shell command will be run with an environment set up to contain all of the
-      current Hadoop configuration variables, with the '_' character replacing any
-      '.' characters in the configuration keys. The configuration used has already had
-      any namenode-specific configurations promoted to their generic forms -- for example
-      <<dfs_namenode_rpc-address>> will contain the RPC address of the target node, even
-      though the configuration may specify that variable as
-      <<dfs.namenode.rpc-address.ns1.nn1>>.
-      
-      Additionally, the following variables referring to the target node to be fenced
-      are also available:
-
-*-----------------------:-----------------------------------+
-| $target_host          | hostname of the node to be fenced |
-*-----------------------:-----------------------------------+
-| $target_port          | IPC port of the node to be fenced |
-*-----------------------:-----------------------------------+
-| $target_address       | the above two, combined as host:port |
-*-----------------------:-----------------------------------+
-| $target_nameserviceid | the nameservice ID of the NN to be fenced |
-*-----------------------:-----------------------------------+
-| $target_namenodeid    | the namenode ID of the NN to be fenced |
-*-----------------------:-----------------------------------+
-      
-      These environment variables may also be used as substitutions in the shell
-      command itself. For example:
-
----
-<property>
-  <name>dfs.ha.fencing.methods</name>
-  <value>shell(/path/to/my/script.sh --nameservice=$target_nameserviceid $target_host:$target_port)</value>
-</property>
----
-      
-      If the shell command returns an exit
-      code of 0, the fencing is determined to be successful. If it returns any other
-      exit code, the fencing was not successful and the next fencing method in the
-      list will be attempted.
-
-      <<Note:>> This fencing method does not implement any timeout. If timeouts are
-      necessary, they should be implemented in the shell script itself (eg by forking
-      a subshell to kill its parent in some number of seconds).
-
-  * <<fs.defaultFS>> - the default path prefix used by the Hadoop FS client when none is given
-
-    Optionally, you may now configure the default path for Hadoop clients to use
-    the new HA-enabled logical URI. If you used "mycluster" as the nameservice ID
-    earlier, this will be the value of the authority portion of all of your HDFS
-    paths. This may be configured like so, in your <<core-site.xml>> file:
-
----
-<property>
-  <name>fs.defaultFS</name>
-  <value>hdfs://mycluster</value>
-</property>
----
-
-
-  * <<dfs.journalnode.edits.dir>> - the path where the JournalNode daemon will store its local state
-
-    This is the absolute path on the JournalNode machines where the edits and
-    other local state used by the JNs will be stored. You may only use a single
-    path for this configuration. Redundancy for this data is provided by running
-    multiple separate JournalNodes, or by configuring this directory on a
-    locally-attached RAID array. For example:
-
----
-<property>
-  <name>dfs.journalnode.edits.dir</name>
-  <value>/path/to/journal/node/local/data</value>
-</property>
----
-
-** Deployment details
-
-  After all of the necessary configuration options have been set, you must
-  start the JournalNode daemons on the set of machines where they will run. This
-  can be done by running the command "<hdfs-daemon.sh journalnode>" and waiting
-  for the daemon to start on each of the relevant machines.
-
-  Once the JournalNodes have been started, one must initially synchronize the
-  two HA NameNodes' on-disk metadata.
-
-    * If you are setting up a fresh HDFS cluster, you should first run the format
-    command (<hdfs namenode -format>) on one of NameNodes.
-  
-    * If you have already formatted the NameNode, or are converting a
-    non-HA-enabled cluster to be HA-enabled, you should now copy over the
-    contents of your NameNode metadata directories to the other, unformatted
-    NameNode by running the command "<hdfs namenode -bootstrapStandby>" on the
-    unformatted NameNode. Running this command will also ensure that the
-    JournalNodes (as configured by <<dfs.namenode.shared.edits.dir>>) contain
-    sufficient edits transactions to be able to start both NameNodes.
-  
-    * If you are converting a non-HA NameNode to be HA, you should run the
-    command "<hdfs -initializeSharedEdits>", which will initialize the
-    JournalNodes with the edits data from the local NameNode edits directories.
-
-  At this point you may start both of your HA NameNodes as you normally would
-  start a NameNode.
-
-  You can visit each of the NameNodes' web pages separately by browsing to their
-  configured HTTP addresses. You should notice that next to the configured
-  address will be the HA state of the NameNode (either "standby" or "active".)
-  Whenever an HA NameNode starts, it is initially in the Standby state.
-
-** Administrative commands
-
-  Now that your HA NameNodes are configured and started, you will have access
-  to some additional commands to administer your HA HDFS cluster. Specifically,
-  you should familiarize yourself with all of the subcommands of the "<hdfs
-  haadmin>" command. Running this command without any additional arguments will
-  display the following usage information:
-
----
-Usage: DFSHAAdmin [-ns <nameserviceId>]
-    [-transitionToActive <serviceId>]
-    [-transitionToStandby <serviceId>]
-    [-failover [--forcefence] [--forceactive] <serviceId> <serviceId>]
-    [-getServiceState <serviceId>]
-    [-checkHealth <serviceId>]
-    [-help <command>]
----
-
-  This guide describes high-level uses of each of these subcommands. For
-  specific usage information of each subcommand, you should run "<hdfs haadmin
-  -help <command>>".
-
-  * <<transitionToActive>> and <<transitionToStandby>> - transition the state of the given NameNode to Active or Standby
-
-    These subcommands cause a given NameNode to transition to the Active or Standby
-    state, respectively. <<These commands do not attempt to perform any fencing,
-    and thus should rarely be used.>> Instead, one should almost always prefer to
-    use the "<hdfs haadmin -failover>" subcommand.
-
-  * <<failover>> - initiate a failover between two NameNodes
-
-    This subcommand causes a failover from the first provided NameNode to the
-    second. If the first NameNode is in the Standby state, this command simply
-    transitions the second to the Active state without error. If the first NameNode
-    is in the Active state, an attempt will be made to gracefully transition it to
-    the Standby state. If this fails, the fencing methods (as configured by
-    <<dfs.ha.fencing.methods>>) will be attempted in order until one
-    succeeds. Only after this process will the second NameNode be transitioned to
-    the Active state. If no fencing method succeeds, the second NameNode will not
-    be transitioned to the Active state, and an error will be returned.
-
-  * <<getServiceState>> - determine whether the given NameNode is Active or Standby
-
-    Connect to the provided NameNode to determine its current state, printing
-    either "standby" or "active" to STDOUT appropriately. This subcommand might be
-    used by cron jobs or monitoring scripts which need to behave differently based
-    on whether the NameNode is currently Active or Standby.
-
-  * <<checkHealth>> - check the health of the given NameNode
-
-    Connect to the provided NameNode to check its health. The NameNode is capable
-    of performing some diagnostics on itself, including checking if internal
-    services are running as expected. This command will return 0 if the NameNode is
-    healthy, non-zero otherwise. One might use this command for monitoring
-    purposes.
-
-    <<Note:>> This is not yet implemented, and at present will always return
-    success, unless the given NameNode is completely down.
-
-* {Automatic Failover}
-
-** Introduction
-
-  The above sections describe how to configure manual failover. In that mode,
-  the system will not automatically trigger a failover from the active to the
-  standby NameNode, even if the active node has failed. This section describes
-  how to configure and deploy automatic failover.
-
-** Components
-
-  Automatic failover adds two new components to an HDFS deployment: a ZooKeeper
-  quorum, and the ZKFailoverController process (abbreviated as ZKFC).
-
-  Apache ZooKeeper is a highly available service for maintaining small amounts
-  of coordination data, notifying clients of changes in that data, and
-  monitoring clients for failures. The implementation of automatic HDFS failover
-  relies on ZooKeeper for the following things:
-  
-    * <<Failure detection>> - each of the NameNode machines in the cluster
-    maintains a persistent session in ZooKeeper. If the machine crashes, the
-    ZooKeeper session will expire, notifying the other NameNode that a failover
-    should be triggered.
-
-    * <<Active NameNode election>> - ZooKeeper provides a simple mechanism to
-    exclusively elect a node as active. If the current active NameNode crashes,
-    another node may take a special exclusive lock in ZooKeeper indicating that
-    it should become the next active.
-
-  The ZKFailoverController (ZKFC) is a new component which is a ZooKeeper client
-  which also monitors and manages the state of the NameNode.  Each of the
-  machines which runs a NameNode also runs a ZKFC, and that ZKFC is responsible
-  for:
-
-    * <<Health monitoring>> - the ZKFC pings its local NameNode on a periodic
-    basis with a health-check command. So long as the NameNode responds in a
-    timely fashion with a healthy status, the ZKFC considers the node
-    healthy. If the node has crashed, frozen, or otherwise entered an unhealthy
-    state, the health monitor will mark it as unhealthy.
-
-    * <<ZooKeeper session management>> - when the local NameNode is healthy, the
-    ZKFC holds a session open in ZooKeeper. If the local NameNode is active, it
-    also holds a special "lock" znode. This lock uses ZooKeeper's support for
-    "ephemeral" nodes; if the session expires, the lock node will be
-    automatically deleted.
-
-    * <<ZooKeeper-based election>> - if the local NameNode is healthy, and the
-    ZKFC sees that no other node currently holds the lock znode, it will itself
-    try to acquire the lock. If it succeeds, then it has "won the election", and
-    is responsible for running a failover to make its local NameNode active. The
-    failover process is similar to the manual failover described above: first,
-    the previous active is fenced if necessary, and then the local NameNode
-    transitions to active state.
-
-  For more details on the design of automatic failover, refer to the design
-  document attached to HDFS-2185 on the Apache HDFS JIRA.
-
-** Deploying ZooKeeper
-
-  In a typical deployment, ZooKeeper daemons are configured to run on three or
-  five nodes. Since ZooKeeper itself has light resource requirements, it is
-  acceptable to collocate the ZooKeeper nodes on the same hardware as the HDFS
-  NameNode and Standby Node. Many operators choose to deploy the third ZooKeeper
-  process on the same node as the YARN ResourceManager. It is advisable to
-  configure the ZooKeeper nodes to store their data on separate disk drives from
-  the HDFS metadata for best performance and isolation.
-
-  The setup of ZooKeeper is out of scope for this document. We will assume that
-  you have set up a ZooKeeper cluster running on three or more nodes, and have
-  verified its correct operation by connecting using the ZK CLI.
-
-** Before you begin
-
-  Before you begin configuring automatic failover, you should shut down your
-  cluster. It is not currently possible to transition from a manual failover
-  setup to an automatic failover setup while the cluster is running.
-
-** Configuring automatic failover
-
-  The configuration of automatic failover requires the addition of two new
-  parameters to your configuration. In your <<<hdfs-site.xml>>> file, add:
-
-----
- <property>
-   <name>dfs.ha.automatic-failover.enabled</name>
-   <value>true</value>
- </property>
-----
-
-  This specifies that the cluster should be set up for automatic failover.
-  In your <<<core-site.xml>>> file, add:
-
-----
- <property>
-   <name>ha.zookeeper.quorum</name>
-   <value>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181</value>
- </property>
-----
-
-  This lists the host-port pairs running the ZooKeeper service.
-
-  As with the parameters described earlier in the document, these settings may
-  be configured on a per-nameservice basis by suffixing the configuration key
-  with the nameservice ID. For example, in a cluster with federation enabled,
-  you can explicitly enable automatic failover for only one of the nameservices
-  by setting <<<dfs.ha.automatic-failover.enabled.my-nameservice-id>>>.
-
-  There are also several other configuration parameters which may be set to
-  control the behavior of automatic failover; however, they are not necessary
-  for most installations. Please refer to the configuration key specific
-  documentation for details.
-
-** Initializing HA state in ZooKeeper
-
-  After the configuration keys have been added, the next step is to initialize
-  required state in ZooKeeper. You can do so by running the following command
-  from one of the NameNode hosts.
-
-----
-$ hdfs zkfc -formatZK
-----
-
-  This will create a znode in ZooKeeper inside of which the automatic failover
-  system stores its data.
-
-** Starting the cluster with <<<start-dfs.sh>>>
-
-  Since automatic failover has been enabled in the configuration, the
-  <<<start-dfs.sh>>> script will now automatically start a ZKFC daemon on any
-  machine that runs a NameNode. When the ZKFCs start, they will automatically
-  select one of the NameNodes to become active.
-
-** Starting the cluster manually
-
-  If you manually manage the services on your cluster, you will need to manually
-  start the <<<zkfc>>> daemon on each of the machines that runs a NameNode. You
-  can start the daemon by running:
-
-----
-$ hadoop-daemon.sh start zkfc
-----
-
-** Securing access to ZooKeeper
-
-  If you are running a secure cluster, you will likely want to ensure that the
-  information stored in ZooKeeper is also secured. This prevents malicious
-  clients from modifying the metadata in ZooKeeper or potentially triggering a
-  false failover.
-
-  In order to secure the information in ZooKeeper, first add the following to
-  your <<<core-site.xml>>> file:
-
-----
- <property>
-   <name>ha.zookeeper.auth</name>
-   <value>@/path/to/zk-auth.txt</value>
- </property>
- <property>
-   <name>ha.zookeeper.acl</name>
-   <value>@/path/to/zk-acl.txt</value>
- </property>
-----
-
-  Please note the '@' character in these values -- this specifies that the
-  configurations are not inline, but rather point to a file on disk.
-
-  The first configured file specifies a list of ZooKeeper authentications, in
-  the same format as used by the ZK CLI. For example, you may specify something
-  like:
-
-----
-digest:hdfs-zkfcs:mypassword
-----
-  ...where <<<hdfs-zkfcs>>> is a unique username for ZooKeeper, and
-  <<<mypassword>>> is some unique string used as a password.
-
-  Next, generate a ZooKeeper ACL that corresponds to this authentication, using
-  a command like the following:
-
-----
-$ java -cp $ZK_HOME/lib/*:$ZK_HOME/zookeeper-3.4.2.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:mypassword
-output: hdfs-zkfcs:mypassword->hdfs-zkfcs:P/OQvnYyU/nF/mGYvB/xurX8dYs=
-----
-
-  Copy and paste the section of this output after the '->' string into the file
-  <<<zk-acls.txt>>>, prefixed by the string "<<<digest:>>>". For example:
-
-----
-digest:hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=:rwcda
-----
-
-  In order for these ACLs to take effect, you should then rerun the
-  <<<zkfc -formatZK>>> command as described above.
-
-  After doing so, you may verify the ACLs from the ZK CLI as follows:
-
-----
-[zk: localhost:2181(CONNECTED) 1] getAcl /hadoop-ha
-'digest,'hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=
-: cdrwa
-----
-
-** Verifying automatic failover
-
-  Once automatic failover has been set up, you should test its operation. To do
-  so, first locate the active NameNode. You can tell which node is active by
-  visiting the NameNode web interfaces -- each node reports its HA state at the
-  top of the page.
-
-  Once you have located your active NameNode, you may cause a failure on that
-  node.  For example, you can use <<<kill -9 <pid of NN>>>> to simulate a JVM
-  crash. Or, you could power cycle the machine or unplug its network interface
-  to simulate a different kind of outage.  After triggering the outage you wish
-  to test, the other NameNode should automatically become active within several
-  seconds. The amount of time required to detect a failure and trigger a
-  fail-over depends on the configuration of
-  <<<ha.zookeeper.session-timeout.ms>>>, but defaults to 5 seconds.
-
-  If the test does not succeed, you may have a misconfiguration. Check the logs
-  for the <<<zkfc>>> daemons as well as the NameNode daemons in order to further
-  diagnose the issue.
-
-
-* Automatic Failover FAQ
-
-  * <<Is it important that I start the ZKFC and NameNode daemons in any
-    particular order?>>
-
-  No. On any given node you may start the ZKFC before or after its corresponding
-  NameNode.
-
-  * <<What additional monitoring should I put in place?>>
-
-  You should add monitoring on each host that runs a NameNode to ensure that the
-  ZKFC remains running. In some types of ZooKeeper failures, for example, the
-  ZKFC may unexpectedly exit, and should be restarted to ensure that the system
-  is ready for automatic failover.
-
-  Additionally, you should monitor each of the servers in the ZooKeeper
-  quorum. If ZooKeeper crashes, then automatic failover will not function.
-
-  * <<What happens if ZooKeeper goes down?>>
-
-  If the ZooKeeper cluster crashes, no automatic failovers will be triggered.
-  However, HDFS will continue to run without any impact. When ZooKeeper is
-  restarted, HDFS will reconnect with no issues.
-
-  * <<Can I designate one of my NameNodes as primary/preferred?>>
-
-  No. Currently, this is not supported. Whichever NameNode is started first will
-  become active. You may choose to start the cluster in a specific order such
-  that your preferred node starts first.
-
-  * <<How can I initiate a manual failover when automatic failover is
-    configured?>>
-
-  Even if automatic failover is configured, you may initiate a manual failover
-  using the same <<<hdfs haadmin>>> command. It will perform a coordinated
-  failover.
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/SingleCluster.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/SingleCluster.apt.vm
deleted file mode 100644
index 7ea4285..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/SingleCluster.apt.vm
+++ /dev/null
@@ -1,194 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop MapReduce Next Generation ${project.version} - Setting up a Single Node Cluster.
-  ---
-  ---
-  ${maven.build.timestamp}
-
-Hadoop MapReduce Next Generation - Setting up a Single Node Cluster.
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-* Mapreduce Tarball
-
-  You should be able to obtain the MapReduce tarball from the release.
-  If not, you should be able to create a tarball from the source.
-
-+---+
-$ mvn clean install -DskipTests
-$ cd hadoop-mapreduce-project
-$ mvn clean install assembly:assembly -Pnative
-+---+
-  <<NOTE:>> You will need protoc installed of version 2.4.1 or greater.
-
-  To ignore the native builds in mapreduce you can omit the <<<-Pnative>>> argument
-  for maven. The tarball should be available in <<<target/>>> directory. 
-
-  
-* Setting up the environment.
-
-  Assuming you have installed hadoop-common/hadoop-hdfs and exported
-  <<$HADOOP_COMMON_HOME>>/<<$HADOOP_HDFS_HOME>>, untar hadoop mapreduce 
-  tarball and set environment variable <<$HADOOP_MAPRED_HOME>> to the 
-  untarred directory. Set <<$YARN_HOME>> the same as <<$HADOOP_MAPRED_HOME>>. 
- 
-  <<NOTE:>> The following instructions assume you have hdfs running.
-
-* Setting up Configuration.
-
-  To start the ResourceManager and NodeManager, you will have to update the configs.
-  Assuming your $HADOOP_CONF_DIR is the configuration directory and has the installed
-  configs for HDFS and <<<core-site.xml>>>. There are 2 config files you will have to setup
-  <<<mapred-site.xml>>> and <<<yarn-site.xml>>>.
-
-** Setting up <<<mapred-site.xml>>>
-
-  Add the following configs to your <<<mapred-site.xml>>>.
-
-+---+
-  <property>
-    <name>mapreduce.cluster.temp.dir</name>
-    <value></value>
-    <description>No description</description>
-    <final>true</final>
-  </property>
-
-  <property>
-    <name>mapreduce.cluster.local.dir</name>
-    <value></value>
-    <description>No description</description>
-    <final>true</final>
-  </property>
-+---+
-
-** Setting up <<<yarn-site.xml>>>
-
-Add the following configs to your <<<yarn-site.xml>>>
-
-+---+
-  <property>
-    <name>yarn.resourcemanager.resource-tracker.address</name>
-    <value>host:port</value>
-    <description>host is the hostname of the resource manager and 
-    port is the port on which the NodeManagers contact the Resource Manager.
-    </description>
-  </property>
-
-  <property>
-    <name>yarn.resourcemanager.scheduler.address</name>
-    <value>host:port</value>
-    <description>host is the hostname of the resourcemanager and port is the port
-    on which the Applications in the cluster talk to the Resource Manager.
-    </description>
-  </property>
-
-  <property>
-    <name>yarn.resourcemanager.scheduler.class</name>
-    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler</value>
-    <description>In case you do not want to use the default scheduler</description>
-  </property>
-
-  <property>
-    <name>yarn.resourcemanager.address</name>
-    <value>host:port</value>
-    <description>the host is the hostname of the ResourceManager and the port is the port on
-    which the clients can talk to the Resource Manager. </description>
-  </property>
-
-  <property>
-    <name>yarn.nodemanager.local-dirs</name>
-    <value></value>
-    <description>the local directories used by the nodemanager</description>
-  </property>
-
-  <property>
-    <name>yarn.nodemanager.address</name>
-    <value>0.0.0.0:port</value>
-    <description>the nodemanagers bind to this port</description>
-  </property>  
-
-  <property>
-    <name>yarn.nodemanager.resource.memory-mb</name>
-    <value>10240</value>
-    <description>the amount of memory on the NodeManager in GB</description>
-  </property>
- 
-  <property>
-    <name>yarn.nodemanager.remote-app-log-dir</name>
-    <value>/app-logs</value>
-    <description>directory on hdfs where the application logs are moved to </description>
-  </property>
-
-   <property>
-    <name>yarn.nodemanager.log-dirs</name>
-    <value></value>
-    <description>the directories used by Nodemanagers as log directories</description>
-  </property>
-
-  <property>
-    <name>yarn.nodemanager.aux-services</name>
-    <value>mapreduce.shuffle</value>
-    <description>shuffle service that needs to be set for Map Reduce to run </description>
-  </property>
-+---+
-
-* Setting up <<<capacity-scheduler.xml>>>
-
-   Make sure you populate the root queues in <<<capacity-scheduler.xml>>>.
-
-+---+
-  <property>
-    <name>yarn.scheduler.capacity.root.queues</name>
-    <value>unfunded,default</value>
-  </property>
-  
-  <property>
-    <name>yarn.scheduler.capacity.root.capacity</name>
-    <value>100</value>
-  </property>
-  
-  <property>
-    <name>yarn.scheduler.capacity.root.unfunded.capacity</name>
-    <value>50</value>
-  </property>
-  
-  <property>
-    <name>yarn.scheduler.capacity.root.default.capacity</name>
-    <value>50</value>
-  </property>
-+---+
-
-* Running daemons.
-
-  Assuming that the environment variables <<$HADOOP_COMMON_HOME>>, <<$HADOOP_HDFS_HOME>>, <<$HADOO_MAPRED_HOME>>,
-  <<$YARN_HOME>>, <<$JAVA_HOME>> and <<$HADOOP_CONF_DIR>> have been set appropriately.
-  Set $<<$YARN_CONF_DIR>> the same as $<<HADOOP_CONF_DIR>>
- 
-  Run ResourceManager and NodeManager as:
-  
-+---+
-$ cd $HADOOP_MAPRED_HOME
-$ sbin/yarn-daemon.sh start resourcemanager
-$ sbin/yarn-daemon.sh start nodemanager
-+---+
-
-  You should be up and running. You can run randomwriter as:
-
-+---+
-$ $HADOOP_COMMON_HOME/bin/hadoop jar hadoop-examples.jar randomwriter out
-+---+
-
-Good luck.
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm
deleted file mode 100644
index 38b8dc8..0000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm
+++ /dev/null
@@ -1,1769 +0,0 @@
-~~ Licensed under the Apache License, Version 2.0 (the "License");
-~~ you may not use this file except in compliance with the License.
-~~ You may obtain a copy of the License at
-~~
-~~   http://www.apache.org/licenses/LICENSE-2.0
-~~
-~~ Unless required by applicable law or agreed to in writing, software
-~~ distributed under the License is distributed on an "AS IS" BASIS,
-~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-~~ See the License for the specific language governing permissions and
-~~ limitations under the License. See accompanying LICENSE file.
-
-  ---
-  Hadoop Distributed File System-${project.version} - WebHDFS REST API
-  ---
-  ---
-  ${maven.build.timestamp}
-
-WebHDFS REST API
-
-  \[ {{{./index.html}Go Back}} \]
-
-%{toc|section=1|fromDepth=0}
-
-* {Document Conventions}
-
-*----------------------+-------------------------------------------------------------------------------+
-| <<<Monospaced>>>     | Used for commands, HTTP request and responses and code blocks.                |
-*----------------------+-------------------------------------------------------------------------------+
-| <<<\<Monospaced\>>>> | User entered values.                                                          |
-*----------------------+-------------------------------------------------------------------------------+
-| <<<[Monospaced]>>>   | Optional values.  When the value is not specified, the default value is used. |
-*----------------------+-------------------------------------------------------------------------------+
-| <Italics>            | Important phrases and words.                                                  |
-*----------------------+-------------------------------------------------------------------------------+
-
-
-* {Introduction}
-
-  The HTTP REST API supports the complete
-  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}/{{{../../api/org/apache/hadoop/fs/FileContext.html}FileContext}}
-  interface for HDFS.
-  The operations and the corresponding FileSystem/FileContext methods are shown in the next section.
-  The Section {{HTTP Query Parameter Dictionary}} specifies the parameter details
-  such as the defaults and the valid values.
-
-** {Operations}
-
-  * HTTP GET
-
-    * {{{Open and Read a File}<<<OPEN>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.open)
-
-    * {{{Status of a File/Directory}<<<GETFILESTATUS>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileStatus)
-
-    * {{<<<LISTSTATUS>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.listStatus)
-
-    * {{{Get Content Summary of a Directory}<<<GETCONTENTSUMMARY>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getContentSummary)
-
-    * {{{Get File Checksum}<<<GETFILECHECKSUM>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileChecksum)
-
-    * {{{Get Home Directory}<<<GETHOMEDIRECTORY>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getHomeDirectory)
-
-    * {{{Get Delegation Token}<<<GETDELEGATIONTOKEN>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationToken)
-
-    * {{{Get Delegation Tokens}<<<GETDELEGATIONTOKENS>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationTokens)
-
-  * HTTP PUT
-
-    * {{{Create and Write to a File}<<<CREATE>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.create)
-
-    * {{{Make a Directory}<<<MKDIRS>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.mkdirs)
-
-    * {{{Create a Symbolic Link}<<<CREATESYMLINK>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileContext.html}FileContext}}.createSymlink)
-
-    * {{{Rename a File/Directory}<<<RENAME>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.rename)
-
-    * {{{Set Replication Factor}<<<SETREPLICATION>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setReplication)
-
-    * {{{Set Owner}<<<SETOWNER>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setOwner)
-
-    * {{{Set Permission}<<<SETPERMISSION>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setPermission)
-
-    * {{{Set Access or Modification Time}<<<SETTIMES>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setTimes)
-
-    * {{{Renew Delegation Token}<<<RENEWDELEGATIONTOKEN>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.renewDelegationToken)
-
-    * {{{Cancel Delegation Token}<<<CANCELDELEGATIONTOKEN>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.cancelDelegationToken)
-
-  * HTTP POST
-
-    * {{{Append to a File}<<<APPEND>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.append)
-
-  * HTTP DELETE
-
-    * {{{Delete a File/Directory}<<<DELETE>>>}}
-        (see  {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.delete)
-
-** {FileSystem URIs vs HTTP URLs}
-
-  The FileSystem scheme of WebHDFS is "<<<webhdfs://>>>".
-  A WebHDFS FileSystem URI has the following format.
-
-+---------------------------------
-  webhdfs://<HOST>:<HTTP_PORT>/<PATH>
-+---------------------------------
-
-  The above WebHDFS URI corresponds to the below HDFS URI.
-
-+---------------------------------
-  hdfs://<HOST>:<RPC_PORT>/<PATH>
-+---------------------------------
-
-  In the REST API, the prefix "<<</webhdfs/v1>>>" is inserted in the path and a query is appended at the end.
-  Therefore, the corresponding HTTP URL has the following format.
-
-+---------------------------------
-  http://<HOST>:<HTTP_PORT>/webhdfs/v1/<PATH>?op=...
-+---------------------------------
-
-** {HDFS Configuration Options}
-
-  Below are the HDFS configuration options for WebHDFS.
-
-*-------------------------------------------------+---------------------------------------------------+
-|| Property Name	                          || Description                                      |
-*-------------------------------------------------+---------------------------------------------------+
-| <<<dfs.webhdfs.enabled                      >>> | Enable/disable WebHDFS in Namenodes and Datanodes |
-*-------------------------------------------------+---------------------------------------------------+
-| <<<dfs.web.authentication.kerberos.principal>>> | The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. The HTTP Kerberos principal MUST start with 'HTTP/' per Kerberos HTTP SPNEGO specification. |
-*-------------------------------------------------+---------------------------------------------------+
-| <<<dfs.web.authentication.kerberos.keytab   >>> | The Kerberos keytab file with the credentials for the HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. |
-*-------------------------------------------------+---------------------------------------------------+
-
-* {Authentication}
-
-  When security is <off>, the authenticated user is the username specified in the <<<user.name>>> query parameter.
-  If the <<<user.name>>> parameter is not set,
-  the server may either set the authenticated user to a default web user, if there is any, or return an error response.
-
-
-  When security is <on>, authentication is performed by either Hadoop delegation token or Kerberos SPNEGO.
-  If a token is set in the <<<delegation>>> query parameter, the authenticated user is the user encoded in the token.
-  If the <<<delegation>>> parameter is not set, the user is authenticated by Kerberos SPNEGO.
-
-
-  Below are examples using the <<<curl>>> command tool.
-
-  [[1]] Authentication when security is off:
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?[user.name=<USER>&]op=..."
-+---------------------------------
- 
-  [[1]] Authentication using Kerberos SPNEGO when security is on:
-
-+---------------------------------
-curl -i --negotiate -u : "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=..."
-+---------------------------------
- 
-  [[1]] Authentication using Hadoop delegation token when security is on:
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?delegation=<TOKEN>&op=..."
-+---------------------------------
-
-* {Proxy Users}
-
-  When the proxy user feature is enabled, a proxy user <P> may submit a request on behalf of another user <U>.
-  The username of <U> must be specified in the <<<doas>>> query parameter unless a delegation token is presented in authentication.
-  In such case, the information of both users <P> and <U> must be encoded in the delegation token.
-
-  [[1]] A proxy request when security is off:
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?[user.name=<USER>&]doas=<USER>&op=..."
-+---------------------------------
-
-  [[1]] A proxy request using Kerberos SPNEGO when security is on:
-
-+---------------------------------
-curl -i --negotiate -u : "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?doas=<USER>&op=..."
-+---------------------------------
-
-  [[1]] A proxy request using Hadoop delegation token when security is on:
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?delegation=<TOKEN>&op=..."
-+---------------------------------
-
-
-* {File and Directory Operations}
-
-** {Create and Write to a File}
-
-  * Step 1: Submit a HTTP PUT request without automatically following redirects and without sending the file data.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=CREATE
-                    [&overwrite=<true|false>][&blocksize=<LONG>][&replication=<SHORT>]
-                    [&permission=<OCTAL>][&buffersize=<INT>]"
-+---------------------------------
-
-  The request is redirected to a datanode where the file data is to be written:
-
-+---------------------------------
-HTTP/1.1 307 TEMPORARY_REDIRECT
-Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=CREATE...
-Content-Length: 0
-+---------------------------------
-
-  * Step 2: Submit another HTTP PUT request using the URL in the <<<Location>>> header with the file data to be written.
-
-+---------------------------------
-curl -i -X PUT -T <LOCAL_FILE> "http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=CREATE..."
-+---------------------------------
-
-  The client receives a <<<201 Created>>> response with zero content length
-  and the WebHDFS URI of the file in the <<<Location>>> header:
-
-+---------------------------------
-HTTP/1.1 201 Created
-Location: webhdfs://<HOST>:<PORT>/<PATH>
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  <<Note>> that the reason of having two-step create/append is
-  for preventing clients to send out data before the redirect.
-  This issue is addressed by the "<<<Expect: 100-continue>>>" header in HTTP/1.1;
-  see {{{http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.2.3}RFC 2616, Section 8.2.3}}.
-  Unfortunately, there are software library bugs (e.g. Jetty 6 HTTP server and Java 6 HTTP client),
-  which do not correctly implement "<<<Expect: 100-continue>>>".
-  The two-step create/append is a temporary workaround for the software library bugs.
-
-  See also:
-  {{{Overwrite}<<<overwrite>>>}},
-  {{{Block Size}<<<blocksize>>>}},
-  {{{Replication}<<<replication>>>}},
-  {{{Permission}<<<permission>>>}},
-  {{{Buffer Size}<<<buffersize>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.create
-
-
-** {Append to a File}
-
-  * Step 1: Submit a HTTP POST request without automatically following redirects and without sending the file data.
-
-+---------------------------------
-curl -i -X POST "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=APPEND[&buffersize=<INT>]"
-+---------------------------------
-
-  The request is redirected to a datanode where the file data is to be appended:
-
-+---------------------------------
-HTTP/1.1 307 TEMPORARY_REDIRECT
-Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=APPEND...
-Content-Length: 0
-+---------------------------------
-
-  * Step 2: Submit another HTTP POST request using the URL in the <<<Location>>> header with the file data to be appended.
-
-+---------------------------------
-curl -i -X POST -T <LOCAL_FILE> "http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=APPEND..."
-+---------------------------------
-
-  The client receives a response with zero content length:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  See the note in the previous section for the description of why this operation requires two steps.
-
-  See also:
-  {{{Buffer Size}<<<buffersize>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.append
-
-
-** {Open and Read a File}
-
-  * Submit a HTTP GET request with automatically following redirects.
-
-+---------------------------------
-curl -i -L "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=OPEN
-                    [&offset=<LONG>][&length=<LONG>][&buffersize=<INT>]"
-+---------------------------------
-
-  The request is redirected to a datanode where the file data can be read:
-
-+---------------------------------
-HTTP/1.1 307 TEMPORARY_REDIRECT
-Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=OPEN...
-Content-Length: 0
-+---------------------------------
-
-  The client follows the redirect to the datanode and receives the file data:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/octet-stream
-Content-Length: 22
-
-Hello, webhdfs user!
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Offset}<<<offset>>>}},
-  {{{Length}<<<length>>>}},
-  {{{Buffer Size}<<<buffersize>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.open
-
-
-** {Make a Directory}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/<PATH>?op=MKDIRS[&permission=<OCTAL>]"
-+---------------------------------
-
-  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{"boolean": true}
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Permission}<<<permission>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.mkdirs
-
-
-** {Create a Symbolic Link}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/<PATH>?op=CREATESYMLINK
-                              &destination=<PATH>[&createParent=<true|false>]"
-+---------------------------------
-
-  The client receives a response with zero content length:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Destination}<<<destination>>>}},
-  {{{Create Parent}<<<createParent>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.createSymlink
-
-
-** {Rename a File/Directory}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "<HOST>:<PORT>/webhdfs/v1/<PATH>?op=RENAME&destination=<PATH>"
-+---------------------------------
-
-  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{"boolean": true}
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Destination}<<<destination>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.rename
-
-
-** {Delete a File/Directory}
-
-  * Submit a HTTP DELETE request.
-
-+---------------------------------
-curl -i -X DELETE "http://<host>:<port>/webhdfs/v1/<path>?op=DELETE
-                              [&recursive=<true|false>]"
-+---------------------------------
-
-  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{"boolean": true}
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Recursive}<<<recursive>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.delete
-
-
-** {Status of a File/Directory}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i  "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=GETFILESTATUS"
-+---------------------------------
-
-  The client receives a response with a {{{FileStatus JSON Schema}<<<FileStatus>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "FileStatus":
-  {
-    "accessTime"      : 0,
-    "blockSize"       : 0,
-    "group"           : "supergroup",
-    "length"          : 0,             //in bytes, zero for directories
-    "modificationTime": 1320173277227,
-    "owner"           : "webuser",
-    "pathSuffix"      : "",
-    "permission"      : "777",
-    "replication"     : 0,
-    "type"            : "DIRECTORY"    //enum {FILE, DIRECTORY, SYMLINK}
-  }
-}
-+---------------------------------
-
-  []
-
-  See also:
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileStatus
-
-
-** {List a Directory}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i  "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=LISTSTATUS"
-+---------------------------------
-
-  The client receives a response with a {{{FileStatuses JSON Schema}<<<FileStatuses>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Content-Length: 427
-
-{
-  "FileStatuses":
-  {
-    "FileStatus":
-    [
-      {
-        "accessTime"      : 1320171722771,
-        "blockSize"       : 33554432,
-        "group"           : "supergroup",
-        "length"          : 24930,
-        "modificationTime": 1320171722771,
-        "owner"           : "webuser",
-        "pathSuffix"      : "a.patch",
-        "permission"      : "644",
-        "replication"     : 1,
-        "type"            : "FILE"
-      },
-      {
-        "accessTime"      : 0,
-        "blockSize"       : 0,
-        "group"           : "supergroup",
-        "length"          : 0,
-        "modificationTime": 1320895981256,
-        "owner"           : "szetszwo",
-        "pathSuffix"      : "bar",
-        "permission"      : "711",
-        "replication"     : 0,
-        "type"            : "DIRECTORY"
-      },
-      ...
-    ]
-  }
-}
-+---------------------------------
-
-  []
-
-  See also:
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.listStatus
-
-
-* {Other File System Operations}
-
-** {Get Content Summary of a Directory}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=GETCONTENTSUMMARY"
-+---------------------------------
-
-  The client receives a response with a {{{ContentSummary JSON Schema}<<<ContentSummary>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "ContentSummary":
-  {
-    "directoryCount": 2,
-    "fileCount"     : 1,
-    "length"        : 24930,
-    "quota"         : -1,
-    "spaceConsumed" : 24930,
-    "spaceQuota"    : -1
-  }
-}
-+---------------------------------
-
-  []
-
-  See also:
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getContentSummary
-
-
-** {Get File Checksum}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=GETFILECHECKSUM"
-+---------------------------------
-
-  The request is redirected to a datanode:
-
-+---------------------------------
-HTTP/1.1 307 TEMPORARY_REDIRECT
-Location: http://<DATANODE>:<PORT>/webhdfs/v1/<PATH>?op=GETFILECHECKSUM...
-Content-Length: 0
-+---------------------------------
-
-  The client follows the redirect to the datanode and receives a {{{FileChecksum JSON Schema}<<<FileChecksum>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "FileChecksum":
-  {
-    "algorithm": "MD5-of-1MD5-of-512CRC32",
-    "bytes"    : "eadb10de24aa315748930df6e185c0d ...",
-    "length"   : 28
-  }
-}
-+---------------------------------
-
-  []
-
-  See also:
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getFileChecksum
-
-
-** {Get Home Directory}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/?op=GETHOMEDIRECTORY"
-+---------------------------------
-
-  The client receives a response with a {{{Path JSON Schema}<<<Path>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{"Path": "/user/szetszwo"}
-+---------------------------------
-
-  []
-
-  See also:
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getHomeDirectory
-
-
-** {Set Permission}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETPERMISSION
-                              [&permission=<OCTAL>]"
-+---------------------------------
-
-  The client receives a response with zero content length:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Permission}<<<permission>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setPermission
-
-
-** {Set Owner}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETOWNER
-                              [&owner=<USER>][&group=<GROUP>]"
-+---------------------------------
-
-  The client receives a response with zero content length:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Owner}<<<owner>>>}},
-  {{{Group}<<<group>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setOwner
-
-
-** {Set Replication Factor}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETREPLICATION
-                              [&replication=<SHORT>]"
-+---------------------------------
-
-  The client receives a response with a {{{Boolean JSON Schema}<<<boolean>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{"boolean": true}
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Replication}<<<replication>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setReplication
-
-
-** {Set Access or Modification Time}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETTIMES
-                              [&modificationtime=<TIME>][&accesstime=<TIME>]"
-+---------------------------------
-
-  The client receives a response with zero content length:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Modification Time}<<<modificationtime>>>}},
-  {{{Access Time}<<<accesstime>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.setTimes
-
-
-* {Delegation Token Operations}
-
-** {Get Delegation Token}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/?op=GETDELEGATIONTOKEN&renewer=<USER>"
-+---------------------------------
-
-  The client receives a response with a {{{Token JSON Schema}<<<Token>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "Token":
-  {
-    "urlString": "JQAIaG9y..."
-  }
-}
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Renewer}<<<renewer>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationToken
-
-
-** {Get Delegation Tokens}
-
-  * Submit a HTTP GET request.
-
-+---------------------------------
-curl -i "http://<HOST>:<PORT>/webhdfs/v1/?op=GETDELEGATIONTOKENS&renewer=<USER>"
-+---------------------------------
-
-  The client receives a response with a {{{Tokens JSON Schema}<<<Tokens>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "Tokens":
-  {
-    "Token":
-    [
-      {
-        "urlString":"KAAKSm9i ..."
-      }
-    ]
-  }
-}
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Renewer}<<<renewer>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.getDelegationTokens
-
-
-** {Renew Delegation Token}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&token=<TOKEN>"
-+---------------------------------
-
-  The client receives a response with a {{{Long JSON Schema}<<<long>>> JSON object}}:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{"long": 1320962673997}           //the new expiration time
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Token}<<<token>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.renewDelegationToken
-
-
-** {Cancel Delegation Token}
-
-  * Submit a HTTP PUT request.
-
-+---------------------------------
-curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/?op=CANCELDELEGATIONTOKEN&token=<TOKEN>"
-+---------------------------------
-
-  The client receives a response with zero content length:
-
-+---------------------------------
-HTTP/1.1 200 OK
-Content-Length: 0
-+---------------------------------
-
-  []
-
-  See also:
-  {{{Token}<<<token>>>}},
-   {{{../../api/org/apache/hadoop/fs/FileSystem.html}FileSystem}}.cancelDelegationToken
-
-
-* {Error Responses}
-
-  When an operation fails, the server may throw an exception.
-  The JSON schema of error responses is defined in {{<<<RemoteException>>> JSON schema}}.
-  The table below shows the mapping from exceptions to HTTP response codes.
-
-** {HTTP Response Codes}
-
-*-------------------------------------+---------------------------------+
-|| Exceptions                         || HTTP Response Codes            |
-*-------------------------------------+---------------------------------+
-| <<<IllegalArgumentException     >>> | <<<400 Bad Request          >>> |
-*-------------------------------------+---------------------------------+
-| <<<UnsupportedOperationException>>> | <<<400 Bad Request          >>> |
-*-------------------------------------+---------------------------------+
-| <<<SecurityException            >>> | <<<401 Unauthorized         >>> |
-*-------------------------------------+---------------------------------+
-| <<<IOException                  >>> | <<<403 Forbidden            >>> |
-*-------------------------------------+---------------------------------+
-| <<<FileNotFoundException        >>> | <<<404 Not Found            >>> |
-*-------------------------------------+---------------------------------+
-| <<<RumtimeException             >>> | <<<500 Internal Server Error>>> |
-*-------------------------------------+---------------------------------+
-
-  Below are examples of exception responses.
-
-*** {Illegal Argument Exception}
-
-+---------------------------------
-HTTP/1.1 400 Bad Request
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "RemoteException":
-  {
-    "exception"    : "IllegalArgumentException",
-    "javaClassName": "java.lang.IllegalArgumentException",
-    "message"      : "Invalid value for webhdfs parameter \"permission\": ..."
-  }
-}
-+---------------------------------
-
-
-*** {Security Exception}
-
-+---------------------------------
-HTTP/1.1 401 Unauthorized
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "RemoteException":
-  {
-    "exception"    : "SecurityException",
-    "javaClassName": "java.lang.SecurityException",
-    "message"      : "Failed to obtain user group information: ..."
-  }
-}
-+---------------------------------
-
-
-*** {Access Control Exception}
-
-+---------------------------------
-HTTP/1.1 403 Forbidden
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "RemoteException":
-  {
-    "exception"    : "AccessControlException",
-    "javaClassName": "org.apache.hadoop.security.AccessControlException",
-    "message"      : "Permission denied: ..."
-  }
-}
-+---------------------------------
-
-
-*** {File Not Found Exception}
-
-+---------------------------------
-HTTP/1.1 404 Not Found
-Content-Type: application/json
-Transfer-Encoding: chunked
-
-{
-  "RemoteException":
-  {
-    "exception"    : "FileNotFoundException",
-    "javaClassName": "java.io.FileNotFoundException",
-    "message"      : "File does not exist: /foo/a.patch"
-  }
-}
-+---------------------------------
-
-
-* {JSON Schemas}
-
-  All operations, except for {{{Open and Read a File}<<<OPEN>>>}},
-  either return a zero-length response or a JSON response. 
-  For {{{Open and Read a File}<<<OPEN>>>}}, the response is an octet-stream.
-  The JSON schemas are shown below.
-  See {{{http://tools.ietf.org/id/draft-zyp-json-schema-03.html}draft-zyp-json-schema-03}}
-  for the syntax definitions of the JSON schemas.
-
-
-** {Boolean JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "boolean",
-  "properties":
-  {
-    "boolean":
-    {
-      "description": "A boolean value",
-      "type"       : "boolean",
-      "required"   : true
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Make a Directory}<<<MKDIRS>>>}},
-  {{{Rename a File/Directory}<<<RENAME>>>}},
-  {{{Delete a File/Directory}<<<DELETE>>>}},
-  {{{Set Replication Factor}<<<SETREPLICATION>>>}}
-
-
-** {ContentSummary JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "ContentSummary",
-  "properties":
-  {
-    "ContentSummary":
-    {
-      "type"      : "object",
-      "properties":
-      {
-        "directoryCount":
-        {
-          "description": "The number of directories.",
-          "type"       : "integer",
-          "required"   : true
-        },
-        "fileCount":
-        {
-          "description": "The number of files.",
-          "type"       : "integer",
-          "required"   : true
-        },
-        "length":
-        {
-          "description": "The number of bytes used by the content.",
-          "type"       : "integer",
-          "required"   : true
-        },
-        "quota":
-        {
-          "description": "The namespace quota of this directory.",
-          "type"       : "integer",
-          "required"   : true
-        },
-        "spaceConsumed":
-        {
-          "description": "The disk space consumed by the content.",
-          "type"       : "integer",
-          "required"   : true
-        },
-        "spaceQuota":
-        {
-          "description": "The disk space quota.",
-          "type"       : "integer",
-          "required"   : true
-        }
-      }
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Get Content Summary of a Directory}<<<GETCONTENTSUMMARY>>>}}
-
-
-** {FileChecksum JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "FileChecksum",
-  "properties":
-  {
-    "FileChecksum":
-    {
-      "type"      : "object",
-      "properties":
-      {
-        "algorithm":
-        {
-          "description": "The name of the checksum algorithm.",
-          "type"       : "string",
-          "required"   : true
-        },
-        "bytes":
-        {
-          "description": "The byte sequence of the checksum in hexadecimal.",
-          "type"       : "string",
-          "required"   : true
-        },
-        "length":
-        {
-          "description": "The length of the bytes (not the length of the string).",
-          "type"       : "integer",
-          "required"   : true
-        }
-      }
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Get File Checksum}<<<GETFILECHECKSUM>>>}}
-
-
-** {FileStatus JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "FileStatus",
-  "properties":
-  {
-    "FileStatus": fileStatusProperties      //See FileStatus Properties
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{FileStatus Properties}<<<FileStatus>>> Properties}},
-  {{{Status of a File/Directory}<<<GETFILESTATUS>>>}},
-  {{{../../api/org/apache/hadoop/fs/FileStatus}FileStatus}}
-
-
-*** {FileStatus Properties}
-
-  JavaScript syntax is used to define <<<fileStatusProperties>>>
-  so that it can be referred in both <<<FileStatus>>> and <<<FileStatuses>>> JSON schemas.
-
-+---------------------------------
-var fileStatusProperties =
-{
-  "type"      : "object",
-  "properties":
-  {
-    "accessTime":
-    {
-      "description": "The access time.",
-      "type"       : "integer",
-      "required"   : true
-    },
-    "blockSize":
-    {
-      "description": "The block size of a file.",
-      "type"       : "integer",
-      "required"   : true
-    },
-    "group":
-    {
-      "description": "The group owner.",
-      "type"       : "string",
-      "required"   : true
-    },
-    "length":
-    {
-      "description": "The number of bytes in a file.",
-      "type"       : "integer",
-      "required"   : true
-    },
-    "modificationTime":
-    {
-      "description": "The modification time.",
-      "type"       : "integer",
-      "required"   : true
-    },
-    "owner":
-    {
-      "description": "The user who is the owner.",
-      "type"       : "string",
-      "required"   : true
-    },
-    "pathSuffix":
-    {
-      "description": "The path suffix.",
-      "type"       : "string",
-      "required"   : true
-    },
-    "permission":
-    {
-      "description": "The permission represented as a octal string.",
-      "type"       : "string",
-      "required"   : true
-    },
-    "replication":
-    {
-      "description": "The number of replication of a file.",
-      "type"       : "integer",
-      "required"   : true
-    },
-   "symlink":                                         //an optional property
-    {
-      "description": "The link target of a symlink.",
-      "type"       : "string"
-    },
-   "type":
-    {
-      "description": "The type of the path object.",
-      "enum"       : ["FILE", "DIRECTORY", "SYMLINK"],
-      "required"   : true
-    }
-  }
-};
-+---------------------------------
-
-
-** {FileStatuses JSON Schema}
-
-  A <<<FileStatuses>>> JSON object represents an array of <<<FileStatus>>> JSON objects.
-
-+---------------------------------
-{
-  "name"      : "FileStatuses",
-  "properties":
-  {
-    "FileStatuses":
-    {
-      "type"      : "object",
-      "properties":
-      {
-        "FileStatus":
-        {
-          "description": "An array of FileStatus",
-          "type"       : "array",
-          "items"      : fileStatusProperties      //See FileStatus Properties
-        }
-      }
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{FileStatus Properties}<<<FileStatus>>> Properties}},
-  {{{List a Directory}<<<LISTSTATUS>>>}},
-  {{{../../api/org/apache/hadoop/fs/FileStatus}FileStatus}}
-
-
-** {Long JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "long",
-  "properties":
-  {
-    "long":
-    {
-      "description": "A long integer value",
-      "type"       : "integer",
-      "required"   : true
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Renew Delegation Token}<<<RENEWDELEGATIONTOKEN>>>}},
-
-
-** {Path JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "Path",
-  "properties":
-  {
-    "Path":
-    {
-      "description": "The string representation a Path.",
-      "type"       : "string",
-      "required"   : true
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Get Home Directory}<<<GETHOMEDIRECTORY>>>}},
-  {{{../../api/org/apache/hadoop/fs/Path}Path}}
-
-
-** {RemoteException JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "RemoteException",
-  "properties":
-  {
-    "RemoteException":
-    {
-      "type"      : "object",
-      "properties":
-      {
-        "exception":
-        {
-          "description": "Name of the exception",
-          "type"       : "string",
-          "required"   : true
-        },
-        "message":
-        {
-          "description": "Exception message",
-          "type"       : "string",
-          "required"   : true
-        },
-        "javaClassName":                                     //an optional property
-        {
-          "description": "Java class name of the exception",
-          "type"       : "string",
-        }
-      }
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{Error Responses}}
-
-
-** {Token JSON Schema}
-
-+---------------------------------
-{
-  "name"      : "Token",
-  "properties":
-  {
-    "Token": tokenProperties      //See Token Properties
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Token Properties}<<<Token>>> Properties}},
-  {{{Get Delegation Token}<<<GETDELEGATIONTOKEN>>>}},
-  the note in {{Delegation}}.
-
-*** {Token Properties}
-
-  JavaScript syntax is used to define <<<tokenProperties>>>
-  so that it can be referred in both <<<Token>>> and <<<Tokens>>> JSON schemas.
-
-+---------------------------------
-var tokenProperties =
-{
-  "type"      : "object",
-  "properties":
-  {
-    "urlString":
-    {
-      "description": "A delegation token encoded as a URL safe string.",
-      "type"       : "string",
-      "required"   : true
-    }
-  }
-}
-+---------------------------------
-
-** {Tokens JSON Schema}
-
-  A <<<Tokens>>> JSON object represents an array of <<<Token>>> JSON objects.
-
-+---------------------------------
-{
-  "name"      : "Tokens",
-  "properties":
-  {
-    "Tokens":
-    {
-      "type"      : "object",
-      "properties":
-      {
-        "Token":
-        {
-          "description": "An array of Token",
-          "type"       : "array",
-          "items"      : "Token": tokenProperties      //See Token Properties
-        }
-      }
-    }
-  }
-}
-+---------------------------------
-
-  See also:
-  {{{Token Properties}<<<Token>>> Properties}},
-  {{{Get Delegation Tokens}<<<GETDELEGATIONTOKENS>>>}},
-  the note in {{Delegation}}.
-
-
-* {HTTP Query Parameter Dictionary}
-
-** {Access Time}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<accesstime>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The access time of a file/directory. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | long |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | -1 (means keeping it unchanged) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | -1 or a timestamp |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Set Access or Modification Time}<<<SETTIMES>>>}}
-
-
-** {Block Size}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<blocksize>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The block size of a file. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | long |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | Specified in the configuration. |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | \> 0 |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create and Write to a File}<<<CREATE>>>}}
-
-
-** {Buffer Size}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<buffersize>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The size of the buffer used in transferring data. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | int |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | Specified in the configuration. |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | \> 0 |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create and Write to a File}<<<CREATE>>>}},
-  {{{Append to a File}<<<APPEND>>>}},
-  {{{Open and Read a File}<<<OPEN>>>}}
-
-
-** {Create Parent}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<createparent>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | If the parent directories do not exist, should they be created?   |
-*----------------+-------------------------------------------------------------------+
-|| Type          | boolean |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | false |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | true | false |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | true | false |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create a Symbolic Link}<<<CREATESYMLINK>>>}}
-
-
-** {Delegation}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<delegation>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The delegation token used for authentication. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | \<empty\> |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | An encoded token. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | See the note below. |
-*----------------+-------------------------------------------------------------------+
-
-  <<Note>> that delegation tokens are encoded as a URL safe string;
-  see <<<encodeToUrlString()>>>
-  and <<<decodeFromUrlString(String)>>>
-  in <<<org.apache.hadoop.security.token.Token>>> for the details of the encoding.
-
-
-  See also:
-  {{Authentication}}
-
-
-** {Destination}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<destination>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The destination path. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | Path |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | \<empty\> (an invalid path) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | An absolute FileSystem path without scheme and authority. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any path. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create a Symbolic Link}<<<CREATESYMLINK>>>}},
-  {{{Rename a File/Directory}<<<RENAME>>>}}
-
-
-** {Do As}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<doas>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | Allowing a proxy user to do as another user. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | null |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | Any valid username. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any string. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{Proxy Users}}
-
-
-** {Group}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<group>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The name of a group. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | \<empty\> (means keeping it unchanged) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | Any valid group name. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any string. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Set Owner}<<<SETOWNER>>>}}
-
-
-** {Length}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<length>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The number of bytes to be processed. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | long |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | null (means the entire file) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | \>= 0 or null |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Open and Read a File}<<<OPEN>>>}}
-
-
-** {Modification Time}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<modificationtime>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The modification time of a file/directory. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | long |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | -1 (means keeping it unchanged) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | -1 or a timestamp |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Set Access or Modification Time}<<<SETTIMES>>>}}
-
-
-** {Offset}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<offset>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The starting byte position. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | long |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | 0 |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | \>= 0 |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Open and Read a File}<<<OPEN>>>}}
-
-
-** {Op}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<op>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The name of the operation to be executed. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | enum |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | null (an invalid value) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | Any valid operation name. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any string. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{Operations}}
-
-
-** {Overwrite}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<overwrite>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | If a file already exists, should it be overwritten? |
-*----------------+-------------------------------------------------------------------+
-|| Type          | boolean |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | false |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | true | false |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | true | false |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create and Write to a File}<<<CREATE>>>}}
-
-
-** {Owner}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<owner>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The username who is the owner of a file/directory. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | \<empty\> (means keeping it unchanged) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | Any valid username. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any string. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Set Owner}<<<SETOWNER>>>}}
-
-
-** {Permission}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<permission>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The permission of a file/directory. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | Octal |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | 755 |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | 0 - 1777 |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any radix-8 integer (leading zeros may be omitted.) |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create and Write to a File}<<<CREATE>>>}},
-  {{{Make a Directory}<<<MKDIRS>>>}},
-  {{{Set Permission}<<<SETPERMISSION>>>}}
-
-
-** {Recursive}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<recursive>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | Should the operation act on the content in the subdirectories? |
-*----------------+-------------------------------------------------------------------+
-|| Type          | boolean |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | false |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | true | false |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | true | false |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Rename a File/Directory}<<<RENAME>>>}}
-
-
-** {Renewer}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<renewer>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The username of the renewer of a delegation token. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | \<empty\> (means the current user) |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | Any valid username. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any string. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Get Delegation Token}<<<GETDELEGATIONTOKEN>>>}},
-  {{{Get Delegation Tokens}<<<GETDELEGATIONTOKENS>>>}}
-
-
-** {Replication}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<replication>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The number of replications of a file. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | short |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | Specified in the configuration. |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | \> 0 |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any integer. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Create and Write to a File}<<<CREATE>>>}},
-  {{{Set Replication Factor}<<<SETREPLICATION>>>}}
-
-
-** {Token}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<token>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The delegation token used for the operation. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | \<empty\> |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | An encoded token. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | See the note in {{Delegation}}. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{{Renew Delegation Token}<<<RENEWDELEGATIONTOKEN>>>}},
-  {{{Cancel Delegation Token}<<<CANCELDELEGATIONTOKEN>>>}}
-
-
-** {Username}
-
-*----------------+-------------------------------------------------------------------+
-|| Name          | <<<user.name>>> |
-*----------------+-------------------------------------------------------------------+
-|| Description   | The authenticated user; see {{Authentication}}. |
-*----------------+-------------------------------------------------------------------+
-|| Type          | String |
-*----------------+-------------------------------------------------------------------+
-|| Default Value | null |
-*----------------+-------------------------------------------------------------------+
-|| Valid Values  | Any valid username. |
-*----------------+-------------------------------------------------------------------+
-|| Syntax        | Any string. |
-*----------------+-------------------------------------------------------------------+
-
-  See also:
-  {{Authentication}}
-
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
index 84ecb39..da5ff5f 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/index.apt.vm
@@ -49,8 +49,8 @@ MapReduce NextGen aka YARN aka MRv2
 
   * {{{./WebApplicationProxy.html}Web Application Proxy}}
 
-  * {{{./CLIMiniCluster.html}CLI MiniCluster}}
+  * {{{../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html}CLI MiniCluster}}
 
-  * {{{./EncryptedShuffle.html}Encrypted Shuffle}}
+  * {{{../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html}Encrypted Shuffle}}
 
   * {{{./PluggableShuffleAndPluggableSort.html}Pluggable Shuffle and Pluggable Sort}}
-- 
1.7.0.4

