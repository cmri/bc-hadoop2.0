From 27f39d4beae125e8dca16d94a21c53eb650bd13e Mon Sep 17 00:00:00 2001
From: Alejandro Abdelnur <tucu@apache.org>
Date: Fri, 8 Feb 2013 22:42:47 +0000
Subject: [PATCH 1286/1357] MR1: MAPREDUCE-4643. Make job-history cleanup-period configurable. (sandyr via tucu)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1@1444266 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit e83648b4dd854844e655fc7e50f3e9b3305d73ba)

Author: Sandy Ryza
Ref: CDH-10281
Reason: Customer
---
 .../src/mapred/mapred-default.xml                  |   18 +++
 .../org/apache/hadoop/mapred/JobHistory.java       |  129 +++++++++++---------
 .../org/apache/hadoop/mapred/TestJobHistory.java   |   82 +++++--------
 3 files changed, 123 insertions(+), 106 deletions(-)

diff --git a/hadoop-mapreduce1-project/src/mapred/mapred-default.xml b/hadoop-mapreduce1-project/src/mapred/mapred-default.xml
index 4f0da1d..3a09285 100644
--- a/hadoop-mapreduce1-project/src/mapred/mapred-default.xml
+++ b/hadoop-mapreduce1-project/src/mapred/mapred-default.xml
@@ -35,6 +35,24 @@
   </description>
 </property>
 
+<property>
+  <name>mapreduce.jobhistory.max-age-ms</name>
+  <value>2592000000</value>
+  <description> Job history files older than this many milliseconds will
+  be deleted when the history cleaner runs. Defaults to 2592000000 (30
+  days).
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.jobhistory.cleaner.interval-ms</name>
+  <value>86400000</value>
+  <description> How often the job history cleaner checks for files to delete, 
+  in milliseconds. Defaults to 86400000 (one day). Files are only deleted if
+  they are older than mapreduce.jobhistory.max-age-ms.
+  </description>
+</property>
+
 <!-- i/o properties -->
 
 <property>
diff --git a/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/JobHistory.java b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/JobHistory.java
index d29e813..67f60e2 100644
--- a/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/JobHistory.java
+++ b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/JobHistory.java
@@ -120,9 +120,9 @@ public class JobHistory {
   final static FsPermission HISTORY_FILE_PERMISSION =
     FsPermission.createImmutable((short) 0744); // rwxr--r--
   private static FileSystem LOGDIR_FS; // log dir filesystem
-  private static FileSystem DONEDIR_FS; // Done dir filesystem
+  protected static FileSystem DONEDIR_FS; // Done dir filesystem
   private static JobConf jtConf;
-  private static Path DONE = null; // folder for completed jobs
+  protected static Path DONE = null; // folder for completed jobs
   private static String DONE_BEFORE_SERIAL_TAIL = doneSubdirsBeforeSerialTail();
   private static String DONE_LEAF_FILES = DONE_BEFORE_SERIAL_TAIL + "/*";
   private static boolean aclsEnabled = false;
@@ -531,17 +531,17 @@ public class JobHistory {
     fileManager.start();
     
     HistoryCleaner.cleanupFrequency =
-      conf.getLong("mapreduce.jobhistory.cleaner.interval-ms",
-          HistoryCleaner.DEFAULT_CLEANUP_FREQUENCY);
+    	      conf.getLong("mapreduce.jobhistory.cleaner.interval-ms",
+    	      HistoryCleaner.DEFAULT_CLEANUP_FREQUENCY);
     HistoryCleaner.maxAgeOfHistoryFiles =
-      conf.getLong("mapreduce.jobhistory.max-age-ms",
-          HistoryCleaner.DEFAULT_HISTORY_MAX_AGE);
+    	      conf.getLong("mapreduce.jobhistory.max-age-ms",
+    	      HistoryCleaner.DEFAULT_HISTORY_MAX_AGE);
     LOG.info(String.format("Job History MaxAge is %d ms (%.2f days), " +
-          "Cleanup Frequency is %d ms (%.2f days)",
-          HistoryCleaner.maxAgeOfHistoryFiles,
-          ((float) HistoryCleaner.maxAgeOfHistoryFiles)/(HistoryCleaner.ONE_DAY_IN_MS),
-          HistoryCleaner.cleanupFrequency,
-          ((float) HistoryCleaner.cleanupFrequency)/HistoryCleaner.ONE_DAY_IN_MS));
+    	      "Cleanup Frequency is %d ms (%.2f days)",
+    	      HistoryCleaner.maxAgeOfHistoryFiles,
+    	      ((float) HistoryCleaner.maxAgeOfHistoryFiles)/HistoryCleaner.ONE_DAY_IN_MS,
+    	      HistoryCleaner.cleanupFrequency,
+    	      ((float) HistoryCleaner.cleanupFrequency)/HistoryCleaner.ONE_DAY_IN_MS));
   }
 
 
@@ -2368,26 +2368,28 @@ public class JobHistory {
      */
     public void handle(RecordTypes recType, Map<Keys, String> values) throws IOException; 
   }
- 
-  static long directoryTime(String year, String seg2, String seg3) {
-    // set to current time.  In debug mode, this is where the month
-    // and day get set.
+  
+  /**
+   * Returns the time in milliseconds, truncated to the day.
+   */
+  static long directoryTime(String year, String month, String day) {
     Calendar result = Calendar.getInstance();
-    // canonicalize by filling in unset fields
-    result.setTimeInMillis(System.currentTimeMillis());
 
     result.set(Calendar.YEAR, Integer.parseInt(year));
 
     // months are 0-based in Calendar, but people will expect January
     // to be month #1 .  Therefore the number is bumped before we make the 
     // directory name and must be debumped to seek the time.
-    result.set(Calendar.MONTH, Integer.parseInt(seg2) - 1);
+    result.set(Calendar.MONTH, Integer.parseInt(month) - 1);
 
-    result.set(Calendar.DAY_OF_MONTH, Integer.parseInt(seg3));
-
-    return result.getTimeInMillis();
+    result.set(Calendar.DAY_OF_MONTH, Integer.parseInt(day));
+    
+    // truncate to day granularity
+    long timeInMillis = result.getTimeInMillis();
+    return timeInMillis - 
+        timeInMillis % HistoryCleaner.ONE_DAY_IN_MS;
   }
- 
+
   /**
    * Delete history files older than one month (or a configurable age).
    * Update master index and remove all 
@@ -2397,10 +2399,8 @@ public class JobHistory {
    */
   public static class HistoryCleaner implements Runnable {
     static final long ONE_DAY_IN_MS = 24 * 60 * 60 * 1000L;
-    static final long DEFAULT_CLEANUP_FREQUENCY = ONE_DAY_IN_MS;
     static final long DEFAULT_HISTORY_MAX_AGE = 30 * ONE_DAY_IN_MS;
-    static final long DIRECTORY_LIFE_IN_MS = ONE_DAY_IN_MS;
-    static final long RUN_INTERVAL = ONE_DAY_IN_MS;
+    static final long DEFAULT_CLEANUP_FREQUENCY = ONE_DAY_IN_MS;
     static long cleanupFrequency = DEFAULT_CLEANUP_FREQUENCY;
     static long maxAgeOfHistoryFiles = DEFAULT_HISTORY_MAX_AGE;
     private long now; 
@@ -2418,12 +2418,16 @@ public class JobHistory {
         return; 
       }
       now = System.currentTimeMillis();
-      if (lastRan != 0 && (now - lastRan) < RUN_INTERVAL) {
+      // clean history only once a day at max
+      if (lastRan != 0 && (now - lastRan) < cleanupFrequency) {
         isRunning.set(false);
         return; 
       }
       lastRan = now;
-
+      clean(now);
+    }
+    
+    public void clean(long now) {
       Set<String> deletedPathnames = new HashSet<String>();
 
       // XXXXX debug code
@@ -2434,13 +2438,19 @@ public class JobHistory {
         Path[] datedDirectories
           = FileUtil.stat2Paths(localGlobber(DONEDIR_FS, DONE,
                                              DONE_BEFORE_SERIAL_TAIL, null));
-        // find directories older than 30 days
+
+        // any file with a timestamp earlier than cutoff should be deleted
+        long cutoff = now - maxAgeOfHistoryFiles;
+        Calendar cutoffDay = Calendar.getInstance();
+        cutoffDay.setTimeInMillis(cutoff - cutoff % ONE_DAY_IN_MS);
+        
+        // find directories older than the maximum age
         for (int i = 0; i < datedDirectories.length; ++i) {
           String thisDir = datedDirectories[i].toString();
           Matcher pathMatcher = parseDirectory.matcher(thisDir);
 
           if (pathMatcher.matches()) {
-            long dirTime = directoryTime(pathMatcher.group(1),
+            long dirDay = directoryTime(pathMatcher.group(1),
                                          pathMatcher.group(2),
                                          pathMatcher.group(3));
 
@@ -2449,44 +2459,49 @@ public class JobHistory {
                   + " as year/month/day = " + pathMatcher.group(1) + "/"
                   + pathMatcher.group(2) + "/" + pathMatcher.group(3));
             }
-
-            if (dirTime < now - DIRECTORY_LIFE_IN_MS) {
-
+            
+            if (dirDay <= cutoffDay.getTimeInMillis()) {
               if (LOG.isDebugEnabled()) {
-                Calendar then = Calendar.getInstance();
-                then.setTimeInMillis(dirTime);
                 Calendar nnow = Calendar.getInstance();
                 nnow.setTimeInMillis(now);
+                Calendar then = Calendar.getInstance();
+                then.setTimeInMillis(dirDay);
                 
                 LOG.debug("HistoryCleaner.run directory: " + thisDir
                     + " because its time is " + then + " but it's now " + nnow);
               }
+            }
+            
+            // if dirDay is cutoffDay, some files may be old enough and others not
+            if (dirDay == cutoffDay.getTimeInMillis()) {
+            	LOG.info("cutoff dir: " + thisDir);
+              // remove old enough files in the directory
+              FileStatus[] possibleDeletees = DONEDIR_FS.listStatus(datedDirectories[i]);
+              
+              for (int j = 0; j < possibleDeletees.length; ++j) {
+            	  if (possibleDeletees[j].getModificationTime() < now - 
+            	      maxAgeOfHistoryFiles) {
+            	    Path deletee = possibleDeletees[j].getPath();
+                  if (LOG.isDebugEnabled() && !printedOneDeletee) {
+                    LOG.debug("HistoryCleaner.run deletee: "
+                        + deletee.toString());
+                    printedOneDeletee = true;
+                  }
 
-              // remove every file in the directory and save the name
-              // so we can remove it from jobHistoryFileMap
-              Path[] deletees
-                = FileUtil.stat2Paths(localGlobber(DONEDIR_FS,
-                                                   datedDirectories[i],
-                                                   "/*/*", // sn + individual files
-                                                   null));
-
-              for (int j = 0; j < deletees.length; ++j) {
-
-                if (LOG.isDebugEnabled() && !printedOneDeletee) {
-                  LOG.debug("HistoryCleaner.run deletee: "
-                      + deletees[j].toString());
-                  printedOneDeletee = true;
-                }
-
-                DONEDIR_FS.delete(deletees[j]);
-                deletedPathnames.add(deletees[j].toString());
+                  DONEDIR_FS.delete(deletee);
+                  deletedPathnames.add(deletee.toString());
+            	  }
               }
+            }
+
+            // if the directory is older than cutoffDay, we can flat out
+            // delete it because all the files in it are old enough
+            if (dirDay < cutoffDay.getTimeInMillis()) {
               synchronized (existingDoneSubdirs) {
-                if (!existingDoneSubdirs.contains(datedDirectories[i]))
-                  {
-                    LOG.warn("JobHistory: existingDoneSubdirs doesn't contain "
-                             + datedDirectories[i] + ", but should.");
-                  }
+                if (!existingDoneSubdirs.contains(datedDirectories[i])) {
+                  LOG.warn("JobHistory: existingDoneSubdirs doesn't contain "
+                      + datedDirectories[i] + ", but should.");
+                }
                 DONEDIR_FS.delete(datedDirectories[i], true);
                 existingDoneSubdirs.remove(datedDirectories[i]);
               }
diff --git a/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobHistory.java b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobHistory.java
index 0fe56ae..aaed7c5 100644
--- a/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobHistory.java
+++ b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobHistory.java
@@ -22,6 +22,7 @@ import java.io.File;
 import java.io.IOException;
 import java.text.ParseException;
 import java.util.ArrayList;
+import java.util.Calendar;
 import java.util.List;
 import java.util.HashMap;
 import java.util.Map;
@@ -1250,61 +1251,44 @@ public class TestJobHistory extends TestCase {
     }
   }
   
-  public void testGetJobDetailsFromHistoryFilePath() throws IOException {
-    String[] parts = JobHistory.JobInfo.getJobHistoryFileNameParts(
-        "hostname_1331056103153_job_201203060948_0007_user_my_job");
-    assertEquals("hostname", parts[0]);
-    assertEquals("1331056103153", parts[1]);
-    assertEquals("job_201203060948_0007", parts[2]);
-    assertEquals("user", parts[3]);
-    assertEquals("my_job", parts[4]);
-  }
-
-  // run two jobs and check history has been deleted
   public void testJobHistoryCleaner() throws Exception {
-    // Disabled
-    if (true) return;
-    MiniMRCluster mr = null;
-    try {
-      JobConf conf = new JobConf();
-      // expire history rapidly
-      conf.setInt("mapreduce.jobhistory.cleaner.interval-ms", 0);
-      conf.setInt("mapreduce.jobhistory.max-age-ms", 100);
-      mr = new MiniMRCluster(2, "file:///", 3, null, null, conf);
-
-      // run the TCs
-      conf = mr.createJobConf();
-
-      FileSystem fs = FileSystem.get(conf);
-      // clean up
-      fs.delete(new Path(TEST_ROOT_DIR + "/succeed"), true);
+    JobConf conf = new JobConf();
+    FileSystem fs = FileSystem.get(conf);
+    JobHistory.DONEDIR_FS = fs;
+    JobHistory.DONE = new Path(TEST_ROOT_DIR + "/done");
+    Path histDirOld = new Path(JobHistory.DONE, "jtinstid/2013/02/05/000000/");
+    Path histDirOnLine = new Path(JobHistory.DONE, "jtinstid/2013/02/06/000000/");
+    final int dayMillis = 1000 * 60 * 60 * 24;
 
-      Path inDir = new Path(TEST_ROOT_DIR + "/succeed/input1");
-      Path outDir = new Path(TEST_ROOT_DIR + "/succeed/output1");
-      conf.set("user.name", UserGroupInformation.getCurrentUser().getUserName());
+    try {
+      Calendar runTime = Calendar.getInstance();
+      runTime.set(2013, 1, 8, 12, 0);
+      long runTimeMillis = runTime.getTimeInMillis();
       
-      RunningJob job1 = UtilsForTests.runJobSucceed(conf, inDir, outDir);
-      validateJobHistoryUserLogLocation(job1.getID(), conf);
-      long historyCleanerRanAt1 = JobHistory.HistoryCleaner.getLastRan();
-      assertTrue(historyCleanerRanAt1 != 0);
+      fs.mkdirs(histDirOld);
+      fs.mkdirs(histDirOnLine);
+      Path histFileOldDir = new Path(histDirOld, "jobfile1.txt");
+      Path histFileOnLineDir = new Path(histDirOnLine, "jobfile1.txt");
+      Path histFileDontDelete = new Path(histDirOnLine, "jobfile2.txt");
+      fs.create(histFileOldDir).close();
+      fs.create(histFileOnLineDir).close();
+      fs.create(histFileDontDelete).close();
+      new File(histFileOnLineDir.toUri()).setLastModified(
+          runTimeMillis - dayMillis * 5 / 2);
+      new File(histFileDontDelete.toUri()).setLastModified(
+          runTimeMillis - dayMillis * 3 / 2);
       
-      // wait for the history max age to pass
-      Thread.sleep(200);
-      
-      RunningJob job2 = UtilsForTests.runJobSucceed(conf, inDir, outDir);
-      validateJobHistoryUserLogLocation(job2.getID(), conf);
-      long historyCleanerRanAt2 = JobHistory.HistoryCleaner.getLastRan();
-      assertTrue(historyCleanerRanAt2 > historyCleanerRanAt1);
-
-      Path doneDir = JobHistory.getCompletedJobHistoryLocation();
-      String logFileName = getDoneFile(conf, job1.getID(), doneDir);
-      assertNull("Log file should no longer exist for " + job1.getID(), logFileName);
+      HistoryCleaner.maxAgeOfHistoryFiles = dayMillis * 2; // two days
+      HistoryCleaner historyCleaner = new HistoryCleaner();
+      historyCleaner.clean(runTimeMillis);
       
+      assertFalse(fs.exists(histDirOld));
+      assertTrue(fs.exists(histDirOnLine));
+      assertFalse(fs.exists(histFileOldDir));
+      assertFalse(fs.exists(histFileOnLineDir));
+      assertTrue(fs.exists(histFileDontDelete));
     } finally {
-      if (mr != null) {
-        cleanupLocalFiles(mr);
-        mr.shutdown();
-      }
+      fs.delete(JobHistory.DONE, true);
     }
   }
 }
-- 
1.7.0.4

