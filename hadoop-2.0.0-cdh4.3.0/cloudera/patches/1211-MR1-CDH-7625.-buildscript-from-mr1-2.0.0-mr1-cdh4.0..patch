From d556575c4848403951b630522e98ce74cc7fa0da Mon Sep 17 00:00:00 2001
From: Roman Shaposhnik <rvs@cloudera.com>
Date: Tue, 20 Nov 2012 15:59:31 -0800
Subject: [PATCH 1211/1357] MR1: CDH-7625. buildscript from mr1-2.0.0-mr1-cdh4.0.1.tar.gz is misleading and the needed one is missing

---
 cloudera/README.cloudera            |    9 -
 cloudera/do-release-build           |  156 --------------------
 cloudera/files/hadoop-0.20.1.gz     |  Bin 4201 -> 0 bytes
 cloudera/files/hadoop-fuse-dfs.1.gz |  Bin 1178 -> 0 bytes
 cloudera/getsnappysource.sh         |   69 ---------
 cloudera/install_hadoop.sh          |  277 -----------------------------------
 6 files changed, 0 insertions(+), 511 deletions(-)
 delete mode 100644 cloudera/README.cloudera
 delete mode 100755 cloudera/do-release-build
 delete mode 100644 cloudera/files/hadoop-0.20.1.gz
 delete mode 100644 cloudera/files/hadoop-fuse-dfs.1.gz
 delete mode 100755 cloudera/getsnappysource.sh
 delete mode 100755 cloudera/install_hadoop.sh

diff --git a/cloudera/README.cloudera b/cloudera/README.cloudera
deleted file mode 100644
index 394d47c..0000000
--- a/cloudera/README.cloudera
+++ /dev/null
@@ -1,9 +0,0 @@
-This build was generated by Cloudera's build system in the following manner:
-
-1) The pristine open-source release tarball was unpacked
-
-2) The patches contained within the patches/ directory next to this README
-were applied using the apply-patches script. A complete log of these changes
-is also included in CHANGES.cloudera.txt.
-
-3) The project was built by running the do-release-build script in this directory.
\ No newline at end of file
diff --git a/cloudera/do-release-build b/cloudera/do-release-build
deleted file mode 100755
index f255f1d..0000000
--- a/cloudera/do-release-build
+++ /dev/null
@@ -1,156 +0,0 @@
-#!/bin/bash
-# Copyright (c) 2009 Cloudera, inc
-#
-# Performs a release build
-
-#
-# Setup
-#
-set -e
-
-if [ $(uname -s) = "SunOS" ]; then
-if [ $(isainfo -b) != "64" -a -z "$SKIP_EXTRA_NATIVE" ]; then
-  echo Release build should be done on a 64-bit box to generate 1>&2
-  echo both 64 and 32 bit native libraries. 1>&2
-  exit 1
-fi
-else
-if [ $(uname -m) != "x86_64" -a -z "$SKIP_EXTRA_NATIVE" ]; then
-  echo Release build should be done on a 64-bit box to generate 1>&2
-  echo both 64 and 32 bit native libraries. 1>&2
-  exit 1
-fi
-fi
-
-JAVA32_HOME=${JAVA32_HOME:-$JAVA_HOME}
-JAVA64_HOME=${JAVA64_HOME:-$JAVA_HOME}
-IVY_MIRROR_PROP=${IVY_MIRROR_PROP:-http://repo1.maven.org/maven2/}
-
-# Do the build
-BIN_DIR=$(readlink -f $(dirname $0))
-RELEASE_DIR=$BIN_DIR/..
-
-failIfNotOK() {
-  if [ $? != 0 ]; then
-    echo "Failed!"
-    exit $?
-  fi
-}
-
-# Clean build directory
-#
-BUILDDIR=${RELEASE_DIR}/build
-rm -rf ${BUILDDIR}
-failIfNotOK
-mkdir ${BUILDDIR}
-failIfNotOK
-
-#
-# Building Snappy
-#
-
-echo "Downloading Snappy"
-
-SNAPPY_VERSION=1.0.3
-SNAPPY_JAVA_VERSION=1.0.3.1
-
-# Get Snappy source
-#
-sh -x ${RELEASE_DIR}/cloudera/getsnappysource.sh ${SNAPPY_VERSION} ${SNAPPY_JAVA_VERSION} ${RELEASE_DIR}/cloudera ${BUILDDIR}
-failIfNotOK
-
-echo "Building Snappy"
-
-SNAPPY_DIR=${BUILDDIR}/snappy-${SNAPPY_VERSION}
-cd ${SNAPPY_DIR}
-failIfNotOK
-
-./configure
-failIfNotOK
-
-# Disable rpath
-# TODO: find the correct way of disabling RPATH when running configure
-#
-sed -i 's|^hardcode_libdir_flag_spec=.*|hardcode_libdir_flag_spec=""|g' libtool
-sed -i 's|^runpath_var=LD_RUN_PATH|runpath_var=DIE_RPATH_DIE|g' libtool
-
-SNAPPY_BUILDDIR=${SNAPPY_DIR}/build
-mkdir ${SNAPPY_BUILDDIR}
-
-LIBTOOL_VAR="LIBTOOL=libtool"
-if [ -f "/etc/SuSE-release" ]; then
-  LIBTOOL_VAR=""
-fi
-
-make install CXXFLAGS="-O2 -DNDEBUG" DESTDIR=${SNAPPY_BUILDDIR} ${LIBTOOL_VAR}
-failIfNotOK
-
-echo "Building snappy-java"
-
-cd ${BUILDDIR}/snappy-java-${SNAPPY_JAVA_VERSION}
-failIfNotOK
-
-SNAPPY_ARCHIVE=${BUILDDIR}/snappy-${SNAPPY_VERSION}.tar.gz
-mkdir target
-mvn compile
-make SNAPPY_OUT=target SNAPPY_ARCHIVE=${SNAPPY_ARCHIVE} \
-     SNAPPY_UNPACKED=${SNAPPY_ARCHIVE} SNAPPY_SRC_DIR=${SNAPPY_DIR} native
-failIfNotOK
-
-cp target/libsnappyjava.so ${SNAPPY_BUILDDIR}/usr/local/lib
-failIfNotOK
-
-#
-# Building Hadoop
-#
-cd $RELEASE_DIR
-
-if [ -z "$SKIP_EXTRA_NATIVE" ]; then
-JAVA_HOME=$JAVA32_HOME \
-  CFLAGS=-m32 \
-  CXXFLAGS=-m32 \
-  ant \
-  -Dreactor.repo=file://$HOME/.m2/repository \
-  -Dlibhdfs=true \
-  -Dcompile.native=true \
-  -Dfusedfs=true \
-  -Dcompile.c++=true \
-  -Dforrest.home=$FORREST_HOME \
-  -Dhadoop.conf.dir=/etc/hadoop-0.20/conf \
-  -propertyfile cloudera/build.properties -Drepo.maven.org=${IVY_MIRROR_PROP} \
-  task-controller package-native
-
-JAVA_HOME=$JAVA64_HOME
-fi
-
-if [ -z "$SKIP_JDIFF" ]; then
-ant \
-  -Dreactor.repo=file://$HOME/.m2/repository \
-  -Djdiff.stable=0.20.1 \
-  -Djdiff.build.dir=build/docs/jdiff-cloudera \
-  -propertyfile build.properties -Drepo.maven.org=${IVY_MIRROR_PROP} \
-  -propertyfile cloudera/build.properties api-report
-fi
-
-# Copy them into the main build directory to be included in the tarball
-mkdir -p build/hadoop-$FULL_VERSION/docs/
-
-ant \
-  -Dreactor.repo=file://$HOME/.m2/repository \
-  -Dlibhdfs=true \
-  -Dcompile.native=true \
-  -Dfusedfs=true \
-  -Dcompile.c++=true \
-  -Dforrest.home=$FORREST_HOME \
-  -Dhadoop.conf.dir=/etc/hadoop-0.20/conf \
-  -propertyfile cloudera/build.properties \
-  -Dsnappy.prefix=${SNAPPY_BUILDDIR}/usr/local \
-  -Dbundle.snappy=true -Drepo.maven.org=${IVY_MIRROR_PROP} \
-  compile-core-native compile-c++ compile-c++-examples task-controller tar
-
-if [ -z "$SKIP_MVN_EXPLICIT" ]; then
-# Change to cloudera/maven directory, and install
-# (and if called from CDH nightly build, deploy) artifacts into Maven repository
-cd $BIN_DIR/maven-packaging
-mvn -Dnot.cdh.release.build=false install $DO_MAVEN_DEPLOY
-fi
diff --git a/cloudera/files/hadoop-0.20.1.gz b/cloudera/files/hadoop-0.20.1.gz
deleted file mode 100644
index 2ecdc80d979e9b03f80683614a76fd63ed030c8a..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 4201
zcmV-v5SH&BiwFqC?4n5k188AnZ*OoeF#x?<>vG#Tvi`59KzVi|$(ATfNoE~yCUqQN
z)|yIem*vgO)Nt2<NKoRKB)Ggt(K$JJpYs~$$@Xgiyh?Uvr*>B^%LLHqdpEj4TlOA1
z$uk-9%y)PYY}i6XBH@{E*?hxpq<eh)o=0powx0iSpvN~nNkyWjCLPP(qhB`5c;xcL
zWpN^SI1j|Jn*RP@^yyfxV6!D7O8Gn$PUg#qEd_V|Xwk9eNyqX%mN9D<cdSUTmmTZo
zrek|=ZTk;n+q+dm%hG6KLi0f+N@SMC+!3s|UW$mV1#@`R%~+lahAsU9Z5lJ{<w}hr
z8Wp%c%n8?9vGAcIk<6jSkY(ai*0H>do&E@BJjD?4QEn2n={$0GYyqd9uK-C@0Zw9;
z$`E`ldBU9xiq8@rr2!`mqzqaomAk_8BiJm>5-23K7_;|#!y#MyL7-4A{FMMO%cD6A
z^uS>u=JIqoAi$fc0@OgUujbzE)#f-0gqJBquFPrg-*cIXfg%s!+a_@0FP2#Y5)6H=
zgD6yBBp?-NJIr~a**-mJAd7qlLZ2QSVD*&8eg;@5WWkxAlAT<ZNyFKebV4gATZ%+@
zG7)So$+~mkU2#7kI79PeA<Ai2ArqfswNtW>T?XSuiS)byi)6Nhf@vnxGeE1nW$kB6
zcKZ4aESDwB?(P3L{A~B;E1<lCce*J=V5jz~JG92DuD$FI$8pzAhm!;Id|;=CSgx(Y
z-2~jt{vW#oE*S$3m{g%ICt$kR*CY65&yIT5<@Yc0{<GdV{C<K5W>wo1-bvoAKMzTB
zJ~ScO_z<`<XZa|)!>7?5X>29BNgW-w!KL>0%F%6O1OwDZT^e=giT%GFO9X8VK(QjU
zKi8E29^yKok00xVy4?bqNmo;koPu4z2N!-yQ3Yqp@`PeYMgt^*E_tBIg)p}(J~0s?
zy5LgCJZa{NBq8G~beW$8Sm;@Im;HfCOCq7+jD~5Mu+-ENQ+_5w8cgH9jv+-rJDwpD
z^5g-`D7X6Q^&9fIo9i>kbnP)m=19Q<wvjopx#S2Gcxs$TN2Do9rclW|i}Q@203CRQ
zLZy;W@CZ)q<pJ{$Bw)-^naz$aYR)Fu^e6q8g=|Xv9jj!T*>?1;o$X;T*;AQ{V4{S5
z%MSH36mtOvDs5jNMoa*ajv1cVuX&pJ9wMI<gKa0lns3r-eYuH|k=5#-pfBj-L8J;B
z;2^EUOtfJktVb&<B89VzR4#F)lZVh`3htI0;!7v)h?hK_I1T`mnwiR;zOm4M^}gwc
zsl^?KjjfmL=xB5}866(6+573An3u?q{pIv6dtpubI!Rv$mAGzml43@tC=`zIm89g5
z4Dq`{9#e@a=80mARE7Os?k)rj;bam%;}xpXiHn{L^YtM3=CaT=4L&gID6EDT%z9oM
z{=#f;Tn+z<;f=PAI@(%jd%bZ!?AiDr^u7GsN#5`8TjS?pzd`T22ED@uwPy`t&onI~
zZ0d(GRKS(;f2*|JI?j*v!QbkGL4ELreJP(ukBAGr_QNq{uAUNio$nuw#)m_s>~Ajn
zT^o6L?=x18^SwIxC-;2BlHSv;bJ^ZAJ{*n5#iqaQ*fZEor{5jy0RFNCtWo2r3H#kI
zJ9dn|BF2B)y)>xrtJyFs4c2qHpKgXIokY+t45>}4PQ2r%s2qT~1A4juJsgYl_yLqH
zz*`fu)U_%m0Hk3Q?jwd3g^13dIT6t#wJNtLAJ}UEx}s}z(;rO~%Axf<Qh0tLp<O{o
zFwInCvG!Mm)E(Ao#;&5d?tbYSM3a|c*Y+lMZ@EE?5h69y<m_z7&I;bJ`K;JH9*0Wp
z*h@vgIPBKd?^0H0noVL5tLedbiuz%!Wzckbz&P#Z`;*c0J{1>ZNnn)(cLk30U_zTG
zh6~#BeDtzg;F1NAawkX778E$3Unpl3YVAa@lVa_pTstov#*s-<9Do$@t1`}4s+`z%
z-~Cs0XCyxJFSbiu*OgKt_*#G|v0j7-G>G-0so5e6gRelYQ4gdt$W?dY3QE~H5u7Ab
z?jFQR-yD=Ei{4D0vtsb!&G;3wJN4vp%tZNo#L~1JNUDC#GdxOp(HQ=;sHT;VnCa!F
zoF)&D-FaMv@tvA^cT>%fohrg)nR6YyNl3{V85v0)85~*r^k3Ye(ZM?XiQQ2x#P*il
zl`>{yOx|6xV#M|)hlj6*hm+yt`Cb8phfDp?LB+)Ou1?;c;}N~pcOCl~iNfyh^>6X1
zKkJWy9O<8X0Nm5ZJgb5E){wM7pqTimzV)*$ntv2<v95PK@=cKH;eKR``};xT&wcra
zS?aqG?9mU<Km|fIu39_H9x`SgQLS<mb6&RQ=xSnggK%K9kxQPTRm-SG6-@BdM@PR-
zP;M#*PV$KHI1WtJm1_8+Y+Ugaejdu4PG;$$fy%h0Ei(K0>iT9jooNeSs`|D*`<Ik2
z#BpKeU+kehbY$fD3r2rjKS3dYY;=14{{6|-85$jp_UwFidON+jn_gcvF*g=2(Y$q2
zwkBRx6<8=iq_J>(b!&kl3+3!FklcVTlz>!e?XedM>lD>?ZMIM+I(>wDK+Vbq1wFNg
zx|`POh(*Tdn@mutMp(}wiVUf|Y~roWcc$oied@vIIX0>mgS1%3OWh-P5(x@UkW=fS
zl<mkclo1#y8&jG>`Hpo9aW2qemhi-Rl!tR_TOt(xL?>IyJOv$vEs}1<w}&d*$C=z>
zyPWR4-Ber?dj;DAR?}3W7L?*BwNb-D?;@aHh`<#=m<Xn-q=u6Q(xJN~lE6=m^?*Tq
z$`TO@h?}6JcMYev+88anm9vH2c;mh*<WK>Afjtj0KURv1Rh#Bj%5UzW`;s`2Xg9Qu
zmC!tWNZFjHssltl?gx3Ilvo9zDqdAz!QS)uR=Bx?E-X(Ar5UiZ(gj?0;e(;sCM8jN
z6a$xM7c&GvpY>9Kvfp2T3Y!a@JyyTM3WKbCQfVVxm9bUZlZFt4q04q2l%Q*wJXoxT
zpE-%(<b$PF5}`0vdNI4{<kMl`2Y$90P&RRlL?Nld_@S{ePmmzgl}h1{R*h})l#FiD
zYQ3nYrhS9Fr}7G=TWStN3O*MB=LeuLN8@U83A$$?>bm{WfAAR+!7|I@<I!l1p~YkF
zEQKYL#YpcOSxhIGYn4J;X{$B~r)%XZe}Qb(bim>EdgpqD_Fox@c(c9U>5u9b>GI_4
z`ugVn^!n=J{%m@y?vrfq7G3?yPh>>CgFKt~=H_W8>YLB6K1^?~uil?u-IW`Qt4{49
zS}K!9TVmB`Ay>pf3>L$wK}Q2`g@3tRtakKYEf4(2xRD3{O_lp<wBSe5Men9S&_uhv
z+@V9Kg|viX1z3~k-K_3J)kHv^wvC{iRS0-}d$DA;dntEAKWc3+X1BMidBg$bQH{b&
zormUjHM707QY?e>Iu8_a$V3!^1*SHh0*ktq@|`*gx@2hZF~?ySIilst<*dPEBADs<
zEf#*2^DN=cLmN$PzO+e|1F*{O^fIe-8~d?nNu;L?D>pQty&9a-@lM+H1+%pxO^Z8d
z1M8LmdmSWFKh6WX-)k_R>P?976p`25SWI@}1j$$lZl(++p0L;Wdefp;P8*hKg3M?9
z4hM!4v)5_j<Vo;`y+&fm=&9w1kAih@<ZTo}hHQ!qCFnMZ4naKS;BSSqR@35%v!si4
zduX1n5JAc@sq7}tB>ZpoHTD(<#pcFd+Sl6D(WegtH6@{eFQQaXts@uhFfFD#3z3ad
zq4kRil(Y~)om2v;o@q#vzp1W#cQTt9_M14%mC8|_fa6@Eyy3F;W_2LFm#BlO9#hqn
zHuO>iF;lOI{AeZB#q3C`Dkv1HMAMo1V*2j9m6ppX@H!HiQyfurm^69aTJ5N!Dcyu~
ziIbZJe)Pa%=~MY(iayvjLC90th+|?^o|5{c$`h^|S>q2IoH!lpcBZJ3>b$pCGE-N_
zW}hikPAILVIDYonQH{Z;fQi}qS=JV&JkXRBsuqmUGpM(W`Usu!0Ggy5-<2TR_UGPF
zzpw*(9lBO!DXQE6cJtmwG9z@(l1B@w5d5?QO)2bJ-e5yUi92^v)ex$EF#2jyD~avW
zUaIKQ75jh{F67Aor!l2V!``u?RCcW2u(~X4I7H;4s@j-nAP^H5Lo^;d%Tv|Rp+>b-
zU#O(0Y+Kp9tyL>dMea(~dbXED7-t(0fCD2H;BkDo6HcS313p)Pr$$#F=LqR;k>-fe
zM^IS0UnP|*zg^0;=`d9om)=W)Jnkq4S?Mo5iuzgsc%znw^%}cjuYPHD?z;5)P^_U!
zw0B_>?g?wc6iA#>W|djT+N{XF!{jklZ%v1rrbg6T%`!78#&*Z);3@rtn?g4OtzCMz
zAQaEM18+)HJEJvCy4yhsdEngjwOyJlsFH#jyx+t?Cy!7rlmW5>A1$qUlvPHdJVgqi
zmn1uMpkm#W{M)MmtiENGR;(czWz-zC`7T0khdk`nVQHzK!7+6)@OR(aJjN!4tIoK{
zUvQ)-Dtf@un<_s4_@B)QaJzp;KG>zV8qlkMHfe;LDq);tDuZjO2I_U6`WFIvQO_1q
zx(vM*Cuz}=lZ$J+n73-FeoMO3Wd=-G3v1B!8O05~^RryT)UfP`3<=HL?Rjxx($lU(
z+&tUE)X#+VDGZ)I)eCYZw9XB_?j&PQg0v5JeOrM{XL2Cz-HPaDFZt;Ci|%?Q%!SJ+
zPm|Hyk4h0VPAgruWN5<Gz)Y%l5<jLyL}i{xP1-?EqVBOOrhYj2aB_cnjZ&+u2Bc#G
z%6k$HL~!6EgpfVm4Jc)hc$#&1kkavvNXX-zO}06K%9A~eWuz`}A_09;_9R<lv$~G?
zO}%*EsyRZ#hwS0*4)zIe+w07<6BkM(-IZBfWy4IyaqEVO>h7zuz1hXc?oN0IBUN+!
zdl)&x?xz{~%7wuWj&|H3{2tb3<t4+{we?t|yMw!W@B4e$`a#Z3gZel-k2b$M_-XXR
zzlWhaASgMaCZ^{4%~qzhV4ANgSli84xl&%;stRZ+BU93q(Jwun(4R^Hk6fV{%!&AW
zj`AsGGgNgy@D*oc>$}c+>7!kw;~Xc6E7#~QA&RU2jl*0a{t93#%_9rR!K%bEH+ee+
ztF7t2r!GiMwMCatx2CF6aD~V7AZry^lwAXVUYOqq4B3iWq-=6(lse|jLuTBi8P=nG
ziFy(xt$@h=ZPCh0%16X2)}It{NBIcG&$OKB)1`h*()K0rpi>lq)ppfMx0CU<Snsa?
z{FGE?d9QN<S1iucqK=@uh9=9<=LoYc+th6N6*E0;C39_X{?p0(o2{FJ+AIo}7gV_X
zfrSs~65u~^p6Lm?5XwGfTJ>-0E6d6WTP`-*7oRx2%h<PQRqZMMTdccCL~Q<#v};K4
zVs*4{N;Y*1ee?6}^v{=f?Q7_A`c%dn{jZ?^|9@<9cz8tbvDk^at!K01<>dvvPcTIc
z*+lK2i!0UFxWyaD^K*9cZg$;z16j`6&(PCTDC=f^+#fu4rO^2YG)B%E^eg}X8p>E$

diff --git a/cloudera/files/hadoop-fuse-dfs.1.gz b/cloudera/files/hadoop-fuse-dfs.1.gz
deleted file mode 100644
index 38e2fb5f7265bbd4b3d3078f6b5816a8a1eb00ac..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 1178
zcmV;L1ZDdliwFoSyY))|188AnZ*OodW_5FAEo5eME-?VTR!wi?HV{2mzhdBCS|FC4
z92VFh4eB%r>?W}d+q>wZDGW-aEN&=LASo;U_njf-4<y@mPrXSRa^~^Pn@6S3J(ydQ
z3+KR>(m^4WfL8i4NinpsrYN9aF%A6{bGejIG)}B};nH(jk)*3Jl#Clw_bJlH2%JN2
zR9mUwVt77;9z06jyh87j6BeGBZkTetBuSTJn2w(AlDFHR`|xBM?E$NfZ||3otHycU
z%N)o8q=y!t7-Y)iFTllC<|2*xmS3i``Es(vx0`pc{)9e<zY=`B91N}y%vRXz%d6U0
ze+^fa_G>PjmzsHL^mR;ld$;^;F`2I>v*|Z;CQK>QI>2{}3>tbk;3h8J4xv{N0DH^0
zq~uJU#-s*C45bij&O$jl!(<M<K0kxL;Ua4`z2rfN`W*(_Xf5@I^7v9fVXVNuHJT$3
z4%i5!GVF&6+srn??Nggb0z4O!__Su6TU^vjMhY1xDz*Y@#d1N1bG682u?uCotyr`z
zWnPAFnpHw0R%lJ5IAj73hDQwOyh&k23X_4YqEUntax1V1#)bA)3P%YG=UJ8>KvIKF
zAkyo1;0wF{2XqwKt-Y*-X*!s>(Htopn--M8&0JcMdt-NpG|URfSzZc$$iqB_vc`*l
z&vjV#J_#~qjmJv(fkAF?J$9*Pmf@FhRxrf@48iav^Kj6SD+4du;N+32iBTkp3neH4
z(ovZ$KJ-DPoAe9@B>KyNuj*rcxy5410_Vue+`Chmn_Ac~T47G8A*R)+AjA}8OOsPa
zD>{&3dDav~ROrLao(E<(g62PlI@pI}P@c9F8n~g(m0?5#<L;-2o1Z(R)<6<Hh9Y3J
z+QB~5QX~cjt(AD3?sS+KC)kzAoo1N|^ocm=;d9RPekD>E7T!wtA1gDtS%)GnL3@lO
z9KRuFL>M?j27;``zGT9a9U`BoQS@dMKa#lLqTvN6LnsH=zraQMA-(tzedT`gco$p+
z2Eyk9E(+EtKj`dam|hHnr{IdkGP&<;9u=mcZj>VFvy-6{dMmBbI8q+A%u1F~N%2Z^
zhaxGgsg5^F5)s_}F?yOu5bv!t`tp8A_fB;AaX1_z;E;C^nu#u+Bw<+NAmfN=l=NOz
zoiV*lM8pJ72nop<ag;<=)4rq%T97E%P~mhb`GTODIYXo_wg^knm}D$}iB@}bkL1KY
ze7H|G0ah%L39@HEOu?}O=+KSmDiD|n%A`2r$c*t#9d6{4g}CkweLsFTec=G;HzcLC
zL1^~Owr59q!w`mDyQZHfi`n$)Zn}zk9gl8jv-x^F!&{qJ!3#J{^nQlBNpB^ZY{Z9l
z<Dy~VvY6!e(dW@Wr|47(uF>$7ztA+JRqyucsBfJ}G1cSk`f+lz7%jf6=cCm)8690u
zMAySJebe;bim#QZaD3_dSm6%`>B=)yeJ6NYretOrWjPB-2=G_7Wx#hxyv)1!uKODq
seOirYi{yuq60a~7E&WDaY0)x#|20>p;fONLO?9384KJLlO^68q0IKpenE(I)

diff --git a/cloudera/getsnappysource.sh b/cloudera/getsnappysource.sh
deleted file mode 100755
index dc8f0c9..0000000
--- a/cloudera/getsnappysource.sh
+++ /dev/null
@@ -1,69 +0,0 @@
-#!/bin/sh
-#
-#  Licensed to Cloudera, Inc. under one or more contributor license
-#  agreements.  See the NOTICE file distributed with this work for
-#  additional information regarding copyright ownership.  Cloudera,
-#  Inc. licenses this file to you under the Apache License, Version
-#  2.0 (the "License"); you may not use this file except in compliance
-#  with the License.  You may obtain a copy of the License at
-#
-#  http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-#
-# Copyright (c) 2011 Cloudera, inc.
-#
-
-failIfNotOK() {
-  if [ $? != 0 ]; then
-    echo "Failed!"
-    exit $?
-  fi
-}
-
-SNAPPY_VERSION=$1
-SNAPPY_JAVA_VERSION=$2
-CACHEDIR=$3
-BUILDDIR=$4
-
-SNAPPY_SRC_TAR=snappy-${SNAPPY_VERSION}.tar.gz
-SNAPPY_SRC_TAR_URL="http://snappy.googlecode.com/files/${SNAPPY_SRC_TAR}"
-SNAPPY_JAVA_SRC_TAR=snappy-java-${SNAPPY_JAVA_VERSION}.tar.gz
-SNAPPY_JAVA_SRC_TAR_URL="http://snappy-java.googlecode.com/files/${SNAPPY_JAVA_SRC_TAR}"
-
-SNAPPY_SRC_TAR_PATH=${CACHEDIR}/${SNAPPY_SRC_TAR}
-SNAPPY_JAVA_SRC_TAR_PATH=${CACHEDIR}/${SNAPPY_JAVA_SRC_TAR}
-
-if [ ! -f ${SNAPPY_SRC_TAR_PATH} ]
-then
-  cd ${BUILDDIR}
-  failIfNotOK
-  wget ${SNAPPY_SRC_TAR_URL}
-  failIfNotOK
-  SNAPPY_SRC_TAR_PATH=${BUILDDIR}/${SNAPPY_SRC_TAR}
-fi
-
-if [ ! -f ${SNAPPY_JAVA_SRC_TAR_PATH} ]
-then
-  cd ${BUILDDIR}
-  failIfNotOK
-  wget -O ${SNAPPY_JAVA_SRC_TAR} ${SNAPPY_JAVA_SRC_TAR_URL} 
-  failIfNotOK
-  SNAPPY_JAVA_SRC_TAR_PATH=${BUILDDIR}/${SNAPPY_JAVA_SRC_TAR}
-fi
-
-cd ${BUILDDIR}
-failIfNotOK
-
-tar xzf ${SNAPPY_SRC_TAR_PATH}
-failIfNotOK
-
-tar xzf ${SNAPPY_JAVA_SRC_TAR_PATH}
-failIfNotOK
-
-exit 0
diff --git a/cloudera/install_hadoop.sh b/cloudera/install_hadoop.sh
deleted file mode 100755
index d1bef42..0000000
--- a/cloudera/install_hadoop.sh
+++ /dev/null
@@ -1,277 +0,0 @@
-#!/bin/bash -x
-# Copyright 2009 Cloudera, inc.
-
-set -ex
-
-usage() {
-  echo "
-usage: $0 <options>
-  Required not-so-options:
-     --cloudera-source-dir=DIR   path to cloudera distribution files
-     --build-dir=DIR             path to hive/build/dist
-     --prefix=PREFIX             path to install into
-
-  Optional options:
-     --native-build-string       eg Linux-amd-64 (optional - no native installed if not set)
-     ... [ see source for more similar options ]
-  "
-  exit 1
-}
-
-OPTS=$(getopt \
-  -n $0 \
-  -o '' \
-  -l 'cloudera-source-dir:' \
-  -l 'prefix:' \
-  -l 'build-dir:' \
-  -l 'native-build-string:' \
-  -l 'installed-lib-dir:' \
-  -l 'lib-dir:' \
-  -l 'system-lib-dir:' \
-  -l 'src-dir:' \
-  -l 'etc-dir:' \
-  -l 'doc-dir:' \
-  -l 'man-dir:' \
-  -l 'example-dir:' \
-  -l 'apache-branch:' \
-  -- "$@")
-
-if [ $? != 0 ] ; then
-    usage
-fi
-
-eval set -- "$OPTS"
-while true ; do
-    case "$1" in
-        --cloudera-source-dir)
-        CLOUDERA_SOURCE_DIR=$2 ; shift 2
-        ;;
-        --prefix)
-        PREFIX=$2 ; shift 2
-        ;;
-        --lib-dir)
-        LIB_DIR=$2 ; shift 2
-        ;;
-        --system-lib-dir)
-        SYSTEM_LIB_DIR=$2 ; shift 2
-        ;;
-        --build-dir)
-        BUILD_DIR=$2 ; shift 2
-        ;;
-        --native-build-string)
-        NATIVE_BUILD_STRING=$2 ; shift 2
-        ;;
-        --doc-dir)
-        DOC_DIR=$2 ; shift 2
-        ;;
-        --etc-dir)
-        ETC_DIR=$2 ; shift 2
-        ;;
-        --installed-lib-dir)
-        INSTALLED_LIB_DIR=$2 ; shift 2
-        ;;
-        --man-dir)
-        MAN_DIR=$2 ; shift 2
-        ;;
-        --example-dir)
-        EXAMPLE_DIR=$2 ; shift 2
-        ;;
-        --apache-branch)
-        APACHE_BRANCH=$2 ; shift 2
-        ;;
-        --src-dir)
-        SRC_DIR=$2 ; shift 2
-        ;;
-        --)
-        shift ; break
-        ;;
-        *)
-        echo "Unknown option: $1"
-        usage
-        exit 1
-        ;;
-    esac
-done
-
-for var in CLOUDERA_SOURCE_DIR PREFIX BUILD_DIR APACHE_BRANCH; do
-  if [ -z "$(eval "echo \$$var")" ]; then
-    echo Missing param: $var
-    usage
-  fi
-done
-
-LIB_DIR=${LIB_DIR:-$PREFIX/usr/lib/hadoop-$APACHE_BRANCH}
-SYSTEM_LIB_DIR=${SYSTEM_LIB_DIR:-/usr/lib}
-BIN_DIR=${BIN_DIR:-$PREFIX/usr/bin}
-DOC_DIR=${DOC_DIR:-$PREFIX/usr/share/doc/hadoop-$APACHE_BRANCH}
-MAN_DIR=${MAN_DIR:-$PREFIX/usr/man}
-EXAMPLE_DIR=${EXAMPLE_DIR:-$DOC_DIR/examples}
-SRC_DIR=${SRC_DIR:-$PREFIX/usr/src/hadoop-$APACHE_BRANCH}
-ETC_DIR=${ETC_DIR:-$PREFIX/etc/hadoop-$APACHE_BRANCH}
-
-INSTALLED_LIB_DIR=${INSTALLED_LIB_DIR:-/usr/lib/hadoop-$APACHE_BRANCH}
-
-# TODO(todd) right now we're using bin-package, so we don't copy
-# src/ into the dist. otherwise this would be BUILD_DIR/src
-HADOOP_SRC_DIR=$BUILD_DIR/../../src
-
-mkdir -p $LIB_DIR
-(cd $BUILD_DIR && tar cf - .) | (cd $LIB_DIR && tar xf - )
-
-# Create symlinks to preserve old jar names
-# Also create symlinks of versioned jars to jars without version names, which other
-# packages can depend on
-(cd $LIB_DIR &&
-for j in hadoop-*.jar; do
-  if [[ $j =~ hadoop-([a-zA-Z]+)-(.*).jar ]]; then
-    name=${BASH_REMATCH[1]}
-    ver=${BASH_REMATCH[2]}
-    ln -s hadoop-$name-$ver.jar hadoop-$ver-$name.jar
-    ln -s hadoop-$name-$ver.jar hadoop-$name.jar
-  fi
-done)
-
-# Take out things we've installed elsewhere
-for x in docs lib/native c++ src conf usr/bin/fuse_dfs contrib/fuse ; do
-  rm -rf $LIB_DIR/$x 
-done
-
-# Make bin wrappers
-mkdir -p $BIN_DIR
-
-for bin_wrapper in hadoop ; do
-  wrapper=$BIN_DIR/$bin_wrapper-$APACHE_BRANCH
-  cat > $wrapper <<EOF
-#!/bin/sh
-
-export HADOOP_HOME=$INSTALLED_LIB_DIR
-exec $INSTALLED_LIB_DIR/bin/$bin_wrapper "\$@"
-EOF
-  chmod 755 $wrapper
-done
-
-# Fix some bad permissions in HOD
-chmod 755 $LIB_DIR/contrib/hod/support/checklimits.sh || /bin/true
-chmod 644 $LIB_DIR/contrib/hod/bin/VERSION || /bin/true
-
-# Link examples to /usr/share
-mkdir -p $EXAMPLE_DIR
-for x in $LIB_DIR/*examples*jar ; do
-  INSTALL_LOC=`echo $x | sed -e "s,$LIB_DIR,$INSTALLED_LIB_DIR,"`
-  ln -sf $INSTALL_LOC $EXAMPLE_DIR/
-done
-# And copy the source
-mkdir -p $EXAMPLE_DIR/src
-cp -a $HADOOP_SRC_DIR/examples/* $EXAMPLE_DIR/src
-
-# Install docs
-mkdir -p $DOC_DIR
-cp -r ${BUILD_DIR}/../../docs/* $DOC_DIR
-
-# Install source
-mkdir -p $SRC_DIR
-rm -f ${HADOOP_SRC_DIR}/contrib/fuse-dfs/src/*.o 
-rm -f ${HADOOP_SRC_DIR}/contrib/fuse-dfs/src/fuse_dfs
-rm -f ${HADOOP_SRC_DIR}/contrib/fuse-dfs/fuse_dfs
-rm -rf ${HADOOP_SRC_DIR}/contrib/hod/
-
-
-cp -a ${HADOOP_SRC_DIR}/* $SRC_DIR/
-mv -f $LIB_DIR/cloudera-pom.xml $LIB_DIR/cloudera $SRC_DIR
-
-# Make the empty config
-install -d -m 0755 $ETC_DIR/conf.empty
-(cd ${BUILD_DIR}/conf && tar cf - .) | (cd $ETC_DIR/conf.empty && tar xf -)
-
-# Link the HADOOP_HOME conf, log and pid dir to installed locations
-rm -rf $LIB_DIR/conf
-ln -s ${ETC_DIR#$PREFIX}/conf $LIB_DIR/conf
-rm -rf $LIB_DIR/logs
-ln -s /var/log/hadoop-$APACHE_BRANCH $LIB_DIR/logs
-rm -rf $LIB_DIR/pids
-ln -s /var/run/hadoop-$APACHE_BRANCH $LIB_DIR/pids
-
-# Make the pseudo-distributed config
-for conf in conf.pseudo ; do
-  install -d -m 0755 $ETC_DIR/$conf
-  # Install the default configurations
-  (cd ${BUILD_DIR}/conf && tar -cf - .) | (cd $ETC_DIR/$conf && tar -xf -)
-  # Overlay the -site files
-  (cd ${BUILD_DIR}/../../example-confs/$conf && tar -cf - .) | (cd $ETC_DIR/$conf && tar -xf -)
-done
-
-# man pages
-mkdir -p $MAN_DIR/man1
-cp ${CLOUDERA_SOURCE_DIR}/hadoop-$APACHE_BRANCH.1.gz $MAN_DIR/man1/
-
-############################################################
-# ARCH DEPENDENT STUFF
-############################################################
-
-if [ ! -z "$NATIVE_BUILD_STRING" ]; then
-  cp ${CLOUDERA_SOURCE_DIR}/hadoop-fuse-dfs.1.gz $MAN_DIR/man1/
-  # Fuse 
-  mkdir -p $LIB_DIR/bin
-  mv  ${BUILD_DIR}/contrib/fuse-dfs/* $LIB_DIR/bin
-  rmdir ${BUILD_DIR}/contrib/fuse-dfs 
-
-  fuse_wrapper=${BIN_DIR}/hadoop-fuse-dfs
-  cat > $fuse_wrapper << EOF
-#!/bin/bash
-
-/sbin/modprobe fuse
-
-export HADOOP_HOME=$INSTALLED_LIB_DIR
-
-if [ -f /etc/default/hadoop-0.20-fuse ] 
-  then . /etc/default/hadoop-0.20-fuse
-fi
-
-if [ -f \$HADOOP_HOME/bin/hadoop-config.sh ] 
-  then . \$HADOOP_HOME/bin/hadoop-config.sh
-fi
-
-if [ "\${LD_LIBRARY_PATH}" = "" ]; then
-  export LD_LIBRARY_PATH=/usr/lib
-  for f in \`find \${JAVA_HOME}/jre/lib -name client -prune -o -name libjvm.so -exec dirname {} \;\`; do
-    export LD_LIBRARY_PATH=\$f:\${LD_LIBRARY_PATH}
-  done
-fi
-
-for i in \${HADOOP_HOME}/*.jar \${HADOOP_HOME}/lib/*.jar
-  do CLASSPATH+=\$i:
-done
-
-export PATH=\$PATH:\${HADOOP_HOME}/bin/
-
-env CLASSPATH=\$CLASSPATH \${HADOOP_HOME}/bin/fuse_dfs \$@
-EOF
-
-  chmod 755 $fuse_wrapper
-
-  # Native compression libs
-  mkdir -p $LIB_DIR/lib/native/
-  cp -r ${BUILD_DIR}/lib/native/${NATIVE_BUILD_STRING} $LIB_DIR/lib/native/
-
-  # Pipes
-  mkdir -p $PREFIX/$SYSTEM_LIB_DIR $PREFIX/usr/include
-  cp ${BUILD_DIR}/c++/${NATIVE_BUILD_STRING}/lib/libhadooppipes.a \
-      ${BUILD_DIR}/c++/${NATIVE_BUILD_STRING}/lib/libhadooputils.a \
-      $PREFIX/$SYSTEM_LIB_DIR
-  cp -r ${BUILD_DIR}/c++/${NATIVE_BUILD_STRING}/include/hadoop $PREFIX/usr/include/
-
-  # libhdfs
-  cp ${BUILD_DIR}/c++/${NATIVE_BUILD_STRING}/lib/libhdfs.so.0.0.0 $PREFIX/$SYSTEM_LIB_DIR
-  ln -sf libhdfs.so.0.0.0 $PREFIX/$SYSTEM_LIB_DIR/libhdfs.so.0
-
-  # libhdfs-dev - hadoop doesn't realy install these things in nice places :(
-  mkdir -p $PREFIX/usr/share/doc/libhdfs0-dev/examples
-
-  cp ${HADOOP_SRC_DIR}/c++/libhdfs/hdfs.h $PREFIX/usr/include/
-  cp ${HADOOP_SRC_DIR}/c++/libhdfs/hdfs_*.c $PREFIX/usr/share/doc/libhdfs0-dev/examples
-
-  #    This is somewhat unintuitive, but the -dev package has this symlink (see Debian Library Packaging Guide)
-  ln -sf libhdfs.so.0.0.0 $PREFIX/$SYSTEM_LIB_DIR/libhdfs.so
-  sed -e "s|^libdir='.*'|libdir=\"$SYSTEM_LIB_DIR\"|" \
-      ${BUILD_DIR}/c++/${NATIVE_BUILD_STRING}/lib/libhdfs.la > $PREFIX/$SYSTEM_LIB_DIR/libhdfs.la
-fi
-- 
1.7.0.4

