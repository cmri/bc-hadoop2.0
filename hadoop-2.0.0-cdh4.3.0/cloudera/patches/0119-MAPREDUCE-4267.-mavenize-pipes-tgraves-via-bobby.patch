From b1f41063f905495c6d1e36302b628821a46177f1 Mon Sep 17 00:00:00 2001
From: Robert Joseph Evans <bobby@apache.org>
Date: Tue, 19 Jun 2012 19:40:44 +0000
Subject: [PATCH 0119/1357] MAPREDUCE-4267. mavenize pipes (tgraves via bobby)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1351818 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 92c1005d35c3618ce3894e471df1f1751c7d7c34)
---
 .../src/main/resources/assemblies/hadoop-tools.xml |   67 ++
 hadoop-mapreduce-project/pom.xml                   |   31 -
 .../src/c++/pipes/.autom4te.cfg                    |   42 -
 hadoop-mapreduce-project/src/c++/pipes/Makefile.am |   31 -
 .../src/c++/pipes/api/hadoop/Pipes.hh              |  260 -----
 .../src/c++/pipes/api/hadoop/TemplateFactory.hh    |   96 --
 .../src/c++/pipes/configure.ac                     |   57 -
 .../c++/pipes/debug/pipes-default-gdb-commands.txt |    3 -
 .../src/c++/pipes/debug/pipes-default-script       |    3 -
 .../src/c++/pipes/impl/HadoopPipes.cc              | 1181 --------------------
 .../src/c++/utils/.autom4te.cfg                    |   42 -
 hadoop-mapreduce-project/src/c++/utils/Makefile.am |   33 -
 .../src/c++/utils/api/hadoop/SerialUtils.hh        |  170 ---
 .../src/c++/utils/api/hadoop/StringUtils.hh        |   81 --
 .../src/c++/utils/configure.ac                     |   56 -
 .../src/c++/utils/impl/SerialUtils.cc              |  294 -----
 .../src/c++/utils/impl/StringUtils.cc              |  180 ---
 .../src/c++/utils/m4/hadoop_utils.m4               |   68 --
 .../src/examples/pipes/.autom4te.cfg               |   42 -
 .../src/examples/pipes/Makefile.am                 |   36 -
 .../src/examples/pipes/README.txt                  |   16 -
 .../src/examples/pipes/conf/word-part.xml          |   24 -
 .../src/examples/pipes/conf/word.xml               |   28 -
 .../src/examples/pipes/configure.ac                |   58 -
 .../src/examples/pipes/impl/sort.cc                |   96 --
 .../src/examples/pipes/impl/wordcount-nopipe.cc    |  148 ---
 .../src/examples/pipes/impl/wordcount-part.cc      |   76 --
 .../src/examples/pipes/impl/wordcount-simple.cc    |   67 --
 hadoop-tools/hadoop-pipes/pom.xml                  |  108 ++
 hadoop-tools/hadoop-pipes/src/CMakeLists.txt       |   99 ++
 .../src/main/native/examples/README.txt            |   16 +
 .../src/main/native/examples/conf/word-part.xml    |   24 +
 .../src/main/native/examples/conf/word.xml         |   28 +
 .../src/main/native/examples/impl/sort.cc          |   96 ++
 .../main/native/examples/impl/wordcount-nopipe.cc  |  148 +++
 .../main/native/examples/impl/wordcount-part.cc    |   76 ++
 .../main/native/examples/impl/wordcount-simple.cc  |   67 ++
 .../src/main/native/pipes/api/hadoop/Pipes.hh      |  260 +++++
 .../native/pipes/api/hadoop/TemplateFactory.hh     |   96 ++
 .../pipes/debug/pipes-default-gdb-commands.txt     |    3 +
 .../main/native/pipes/debug/pipes-default-script   |    3 +
 .../src/main/native/pipes/impl/HadoopPipes.cc      | 1181 ++++++++++++++++++++
 .../main/native/utils/api/hadoop/SerialUtils.hh    |  170 +++
 .../main/native/utils/api/hadoop/StringUtils.hh    |   81 ++
 .../src/main/native/utils/impl/SerialUtils.cc      |  294 +++++
 .../src/main/native/utils/impl/StringUtils.cc      |  180 +++
 hadoop-tools/hadoop-tools-dist/pom.xml             |   47 +
 hadoop-tools/pom.xml                               |    1 +
 48 files changed, 3045 insertions(+), 3219 deletions(-)
 create mode 100644 hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/.autom4te.cfg
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/Makefile.am
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/api/hadoop/Pipes.hh
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/api/hadoop/TemplateFactory.hh
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/configure.ac
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-gdb-commands.txt
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-script
 delete mode 100644 hadoop-mapreduce-project/src/c++/pipes/impl/HadoopPipes.cc
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/.autom4te.cfg
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/Makefile.am
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/api/hadoop/SerialUtils.hh
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/api/hadoop/StringUtils.hh
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/configure.ac
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/impl/SerialUtils.cc
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/impl/StringUtils.cc
 delete mode 100644 hadoop-mapreduce-project/src/c++/utils/m4/hadoop_utils.m4
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/.autom4te.cfg
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/Makefile.am
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/README.txt
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/conf/word-part.xml
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/conf/word.xml
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/configure.ac
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/impl/sort.cc
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-nopipe.cc
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-part.cc
 delete mode 100644 hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-simple.cc
 create mode 100644 hadoop-tools/hadoop-pipes/pom.xml
 create mode 100644 hadoop-tools/hadoop-pipes/src/CMakeLists.txt
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc
 create mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc

diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
new file mode 100644
index 0000000..1e3356d
--- /dev/null
+++ b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
@@ -0,0 +1,67 @@
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+<assembly xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd">
+  <id>hadoop-tools</id>
+  <formats>
+    <format>dir</format>
+  </formats>
+  <includeBaseDirectory>false</includeBaseDirectory>
+  <fileSets>
+    <fileSet>
+      <directory>../hadoop-pipes/src/main/native/pipes/api/hadoop</directory>
+      <includes>
+        <include>*.hh</include>
+      </includes>
+      <outputDirectory>/include</outputDirectory>
+    </fileSet>
+    <fileSet>
+      <directory>../hadoop-pipes/src/main/native/utils/api/hadoop</directory>
+      <includes>
+        <include>*.hh</include>
+      </includes>
+      <outputDirectory>/include</outputDirectory>
+    </fileSet>
+    <fileSet>
+      <directory>../hadoop-pipes/target/native</directory>
+      <includes>
+        <include>*.a</include>
+      </includes>
+      <outputDirectory>lib/native</outputDirectory>
+    </fileSet>
+  </fileSets>
+  <dependencySets>
+    <dependencySet>
+      <outputDirectory>/share/hadoop/${hadoop.component}/lib</outputDirectory>
+      <unpack>false</unpack>
+      <scope>runtime</scope>
+      <useProjectArtifact>false</useProjectArtifact>
+      <!-- Exclude hadoop artifacts. They will be found via HADOOP* env -->
+      <excludes>
+        <exclude>org.apache.hadoop:hadoop-common</exclude>
+        <exclude>org.apache.hadoop:hadoop-hdfs</exclude>
+        <exclude>org.apache.hadoop:hadoop-mapreduce</exclude>
+        <!-- pipes is native stuff, this just keeps pom from being package-->
+        <exclude>org.apache.hadoop:hadoop-pipes</exclude>
+        <!-- use slf4j from common to avoid multiple binding warnings -->
+        <exclude>org.slf4j:slf4j-api</exclude>
+        <exclude>org.slf4j:slf4j-log4j12</exclude>
+      </excludes>
+    </dependencySet>
+  </dependencySets>
+</assembly>
diff --git a/hadoop-mapreduce-project/pom.xml b/hadoop-mapreduce-project/pom.xml
index 843e26e..222903c 100644
--- a/hadoop-mapreduce-project/pom.xml
+++ b/hadoop-mapreduce-project/pom.xml
@@ -270,37 +270,6 @@
         </plugins>
       </build>
     </profile>
-    <profile>
-      <id>native</id>
-      <build>
-        <plugins>
-          <plugin>
-            <artifactId>maven-antrun-plugin</artifactId>
-            <executions>
-              <execution>
-                <id>compile-native</id>
-                <phase>compile</phase>
-                <goals>
-                  <goal>run</goal>
-                </goals>
-                <configuration>
-                  <target>
-                    <ant target="compile-c++-examples">
-                      <property name="compile.c++" value="yes"/>
-                      <property name="install.c++" value="${project.build.directory}/native"/>
-                      <property name="install.c++.examples" value="${project.build.directory}/native"/>
-                    </ant>
-                  </target>
-                </configuration>
-              </execution>
-            </executions>
-          </plugin>
-        </plugins>
-      </build>
-      <activation>
-        <activeByDefault>false</activeByDefault>
-      </activation>
-    </profile>
   </profiles>
 
 
diff --git a/hadoop-mapreduce-project/src/c++/pipes/.autom4te.cfg b/hadoop-mapreduce-project/src/c++/pipes/.autom4te.cfg
deleted file mode 100644
index d21d1c9..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/.autom4te.cfg
+++ /dev/null
@@ -1,42 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# autom4te configuration for hadoop utils library
-#
-
-begin-language: "Autoheader-preselections"
-args: --no-cache 
-end-language: "Autoheader-preselections"
-
-begin-language: "Automake-preselections"
-args: --no-cache 
-end-language: "Automake-preselections"
-
-begin-language: "Autoreconf-preselections"
-args: --no-cache 
-end-language: "Autoreconf-preselections"
-
-begin-language: "Autoconf-without-aclocal-m4"
-args: --no-cache 
-end-language: "Autoconf-without-aclocal-m4"
-
-begin-language: "Autoconf"
-args: --no-cache 
-end-language: "Autoconf"
-
diff --git a/hadoop-mapreduce-project/src/c++/pipes/Makefile.am b/hadoop-mapreduce-project/src/c++/pipes/Makefile.am
deleted file mode 100644
index 2c91d7f..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/Makefile.am
+++ /dev/null
@@ -1,31 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-ACLOCAL_AMFLAGS = -I ../utils/m4
-AM_CXXFLAGS=-I$(srcdir)/api -Wall -I$(HADOOP_UTILS_PREFIX)/include
-
-# List the api header files and where they will be installed
-apidir = $(includedir)/hadoop
-api_HEADERS = \
-	api/hadoop/Pipes.hh \
-	api/hadoop/TemplateFactory.hh
-
-# Define the libaries that need to be built
-lib_LIBRARIES = libhadooppipes.a
-
-# Define the sources for lib 
-libhadooppipes_a_SOURCES = \
-	impl/HadoopPipes.cc
-
diff --git a/hadoop-mapreduce-project/src/c++/pipes/api/hadoop/Pipes.hh b/hadoop-mapreduce-project/src/c++/pipes/api/hadoop/Pipes.hh
deleted file mode 100644
index b5d0ddd..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/api/hadoop/Pipes.hh
+++ /dev/null
@@ -1,260 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_PIPES_HH
-#define HADOOP_PIPES_HH
-
-#ifdef SWIG
-%module (directors="1") HadoopPipes
-%include "std_string.i"
-%feature("director") Mapper;
-%feature("director") Reducer;
-%feature("director") Partitioner;
-%feature("director") RecordReader;
-%feature("director") RecordWriter;
-%feature("director") Factory;
-#else
-#include <string>
-#endif
-
-#include <stdint.h>
-
-namespace HadoopPipes {
-
-/**
- * This interface defines the interface between application code and the 
- * foreign code interface to Hadoop Map/Reduce.
- */
-
-/**
- * A JobConf defines the properties for a job.
- */
-class JobConf {
-public:
-  virtual bool hasKey(const std::string& key) const = 0;
-  virtual const std::string& get(const std::string& key) const = 0;
-  virtual int getInt(const std::string& key) const = 0;
-  virtual float getFloat(const std::string& key) const = 0;
-  virtual bool getBoolean(const std::string&key) const = 0;
-  virtual ~JobConf() {}
-};
-
-/**
- * Task context provides the information about the task and job.
- */
-class TaskContext {
-public:
-  /**
-   * Counter to keep track of a property and its value.
-   */
-  class Counter {
-  private:
-    int id;
-  public:
-    Counter(int counterId) : id(counterId) {}
-    Counter(const Counter& counter) : id(counter.id) {}
-
-    int getId() const { return id; }
-  };
-  
-  /**
-   * Get the JobConf for the current task.
-   */
-  virtual const JobConf* getJobConf() = 0;
-
-  /**
-   * Get the current key. 
-   * @return the current key
-   */
-  virtual const std::string& getInputKey() = 0;
-
-  /**
-   * Get the current value. 
-   * @return the current value
-   */
-  virtual const std::string& getInputValue() = 0;
-
-  /**
-   * Generate an output record
-   */
-  virtual void emit(const std::string& key, const std::string& value) = 0;
-
-  /**
-   * Mark your task as having made progress without changing the status 
-   * message.
-   */
-  virtual void progress() = 0;
-
-  /**
-   * Set the status message and call progress.
-   */
-  virtual void setStatus(const std::string& status) = 0;
-
-  /**
-   * Register a counter with the given group and name.
-   */
-  virtual Counter* 
-    getCounter(const std::string& group, const std::string& name) = 0;
-
-  /**
-   * Increment the value of the counter with the given amount.
-   */
-  virtual void incrementCounter(const Counter* counter, uint64_t amount) = 0;
-  
-  virtual ~TaskContext() {}
-};
-
-class MapContext: public TaskContext {
-public:
-
-  /**
-   * Access the InputSplit of the mapper.
-   */
-  virtual const std::string& getInputSplit() = 0;
-
-  /**
-   * Get the name of the key class of the input to this task.
-   */
-  virtual const std::string& getInputKeyClass() = 0;
-
-  /**
-   * Get the name of the value class of the input to this task.
-   */
-  virtual const std::string& getInputValueClass() = 0;
-
-};
-
-class ReduceContext: public TaskContext {
-public:
-  /**
-   * Advance to the next value.
-   */
-  virtual bool nextValue() = 0;
-};
-
-class Closable {
-public:
-  virtual void close() {}
-  virtual ~Closable() {}
-};
-
-/**
- * The application's mapper class to do map.
- */
-class Mapper: public Closable {
-public:
-  virtual void map(MapContext& context) = 0;
-};
-
-/**
- * The application's reducer class to do reduce.
- */
-class Reducer: public Closable {
-public:
-  virtual void reduce(ReduceContext& context) = 0;
-};
-
-/**
- * User code to decide where each key should be sent.
- */
-class Partitioner {
-public:
-  virtual int partition(const std::string& key, int numOfReduces) = 0;
-  virtual ~Partitioner() {}
-};
-
-/**
- * For applications that want to read the input directly for the map function
- * they can define RecordReaders in C++.
- */
-class RecordReader: public Closable {
-public:
-  virtual bool next(std::string& key, std::string& value) = 0;
-
-  /**
-   * The progress of the record reader through the split as a value between
-   * 0.0 and 1.0.
-   */
-  virtual float getProgress() = 0;
-};
-
-/**
- * An object to write key/value pairs as they are emited from the reduce.
- */
-class RecordWriter: public Closable {
-public:
-  virtual void emit(const std::string& key,
-                    const std::string& value) = 0;
-};
-
-/**
- * A factory to create the necessary application objects.
- */
-class Factory {
-public:
-  virtual Mapper* createMapper(MapContext& context) const = 0;
-  virtual Reducer* createReducer(ReduceContext& context) const = 0;
-
-  /**
-   * Create a combiner, if this application has one.
-   * @return the new combiner or NULL, if one is not needed
-   */
-  virtual Reducer* createCombiner(MapContext& context) const {
-    return NULL; 
-  }
-
-  /**
-   * Create an application partitioner object.
-   * @return the new partitioner or NULL, if the default partitioner should be 
-   *     used.
-   */
-  virtual Partitioner* createPartitioner(MapContext& context) const {
-    return NULL;
-  }
-
-  /**
-   * Create an application record reader.
-   * @return the new RecordReader or NULL, if the Java RecordReader should be
-   *    used.
-   */
-  virtual RecordReader* createRecordReader(MapContext& context) const {
-    return NULL; 
-  }
-
-  /**
-   * Create an application record writer.
-   * @return the new RecordWriter or NULL, if the Java RecordWriter should be
-   *    used.
-   */
-  virtual RecordWriter* createRecordWriter(ReduceContext& context) const {
-    return NULL;
-  }
-
-  virtual ~Factory() {}
-};
-
-/**
- * Run the assigned task in the framework.
- * The user's main function should set the various functions using the 
- * set* functions above and then call this.
- * @return true, if the task succeeded.
- */
-bool runTask(const Factory& factory);
-
-}
-
-#endif
diff --git a/hadoop-mapreduce-project/src/c++/pipes/api/hadoop/TemplateFactory.hh b/hadoop-mapreduce-project/src/c++/pipes/api/hadoop/TemplateFactory.hh
deleted file mode 100644
index 22e10ae..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/api/hadoop/TemplateFactory.hh
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_PIPES_TEMPLATE_FACTORY_HH
-#define HADOOP_PIPES_TEMPLATE_FACTORY_HH
-
-namespace HadoopPipes {
-
-  template <class mapper, class reducer>
-  class TemplateFactory2: public Factory {
-  public:
-    Mapper* createMapper(MapContext& context) const {
-      return new mapper(context);
-    }
-    Reducer* createReducer(ReduceContext& context) const {
-      return new reducer(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner>
-  class TemplateFactory3: public TemplateFactory2<mapper,reducer> {
-  public:
-    Partitioner* createPartitioner(MapContext& context) const {
-      return new partitioner(context);
-    }
-  };
-
-  template <class mapper, class reducer>
-  class TemplateFactory3<mapper, reducer, void>
-      : public TemplateFactory2<mapper,reducer> {
-  };
-
-  template <class mapper, class reducer, class partitioner, class combiner>
-  class TemplateFactory4
-   : public TemplateFactory3<mapper,reducer,partitioner>{
-  public:
-    Reducer* createCombiner(MapContext& context) const {
-      return new combiner(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner>
-  class TemplateFactory4<mapper,reducer,partitioner,void>
-   : public TemplateFactory3<mapper,reducer,partitioner>{
-  };
-
-  template <class mapper, class reducer, class partitioner, 
-            class combiner, class recordReader>
-  class TemplateFactory5
-   : public TemplateFactory4<mapper,reducer,partitioner,combiner>{
-  public:
-    RecordReader* createRecordReader(MapContext& context) const {
-      return new recordReader(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner,class combiner>
-  class TemplateFactory5<mapper,reducer,partitioner,combiner,void>
-   : public TemplateFactory4<mapper,reducer,partitioner,combiner>{
-  };
-
-  template <class mapper, class reducer, class partitioner=void, 
-            class combiner=void, class recordReader=void, 
-            class recordWriter=void> 
-  class TemplateFactory
-   : public TemplateFactory5<mapper,reducer,partitioner,combiner,recordReader>{
-  public:
-    RecordWriter* createRecordWriter(ReduceContext& context) const {
-      return new recordWriter(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner, 
-            class combiner, class recordReader>
-  class TemplateFactory<mapper, reducer, partitioner, combiner, recordReader, 
-                        void>
-   : public TemplateFactory5<mapper,reducer,partitioner,combiner,recordReader>{
-  };
-
-}
-
-#endif
diff --git a/hadoop-mapreduce-project/src/c++/pipes/configure.ac b/hadoop-mapreduce-project/src/c++/pipes/configure.ac
deleted file mode 100644
index ac05ece..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/configure.ac
+++ /dev/null
@@ -1,57 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-#                                               -*- Autoconf -*-
-# Process this file with autoconf to produce a configure script.
-
-AC_PREREQ(2.59)
-AC_INIT(hadoop-pipes, 0.13.0, omalley@apache.org)
-
-AC_CONFIG_AUX_DIR([config])
-AC_CONFIG_MACRO_DIR([../utils/m4])
-
-AM_INIT_AUTOMAKE([subdir-objects foreign no-dist])
-
-AC_CONFIG_SRCDIR([impl/HadoopPipes.cc])
-AC_CONFIG_HEADER([impl/config.h])
-AC_CONFIG_FILES([Makefile])
-
-AC_PREFIX_DEFAULT(`pwd`/../install)
-
-USE_HADOOP_UTILS
-HADOOP_PIPES_SETUP
-CHECK_INSTALL_CFLAG
-
-# Checks for programs.
-AC_PROG_CXX
-AC_PROG_LIBTOOL
-
-# Checks for libraries.
-
-# Checks for header files.
-AC_LANG(C++)
-AC_CHECK_HEADERS([unistd.h])
-
-# Checks for typedefs, structures, and compiler characteristics.
-AC_HEADER_STDBOOL
-AC_C_CONST
-AC_TYPE_OFF_T
-AC_TYPE_SIZE_T
-AC_FUNC_STRERROR_R
-
-# Checks for library functions.
-AC_CHECK_FUNCS([mkdir uname])
-AC_OUTPUT
diff --git a/hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-gdb-commands.txt b/hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-gdb-commands.txt
deleted file mode 100644
index 6cfd4d6..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-gdb-commands.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-info threads
-backtrace
-quit
diff --git a/hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-script b/hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-script
deleted file mode 100644
index e7f59e5..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/debug/pipes-default-script
+++ /dev/null
@@ -1,3 +0,0 @@
-core=`find . -name 'core*'`
-#Only pipes programs have 5th argument as program name.
-gdb -quiet $5 -c $core -x $HADOOP_PREFIX/src/c++/pipes/debug/pipes-default-gdb-commands.txt 
diff --git a/hadoop-mapreduce-project/src/c++/pipes/impl/HadoopPipes.cc b/hadoop-mapreduce-project/src/c++/pipes/impl/HadoopPipes.cc
deleted file mode 100644
index 0e26425..0000000
--- a/hadoop-mapreduce-project/src/c++/pipes/impl/HadoopPipes.cc
+++ /dev/null
@@ -1,1181 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/SerialUtils.hh"
-#include "hadoop/StringUtils.hh"
-
-#include <map>
-#include <vector>
-
-#include <errno.h>
-#include <netinet/in.h>
-#include <stdint.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <strings.h>
-#include <sys/socket.h>
-#include <pthread.h>
-#include <iostream>
-#include <fstream>
-
-#include <openssl/hmac.h>
-#include <openssl/buffer.h>
-
-using std::map;
-using std::string;
-using std::vector;
-
-using namespace HadoopUtils;
-
-namespace HadoopPipes {
-
-  class JobConfImpl: public JobConf {
-  private:
-    map<string, string> values;
-  public:
-    void set(const string& key, const string& value) {
-      values[key] = value;
-    }
-
-    virtual bool hasKey(const string& key) const {
-      return values.find(key) != values.end();
-    }
-
-    virtual const string& get(const string& key) const {
-      map<string,string>::const_iterator itr = values.find(key);
-      if (itr == values.end()) {
-        throw Error("Key " + key + " not found in JobConf");
-      }
-      return itr->second;
-    }
-
-    virtual int getInt(const string& key) const {
-      const string& val = get(key);
-      return toInt(val);
-    }
-
-    virtual float getFloat(const string& key) const {
-      const string& val = get(key);
-      return toFloat(val);
-    }
-
-    virtual bool getBoolean(const string&key) const {
-      const string& val = get(key);
-      return toBool(val);
-    }
-  };
-
-  class DownwardProtocol {
-  public:
-    virtual void start(int protocol) = 0;
-    virtual void setJobConf(vector<string> values) = 0;
-    virtual void setInputTypes(string keyType, string valueType) = 0;
-    virtual void runMap(string inputSplit, int numReduces, bool pipedInput)= 0;
-    virtual void mapItem(const string& key, const string& value) = 0;
-    virtual void runReduce(int reduce, bool pipedOutput) = 0;
-    virtual void reduceKey(const string& key) = 0;
-    virtual void reduceValue(const string& value) = 0;
-    virtual void close() = 0;
-    virtual void abort() = 0;
-    virtual ~DownwardProtocol() {}
-  };
-
-  class UpwardProtocol {
-  public:
-    virtual void output(const string& key, const string& value) = 0;
-    virtual void partitionedOutput(int reduce, const string& key,
-                                   const string& value) = 0;
-    virtual void status(const string& message) = 0;
-    virtual void progress(float progress) = 0;
-    virtual void done() = 0;
-    virtual void registerCounter(int id, const string& group, 
-                                 const string& name) = 0;
-    virtual void 
-      incrementCounter(const TaskContext::Counter* counter, uint64_t amount) = 0;
-    virtual ~UpwardProtocol() {}
-  };
-
-  class Protocol {
-  public:
-    virtual void nextEvent() = 0;
-    virtual UpwardProtocol* getUplink() = 0;
-    virtual ~Protocol() {}
-  };
-
-  class TextUpwardProtocol: public UpwardProtocol {
-  private:
-    FILE* stream;
-    static const char fieldSeparator = '\t';
-    static const char lineSeparator = '\n';
-
-    void writeBuffer(const string& buffer) {
-      fprintf(stream, quoteString(buffer, "\t\n").c_str());
-    }
-
-  public:
-    TextUpwardProtocol(FILE* _stream): stream(_stream) {}
-    
-    virtual void output(const string& key, const string& value) {
-      fprintf(stream, "output%c", fieldSeparator);
-      writeBuffer(key);
-      fprintf(stream, "%c", fieldSeparator);
-      writeBuffer(value);
-      fprintf(stream, "%c", lineSeparator);
-    }
-
-    virtual void partitionedOutput(int reduce, const string& key,
-                                   const string& value) {
-      fprintf(stream, "parititionedOutput%c%d%c", fieldSeparator, reduce, 
-              fieldSeparator);
-      writeBuffer(key);
-      fprintf(stream, "%c", fieldSeparator);
-      writeBuffer(value);
-      fprintf(stream, "%c", lineSeparator);
-    }
-
-    virtual void status(const string& message) {
-      fprintf(stream, "status%c%s%c", fieldSeparator, message.c_str(), 
-              lineSeparator);
-    }
-
-    virtual void progress(float progress) {
-      fprintf(stream, "progress%c%f%c", fieldSeparator, progress, 
-              lineSeparator);
-    }
-
-    virtual void registerCounter(int id, const string& group, 
-                                 const string& name) {
-      fprintf(stream, "registerCounter%c%d%c%s%c%s%c", fieldSeparator, id,
-              fieldSeparator, group.c_str(), fieldSeparator, name.c_str(), 
-              lineSeparator);
-    }
-
-    virtual void incrementCounter(const TaskContext::Counter* counter, 
-                                  uint64_t amount) {
-      fprintf(stream, "incrCounter%c%d%c%ld%c", fieldSeparator, counter->getId(), 
-              fieldSeparator, (long)amount, lineSeparator);
-    }
-    
-    virtual void done() {
-      fprintf(stream, "done%c", lineSeparator);
-    }
-  };
-
-  class TextProtocol: public Protocol {
-  private:
-    FILE* downStream;
-    DownwardProtocol* handler;
-    UpwardProtocol* uplink;
-    string key;
-    string value;
-
-    int readUpto(string& buffer, const char* limit) {
-      int ch;
-      buffer.clear();
-      while ((ch = getc(downStream)) != -1) {
-        if (strchr(limit, ch) != NULL) {
-          return ch;
-        }
-        buffer += ch;
-      }
-      return -1;
-    }
-
-    static const char* delim;
-  public:
-
-    TextProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
-      downStream = down;
-      uplink = new TextUpwardProtocol(up);
-      handler = _handler;
-    }
-
-    UpwardProtocol* getUplink() {
-      return uplink;
-    }
-
-    virtual void nextEvent() {
-      string command;
-      string arg;
-      int sep;
-      sep = readUpto(command, delim);
-      if (command == "mapItem") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(key, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(value, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->mapItem(key, value);
-      } else if (command == "reduceValue") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(value, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->reduceValue(value);
-      } else if (command == "reduceKey") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(key, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->reduceKey(key);
-      } else if (command == "start") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->start(toInt(arg));
-      } else if (command == "setJobConf") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        int len = toInt(arg);
-        vector<string> values(len);
-        for(int i=0; i < len; ++i) {
-          HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-          sep = readUpto(arg, delim);
-          values.push_back(arg);
-        }
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->setJobConf(values);
-      } else if (command == "setInputTypes") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(key, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(value, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->setInputTypes(key, value);
-      } else if (command == "runMap") {
-        string split;
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(split, delim);
-        string reduces;
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(reduces, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->runMap(split, toInt(reduces), toBool(arg));
-      } else if (command == "runReduce") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        string piped;
-        sep = readUpto(piped, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->runReduce(toInt(arg), toBool(piped));
-      } else if (command == "abort") { 
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->abort();
-      } else if (command == "close") {
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->close();
-      } else {
-        throw Error("Illegal text protocol command " + command);
-      }
-    }
-
-    ~TextProtocol() {
-      delete uplink;
-    }
-  };
-  const char* TextProtocol::delim = "\t\n";
-
-  enum MESSAGE_TYPE {START_MESSAGE, SET_JOB_CONF, SET_INPUT_TYPES, RUN_MAP, 
-                     MAP_ITEM, RUN_REDUCE, REDUCE_KEY, REDUCE_VALUE, 
-                     CLOSE, ABORT, AUTHENTICATION_REQ,
-                     OUTPUT=50, PARTITIONED_OUTPUT, STATUS, PROGRESS, DONE,
-                     REGISTER_COUNTER, INCREMENT_COUNTER, AUTHENTICATION_RESP};
-
-  class BinaryUpwardProtocol: public UpwardProtocol {
-  private:
-    FileOutStream* stream;
-  public:
-    BinaryUpwardProtocol(FILE* _stream) {
-      stream = new FileOutStream();
-      HADOOP_ASSERT(stream->open(_stream), "problem opening stream");
-    }
-
-    virtual void authenticate(const string &responseDigest) {
-      serializeInt(AUTHENTICATION_RESP, *stream);
-      serializeString(responseDigest, *stream);
-      stream->flush();
-    }
-
-    virtual void output(const string& key, const string& value) {
-      serializeInt(OUTPUT, *stream);
-      serializeString(key, *stream);
-      serializeString(value, *stream);
-    }
-
-    virtual void partitionedOutput(int reduce, const string& key,
-                                   const string& value) {
-      serializeInt(PARTITIONED_OUTPUT, *stream);
-      serializeInt(reduce, *stream);
-      serializeString(key, *stream);
-      serializeString(value, *stream);
-    }
-
-    virtual void status(const string& message) {
-      serializeInt(STATUS, *stream);
-      serializeString(message, *stream);
-    }
-
-    virtual void progress(float progress) {
-      serializeInt(PROGRESS, *stream);
-      serializeFloat(progress, *stream);
-      stream->flush();
-    }
-
-    virtual void done() {
-      serializeInt(DONE, *stream);
-    }
-
-    virtual void registerCounter(int id, const string& group, 
-                                 const string& name) {
-      serializeInt(REGISTER_COUNTER, *stream);
-      serializeInt(id, *stream);
-      serializeString(group, *stream);
-      serializeString(name, *stream);
-    }
-
-    virtual void incrementCounter(const TaskContext::Counter* counter, 
-                                  uint64_t amount) {
-      serializeInt(INCREMENT_COUNTER, *stream);
-      serializeInt(counter->getId(), *stream);
-      serializeLong(amount, *stream);
-    }
-    
-    ~BinaryUpwardProtocol() {
-      delete stream;
-    }
-  };
-
-  class BinaryProtocol: public Protocol {
-  private:
-    FileInStream* downStream;
-    DownwardProtocol* handler;
-    BinaryUpwardProtocol * uplink;
-    string key;
-    string value;
-    string password;
-    bool authDone;
-    void getPassword(string &password) {
-      const char *passwordFile = getenv("hadoop.pipes.shared.secret.location");
-      if (passwordFile == NULL) {
-        return;
-      }
-      std::ifstream fstr(passwordFile, std::fstream::binary);
-      if (fstr.fail()) {
-        std::cerr << "Could not open the password file" << std::endl;
-        return;
-      } 
-      unsigned char * passBuff = new unsigned char [512];
-      fstr.read((char *)passBuff, 512);
-      int passwordLength = fstr.gcount();
-      fstr.close();
-      passBuff[passwordLength] = 0;
-      password.replace(0, passwordLength, (const char *) passBuff, passwordLength);
-      delete [] passBuff;
-      return; 
-    }
-
-    void verifyDigestAndRespond(string& digest, string& challenge) {
-      if (password.empty()) {
-        //password can be empty if process is running in debug mode from
-        //command file.
-        authDone = true;
-        return;
-      }
-
-      if (!verifyDigest(password, digest, challenge)) {
-        std::cerr << "Server failed to authenticate. Exiting" << std::endl;
-        exit(-1);
-      }
-      authDone = true;
-      string responseDigest = createDigest(password, digest);
-      uplink->authenticate(responseDigest);
-    }
-
-    bool verifyDigest(string &password, string& digest, string& challenge) {
-      string expectedDigest = createDigest(password, challenge);
-      if (digest == expectedDigest) {
-        return true;
-      } else {
-        return false;
-      }
-    }
-
-    string createDigest(string &password, string& msg) {
-      HMAC_CTX ctx;
-      unsigned char digest[EVP_MAX_MD_SIZE];
-      HMAC_Init(&ctx, (const unsigned char *)password.c_str(), 
-          password.length(), EVP_sha1());
-      HMAC_Update(&ctx, (const unsigned char *)msg.c_str(), msg.length());
-      unsigned int digestLen;
-      HMAC_Final(&ctx, digest, &digestLen);
-      HMAC_cleanup(&ctx);
-
-      //now apply base64 encoding
-      BIO *bmem, *b64;
-      BUF_MEM *bptr;
-
-      b64 = BIO_new(BIO_f_base64());
-      bmem = BIO_new(BIO_s_mem());
-      b64 = BIO_push(b64, bmem);
-      BIO_write(b64, digest, digestLen);
-      BIO_flush(b64);
-      BIO_get_mem_ptr(b64, &bptr);
-
-      char digestBuffer[bptr->length];
-      memcpy(digestBuffer, bptr->data, bptr->length-1);
-      digestBuffer[bptr->length-1] = 0;
-      BIO_free_all(b64);
-
-      return string(digestBuffer);
-    }
-
-  public:
-    BinaryProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
-      downStream = new FileInStream();
-      downStream->open(down);
-      uplink = new BinaryUpwardProtocol(up);
-      handler = _handler;
-      authDone = false;
-      getPassword(password);
-    }
-
-    UpwardProtocol* getUplink() {
-      return uplink;
-    }
-
-    virtual void nextEvent() {
-      int32_t cmd;
-      cmd = deserializeInt(*downStream);
-      if (!authDone && cmd != AUTHENTICATION_REQ) {
-        //Authentication request must be the first message if
-        //authentication is not complete
-        std::cerr << "Command:" << cmd << "received before authentication. " 
-            << "Exiting.." << std::endl;
-        exit(-1);
-      }
-      switch (cmd) {
-      case AUTHENTICATION_REQ: {
-        string digest;
-        string challenge;
-        deserializeString(digest, *downStream);
-        deserializeString(challenge, *downStream);
-        verifyDigestAndRespond(digest, challenge);
-        break;
-      }
-      case START_MESSAGE: {
-        int32_t prot;
-        prot = deserializeInt(*downStream);
-        handler->start(prot);
-        break;
-      }
-      case SET_JOB_CONF: {
-        int32_t entries;
-        entries = deserializeInt(*downStream);
-        vector<string> result(entries);
-        for(int i=0; i < entries; ++i) {
-          string item;
-          deserializeString(item, *downStream);
-          result.push_back(item);
-        }
-        handler->setJobConf(result);
-        break;
-      }
-      case SET_INPUT_TYPES: {
-        string keyType;
-        string valueType;
-        deserializeString(keyType, *downStream);
-        deserializeString(valueType, *downStream);
-        handler->setInputTypes(keyType, valueType);
-        break;
-      }
-      case RUN_MAP: {
-        string split;
-        int32_t numReduces;
-        int32_t piped;
-        deserializeString(split, *downStream);
-        numReduces = deserializeInt(*downStream);
-        piped = deserializeInt(*downStream);
-        handler->runMap(split, numReduces, piped);
-        break;
-      }
-      case MAP_ITEM: {
-        deserializeString(key, *downStream);
-        deserializeString(value, *downStream);
-        handler->mapItem(key, value);
-        break;
-      }
-      case RUN_REDUCE: {
-        int32_t reduce;
-        int32_t piped;
-        reduce = deserializeInt(*downStream);
-        piped = deserializeInt(*downStream);
-        handler->runReduce(reduce, piped);
-        break;
-      }
-      case REDUCE_KEY: {
-        deserializeString(key, *downStream);
-        handler->reduceKey(key);
-        break;
-      }
-      case REDUCE_VALUE: {
-        deserializeString(value, *downStream);
-        handler->reduceValue(value);
-        break;
-      }
-      case CLOSE:
-        handler->close();
-        break;
-      case ABORT:
-        handler->abort();
-        break;
-      default:
-        HADOOP_ASSERT(false, "Unknown binary command " + toString(cmd));
-      }
-    }
-
-    virtual ~BinaryProtocol() {
-      delete downStream;
-      delete uplink;
-    }
-  };
-
-  /**
-   * Define a context object to give to combiners that will let them
-   * go through the values and emit their results correctly.
-   */
-  class CombineContext: public ReduceContext {
-  private:
-    ReduceContext* baseContext;
-    Partitioner* partitioner;
-    int numReduces;
-    UpwardProtocol* uplink;
-    bool firstKey;
-    bool firstValue;
-    map<string, vector<string> >::iterator keyItr;
-    map<string, vector<string> >::iterator endKeyItr;
-    vector<string>::iterator valueItr;
-    vector<string>::iterator endValueItr;
-
-  public:
-    CombineContext(ReduceContext* _baseContext,
-                   Partitioner* _partitioner,
-                   int _numReduces,
-                   UpwardProtocol* _uplink,
-                   map<string, vector<string> >& data) {
-      baseContext = _baseContext;
-      partitioner = _partitioner;
-      numReduces = _numReduces;
-      uplink = _uplink;
-      keyItr = data.begin();
-      endKeyItr = data.end();
-      firstKey = true;
-      firstValue = true;
-    }
-
-    virtual const JobConf* getJobConf() {
-      return baseContext->getJobConf();
-    }
-
-    virtual const std::string& getInputKey() {
-      return keyItr->first;
-    }
-
-    virtual const std::string& getInputValue() {
-      return *valueItr;
-    }
-
-    virtual void emit(const std::string& key, const std::string& value) {
-      if (partitioner != NULL) {
-        uplink->partitionedOutput(partitioner->partition(key, numReduces),
-                                  key, value);
-      } else {
-        uplink->output(key, value);
-      }
-    }
-
-    virtual void progress() {
-      baseContext->progress();
-    }
-
-    virtual void setStatus(const std::string& status) {
-      baseContext->setStatus(status);
-    }
-
-    bool nextKey() {
-      if (firstKey) {
-        firstKey = false;
-      } else {
-        ++keyItr;
-      }
-      if (keyItr != endKeyItr) {
-        valueItr = keyItr->second.begin();
-        endValueItr = keyItr->second.end();
-        firstValue = true;
-        return true;
-      }
-      return false;
-    }
-
-    virtual bool nextValue() {
-      if (firstValue) {
-        firstValue = false;
-      } else {
-        ++valueItr;
-      }
-      return valueItr != endValueItr;
-    }
-    
-    virtual Counter* getCounter(const std::string& group, 
-                               const std::string& name) {
-      return baseContext->getCounter(group, name);
-    }
-
-    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
-      baseContext->incrementCounter(counter, amount);
-    }
-  };
-
-  /**
-   * A RecordWriter that will take the map outputs, buffer them up and then
-   * combine then when the buffer is full.
-   */
-  class CombineRunner: public RecordWriter {
-  private:
-    map<string, vector<string> > data;
-    int64_t spillSize;
-    int64_t numBytes;
-    ReduceContext* baseContext;
-    Partitioner* partitioner;
-    int numReduces;
-    UpwardProtocol* uplink;
-    Reducer* combiner;
-  public:
-    CombineRunner(int64_t _spillSize, ReduceContext* _baseContext, 
-                  Reducer* _combiner, UpwardProtocol* _uplink, 
-                  Partitioner* _partitioner, int _numReduces) {
-      numBytes = 0;
-      spillSize = _spillSize;
-      baseContext = _baseContext;
-      partitioner = _partitioner;
-      numReduces = _numReduces;
-      uplink = _uplink;
-      combiner = _combiner;
-    }
-
-    virtual void emit(const std::string& key,
-                      const std::string& value) {
-      numBytes += key.length() + value.length();
-      data[key].push_back(value);
-      if (numBytes >= spillSize) {
-        spillAll();
-      }
-    }
-
-    virtual void close() {
-      spillAll();
-    }
-
-  private:
-    void spillAll() {
-      CombineContext context(baseContext, partitioner, numReduces, 
-                             uplink, data);
-      while (context.nextKey()) {
-        combiner->reduce(context);
-      }
-      data.clear();
-      numBytes = 0;
-    }
-  };
-
-  class TaskContextImpl: public MapContext, public ReduceContext, 
-                         public DownwardProtocol {
-  private:
-    bool done;
-    JobConf* jobConf;
-    string key;
-    const string* newKey;
-    const string* value;
-    bool hasTask;
-    bool isNewKey;
-    bool isNewValue;
-    string* inputKeyClass;
-    string* inputValueClass;
-    string status;
-    float progressFloat;
-    uint64_t lastProgress;
-    bool statusSet;
-    Protocol* protocol;
-    UpwardProtocol *uplink;
-    string* inputSplit;
-    RecordReader* reader;
-    Mapper* mapper;
-    Reducer* reducer;
-    RecordWriter* writer;
-    Partitioner* partitioner;
-    int numReduces;
-    const Factory* factory;
-    pthread_mutex_t mutexDone;
-    std::vector<int> registeredCounterIds;
-
-  public:
-
-    TaskContextImpl(const Factory& _factory) {
-      statusSet = false;
-      done = false;
-      newKey = NULL;
-      factory = &_factory;
-      jobConf = NULL;
-      inputKeyClass = NULL;
-      inputValueClass = NULL;
-      inputSplit = NULL;
-      mapper = NULL;
-      reducer = NULL;
-      reader = NULL;
-      writer = NULL;
-      partitioner = NULL;
-      protocol = NULL;
-      isNewKey = false;
-      isNewValue = false;
-      lastProgress = 0;
-      progressFloat = 0.0f;
-      hasTask = false;
-      pthread_mutex_init(&mutexDone, NULL);
-    }
-
-    void setProtocol(Protocol* _protocol, UpwardProtocol* _uplink) {
-
-      protocol = _protocol;
-      uplink = _uplink;
-    }
-
-    virtual void start(int protocol) {
-      if (protocol != 0) {
-        throw Error("Protocol version " + toString(protocol) + 
-                    " not supported");
-      }
-    }
-
-    virtual void setJobConf(vector<string> values) {
-      int len = values.size();
-      JobConfImpl* result = new JobConfImpl();
-      HADOOP_ASSERT(len % 2 == 0, "Odd length of job conf values");
-      for(int i=0; i < len; i += 2) {
-        result->set(values[i], values[i+1]);
-      }
-      jobConf = result;
-    }
-
-    virtual void setInputTypes(string keyType, string valueType) {
-      inputKeyClass = new string(keyType);
-      inputValueClass = new string(valueType);
-    }
-
-    virtual void runMap(string _inputSplit, int _numReduces, bool pipedInput) {
-      inputSplit = new string(_inputSplit);
-      reader = factory->createRecordReader(*this);
-      HADOOP_ASSERT((reader == NULL) == pipedInput,
-                    pipedInput ? "RecordReader defined when not needed.":
-                    "RecordReader not defined");
-      if (reader != NULL) {
-        value = new string();
-      }
-      mapper = factory->createMapper(*this);
-      numReduces = _numReduces;
-      if (numReduces != 0) { 
-        reducer = factory->createCombiner(*this);
-        partitioner = factory->createPartitioner(*this);
-      }
-      if (reducer != NULL) {
-        int64_t spillSize = 100;
-        if (jobConf->hasKey("mapreduce.task.io.sort.mb")) {
-          spillSize = jobConf->getInt("mapreduce.task.io.sort.mb");
-        }
-        writer = new CombineRunner(spillSize * 1024 * 1024, this, reducer, 
-                                   uplink, partitioner, numReduces);
-      }
-      hasTask = true;
-    }
-
-    virtual void mapItem(const string& _key, const string& _value) {
-      newKey = &_key;
-      value = &_value;
-      isNewKey = true;
-    }
-
-    virtual void runReduce(int reduce, bool pipedOutput) {
-      reducer = factory->createReducer(*this);
-      writer = factory->createRecordWriter(*this);
-      HADOOP_ASSERT((writer == NULL) == pipedOutput,
-                    pipedOutput ? "RecordWriter defined when not needed.":
-                    "RecordWriter not defined");
-      hasTask = true;
-    }
-
-    virtual void reduceKey(const string& _key) {
-      isNewKey = true;
-      newKey = &_key;
-    }
-
-    virtual void reduceValue(const string& _value) {
-      isNewValue = true;
-      value = &_value;
-    }
-    
-    virtual bool isDone() {
-      pthread_mutex_lock(&mutexDone);
-      bool doneCopy = done;
-      pthread_mutex_unlock(&mutexDone);
-      return doneCopy;
-    }
-
-    virtual void close() {
-      pthread_mutex_lock(&mutexDone);
-      done = true;
-      pthread_mutex_unlock(&mutexDone);
-    }
-
-    virtual void abort() {
-      throw Error("Aborted by driver");
-    }
-
-    void waitForTask() {
-      while (!done && !hasTask) {
-        protocol->nextEvent();
-      }
-    }
-
-    bool nextKey() {
-      if (reader == NULL) {
-        while (!isNewKey) {
-          nextValue();
-          if (done) {
-            return false;
-          }
-        }
-        key = *newKey;
-      } else {
-        if (!reader->next(key, const_cast<string&>(*value))) {
-          pthread_mutex_lock(&mutexDone);
-          done = true;
-          pthread_mutex_unlock(&mutexDone);
-          return false;
-        }
-        progressFloat = reader->getProgress();
-      }
-      isNewKey = false;
-      if (mapper != NULL) {
-        mapper->map(*this);
-      } else {
-        reducer->reduce(*this);
-      }
-      return true;
-    }
-
-    /**
-     * Advance to the next value.
-     */
-    virtual bool nextValue() {
-      if (isNewKey || done) {
-        return false;
-      }
-      isNewValue = false;
-      progress();
-      protocol->nextEvent();
-      return isNewValue;
-    }
-
-    /**
-     * Get the JobConf for the current task.
-     */
-    virtual JobConf* getJobConf() {
-      return jobConf;
-    }
-
-    /**
-     * Get the current key. 
-     * @return the current key or NULL if called before the first map or reduce
-     */
-    virtual const string& getInputKey() {
-      return key;
-    }
-
-    /**
-     * Get the current value. 
-     * @return the current value or NULL if called before the first map or 
-     *    reduce
-     */
-    virtual const string& getInputValue() {
-      return *value;
-    }
-
-    /**
-     * Mark your task as having made progress without changing the status 
-     * message.
-     */
-    virtual void progress() {
-      if (uplink != 0) {
-        uint64_t now = getCurrentMillis();
-        if (now - lastProgress > 1000) {
-          lastProgress = now;
-          if (statusSet) {
-            uplink->status(status);
-            statusSet = false;
-          }
-          uplink->progress(progressFloat);
-        }
-      }
-    }
-
-    /**
-     * Set the status message and call progress.
-     */
-    virtual void setStatus(const string& status) {
-      this->status = status;
-      statusSet = true;
-      progress();
-    }
-
-    /**
-     * Get the name of the key class of the input to this task.
-     */
-    virtual const string& getInputKeyClass() {
-      return *inputKeyClass;
-    }
-
-    /**
-     * Get the name of the value class of the input to this task.
-     */
-    virtual const string& getInputValueClass() {
-      return *inputValueClass;
-    }
-
-    /**
-     * Access the InputSplit of the mapper.
-     */
-    virtual const std::string& getInputSplit() {
-      return *inputSplit;
-    }
-
-    virtual void emit(const string& key, const string& value) {
-      progress();
-      if (writer != NULL) {
-        writer->emit(key, value);
-      } else if (partitioner != NULL) {
-        int part = partitioner->partition(key, numReduces);
-        uplink->partitionedOutput(part, key, value);
-      } else {
-        uplink->output(key, value);
-      }
-    }
-
-    /**
-     * Register a counter with the given group and name.
-     */
-    virtual Counter* getCounter(const std::string& group, 
-                               const std::string& name) {
-      int id = registeredCounterIds.size();
-      registeredCounterIds.push_back(id);
-      uplink->registerCounter(id, group, name);
-      return new Counter(id);
-    }
-
-    /**
-     * Increment the value of the counter with the given amount.
-     */
-    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
-      uplink->incrementCounter(counter, amount); 
-    }
-
-    void closeAll() {
-      if (reader) {
-        reader->close();
-      }
-      if (mapper) {
-        mapper->close();
-      }
-      if (reducer) {
-        reducer->close();
-      }
-      if (writer) {
-        writer->close();
-      }
-    }
-
-    virtual ~TaskContextImpl() {
-      delete jobConf;
-      delete inputKeyClass;
-      delete inputValueClass;
-      delete inputSplit;
-      if (reader) {
-        delete value;
-      }
-      delete reader;
-      delete mapper;
-      delete reducer;
-      delete writer;
-      delete partitioner;
-      pthread_mutex_destroy(&mutexDone);
-    }
-  };
-
-  /**
-   * Ping the parent every 5 seconds to know if it is alive 
-   */
-  void* ping(void* ptr) {
-    TaskContextImpl* context = (TaskContextImpl*) ptr;
-    char* portStr = getenv("mapreduce.pipes.command.port");
-    int MAX_RETRIES = 3;
-    int remaining_retries = MAX_RETRIES;
-    while (!context->isDone()) {
-      try{
-        sleep(5);
-        int sock = -1;
-        if (portStr) {
-          sock = socket(PF_INET, SOCK_STREAM, 0);
-          HADOOP_ASSERT(sock != - 1,
-                        string("problem creating socket: ") + strerror(errno));
-          sockaddr_in addr;
-          addr.sin_family = AF_INET;
-          addr.sin_port = htons(toInt(portStr));
-          addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
-          HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
-                        string("problem connecting command socket: ") +
-                        strerror(errno));
-
-        }
-        if (sock != -1) {
-          int result = shutdown(sock, SHUT_RDWR);
-          HADOOP_ASSERT(result == 0, "problem shutting socket");
-          result = close(sock);
-          HADOOP_ASSERT(result == 0, "problem closing socket");
-        }
-        remaining_retries = MAX_RETRIES;
-      } catch (Error& err) {
-        if (!context->isDone()) {
-          fprintf(stderr, "Hadoop Pipes Exception: in ping %s\n", 
-                err.getMessage().c_str());
-          remaining_retries -= 1;
-          if (remaining_retries == 0) {
-            exit(1);
-          }
-        } else {
-          return NULL;
-        }
-      }
-    }
-    return NULL;
-  }
-
-  /**
-   * Run the assigned task in the framework.
-   * The user's main function should set the various functions using the 
-   * set* functions above and then call this.
-   * @return true, if the task succeeded.
-   */
-  bool runTask(const Factory& factory) {
-    try {
-      TaskContextImpl* context = new TaskContextImpl(factory);
-      Protocol* connection;
-      char* portStr = getenv("mapreduce.pipes.command.port");
-      int sock = -1;
-      FILE* stream = NULL;
-      FILE* outStream = NULL;
-      char *bufin = NULL;
-      char *bufout = NULL;
-      if (portStr) {
-        sock = socket(PF_INET, SOCK_STREAM, 0);
-        HADOOP_ASSERT(sock != - 1,
-                      string("problem creating socket: ") + strerror(errno));
-        sockaddr_in addr;
-        addr.sin_family = AF_INET;
-        addr.sin_port = htons(toInt(portStr));
-        addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
-        HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
-                      string("problem connecting command socket: ") +
-                      strerror(errno));
-
-        stream = fdopen(sock, "r");
-        outStream = fdopen(sock, "w");
-
-        // increase buffer size
-        int bufsize = 128*1024;
-        int setbuf;
-        bufin = new char[bufsize];
-        bufout = new char[bufsize];
-        setbuf = setvbuf(stream, bufin, _IOFBF, bufsize);
-        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for inStream: ")
-                                     + strerror(errno));
-        setbuf = setvbuf(outStream, bufout, _IOFBF, bufsize);
-        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for outStream: ")
-                                     + strerror(errno));
-        connection = new BinaryProtocol(stream, context, outStream);
-      } else if (getenv("mapreduce.pipes.commandfile")) {
-        char* filename = getenv("mapreduce.pipes.commandfile");
-        string outFilename = filename;
-        outFilename += ".out";
-        stream = fopen(filename, "r");
-        outStream = fopen(outFilename.c_str(), "w");
-        connection = new BinaryProtocol(stream, context, outStream);
-      } else {
-        connection = new TextProtocol(stdin, context, stdout);
-      }
-      context->setProtocol(connection, connection->getUplink());
-      pthread_t pingThread;
-      pthread_create(&pingThread, NULL, ping, (void*)(context));
-      context->waitForTask();
-      while (!context->isDone()) {
-        context->nextKey();
-      }
-      context->closeAll();
-      connection->getUplink()->done();
-      pthread_join(pingThread,NULL);
-      delete context;
-      delete connection;
-      if (stream != NULL) {
-        fflush(stream);
-      }
-      if (outStream != NULL) {
-        fflush(outStream);
-      }
-      fflush(stdout);
-      if (sock != -1) {
-        int result = shutdown(sock, SHUT_RDWR);
-        HADOOP_ASSERT(result == 0, "problem shutting socket");
-        result = close(sock);
-        HADOOP_ASSERT(result == 0, "problem closing socket");
-      }
-      if (stream != NULL) {
-        //fclose(stream);
-      }
-      if (outStream != NULL) {
-        //fclose(outStream);
-      } 
-      delete bufin;
-      delete bufout;
-      return true;
-    } catch (Error& err) {
-      fprintf(stderr, "Hadoop Pipes Exception: %s\n", 
-              err.getMessage().c_str());
-      return false;
-    }
-  }
-}
-
diff --git a/hadoop-mapreduce-project/src/c++/utils/.autom4te.cfg b/hadoop-mapreduce-project/src/c++/utils/.autom4te.cfg
deleted file mode 100644
index d21d1c9..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/.autom4te.cfg
+++ /dev/null
@@ -1,42 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# autom4te configuration for hadoop utils library
-#
-
-begin-language: "Autoheader-preselections"
-args: --no-cache 
-end-language: "Autoheader-preselections"
-
-begin-language: "Automake-preselections"
-args: --no-cache 
-end-language: "Automake-preselections"
-
-begin-language: "Autoreconf-preselections"
-args: --no-cache 
-end-language: "Autoreconf-preselections"
-
-begin-language: "Autoconf-without-aclocal-m4"
-args: --no-cache 
-end-language: "Autoconf-without-aclocal-m4"
-
-begin-language: "Autoconf"
-args: --no-cache 
-end-language: "Autoconf"
-
diff --git a/hadoop-mapreduce-project/src/c++/utils/Makefile.am b/hadoop-mapreduce-project/src/c++/utils/Makefile.am
deleted file mode 100644
index d99ea14..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/Makefile.am
+++ /dev/null
@@ -1,33 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-ACLOCAL_AMFLAGS = -I m4
-AM_CXXFLAGS=-I$(srcdir)/api -Wall
-
-# List the api header files and where they will be installed
-apidir = $(includedir)/hadoop
-api_HEADERS = \
-	api/hadoop/StringUtils.hh \
-	api/hadoop/SerialUtils.hh
-
-
-# Define the libaries that need to be built
-lib_LIBRARIES = libhadooputils.a
-
-# Define the sources for lib 
-libhadooputils_a_SOURCES = \
-	impl/StringUtils.cc \
-	impl/SerialUtils.cc
-
diff --git a/hadoop-mapreduce-project/src/c++/utils/api/hadoop/SerialUtils.hh b/hadoop-mapreduce-project/src/c++/utils/api/hadoop/SerialUtils.hh
deleted file mode 100644
index cadfd76..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/api/hadoop/SerialUtils.hh
+++ /dev/null
@@ -1,170 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_SERIAL_UTILS_HH
-#define HADOOP_SERIAL_UTILS_HH
-
-#include <string>
-#include <stdint.h>
-
-namespace HadoopUtils {
-
-  /**
-   * A simple exception class that records a message for the user.
-   */
-  class Error {
-  private:
-    std::string error;
-  public:
-
-    /**
-     * Create an error object with the given message.
-     */
-    Error(const std::string& msg);
-
-    /**
-     * Construct an error object with the given message that was created on
-     * the given file, line, and functino.
-     */
-    Error(const std::string& msg, 
-          const std::string& file, int line, const std::string& function);
-
-    /**
-     * Get the error message.
-     */
-    const std::string& getMessage() const;
-  };
-
-  /**
-   * Check to make sure that the condition is true, and throw an exception
-   * if it is not. The exception will contain the message and a description
-   * of the source location.
-   */
-  #define HADOOP_ASSERT(CONDITION, MESSAGE) \
-    { \
-      if (!(CONDITION)) { \
-        throw HadoopUtils::Error((MESSAGE), __FILE__, __LINE__, \
-                                    __func__); \
-      } \
-    }
-
-  /**
-   * An interface for an input stream.
-   */
-  class InStream {
-  public:
-    /**
-     * Reads len bytes from the stream into the buffer.
-     * @param buf the buffer to read into
-     * @param buflen the length of the buffer
-     * @throws Error if there are problems reading
-     */
-    virtual void read(void *buf, size_t len) = 0;
-    virtual ~InStream() {}
-  };
-
-  /**
-   * An interface for an output stream.
-   */
-  class OutStream {
-  public:
-    /**
-     * Write the given buffer to the stream.
-     * @param buf the data to write
-     * @param len the number of bytes to write
-     * @throws Error if there are problems writing
-     */
-    virtual void write(const void *buf, size_t len) = 0;
-    /**
-     * Flush the data to the underlying store.
-     */
-    virtual void flush() = 0;
-    virtual ~OutStream() {}
-  };
-
-  /**
-   * A class to read a file as a stream.
-   */
-  class FileInStream : public InStream {
-  public:
-    FileInStream();
-    bool open(const std::string& name);
-    bool open(FILE* file);
-    void read(void *buf, size_t buflen);
-    bool skip(size_t nbytes);
-    bool close();
-    virtual ~FileInStream();
-  private:
-    /**
-     * The file to write to.
-     */
-    FILE *mFile;
-    /**
-     * Does is this class responsible for closing the FILE*?
-     */
-    bool isOwned;
-  };
-
-  /**
-   * A class to write a stream to a file.
-   */
-  class FileOutStream: public OutStream {
-  public:
-
-    /**
-     * Create a stream that isn't bound to anything.
-     */
-    FileOutStream();
-
-    /**
-     * Create the given file, potentially overwriting an existing file.
-     */
-    bool open(const std::string& name, bool overwrite);
-    bool open(FILE* file);
-    void write(const void* buf, size_t len);
-    bool advance(size_t nbytes);
-    void flush();
-    bool close();
-    virtual ~FileOutStream();
-  private:
-    FILE *mFile;
-    bool isOwned;
-  };
-
-  /**
-   * A stream that reads from a string.
-   */
-  class StringInStream: public InStream {
-  public:
-    StringInStream(const std::string& str);
-    virtual void read(void *buf, size_t buflen);
-  private:
-    const std::string& buffer;
-    std::string::const_iterator itr;
-  };
-
-  void serializeInt(int32_t t, OutStream& stream);
-  int32_t deserializeInt(InStream& stream);
-  void serializeLong(int64_t t, OutStream& stream);
-  int64_t deserializeLong(InStream& stream);
-  void serializeFloat(float t, OutStream& stream);
-  float deserializeFloat(InStream& stream);
-  void serializeString(const std::string& t, OutStream& stream);
-  void deserializeString(std::string& t, InStream& stream);
-}
-
-#endif
diff --git a/hadoop-mapreduce-project/src/c++/utils/api/hadoop/StringUtils.hh b/hadoop-mapreduce-project/src/c++/utils/api/hadoop/StringUtils.hh
deleted file mode 100644
index 4720172..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/api/hadoop/StringUtils.hh
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_STRING_UTILS_HH
-#define HADOOP_STRING_UTILS_HH
-
-#include <stdint.h>
-#include <string>
-#include <vector>
-
-namespace HadoopUtils {
-
-  /**
-   * Convert an integer to a string.
-   */
-  std::string toString(int32_t x);
-
-  /**
-   * Convert a string to an integer.
-   * @throws Error if the string is not a valid integer
-   */
-  int32_t toInt(const std::string& val);
-
-  /**
-   * Convert the string to a float.
-   * @throws Error if the string is not a valid float
-   */
-  float toFloat(const std::string& val);
-
-  /**
-   * Convert the string to a boolean.
-   * @throws Error if the string is not a valid boolean value
-   */
-  bool toBool(const std::string& val);
-
-  /**
-   * Get the current time in the number of milliseconds since 1970.
-   */
-  uint64_t getCurrentMillis();
-
-  /**
-   * Split a string into "words". Multiple deliminators are treated as a single
-   * word break, so no zero-length words are returned.
-   * @param str the string to split
-   * @param separator a list of characters that divide words
-   */
-  std::vector<std::string> splitString(const std::string& str,
-                                       const char* separator);
-
-  /**
-   * Quote a string to avoid "\", non-printable characters, and the 
-   * deliminators.
-   * @param str the string to quote
-   * @param deliminators the set of characters to always quote
-   */
-  std::string quoteString(const std::string& str,
-                          const char* deliminators);
-
-  /**
-   * Unquote the given string to return the original string.
-   * @param str the string to unquote
-   */
-  std::string unquoteString(const std::string& str);
-
-}
-
-#endif
diff --git a/hadoop-mapreduce-project/src/c++/utils/configure.ac b/hadoop-mapreduce-project/src/c++/utils/configure.ac
deleted file mode 100644
index 9283181..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/configure.ac
+++ /dev/null
@@ -1,56 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-#                                               -*- Autoconf -*-
-# Process this file with autoconf to produce a configure script.
-
-AC_PREREQ(2.59)
-AC_INIT(hadoop-utils, 0.13.0, omalley@apache.org)
-
-AC_CONFIG_AUX_DIR([config])
-AC_CONFIG_MACRO_DIR([m4])
-
-AM_INIT_AUTOMAKE([subdir-objects foreign no-dist])
-
-AC_CONFIG_SRCDIR([impl/SerialUtils.cc])
-AC_CONFIG_HEADER([impl/config.h])
-AC_CONFIG_FILES([Makefile])
-
-AC_PREFIX_DEFAULT(`pwd`/../install)
-
-CHECK_INSTALL_CFLAG
-HADOOP_UTILS_SETUP
-
-# Checks for programs.
-AC_PROG_CXX
-AC_PROG_LIBTOOL
-
-# Checks for libraries.
-
-# Checks for header files.
-AC_LANG(C++)
-AC_CHECK_HEADERS([unistd.h])
-
-# Checks for typedefs, structures, and compiler characteristics.
-AC_HEADER_STDBOOL
-AC_C_CONST
-AC_TYPE_OFF_T
-AC_TYPE_SIZE_T
-AC_FUNC_STRERROR_R
-
-# Checks for library functions.
-AC_CHECK_FUNCS([mkdir uname])
-AC_OUTPUT
diff --git a/hadoop-mapreduce-project/src/c++/utils/impl/SerialUtils.cc b/hadoop-mapreduce-project/src/c++/utils/impl/SerialUtils.cc
deleted file mode 100644
index 03d009b..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/impl/SerialUtils.cc
+++ /dev/null
@@ -1,294 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#include "hadoop/SerialUtils.hh"
-#include "hadoop/StringUtils.hh"
-
-#include <errno.h>
-#include <rpc/types.h>
-#include <rpc/xdr.h>
-#include <string>
-#include <string.h>
-
-using std::string;
-
-namespace HadoopUtils {
-
-  Error::Error(const std::string& msg): error(msg) {
-  }
-
-  Error::Error(const std::string& msg, 
-               const std::string& file, int line, 
-               const std::string& function) {
-    error = msg + " at " + file + ":" + toString(line) + 
-            " in " + function;
-  }
-
-  const std::string& Error::getMessage() const {
-    return error;
-  }
-
-  FileInStream::FileInStream()
-  {
-    mFile = NULL;
-    isOwned = false;
-  }
-
-  bool FileInStream::open(const std::string& name)
-  {
-    mFile = fopen(name.c_str(), "rb");
-    isOwned = true;
-    return (mFile != NULL);
-  }
-
-  bool FileInStream::open(FILE* file)
-  {
-    mFile = file;
-    isOwned = false;
-    return (mFile != NULL);
-  }
-
-  void FileInStream::read(void *buf, size_t len)
-  {
-    size_t result = fread(buf, len, 1, mFile);
-    if (result == 0) {
-      if (feof(mFile)) {
-        HADOOP_ASSERT(false, "end of file");
-      } else {
-        HADOOP_ASSERT(false, string("read error on file: ") + strerror(errno));
-      }
-    }
-  }
-
-  bool FileInStream::skip(size_t nbytes)
-  {
-    return (0==fseek(mFile, nbytes, SEEK_CUR));
-  }
-
-  bool FileInStream::close()
-  {
-    int ret = 0;
-    if (mFile != NULL && isOwned) {
-      ret = fclose(mFile);
-    }
-    mFile = NULL;
-    return (ret==0);
-  }
-
-  FileInStream::~FileInStream()
-  {
-    if (mFile != NULL) {
-      close();
-    }
-  }
-
-  FileOutStream::FileOutStream()
-  {
-    mFile = NULL;
-    isOwned = false;
-  }
-
-  bool FileOutStream::open(const std::string& name, bool overwrite)
-  {
-    if (!overwrite) {
-      mFile = fopen(name.c_str(), "rb");
-      if (mFile != NULL) {
-        fclose(mFile);
-        return false;
-      }
-    }
-    mFile = fopen(name.c_str(), "wb");
-    isOwned = true;
-    return (mFile != NULL);
-  }
-
-  bool FileOutStream::open(FILE* file)
-  {
-    mFile = file;
-    isOwned = false;
-    return (mFile != NULL);
-  }
-
-  void FileOutStream::write(const void* buf, size_t len)
-  {
-    size_t result = fwrite(buf, len, 1, mFile);
-    HADOOP_ASSERT(result == 1,
-                  string("write error to file: ") + strerror(errno));
-  }
-
-  bool FileOutStream::advance(size_t nbytes)
-  {
-    return (0==fseek(mFile, nbytes, SEEK_CUR));
-  }
-
-  bool FileOutStream::close()
-  {
-    int ret = 0;
-    if (mFile != NULL && isOwned) {
-      ret = fclose(mFile);
-    }
-    mFile = NULL;
-    return (ret == 0);
-  }
-
-  void FileOutStream::flush()
-  {
-    fflush(mFile);
-  }
-
-  FileOutStream::~FileOutStream()
-  {
-    if (mFile != NULL) {
-      close();
-    }
-  }
-
-  StringInStream::StringInStream(const std::string& str): buffer(str) {
-    itr = buffer.begin();
-  }
-
-  void StringInStream::read(void *buf, size_t buflen) {
-    size_t bytes = 0;
-    char* output = (char*) buf;
-    std::string::const_iterator end = buffer.end();
-    while (bytes < buflen) {
-      output[bytes++] = *itr;
-      ++itr;
-      if (itr == end) {
-        break;
-      }
-    }
-    HADOOP_ASSERT(bytes == buflen, "unexpected end of string reached");
-  }
-
-  void serializeInt(int32_t t, OutStream& stream) {
-    serializeLong(t,stream);
-  }
-
-  void serializeLong(int64_t t, OutStream& stream)
-  {
-    if (t >= -112 && t <= 127) {
-      int8_t b = t;
-      stream.write(&b, 1);
-      return;
-    }
-        
-    int8_t len = -112;
-    if (t < 0) {
-      t ^= -1ll; // reset the sign bit
-      len = -120;
-    }
-        
-    uint64_t tmp = t;
-    while (tmp != 0) {
-      tmp = tmp >> 8;
-      len--;
-    }
-  
-    stream.write(&len, 1);      
-    len = (len < -120) ? -(len + 120) : -(len + 112);
-        
-    for (uint32_t idx = len; idx != 0; idx--) {
-      uint32_t shiftbits = (idx - 1) * 8;
-      uint64_t mask = 0xFFll << shiftbits;
-      uint8_t b = (t & mask) >> shiftbits;
-      stream.write(&b, 1);
-    }
-  }
-
-  int32_t deserializeInt(InStream& stream) {
-    return deserializeLong(stream);
-  }
-
-  int64_t deserializeLong(InStream& stream)
-  {
-    int8_t b;
-    stream.read(&b, 1);
-    if (b >= -112) {
-      return b;
-    }
-    bool negative;
-    int len;
-    if (b < -120) {
-      negative = true;
-      len = -120 - b;
-    } else {
-      negative = false;
-      len = -112 - b;
-    }
-    uint8_t barr[len];
-    stream.read(barr, len);
-    int64_t t = 0;
-    for (int idx = 0; idx < len; idx++) {
-      t = t << 8;
-      t |= (barr[idx] & 0xFF);
-    }
-    if (negative) {
-      t ^= -1ll;
-    }
-    return t;
-  }
-
-  void serializeFloat(float t, OutStream& stream)
-  {
-    char buf[sizeof(float)];
-    XDR xdrs;
-    xdrmem_create(&xdrs, buf, sizeof(float), XDR_ENCODE);
-    xdr_float(&xdrs, &t);
-    stream.write(buf, sizeof(float));
-  }
-
-  void deserializeFloat(float& t, InStream& stream)
-  {
-    char buf[sizeof(float)];
-    stream.read(buf, sizeof(float));
-    XDR xdrs;
-    xdrmem_create(&xdrs, buf, sizeof(float), XDR_DECODE);
-    xdr_float(&xdrs, &t);
-  }
-
-  void serializeString(const std::string& t, OutStream& stream)
-  {
-    serializeInt(t.length(), stream);
-    if (t.length() > 0) {
-      stream.write(t.data(), t.length());
-    }
-  }
-
-  void deserializeString(std::string& t, InStream& stream)
-  {
-    int32_t len = deserializeInt(stream);
-    if (len > 0) {
-      // resize the string to the right length
-      t.resize(len);
-      // read into the string in 64k chunks
-      const int bufSize = 65536;
-      int offset = 0;
-      char buf[bufSize];
-      while (len > 0) {
-        int chunkLength = len > bufSize ? bufSize : len;
-        stream.read(buf, chunkLength);
-        t.replace(offset, chunkLength, buf, chunkLength);
-        offset += chunkLength;
-        len -= chunkLength;
-      }
-    } else {
-      t.clear();
-    }
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/c++/utils/impl/StringUtils.cc b/hadoop-mapreduce-project/src/c++/utils/impl/StringUtils.cc
deleted file mode 100644
index 0d2a878..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/impl/StringUtils.cc
+++ /dev/null
@@ -1,180 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#include "hadoop/StringUtils.hh"
-#include "hadoop/SerialUtils.hh"
-
-#include <errno.h>
-#include <stdint.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <strings.h>
-#include <sys/time.h>
-
-using std::string;
-using std::vector;
-
-namespace HadoopUtils {
-
-  string toString(int32_t x) {
-    char str[100];
-    sprintf(str, "%d", x);
-    return str;
-  }
-
-  int toInt(const string& val) {
-    int result;
-    char trash;
-    int num = sscanf(val.c_str(), "%d%c", &result, &trash);
-    HADOOP_ASSERT(num == 1,
-                  "Problem converting " + val + " to integer.");
-    return result;
-  }
-
-  float toFloat(const string& val) {
-    float result;
-    char trash;
-    int num = sscanf(val.c_str(), "%f%c", &result, &trash);
-    HADOOP_ASSERT(num == 1,
-                  "Problem converting " + val + " to float.");
-    return result;
-  }
-
-  bool toBool(const string& val) {
-    if (val == "true") {
-      return true;
-    } else if (val == "false") {
-      return false;
-    } else {
-      HADOOP_ASSERT(false,
-                    "Problem converting " + val + " to boolean.");
-    }
-  }
-
-  /**
-   * Get the current time in the number of milliseconds since 1970.
-   */
-  uint64_t getCurrentMillis() {
-    struct timeval tv;
-    struct timezone tz;
-    int sys = gettimeofday(&tv, &tz);
-    HADOOP_ASSERT(sys != -1, strerror(errno));
-    return tv.tv_sec * 1000 + tv.tv_usec / 1000;
-  }
-
-  vector<string> splitString(const std::string& str,
-			     const char* separator) {
-    vector<string> result;
-    string::size_type prev_pos=0;
-    string::size_type pos=0;
-    while ((pos = str.find_first_of(separator, prev_pos)) != string::npos) {
-      if (prev_pos < pos) {
-	result.push_back(str.substr(prev_pos, pos-prev_pos));
-      }
-      prev_pos = pos + 1;
-    }
-    if (prev_pos < str.size()) {
-      result.push_back(str.substr(prev_pos));
-    }
-    return result;
-  }
-
-  string quoteString(const string& str,
-                     const char* deliminators) {
-    
-    string result(str);
-    for(int i=result.length() -1; i >= 0; --i) {
-      char ch = result[i];
-      if (!isprint(ch) ||
-          ch == '\\' || 
-          strchr(deliminators, ch)) {
-        switch (ch) {
-        case '\\':
-          result.replace(i, 1, "\\\\");
-          break;
-        case '\t':
-          result.replace(i, 1, "\\t");
-          break;
-        case '\n':
-          result.replace(i, 1, "\\n");
-          break;
-        case ' ':
-          result.replace(i, 1, "\\s");
-          break;
-        default:
-          char buff[4];
-          sprintf(buff, "\\%02x", static_cast<unsigned char>(result[i]));
-          result.replace(i, 1, buff);
-        }
-      }
-    }
-    return result;
-  }
-
-  string unquoteString(const string& str) {
-    string result(str);
-    string::size_type current = result.find('\\');
-    while (current != string::npos) {
-      if (current + 1 < result.size()) {
-        char new_ch;
-        int num_chars;
-        if (isxdigit(result[current+1])) {
-          num_chars = 2;
-          HADOOP_ASSERT(current + num_chars < result.size(),
-                     "escape pattern \\<hex><hex> is missing second digit in '"
-                     + str + "'");
-          char sub_str[3];
-          sub_str[0] = result[current+1];
-          sub_str[1] = result[current+2];
-          sub_str[2] = '\0';
-          char* end_ptr = NULL;
-          long int int_val = strtol(sub_str, &end_ptr, 16);
-          HADOOP_ASSERT(*end_ptr == '\0' && int_val >= 0,
-                     "escape pattern \\<hex><hex> is broken in '" + str + "'");
-          new_ch = static_cast<char>(int_val);
-        } else {
-          num_chars = 1;
-          switch(result[current+1]) {
-          case '\\':
-            new_ch = '\\';
-            break;
-          case 't':
-            new_ch = '\t';
-            break;
-          case 'n':
-            new_ch = '\n';
-            break;
-          case 's':
-            new_ch = ' ';
-            break;
-          default:
-            string msg("unknow n escape character '");
-            msg += result[current+1];
-            HADOOP_ASSERT(false, msg + "' found in '" + str + "'");
-          }
-        }
-        result.replace(current, 1 + num_chars, 1, new_ch);
-        current = result.find('\\', current+1);
-      } else {
-        HADOOP_ASSERT(false, "trailing \\ in '" + str + "'");
-      }
-    }
-    return result;
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/c++/utils/m4/hadoop_utils.m4 b/hadoop-mapreduce-project/src/c++/utils/m4/hadoop_utils.m4
deleted file mode 100644
index d0ed6c4..0000000
--- a/hadoop-mapreduce-project/src/c++/utils/m4/hadoop_utils.m4
+++ /dev/null
@@ -1,68 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# hadoop_utils.m4
-
-# Check to see if the install program supports -C
-# If so, use "install -C" for the headers. Otherwise, every install
-# updates the timestamps on the installed headers, which causes a recompilation
-# of any downstream libraries.
-AC_DEFUN([CHECK_INSTALL_CFLAG],[
-AC_REQUIRE([AC_PROG_INSTALL])
-touch foo
-if $INSTALL -C foo bar; then
-  INSTALL_DATA="$INSTALL_DATA -C"
-fi
-rm -f foo bar
-])
-
-# Set up the things we need for compiling hadoop utils
-AC_DEFUN([HADOOP_UTILS_SETUP],[
-AC_REQUIRE([AC_GNU_SOURCE])
-AC_REQUIRE([AC_SYS_LARGEFILE])
-])
-
-# define a macro for using hadoop utils
-AC_DEFUN([USE_HADOOP_UTILS],[
-AC_REQUIRE([HADOOP_UTILS_SETUP])
-AC_ARG_WITH([hadoop-utils],
-            AS_HELP_STRING([--with-hadoop-utils=<dir>],
-                           [directory to get hadoop_utils from]),
-            [HADOOP_UTILS_PREFIX="$withval"],
-            [HADOOP_UTILS_PREFIX="\${prefix}"])
-AC_SUBST(HADOOP_UTILS_PREFIX)
-])
-
-AC_DEFUN([HADOOP_PIPES_SETUP],[
-AC_CHECK_HEADERS([pthread.h], [], 
-  AC_MSG_ERROR(Please check if you have installed the pthread library)) 
-AC_CHECK_LIB([pthread], [pthread_create], [], 
-  AC_MSG_ERROR(Cannot find libpthread.so, please check))
-AC_CHECK_LIB([crypto], [HMAC_Init], [], 
-  AC_MSG_ERROR(Cannot find libcrypto.so, please check))
-])
-
-# define a macro for using hadoop pipes
-AC_DEFUN([USE_HADOOP_PIPES],[
-AC_REQUIRE([USE_HADOOP_UTILS])
-AC_REQUIRE([HADOOP_PIPES_SETUP])
-AC_ARG_WITH([hadoop-pipes],
-            AS_HELP_STRING([--with-hadoop-pipes=<dir>],
-                           [directory to get hadoop pipes from]),
-            [HADOOP_PIPES_PREFIX="$withval"],
-            [HADOOP_PIPES_PREFIX="\${prefix}"])
-AC_SUBST(HADOOP_PIPES_PREFIX)
-])
diff --git a/hadoop-mapreduce-project/src/examples/pipes/.autom4te.cfg b/hadoop-mapreduce-project/src/examples/pipes/.autom4te.cfg
deleted file mode 100644
index d21d1c9..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/.autom4te.cfg
+++ /dev/null
@@ -1,42 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# autom4te configuration for hadoop utils library
-#
-
-begin-language: "Autoheader-preselections"
-args: --no-cache 
-end-language: "Autoheader-preselections"
-
-begin-language: "Automake-preselections"
-args: --no-cache 
-end-language: "Automake-preselections"
-
-begin-language: "Autoreconf-preselections"
-args: --no-cache 
-end-language: "Autoreconf-preselections"
-
-begin-language: "Autoconf-without-aclocal-m4"
-args: --no-cache 
-end-language: "Autoconf-without-aclocal-m4"
-
-begin-language: "Autoconf"
-args: --no-cache 
-end-language: "Autoconf"
-
diff --git a/hadoop-mapreduce-project/src/examples/pipes/Makefile.am b/hadoop-mapreduce-project/src/examples/pipes/Makefile.am
deleted file mode 100644
index 731ab1e..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/Makefile.am
+++ /dev/null
@@ -1,36 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-ACLOCAL_AMFLAGS = -I ../../c++/utils/m4
-AM_CXXFLAGS=-Wall -I$(HADOOP_UTILS_PREFIX)/include \
-            -I$(HADOOP_PIPES_PREFIX)/include
-LDADD=-L$(HADOOP_UTILS_PREFIX)/lib -L$(HADOOP_PIPES_PREFIX)/lib \
-      -lhadooppipes -lhadooputils
-
-bin_PROGRAMS= wordcount-simple wordcount-part wordcount-nopipe pipes-sort
-
-# Define the sources for each program
-wordcount_simple_SOURCES = \
-	impl/wordcount-simple.cc
-
-wordcount_part_SOURCES = \
-	impl/wordcount-part.cc
-
-wordcount_nopipe_SOURCES = \
-	impl/wordcount-nopipe.cc
-
-pipes_sort_SOURCES = \
-        impl/sort.cc
-
diff --git a/hadoop-mapreduce-project/src/examples/pipes/README.txt b/hadoop-mapreduce-project/src/examples/pipes/README.txt
deleted file mode 100644
index 4685304..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/README.txt
+++ /dev/null
@@ -1,16 +0,0 @@
-To run the examples, first compile them:
-
-% ant -Dcompile.c++=yes examples
-
-and then copy the binaries to dfs:
-
-% bin/hadoop fs -put build/c++-examples/Linux-i386-32/bin /examples/bin
-
-create an input directory with text files:
-
-% bin/hadoop fs -put my-data in-dir
-
-and run the word count example:
-
-% bin/hadoop pipes -conf src/examples/pipes/conf/word.xml \
-                   -input in-dir -output out-dir
diff --git a/hadoop-mapreduce-project/src/examples/pipes/conf/word-part.xml b/hadoop-mapreduce-project/src/examples/pipes/conf/word-part.xml
deleted file mode 100644
index b552a1c..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/conf/word-part.xml
+++ /dev/null
@@ -1,24 +0,0 @@
-<?xml version="1.0"?>
-<configuration>
-
-<property>
-  <name>mapreduce.job.reduces</name>
-  <value>2</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.executable</name>
-  <value>hdfs:/examples/bin/wordcount-part</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordreader</name>
-  <value>true</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordwriter</name>
-  <value>true</value>
-</property>
-
-</configuration>
diff --git a/hadoop-mapreduce-project/src/examples/pipes/conf/word.xml b/hadoop-mapreduce-project/src/examples/pipes/conf/word.xml
deleted file mode 100644
index ed727dd..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/conf/word.xml
+++ /dev/null
@@ -1,28 +0,0 @@
-<?xml version="1.0"?>
-<configuration>
-
-<property>
-  <name>mapreduce.job.reduces</name>
-  <value>2</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.executable</name>
-  <value>/examples/bin/wordcount-simple#wordcount-simple</value>
-  <description> Executable path is given as "path#executable-name"
-                sothat the executable will have a symlink in working directory.
-                This can be used for gdb debugging etc.
-  </description>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordreader</name>
-  <value>true</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordwriter</name>
-  <value>true</value>
-</property>
-
-</configuration>
diff --git a/hadoop-mapreduce-project/src/examples/pipes/configure.ac b/hadoop-mapreduce-project/src/examples/pipes/configure.ac
deleted file mode 100644
index 7959ba6..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/configure.ac
+++ /dev/null
@@ -1,58 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-#                                               -*- Autoconf -*-
-# Process this file with autoconf to produce a configure script.
-
-AC_PREREQ(2.59)
-AC_INIT(hadoop-pipes-examples, 0.13.0, omalley@apache.org)
-
-AC_CONFIG_AUX_DIR([config])
-AC_CONFIG_MACRO_DIR([../../c++/utils/m4])
-
-AM_INIT_AUTOMAKE([subdir-objects foreign no-dist])
-
-AC_CONFIG_SRCDIR([impl/wordcount-simple.cc])
-AC_CONFIG_HEADER([impl/config.h])
-AC_CONFIG_FILES([Makefile])
-
-AC_PREFIX_DEFAULT(`pwd`/../install)
-
-USE_HADOOP_PIPES
-
-# Checks for programs.
-AC_PROG_CXX
-AC_PROG_INSTALL
-AC_PROG_LIBTOOL
-
-# Checks for libraries.
-
-# Checks for header files.
-AC_LANG(C++)
-AC_CHECK_HEADERS([unistd.h])
-
-# Checks for typedefs, structures, and compiler characteristics.
-AC_HEADER_STDBOOL
-AC_C_CONST
-AC_TYPE_OFF_T
-AC_TYPE_SIZE_T
-AC_FUNC_STRERROR_R
-
-# Checks for library functions.
-AC_CHECK_FUNCS([mkdir uname])
-AC_CHECK_LIB([socket],[shutdown])
-AC_CHECK_LIB([nsl],[xdr_float])
-AC_OUTPUT
diff --git a/hadoop-mapreduce-project/src/examples/pipes/impl/sort.cc b/hadoop-mapreduce-project/src/examples/pipes/impl/sort.cc
deleted file mode 100644
index 529d6fb..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/impl/sort.cc
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-
-class SortMap: public HadoopPipes::Mapper {
-private:
-  /* the fraction 0.0 to 1.0 of records to keep */
-  float keepFraction;
-  /* the number of records kept so far */
-  long long keptRecords;
-  /* the total number of records */
-  long long totalRecords;
-  static const std::string MAP_KEEP_PERCENT;
-public:
-  /*
-   * Look in the config to find the fraction of records to keep.
-   */
-  SortMap(HadoopPipes::TaskContext& context){
-    const HadoopPipes::JobConf* conf = context.getJobConf();
-    if (conf->hasKey(MAP_KEEP_PERCENT)) {
-      keepFraction = conf->getFloat(MAP_KEEP_PERCENT) / 100.0;
-    } else {
-      keepFraction = 1.0;
-    }
-    keptRecords = 0;
-    totalRecords = 0;
-  }
-
-  void map(HadoopPipes::MapContext& context) {
-    totalRecords += 1;
-    while ((float) keptRecords / totalRecords < keepFraction) {
-      keptRecords += 1;
-      context.emit(context.getInputKey(), context.getInputValue());
-    }
-  }
-};
-
-const std::string SortMap::MAP_KEEP_PERCENT("mapreduce.loadgen.sort.map.preserve.percent");
-
-class SortReduce: public HadoopPipes::Reducer {
-private:
-  /* the fraction 0.0 to 1.0 of records to keep */
-  float keepFraction;
-  /* the number of records kept so far */
-  long long keptRecords;
-  /* the total number of records */
-  long long totalRecords;
-  static const std::string REDUCE_KEEP_PERCENT;
-public:
-  SortReduce(HadoopPipes::TaskContext& context){
-    const HadoopPipes::JobConf* conf = context.getJobConf();
-    if (conf->hasKey(REDUCE_KEEP_PERCENT)) {
-      keepFraction = conf->getFloat(REDUCE_KEEP_PERCENT) / 100.0;
-    } else {
-      keepFraction = 1.0;
-    }
-    keptRecords = 0;
-    totalRecords = 0;
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    while (context.nextValue()) {
-      totalRecords += 1;
-      while ((float) keptRecords / totalRecords < keepFraction) {
-        keptRecords += 1;
-        context.emit(context.getInputKey(), context.getInputValue());
-      }
-    }
-  }
-};
-
-const std::string 
-  SortReduce::REDUCE_KEEP_PERCENT("mapreduce.loadgen.sort.reduce.preserve.percent");
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<SortMap,
-                                                           SortReduce>());
-}
-
diff --git a/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-nopipe.cc b/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-nopipe.cc
deleted file mode 100644
index a5fe731..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-nopipe.cc
+++ /dev/null
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-#include "hadoop/StringUtils.hh"
-#include "hadoop/SerialUtils.hh"
-
-#include <stdio.h>
-#include <sys/types.h>
-#include <sys/stat.h>
-
-const std::string WORDCOUNT = "WORDCOUNT";
-const std::string INPUT_WORDS = "INPUT_WORDS";
-const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
-
-class WordCountMap: public HadoopPipes::Mapper {
-public:
-  HadoopPipes::TaskContext::Counter* inputWords;
-  
-  WordCountMap(HadoopPipes::TaskContext& context) {
-    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
-  }
-  
-  void map(HadoopPipes::MapContext& context) {
-    std::vector<std::string> words = 
-      HadoopUtils::splitString(context.getInputValue(), " ");
-    for(unsigned int i=0; i < words.size(); ++i) {
-      context.emit(words[i], "1");
-    }
-    context.incrementCounter(inputWords, words.size());
-  }
-};
-
-class WordCountReduce: public HadoopPipes::Reducer {
-public:
-  HadoopPipes::TaskContext::Counter* outputWords;
-
-  WordCountReduce(HadoopPipes::TaskContext& context) {
-    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    int sum = 0;
-    while (context.nextValue()) {
-      sum += HadoopUtils::toInt(context.getInputValue());
-    }
-    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
-    context.incrementCounter(outputWords, 1); 
-  }
-};
-
-class WordCountReader: public HadoopPipes::RecordReader {
-private:
-  int64_t bytesTotal;
-  int64_t bytesRead;
-  FILE* file;
-public:
-  WordCountReader(HadoopPipes::MapContext& context) {
-    std::string filename;
-    HadoopUtils::StringInStream stream(context.getInputSplit());
-    HadoopUtils::deserializeString(filename, stream);
-    struct stat statResult;
-    stat(filename.c_str(), &statResult);
-    bytesTotal = statResult.st_size;
-    bytesRead = 0;
-    file = fopen(filename.c_str(), "rt");
-    HADOOP_ASSERT(file != NULL, "failed to open " + filename);
-  }
-
-  ~WordCountReader() {
-    fclose(file);
-  }
-
-  virtual bool next(std::string& key, std::string& value) {
-    key = HadoopUtils::toString(ftell(file));
-    int ch = getc(file);
-    bytesRead += 1;
-    value.clear();
-    while (ch != -1 && ch != '\n') {
-      value += ch;
-      ch = getc(file);
-      bytesRead += 1;
-    }
-    return ch != -1;
-  }
-
-  /**
-   * The progress of the record reader through the split as a value between
-   * 0.0 and 1.0.
-   */
-  virtual float getProgress() {
-    if (bytesTotal > 0) {
-      return (float)bytesRead / bytesTotal;
-    } else {
-      return 1.0f;
-    }
-  }
-};
-
-class WordCountWriter: public HadoopPipes::RecordWriter {
-private:
-  FILE* file;
-public:
-  WordCountWriter(HadoopPipes::ReduceContext& context) {
-    const HadoopPipes::JobConf* job = context.getJobConf();
-    int part = job->getInt("mapreduce.task.partition");
-    std::string outDir = job->get("mapreduce.task.output.dir");
-    // remove the file: schema substring
-    std::string::size_type posn = outDir.find(":");
-    HADOOP_ASSERT(posn != std::string::npos, 
-                  "no schema found in output dir: " + outDir);
-    outDir.erase(0, posn+1);
-    mkdir(outDir.c_str(), 0777);
-    std::string outFile = outDir + "/part-" + HadoopUtils::toString(part);
-    file = fopen(outFile.c_str(), "wt");
-    HADOOP_ASSERT(file != NULL, "can't open file for writing: " + outFile);
-  }
-
-  ~WordCountWriter() {
-    fclose(file);
-  }
-
-  void emit(const std::string& key, const std::string& value) {
-    fprintf(file, "%s -> %s\n", key.c_str(), value.c_str());
-  }
-};
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
-                              WordCountReduce, void, void, WordCountReader,
-                              WordCountWriter>());
-}
-
diff --git a/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-part.cc b/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-part.cc
deleted file mode 100644
index 37dc199..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-part.cc
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-#include "hadoop/StringUtils.hh"
-
-const std::string WORDCOUNT = "WORDCOUNT";
-const std::string INPUT_WORDS = "INPUT_WORDS";
-const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
-
-class WordCountMap: public HadoopPipes::Mapper {
-public:
-  HadoopPipes::TaskContext::Counter* inputWords;
-  
-  WordCountMap(HadoopPipes::TaskContext& context) {
-    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
-  }
-  
-  void map(HadoopPipes::MapContext& context) {
-    std::vector<std::string> words = 
-      HadoopUtils::splitString(context.getInputValue(), " ");
-    for(unsigned int i=0; i < words.size(); ++i) {
-      context.emit(words[i], "1");
-    }
-    context.incrementCounter(inputWords, words.size());
-  }
-};
-
-class WordCountReduce: public HadoopPipes::Reducer {
-public:
-  HadoopPipes::TaskContext::Counter* outputWords;
-
-  WordCountReduce(HadoopPipes::TaskContext& context) {
-    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    int sum = 0;
-    while (context.nextValue()) {
-      sum += HadoopUtils::toInt(context.getInputValue());
-    }
-    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
-    context.incrementCounter(outputWords, 1); 
-  }
-};
-
-class WordCountPartitioner: public HadoopPipes::Partitioner {
-public:
-  WordCountPartitioner(HadoopPipes::TaskContext& context){}
-  virtual int partition(const std::string& key, int numOfReduces) {
-    return 0;
-  }
-};
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
-                              WordCountReduce,WordCountPartitioner,
-                              WordCountReduce>());
-}
-
diff --git a/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-simple.cc b/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-simple.cc
deleted file mode 100644
index 64dd801..0000000
--- a/hadoop-mapreduce-project/src/examples/pipes/impl/wordcount-simple.cc
+++ /dev/null
@@ -1,67 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-#include "hadoop/StringUtils.hh"
-
-const std::string WORDCOUNT = "WORDCOUNT";
-const std::string INPUT_WORDS = "INPUT_WORDS";
-const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
-
-class WordCountMap: public HadoopPipes::Mapper {
-public:
-  HadoopPipes::TaskContext::Counter* inputWords;
-  
-  WordCountMap(HadoopPipes::TaskContext& context) {
-    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
-  }
-  
-  void map(HadoopPipes::MapContext& context) {
-    std::vector<std::string> words = 
-      HadoopUtils::splitString(context.getInputValue(), " ");
-    for(unsigned int i=0; i < words.size(); ++i) {
-      context.emit(words[i], "1");
-    }
-    context.incrementCounter(inputWords, words.size());
-  }
-};
-
-class WordCountReduce: public HadoopPipes::Reducer {
-public:
-  HadoopPipes::TaskContext::Counter* outputWords;
-
-  WordCountReduce(HadoopPipes::TaskContext& context) {
-    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    int sum = 0;
-    while (context.nextValue()) {
-      sum += HadoopUtils::toInt(context.getInputValue());
-    }
-    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
-    context.incrementCounter(outputWords, 1); 
-  }
-};
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
-                              WordCountReduce>());
-}
-
diff --git a/hadoop-tools/hadoop-pipes/pom.xml b/hadoop-tools/hadoop-pipes/pom.xml
new file mode 100644
index 0000000..d80ea73
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/pom.xml
@@ -0,0 +1,108 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<project>
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.hadoop</groupId>
+    <artifactId>hadoop-project</artifactId>
+    <version>2.0.0-cdh4.1.0-SNAPSHOT</version>
+    <relativePath>../../hadoop-project</relativePath>
+  </parent>
+  <groupId>org.apache.hadoop</groupId>
+  <artifactId>hadoop-pipes</artifactId>
+  <version>2.0.0-cdh4.1.0-SNAPSHOT</version>
+  <description>Apache Hadoop Pipes</description>
+  <name>Apache Hadoop Pipes</name>
+  <packaging>pom</packaging>
+
+  <properties>
+    <hadoop.log.dir>${project.build.directory}/log</hadoop.log.dir>
+  </properties>
+
+  <profiles>
+    <profile>
+      <id>native</id>
+      <activation>
+        <activeByDefault>false</activeByDefault>
+      </activation>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-antrun-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>make</id>
+                <phase>compile</phase>
+                <goals><goal>run</goal></goals>
+                <configuration>
+                  <target>
+                    <mkdir dir="${project.build.directory}/native"/>
+                    <exec executable="cmake" dir="${project.build.directory}/native" 
+                        failonerror="true">
+                      <arg line="${basedir}/src/ -DJVM_ARCH_DATA_MODEL=${sun.arch.data.model}"/>
+                    </exec>
+                    <exec executable="make" dir="${project.build.directory}/native" failonerror="true">
+                      <arg line="VERBOSE=1"/>
+                    </exec>
+                  </target>
+                </configuration>
+              </execution>
+              <!-- TODO wire here native testcases
+              <execution>
+                <id>test</id>
+                <phase>test</phase>
+                <goals>
+                  <goal>test</goal>
+                </goals>
+                <configuration>
+                  <destDir>${project.build.directory}/native/target</destDir>
+                </configuration>
+              </execution>
+              -->
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+  </profiles> 
+
+<!--
+  <build>
+    <plugins>
+     <plugin>
+        <artifactId>maven-antrun-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>compile</id>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>run</goal>
+            </goals>
+            <configuration>
+              <target>
+                <mkdir dir="${basedir}/../target/native"/>
+                <copy toDir="${basedir}/../target/native">
+                  <fileset dir="${basedir}/src/main/native"/>
+                </copy>
+              </target>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+    </plugins>
+  </build>
+-->
+</project>
diff --git a/hadoop-tools/hadoop-pipes/src/CMakeLists.txt b/hadoop-tools/hadoop-pipes/src/CMakeLists.txt
new file mode 100644
index 0000000..8ab7d27
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/CMakeLists.txt
@@ -0,0 +1,99 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+ 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+cmake_minimum_required(VERSION 2.6 FATAL_ERROR)
+find_package(OpenSSL REQUIRED)
+
+set(CMAKE_BUILD_TYPE, Release)
+
+set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -g -Wall -O2")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -g -Wall -O2")
+set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -D_REENTRANT -D_FILE_OFFSET_BITS=64")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -D_REENTRANT -D_FILE_OFFSET_BITS=64")
+
+if (JVM_ARCH_DATA_MODEL EQUAL 32)
+    set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -m32")
+    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -m32")
+    set(CMAKE_LD_FLAGS "${CMAKE_LD_FLAGS} -m32")
+    if (CMAKE_SYSTEM_PROCESSOR STREQUAL "x86_64" OR CMAKE_SYSTEM_PROCESSOR STREQUAL "amd64")
+        set(CMAKE_SYSTEM_PROCESSOR "i686")
+    endif ()
+endif (JVM_ARCH_DATA_MODEL EQUAL 32)
+
+function(output_directory TGT DIR)
+    SET_TARGET_PROPERTIES(${TGT} PROPERTIES
+        RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/${DIR}")
+   SET_TARGET_PROPERTIES(${TGT} PROPERTIES
+        ARCHIVE_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/${DIR}")
+    SET_TARGET_PROPERTIES(${TGT} PROPERTIES
+        LIBRARY_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/${DIR}")
+endfunction(output_directory TGT DIR)
+
+include_directories(
+    main/native/utils/api
+    main/native/pipes/api
+    ${CMAKE_CURRENT_SOURCE_DIR}
+    ${OPENSSL_INCLUDE_DIR}
+)
+
+# Example programs
+add_executable(wordcount-simple main/native/examples/impl/wordcount-simple.cc)
+target_link_libraries(wordcount-simple hadooppipes hadooputils)
+
+add_executable(wordcount-part main/native/examples/impl/wordcount-part.cc)
+target_link_libraries(wordcount-part hadooppipes hadooputils)
+
+add_executable(wordcount-nopipe main/native/examples/impl/wordcount-nopipe.cc)
+target_link_libraries(wordcount-nopipe hadooppipes hadooputils)
+
+add_executable(pipes-sort main/native/examples/impl/sort.cc)
+target_link_libraries(pipes-sort hadooppipes hadooputils)
+
+install(TARGETS wordcount-simple wordcount-part wordcount-nopipe pipes-sort 
+    RUNTIME DESTINATION bin
+)
+
+add_library(hadooputils STATIC
+    main/native/utils/impl/StringUtils.cc
+    main/native/utils/impl/SerialUtils.cc
+)
+
+install(FILES
+    main/native/utils/api/hadoop/SerialUtils.hh
+    main/native/utils/api/hadoop/StringUtils.hh
+    DESTINATION api/hadoop
+    COMPONENT headers
+)
+install(TARGETS hadooputils DESTINATION lib)
+
+add_library(hadooppipes STATIC
+    main/native/pipes/impl/HadoopPipes.cc
+)
+target_link_libraries(hadooppipes
+    ${JAVA_JVM_LIBRARY}
+    ${OPENSSL_LIBRARIES}
+    pthread
+)
+
+install(FILES
+    main/native/pipes/api/hadoop/Pipes.hh
+    main/native/pipes/api/hadoop/TemplateFactory.hh
+    DESTINATION api/hadoop
+    COMPONENT headers
+)
+install(TARGETS hadooppipes DESTINATION lib)
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt b/hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt
new file mode 100644
index 0000000..b9448f5
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt
@@ -0,0 +1,16 @@
+To run the examples, first compile them:
+
+% mvn install 
+
+and then copy the binaries to dfs:
+
+% hadoop fs -put target/native/wordcount-simple /examples/bin/
+
+create an input directory with text files:
+
+% hadoop fs -put my-data in-dir
+
+and run the word count example:
+
+% hadoop pipes -conf src/main/native/examples/conf/word.xml \
+                   -input in-dir -output out-dir
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml b/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml
new file mode 100644
index 0000000..b552a1c
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml
@@ -0,0 +1,24 @@
+<?xml version="1.0"?>
+<configuration>
+
+<property>
+  <name>mapreduce.job.reduces</name>
+  <value>2</value>
+</property>
+
+<property>
+  <name>mapreduce.pipes.executable</name>
+  <value>hdfs:/examples/bin/wordcount-part</value>
+</property>
+
+<property>
+  <name>mapreduce.pipes.isjavarecordreader</name>
+  <value>true</value>
+</property>
+
+<property>
+  <name>mapreduce.pipes.isjavarecordwriter</name>
+  <value>true</value>
+</property>
+
+</configuration>
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml b/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml
new file mode 100644
index 0000000..ed727dd
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml
@@ -0,0 +1,28 @@
+<?xml version="1.0"?>
+<configuration>
+
+<property>
+  <name>mapreduce.job.reduces</name>
+  <value>2</value>
+</property>
+
+<property>
+  <name>mapreduce.pipes.executable</name>
+  <value>/examples/bin/wordcount-simple#wordcount-simple</value>
+  <description> Executable path is given as "path#executable-name"
+                sothat the executable will have a symlink in working directory.
+                This can be used for gdb debugging etc.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.pipes.isjavarecordreader</name>
+  <value>true</value>
+</property>
+
+<property>
+  <name>mapreduce.pipes.isjavarecordwriter</name>
+  <value>true</value>
+</property>
+
+</configuration>
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc
new file mode 100644
index 0000000..529d6fb
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc
@@ -0,0 +1,96 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hadoop/Pipes.hh"
+#include "hadoop/TemplateFactory.hh"
+
+class SortMap: public HadoopPipes::Mapper {
+private:
+  /* the fraction 0.0 to 1.0 of records to keep */
+  float keepFraction;
+  /* the number of records kept so far */
+  long long keptRecords;
+  /* the total number of records */
+  long long totalRecords;
+  static const std::string MAP_KEEP_PERCENT;
+public:
+  /*
+   * Look in the config to find the fraction of records to keep.
+   */
+  SortMap(HadoopPipes::TaskContext& context){
+    const HadoopPipes::JobConf* conf = context.getJobConf();
+    if (conf->hasKey(MAP_KEEP_PERCENT)) {
+      keepFraction = conf->getFloat(MAP_KEEP_PERCENT) / 100.0;
+    } else {
+      keepFraction = 1.0;
+    }
+    keptRecords = 0;
+    totalRecords = 0;
+  }
+
+  void map(HadoopPipes::MapContext& context) {
+    totalRecords += 1;
+    while ((float) keptRecords / totalRecords < keepFraction) {
+      keptRecords += 1;
+      context.emit(context.getInputKey(), context.getInputValue());
+    }
+  }
+};
+
+const std::string SortMap::MAP_KEEP_PERCENT("mapreduce.loadgen.sort.map.preserve.percent");
+
+class SortReduce: public HadoopPipes::Reducer {
+private:
+  /* the fraction 0.0 to 1.0 of records to keep */
+  float keepFraction;
+  /* the number of records kept so far */
+  long long keptRecords;
+  /* the total number of records */
+  long long totalRecords;
+  static const std::string REDUCE_KEEP_PERCENT;
+public:
+  SortReduce(HadoopPipes::TaskContext& context){
+    const HadoopPipes::JobConf* conf = context.getJobConf();
+    if (conf->hasKey(REDUCE_KEEP_PERCENT)) {
+      keepFraction = conf->getFloat(REDUCE_KEEP_PERCENT) / 100.0;
+    } else {
+      keepFraction = 1.0;
+    }
+    keptRecords = 0;
+    totalRecords = 0;
+  }
+
+  void reduce(HadoopPipes::ReduceContext& context) {
+    while (context.nextValue()) {
+      totalRecords += 1;
+      while ((float) keptRecords / totalRecords < keepFraction) {
+        keptRecords += 1;
+        context.emit(context.getInputKey(), context.getInputValue());
+      }
+    }
+  }
+};
+
+const std::string 
+  SortReduce::REDUCE_KEEP_PERCENT("mapreduce.loadgen.sort.reduce.preserve.percent");
+
+int main(int argc, char *argv[]) {
+  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<SortMap,
+                                                           SortReduce>());
+}
+
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc
new file mode 100644
index 0000000..a5fe731
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc
@@ -0,0 +1,148 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "hadoop/Pipes.hh"
+#include "hadoop/TemplateFactory.hh"
+#include "hadoop/StringUtils.hh"
+#include "hadoop/SerialUtils.hh"
+
+#include <stdio.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+
+const std::string WORDCOUNT = "WORDCOUNT";
+const std::string INPUT_WORDS = "INPUT_WORDS";
+const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
+
+class WordCountMap: public HadoopPipes::Mapper {
+public:
+  HadoopPipes::TaskContext::Counter* inputWords;
+  
+  WordCountMap(HadoopPipes::TaskContext& context) {
+    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
+  }
+  
+  void map(HadoopPipes::MapContext& context) {
+    std::vector<std::string> words = 
+      HadoopUtils::splitString(context.getInputValue(), " ");
+    for(unsigned int i=0; i < words.size(); ++i) {
+      context.emit(words[i], "1");
+    }
+    context.incrementCounter(inputWords, words.size());
+  }
+};
+
+class WordCountReduce: public HadoopPipes::Reducer {
+public:
+  HadoopPipes::TaskContext::Counter* outputWords;
+
+  WordCountReduce(HadoopPipes::TaskContext& context) {
+    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
+  }
+
+  void reduce(HadoopPipes::ReduceContext& context) {
+    int sum = 0;
+    while (context.nextValue()) {
+      sum += HadoopUtils::toInt(context.getInputValue());
+    }
+    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
+    context.incrementCounter(outputWords, 1); 
+  }
+};
+
+class WordCountReader: public HadoopPipes::RecordReader {
+private:
+  int64_t bytesTotal;
+  int64_t bytesRead;
+  FILE* file;
+public:
+  WordCountReader(HadoopPipes::MapContext& context) {
+    std::string filename;
+    HadoopUtils::StringInStream stream(context.getInputSplit());
+    HadoopUtils::deserializeString(filename, stream);
+    struct stat statResult;
+    stat(filename.c_str(), &statResult);
+    bytesTotal = statResult.st_size;
+    bytesRead = 0;
+    file = fopen(filename.c_str(), "rt");
+    HADOOP_ASSERT(file != NULL, "failed to open " + filename);
+  }
+
+  ~WordCountReader() {
+    fclose(file);
+  }
+
+  virtual bool next(std::string& key, std::string& value) {
+    key = HadoopUtils::toString(ftell(file));
+    int ch = getc(file);
+    bytesRead += 1;
+    value.clear();
+    while (ch != -1 && ch != '\n') {
+      value += ch;
+      ch = getc(file);
+      bytesRead += 1;
+    }
+    return ch != -1;
+  }
+
+  /**
+   * The progress of the record reader through the split as a value between
+   * 0.0 and 1.0.
+   */
+  virtual float getProgress() {
+    if (bytesTotal > 0) {
+      return (float)bytesRead / bytesTotal;
+    } else {
+      return 1.0f;
+    }
+  }
+};
+
+class WordCountWriter: public HadoopPipes::RecordWriter {
+private:
+  FILE* file;
+public:
+  WordCountWriter(HadoopPipes::ReduceContext& context) {
+    const HadoopPipes::JobConf* job = context.getJobConf();
+    int part = job->getInt("mapreduce.task.partition");
+    std::string outDir = job->get("mapreduce.task.output.dir");
+    // remove the file: schema substring
+    std::string::size_type posn = outDir.find(":");
+    HADOOP_ASSERT(posn != std::string::npos, 
+                  "no schema found in output dir: " + outDir);
+    outDir.erase(0, posn+1);
+    mkdir(outDir.c_str(), 0777);
+    std::string outFile = outDir + "/part-" + HadoopUtils::toString(part);
+    file = fopen(outFile.c_str(), "wt");
+    HADOOP_ASSERT(file != NULL, "can't open file for writing: " + outFile);
+  }
+
+  ~WordCountWriter() {
+    fclose(file);
+  }
+
+  void emit(const std::string& key, const std::string& value) {
+    fprintf(file, "%s -> %s\n", key.c_str(), value.c_str());
+  }
+};
+
+int main(int argc, char *argv[]) {
+  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
+                              WordCountReduce, void, void, WordCountReader,
+                              WordCountWriter>());
+}
+
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc
new file mode 100644
index 0000000..37dc199
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hadoop/Pipes.hh"
+#include "hadoop/TemplateFactory.hh"
+#include "hadoop/StringUtils.hh"
+
+const std::string WORDCOUNT = "WORDCOUNT";
+const std::string INPUT_WORDS = "INPUT_WORDS";
+const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
+
+class WordCountMap: public HadoopPipes::Mapper {
+public:
+  HadoopPipes::TaskContext::Counter* inputWords;
+  
+  WordCountMap(HadoopPipes::TaskContext& context) {
+    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
+  }
+  
+  void map(HadoopPipes::MapContext& context) {
+    std::vector<std::string> words = 
+      HadoopUtils::splitString(context.getInputValue(), " ");
+    for(unsigned int i=0; i < words.size(); ++i) {
+      context.emit(words[i], "1");
+    }
+    context.incrementCounter(inputWords, words.size());
+  }
+};
+
+class WordCountReduce: public HadoopPipes::Reducer {
+public:
+  HadoopPipes::TaskContext::Counter* outputWords;
+
+  WordCountReduce(HadoopPipes::TaskContext& context) {
+    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
+  }
+
+  void reduce(HadoopPipes::ReduceContext& context) {
+    int sum = 0;
+    while (context.nextValue()) {
+      sum += HadoopUtils::toInt(context.getInputValue());
+    }
+    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
+    context.incrementCounter(outputWords, 1); 
+  }
+};
+
+class WordCountPartitioner: public HadoopPipes::Partitioner {
+public:
+  WordCountPartitioner(HadoopPipes::TaskContext& context){}
+  virtual int partition(const std::string& key, int numOfReduces) {
+    return 0;
+  }
+};
+
+int main(int argc, char *argv[]) {
+  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
+                              WordCountReduce,WordCountPartitioner,
+                              WordCountReduce>());
+}
+
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc
new file mode 100644
index 0000000..64dd801
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc
@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hadoop/Pipes.hh"
+#include "hadoop/TemplateFactory.hh"
+#include "hadoop/StringUtils.hh"
+
+const std::string WORDCOUNT = "WORDCOUNT";
+const std::string INPUT_WORDS = "INPUT_WORDS";
+const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
+
+class WordCountMap: public HadoopPipes::Mapper {
+public:
+  HadoopPipes::TaskContext::Counter* inputWords;
+  
+  WordCountMap(HadoopPipes::TaskContext& context) {
+    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
+  }
+  
+  void map(HadoopPipes::MapContext& context) {
+    std::vector<std::string> words = 
+      HadoopUtils::splitString(context.getInputValue(), " ");
+    for(unsigned int i=0; i < words.size(); ++i) {
+      context.emit(words[i], "1");
+    }
+    context.incrementCounter(inputWords, words.size());
+  }
+};
+
+class WordCountReduce: public HadoopPipes::Reducer {
+public:
+  HadoopPipes::TaskContext::Counter* outputWords;
+
+  WordCountReduce(HadoopPipes::TaskContext& context) {
+    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
+  }
+
+  void reduce(HadoopPipes::ReduceContext& context) {
+    int sum = 0;
+    while (context.nextValue()) {
+      sum += HadoopUtils::toInt(context.getInputValue());
+    }
+    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
+    context.incrementCounter(outputWords, 1); 
+  }
+};
+
+int main(int argc, char *argv[]) {
+  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
+                              WordCountReduce>());
+}
+
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh b/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh
new file mode 100644
index 0000000..b5d0ddd
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh
@@ -0,0 +1,260 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef HADOOP_PIPES_HH
+#define HADOOP_PIPES_HH
+
+#ifdef SWIG
+%module (directors="1") HadoopPipes
+%include "std_string.i"
+%feature("director") Mapper;
+%feature("director") Reducer;
+%feature("director") Partitioner;
+%feature("director") RecordReader;
+%feature("director") RecordWriter;
+%feature("director") Factory;
+#else
+#include <string>
+#endif
+
+#include <stdint.h>
+
+namespace HadoopPipes {
+
+/**
+ * This interface defines the interface between application code and the 
+ * foreign code interface to Hadoop Map/Reduce.
+ */
+
+/**
+ * A JobConf defines the properties for a job.
+ */
+class JobConf {
+public:
+  virtual bool hasKey(const std::string& key) const = 0;
+  virtual const std::string& get(const std::string& key) const = 0;
+  virtual int getInt(const std::string& key) const = 0;
+  virtual float getFloat(const std::string& key) const = 0;
+  virtual bool getBoolean(const std::string&key) const = 0;
+  virtual ~JobConf() {}
+};
+
+/**
+ * Task context provides the information about the task and job.
+ */
+class TaskContext {
+public:
+  /**
+   * Counter to keep track of a property and its value.
+   */
+  class Counter {
+  private:
+    int id;
+  public:
+    Counter(int counterId) : id(counterId) {}
+    Counter(const Counter& counter) : id(counter.id) {}
+
+    int getId() const { return id; }
+  };
+  
+  /**
+   * Get the JobConf for the current task.
+   */
+  virtual const JobConf* getJobConf() = 0;
+
+  /**
+   * Get the current key. 
+   * @return the current key
+   */
+  virtual const std::string& getInputKey() = 0;
+
+  /**
+   * Get the current value. 
+   * @return the current value
+   */
+  virtual const std::string& getInputValue() = 0;
+
+  /**
+   * Generate an output record
+   */
+  virtual void emit(const std::string& key, const std::string& value) = 0;
+
+  /**
+   * Mark your task as having made progress without changing the status 
+   * message.
+   */
+  virtual void progress() = 0;
+
+  /**
+   * Set the status message and call progress.
+   */
+  virtual void setStatus(const std::string& status) = 0;
+
+  /**
+   * Register a counter with the given group and name.
+   */
+  virtual Counter* 
+    getCounter(const std::string& group, const std::string& name) = 0;
+
+  /**
+   * Increment the value of the counter with the given amount.
+   */
+  virtual void incrementCounter(const Counter* counter, uint64_t amount) = 0;
+  
+  virtual ~TaskContext() {}
+};
+
+class MapContext: public TaskContext {
+public:
+
+  /**
+   * Access the InputSplit of the mapper.
+   */
+  virtual const std::string& getInputSplit() = 0;
+
+  /**
+   * Get the name of the key class of the input to this task.
+   */
+  virtual const std::string& getInputKeyClass() = 0;
+
+  /**
+   * Get the name of the value class of the input to this task.
+   */
+  virtual const std::string& getInputValueClass() = 0;
+
+};
+
+class ReduceContext: public TaskContext {
+public:
+  /**
+   * Advance to the next value.
+   */
+  virtual bool nextValue() = 0;
+};
+
+class Closable {
+public:
+  virtual void close() {}
+  virtual ~Closable() {}
+};
+
+/**
+ * The application's mapper class to do map.
+ */
+class Mapper: public Closable {
+public:
+  virtual void map(MapContext& context) = 0;
+};
+
+/**
+ * The application's reducer class to do reduce.
+ */
+class Reducer: public Closable {
+public:
+  virtual void reduce(ReduceContext& context) = 0;
+};
+
+/**
+ * User code to decide where each key should be sent.
+ */
+class Partitioner {
+public:
+  virtual int partition(const std::string& key, int numOfReduces) = 0;
+  virtual ~Partitioner() {}
+};
+
+/**
+ * For applications that want to read the input directly for the map function
+ * they can define RecordReaders in C++.
+ */
+class RecordReader: public Closable {
+public:
+  virtual bool next(std::string& key, std::string& value) = 0;
+
+  /**
+   * The progress of the record reader through the split as a value between
+   * 0.0 and 1.0.
+   */
+  virtual float getProgress() = 0;
+};
+
+/**
+ * An object to write key/value pairs as they are emited from the reduce.
+ */
+class RecordWriter: public Closable {
+public:
+  virtual void emit(const std::string& key,
+                    const std::string& value) = 0;
+};
+
+/**
+ * A factory to create the necessary application objects.
+ */
+class Factory {
+public:
+  virtual Mapper* createMapper(MapContext& context) const = 0;
+  virtual Reducer* createReducer(ReduceContext& context) const = 0;
+
+  /**
+   * Create a combiner, if this application has one.
+   * @return the new combiner or NULL, if one is not needed
+   */
+  virtual Reducer* createCombiner(MapContext& context) const {
+    return NULL; 
+  }
+
+  /**
+   * Create an application partitioner object.
+   * @return the new partitioner or NULL, if the default partitioner should be 
+   *     used.
+   */
+  virtual Partitioner* createPartitioner(MapContext& context) const {
+    return NULL;
+  }
+
+  /**
+   * Create an application record reader.
+   * @return the new RecordReader or NULL, if the Java RecordReader should be
+   *    used.
+   */
+  virtual RecordReader* createRecordReader(MapContext& context) const {
+    return NULL; 
+  }
+
+  /**
+   * Create an application record writer.
+   * @return the new RecordWriter or NULL, if the Java RecordWriter should be
+   *    used.
+   */
+  virtual RecordWriter* createRecordWriter(ReduceContext& context) const {
+    return NULL;
+  }
+
+  virtual ~Factory() {}
+};
+
+/**
+ * Run the assigned task in the framework.
+ * The user's main function should set the various functions using the 
+ * set* functions above and then call this.
+ * @return true, if the task succeeded.
+ */
+bool runTask(const Factory& factory);
+
+}
+
+#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh b/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh
new file mode 100644
index 0000000..22e10ae
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh
@@ -0,0 +1,96 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef HADOOP_PIPES_TEMPLATE_FACTORY_HH
+#define HADOOP_PIPES_TEMPLATE_FACTORY_HH
+
+namespace HadoopPipes {
+
+  template <class mapper, class reducer>
+  class TemplateFactory2: public Factory {
+  public:
+    Mapper* createMapper(MapContext& context) const {
+      return new mapper(context);
+    }
+    Reducer* createReducer(ReduceContext& context) const {
+      return new reducer(context);
+    }
+  };
+
+  template <class mapper, class reducer, class partitioner>
+  class TemplateFactory3: public TemplateFactory2<mapper,reducer> {
+  public:
+    Partitioner* createPartitioner(MapContext& context) const {
+      return new partitioner(context);
+    }
+  };
+
+  template <class mapper, class reducer>
+  class TemplateFactory3<mapper, reducer, void>
+      : public TemplateFactory2<mapper,reducer> {
+  };
+
+  template <class mapper, class reducer, class partitioner, class combiner>
+  class TemplateFactory4
+   : public TemplateFactory3<mapper,reducer,partitioner>{
+  public:
+    Reducer* createCombiner(MapContext& context) const {
+      return new combiner(context);
+    }
+  };
+
+  template <class mapper, class reducer, class partitioner>
+  class TemplateFactory4<mapper,reducer,partitioner,void>
+   : public TemplateFactory3<mapper,reducer,partitioner>{
+  };
+
+  template <class mapper, class reducer, class partitioner, 
+            class combiner, class recordReader>
+  class TemplateFactory5
+   : public TemplateFactory4<mapper,reducer,partitioner,combiner>{
+  public:
+    RecordReader* createRecordReader(MapContext& context) const {
+      return new recordReader(context);
+    }
+  };
+
+  template <class mapper, class reducer, class partitioner,class combiner>
+  class TemplateFactory5<mapper,reducer,partitioner,combiner,void>
+   : public TemplateFactory4<mapper,reducer,partitioner,combiner>{
+  };
+
+  template <class mapper, class reducer, class partitioner=void, 
+            class combiner=void, class recordReader=void, 
+            class recordWriter=void> 
+  class TemplateFactory
+   : public TemplateFactory5<mapper,reducer,partitioner,combiner,recordReader>{
+  public:
+    RecordWriter* createRecordWriter(ReduceContext& context) const {
+      return new recordWriter(context);
+    }
+  };
+
+  template <class mapper, class reducer, class partitioner, 
+            class combiner, class recordReader>
+  class TemplateFactory<mapper, reducer, partitioner, combiner, recordReader, 
+                        void>
+   : public TemplateFactory5<mapper,reducer,partitioner,combiner,recordReader>{
+  };
+
+}
+
+#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt b/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt
new file mode 100644
index 0000000..6cfd4d6
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt
@@ -0,0 +1,3 @@
+info threads
+backtrace
+quit
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script b/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script
new file mode 100644
index 0000000..e7f59e5
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script
@@ -0,0 +1,3 @@
+core=`find . -name 'core*'`
+#Only pipes programs have 5th argument as program name.
+gdb -quiet $5 -c $core -x $HADOOP_PREFIX/src/c++/pipes/debug/pipes-default-gdb-commands.txt 
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc b/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
new file mode 100644
index 0000000..0e26425
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
@@ -0,0 +1,1181 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hadoop/Pipes.hh"
+#include "hadoop/SerialUtils.hh"
+#include "hadoop/StringUtils.hh"
+
+#include <map>
+#include <vector>
+
+#include <errno.h>
+#include <netinet/in.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <strings.h>
+#include <sys/socket.h>
+#include <pthread.h>
+#include <iostream>
+#include <fstream>
+
+#include <openssl/hmac.h>
+#include <openssl/buffer.h>
+
+using std::map;
+using std::string;
+using std::vector;
+
+using namespace HadoopUtils;
+
+namespace HadoopPipes {
+
+  class JobConfImpl: public JobConf {
+  private:
+    map<string, string> values;
+  public:
+    void set(const string& key, const string& value) {
+      values[key] = value;
+    }
+
+    virtual bool hasKey(const string& key) const {
+      return values.find(key) != values.end();
+    }
+
+    virtual const string& get(const string& key) const {
+      map<string,string>::const_iterator itr = values.find(key);
+      if (itr == values.end()) {
+        throw Error("Key " + key + " not found in JobConf");
+      }
+      return itr->second;
+    }
+
+    virtual int getInt(const string& key) const {
+      const string& val = get(key);
+      return toInt(val);
+    }
+
+    virtual float getFloat(const string& key) const {
+      const string& val = get(key);
+      return toFloat(val);
+    }
+
+    virtual bool getBoolean(const string&key) const {
+      const string& val = get(key);
+      return toBool(val);
+    }
+  };
+
+  class DownwardProtocol {
+  public:
+    virtual void start(int protocol) = 0;
+    virtual void setJobConf(vector<string> values) = 0;
+    virtual void setInputTypes(string keyType, string valueType) = 0;
+    virtual void runMap(string inputSplit, int numReduces, bool pipedInput)= 0;
+    virtual void mapItem(const string& key, const string& value) = 0;
+    virtual void runReduce(int reduce, bool pipedOutput) = 0;
+    virtual void reduceKey(const string& key) = 0;
+    virtual void reduceValue(const string& value) = 0;
+    virtual void close() = 0;
+    virtual void abort() = 0;
+    virtual ~DownwardProtocol() {}
+  };
+
+  class UpwardProtocol {
+  public:
+    virtual void output(const string& key, const string& value) = 0;
+    virtual void partitionedOutput(int reduce, const string& key,
+                                   const string& value) = 0;
+    virtual void status(const string& message) = 0;
+    virtual void progress(float progress) = 0;
+    virtual void done() = 0;
+    virtual void registerCounter(int id, const string& group, 
+                                 const string& name) = 0;
+    virtual void 
+      incrementCounter(const TaskContext::Counter* counter, uint64_t amount) = 0;
+    virtual ~UpwardProtocol() {}
+  };
+
+  class Protocol {
+  public:
+    virtual void nextEvent() = 0;
+    virtual UpwardProtocol* getUplink() = 0;
+    virtual ~Protocol() {}
+  };
+
+  class TextUpwardProtocol: public UpwardProtocol {
+  private:
+    FILE* stream;
+    static const char fieldSeparator = '\t';
+    static const char lineSeparator = '\n';
+
+    void writeBuffer(const string& buffer) {
+      fprintf(stream, quoteString(buffer, "\t\n").c_str());
+    }
+
+  public:
+    TextUpwardProtocol(FILE* _stream): stream(_stream) {}
+    
+    virtual void output(const string& key, const string& value) {
+      fprintf(stream, "output%c", fieldSeparator);
+      writeBuffer(key);
+      fprintf(stream, "%c", fieldSeparator);
+      writeBuffer(value);
+      fprintf(stream, "%c", lineSeparator);
+    }
+
+    virtual void partitionedOutput(int reduce, const string& key,
+                                   const string& value) {
+      fprintf(stream, "parititionedOutput%c%d%c", fieldSeparator, reduce, 
+              fieldSeparator);
+      writeBuffer(key);
+      fprintf(stream, "%c", fieldSeparator);
+      writeBuffer(value);
+      fprintf(stream, "%c", lineSeparator);
+    }
+
+    virtual void status(const string& message) {
+      fprintf(stream, "status%c%s%c", fieldSeparator, message.c_str(), 
+              lineSeparator);
+    }
+
+    virtual void progress(float progress) {
+      fprintf(stream, "progress%c%f%c", fieldSeparator, progress, 
+              lineSeparator);
+    }
+
+    virtual void registerCounter(int id, const string& group, 
+                                 const string& name) {
+      fprintf(stream, "registerCounter%c%d%c%s%c%s%c", fieldSeparator, id,
+              fieldSeparator, group.c_str(), fieldSeparator, name.c_str(), 
+              lineSeparator);
+    }
+
+    virtual void incrementCounter(const TaskContext::Counter* counter, 
+                                  uint64_t amount) {
+      fprintf(stream, "incrCounter%c%d%c%ld%c", fieldSeparator, counter->getId(), 
+              fieldSeparator, (long)amount, lineSeparator);
+    }
+    
+    virtual void done() {
+      fprintf(stream, "done%c", lineSeparator);
+    }
+  };
+
+  class TextProtocol: public Protocol {
+  private:
+    FILE* downStream;
+    DownwardProtocol* handler;
+    UpwardProtocol* uplink;
+    string key;
+    string value;
+
+    int readUpto(string& buffer, const char* limit) {
+      int ch;
+      buffer.clear();
+      while ((ch = getc(downStream)) != -1) {
+        if (strchr(limit, ch) != NULL) {
+          return ch;
+        }
+        buffer += ch;
+      }
+      return -1;
+    }
+
+    static const char* delim;
+  public:
+
+    TextProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
+      downStream = down;
+      uplink = new TextUpwardProtocol(up);
+      handler = _handler;
+    }
+
+    UpwardProtocol* getUplink() {
+      return uplink;
+    }
+
+    virtual void nextEvent() {
+      string command;
+      string arg;
+      int sep;
+      sep = readUpto(command, delim);
+      if (command == "mapItem") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(key, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(value, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->mapItem(key, value);
+      } else if (command == "reduceValue") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(value, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->reduceValue(value);
+      } else if (command == "reduceKey") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(key, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->reduceKey(key);
+      } else if (command == "start") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->start(toInt(arg));
+      } else if (command == "setJobConf") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        int len = toInt(arg);
+        vector<string> values(len);
+        for(int i=0; i < len; ++i) {
+          HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+          sep = readUpto(arg, delim);
+          values.push_back(arg);
+        }
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->setJobConf(values);
+      } else if (command == "setInputTypes") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(key, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(value, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->setInputTypes(key, value);
+      } else if (command == "runMap") {
+        string split;
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(split, delim);
+        string reduces;
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(reduces, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->runMap(split, toInt(reduces), toBool(arg));
+      } else if (command == "runReduce") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        string piped;
+        sep = readUpto(piped, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->runReduce(toInt(arg), toBool(piped));
+      } else if (command == "abort") { 
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->abort();
+      } else if (command == "close") {
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->close();
+      } else {
+        throw Error("Illegal text protocol command " + command);
+      }
+    }
+
+    ~TextProtocol() {
+      delete uplink;
+    }
+  };
+  const char* TextProtocol::delim = "\t\n";
+
+  enum MESSAGE_TYPE {START_MESSAGE, SET_JOB_CONF, SET_INPUT_TYPES, RUN_MAP, 
+                     MAP_ITEM, RUN_REDUCE, REDUCE_KEY, REDUCE_VALUE, 
+                     CLOSE, ABORT, AUTHENTICATION_REQ,
+                     OUTPUT=50, PARTITIONED_OUTPUT, STATUS, PROGRESS, DONE,
+                     REGISTER_COUNTER, INCREMENT_COUNTER, AUTHENTICATION_RESP};
+
+  class BinaryUpwardProtocol: public UpwardProtocol {
+  private:
+    FileOutStream* stream;
+  public:
+    BinaryUpwardProtocol(FILE* _stream) {
+      stream = new FileOutStream();
+      HADOOP_ASSERT(stream->open(_stream), "problem opening stream");
+    }
+
+    virtual void authenticate(const string &responseDigest) {
+      serializeInt(AUTHENTICATION_RESP, *stream);
+      serializeString(responseDigest, *stream);
+      stream->flush();
+    }
+
+    virtual void output(const string& key, const string& value) {
+      serializeInt(OUTPUT, *stream);
+      serializeString(key, *stream);
+      serializeString(value, *stream);
+    }
+
+    virtual void partitionedOutput(int reduce, const string& key,
+                                   const string& value) {
+      serializeInt(PARTITIONED_OUTPUT, *stream);
+      serializeInt(reduce, *stream);
+      serializeString(key, *stream);
+      serializeString(value, *stream);
+    }
+
+    virtual void status(const string& message) {
+      serializeInt(STATUS, *stream);
+      serializeString(message, *stream);
+    }
+
+    virtual void progress(float progress) {
+      serializeInt(PROGRESS, *stream);
+      serializeFloat(progress, *stream);
+      stream->flush();
+    }
+
+    virtual void done() {
+      serializeInt(DONE, *stream);
+    }
+
+    virtual void registerCounter(int id, const string& group, 
+                                 const string& name) {
+      serializeInt(REGISTER_COUNTER, *stream);
+      serializeInt(id, *stream);
+      serializeString(group, *stream);
+      serializeString(name, *stream);
+    }
+
+    virtual void incrementCounter(const TaskContext::Counter* counter, 
+                                  uint64_t amount) {
+      serializeInt(INCREMENT_COUNTER, *stream);
+      serializeInt(counter->getId(), *stream);
+      serializeLong(amount, *stream);
+    }
+    
+    ~BinaryUpwardProtocol() {
+      delete stream;
+    }
+  };
+
+  class BinaryProtocol: public Protocol {
+  private:
+    FileInStream* downStream;
+    DownwardProtocol* handler;
+    BinaryUpwardProtocol * uplink;
+    string key;
+    string value;
+    string password;
+    bool authDone;
+    void getPassword(string &password) {
+      const char *passwordFile = getenv("hadoop.pipes.shared.secret.location");
+      if (passwordFile == NULL) {
+        return;
+      }
+      std::ifstream fstr(passwordFile, std::fstream::binary);
+      if (fstr.fail()) {
+        std::cerr << "Could not open the password file" << std::endl;
+        return;
+      } 
+      unsigned char * passBuff = new unsigned char [512];
+      fstr.read((char *)passBuff, 512);
+      int passwordLength = fstr.gcount();
+      fstr.close();
+      passBuff[passwordLength] = 0;
+      password.replace(0, passwordLength, (const char *) passBuff, passwordLength);
+      delete [] passBuff;
+      return; 
+    }
+
+    void verifyDigestAndRespond(string& digest, string& challenge) {
+      if (password.empty()) {
+        //password can be empty if process is running in debug mode from
+        //command file.
+        authDone = true;
+        return;
+      }
+
+      if (!verifyDigest(password, digest, challenge)) {
+        std::cerr << "Server failed to authenticate. Exiting" << std::endl;
+        exit(-1);
+      }
+      authDone = true;
+      string responseDigest = createDigest(password, digest);
+      uplink->authenticate(responseDigest);
+    }
+
+    bool verifyDigest(string &password, string& digest, string& challenge) {
+      string expectedDigest = createDigest(password, challenge);
+      if (digest == expectedDigest) {
+        return true;
+      } else {
+        return false;
+      }
+    }
+
+    string createDigest(string &password, string& msg) {
+      HMAC_CTX ctx;
+      unsigned char digest[EVP_MAX_MD_SIZE];
+      HMAC_Init(&ctx, (const unsigned char *)password.c_str(), 
+          password.length(), EVP_sha1());
+      HMAC_Update(&ctx, (const unsigned char *)msg.c_str(), msg.length());
+      unsigned int digestLen;
+      HMAC_Final(&ctx, digest, &digestLen);
+      HMAC_cleanup(&ctx);
+
+      //now apply base64 encoding
+      BIO *bmem, *b64;
+      BUF_MEM *bptr;
+
+      b64 = BIO_new(BIO_f_base64());
+      bmem = BIO_new(BIO_s_mem());
+      b64 = BIO_push(b64, bmem);
+      BIO_write(b64, digest, digestLen);
+      BIO_flush(b64);
+      BIO_get_mem_ptr(b64, &bptr);
+
+      char digestBuffer[bptr->length];
+      memcpy(digestBuffer, bptr->data, bptr->length-1);
+      digestBuffer[bptr->length-1] = 0;
+      BIO_free_all(b64);
+
+      return string(digestBuffer);
+    }
+
+  public:
+    BinaryProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
+      downStream = new FileInStream();
+      downStream->open(down);
+      uplink = new BinaryUpwardProtocol(up);
+      handler = _handler;
+      authDone = false;
+      getPassword(password);
+    }
+
+    UpwardProtocol* getUplink() {
+      return uplink;
+    }
+
+    virtual void nextEvent() {
+      int32_t cmd;
+      cmd = deserializeInt(*downStream);
+      if (!authDone && cmd != AUTHENTICATION_REQ) {
+        //Authentication request must be the first message if
+        //authentication is not complete
+        std::cerr << "Command:" << cmd << "received before authentication. " 
+            << "Exiting.." << std::endl;
+        exit(-1);
+      }
+      switch (cmd) {
+      case AUTHENTICATION_REQ: {
+        string digest;
+        string challenge;
+        deserializeString(digest, *downStream);
+        deserializeString(challenge, *downStream);
+        verifyDigestAndRespond(digest, challenge);
+        break;
+      }
+      case START_MESSAGE: {
+        int32_t prot;
+        prot = deserializeInt(*downStream);
+        handler->start(prot);
+        break;
+      }
+      case SET_JOB_CONF: {
+        int32_t entries;
+        entries = deserializeInt(*downStream);
+        vector<string> result(entries);
+        for(int i=0; i < entries; ++i) {
+          string item;
+          deserializeString(item, *downStream);
+          result.push_back(item);
+        }
+        handler->setJobConf(result);
+        break;
+      }
+      case SET_INPUT_TYPES: {
+        string keyType;
+        string valueType;
+        deserializeString(keyType, *downStream);
+        deserializeString(valueType, *downStream);
+        handler->setInputTypes(keyType, valueType);
+        break;
+      }
+      case RUN_MAP: {
+        string split;
+        int32_t numReduces;
+        int32_t piped;
+        deserializeString(split, *downStream);
+        numReduces = deserializeInt(*downStream);
+        piped = deserializeInt(*downStream);
+        handler->runMap(split, numReduces, piped);
+        break;
+      }
+      case MAP_ITEM: {
+        deserializeString(key, *downStream);
+        deserializeString(value, *downStream);
+        handler->mapItem(key, value);
+        break;
+      }
+      case RUN_REDUCE: {
+        int32_t reduce;
+        int32_t piped;
+        reduce = deserializeInt(*downStream);
+        piped = deserializeInt(*downStream);
+        handler->runReduce(reduce, piped);
+        break;
+      }
+      case REDUCE_KEY: {
+        deserializeString(key, *downStream);
+        handler->reduceKey(key);
+        break;
+      }
+      case REDUCE_VALUE: {
+        deserializeString(value, *downStream);
+        handler->reduceValue(value);
+        break;
+      }
+      case CLOSE:
+        handler->close();
+        break;
+      case ABORT:
+        handler->abort();
+        break;
+      default:
+        HADOOP_ASSERT(false, "Unknown binary command " + toString(cmd));
+      }
+    }
+
+    virtual ~BinaryProtocol() {
+      delete downStream;
+      delete uplink;
+    }
+  };
+
+  /**
+   * Define a context object to give to combiners that will let them
+   * go through the values and emit their results correctly.
+   */
+  class CombineContext: public ReduceContext {
+  private:
+    ReduceContext* baseContext;
+    Partitioner* partitioner;
+    int numReduces;
+    UpwardProtocol* uplink;
+    bool firstKey;
+    bool firstValue;
+    map<string, vector<string> >::iterator keyItr;
+    map<string, vector<string> >::iterator endKeyItr;
+    vector<string>::iterator valueItr;
+    vector<string>::iterator endValueItr;
+
+  public:
+    CombineContext(ReduceContext* _baseContext,
+                   Partitioner* _partitioner,
+                   int _numReduces,
+                   UpwardProtocol* _uplink,
+                   map<string, vector<string> >& data) {
+      baseContext = _baseContext;
+      partitioner = _partitioner;
+      numReduces = _numReduces;
+      uplink = _uplink;
+      keyItr = data.begin();
+      endKeyItr = data.end();
+      firstKey = true;
+      firstValue = true;
+    }
+
+    virtual const JobConf* getJobConf() {
+      return baseContext->getJobConf();
+    }
+
+    virtual const std::string& getInputKey() {
+      return keyItr->first;
+    }
+
+    virtual const std::string& getInputValue() {
+      return *valueItr;
+    }
+
+    virtual void emit(const std::string& key, const std::string& value) {
+      if (partitioner != NULL) {
+        uplink->partitionedOutput(partitioner->partition(key, numReduces),
+                                  key, value);
+      } else {
+        uplink->output(key, value);
+      }
+    }
+
+    virtual void progress() {
+      baseContext->progress();
+    }
+
+    virtual void setStatus(const std::string& status) {
+      baseContext->setStatus(status);
+    }
+
+    bool nextKey() {
+      if (firstKey) {
+        firstKey = false;
+      } else {
+        ++keyItr;
+      }
+      if (keyItr != endKeyItr) {
+        valueItr = keyItr->second.begin();
+        endValueItr = keyItr->second.end();
+        firstValue = true;
+        return true;
+      }
+      return false;
+    }
+
+    virtual bool nextValue() {
+      if (firstValue) {
+        firstValue = false;
+      } else {
+        ++valueItr;
+      }
+      return valueItr != endValueItr;
+    }
+    
+    virtual Counter* getCounter(const std::string& group, 
+                               const std::string& name) {
+      return baseContext->getCounter(group, name);
+    }
+
+    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
+      baseContext->incrementCounter(counter, amount);
+    }
+  };
+
+  /**
+   * A RecordWriter that will take the map outputs, buffer them up and then
+   * combine then when the buffer is full.
+   */
+  class CombineRunner: public RecordWriter {
+  private:
+    map<string, vector<string> > data;
+    int64_t spillSize;
+    int64_t numBytes;
+    ReduceContext* baseContext;
+    Partitioner* partitioner;
+    int numReduces;
+    UpwardProtocol* uplink;
+    Reducer* combiner;
+  public:
+    CombineRunner(int64_t _spillSize, ReduceContext* _baseContext, 
+                  Reducer* _combiner, UpwardProtocol* _uplink, 
+                  Partitioner* _partitioner, int _numReduces) {
+      numBytes = 0;
+      spillSize = _spillSize;
+      baseContext = _baseContext;
+      partitioner = _partitioner;
+      numReduces = _numReduces;
+      uplink = _uplink;
+      combiner = _combiner;
+    }
+
+    virtual void emit(const std::string& key,
+                      const std::string& value) {
+      numBytes += key.length() + value.length();
+      data[key].push_back(value);
+      if (numBytes >= spillSize) {
+        spillAll();
+      }
+    }
+
+    virtual void close() {
+      spillAll();
+    }
+
+  private:
+    void spillAll() {
+      CombineContext context(baseContext, partitioner, numReduces, 
+                             uplink, data);
+      while (context.nextKey()) {
+        combiner->reduce(context);
+      }
+      data.clear();
+      numBytes = 0;
+    }
+  };
+
+  class TaskContextImpl: public MapContext, public ReduceContext, 
+                         public DownwardProtocol {
+  private:
+    bool done;
+    JobConf* jobConf;
+    string key;
+    const string* newKey;
+    const string* value;
+    bool hasTask;
+    bool isNewKey;
+    bool isNewValue;
+    string* inputKeyClass;
+    string* inputValueClass;
+    string status;
+    float progressFloat;
+    uint64_t lastProgress;
+    bool statusSet;
+    Protocol* protocol;
+    UpwardProtocol *uplink;
+    string* inputSplit;
+    RecordReader* reader;
+    Mapper* mapper;
+    Reducer* reducer;
+    RecordWriter* writer;
+    Partitioner* partitioner;
+    int numReduces;
+    const Factory* factory;
+    pthread_mutex_t mutexDone;
+    std::vector<int> registeredCounterIds;
+
+  public:
+
+    TaskContextImpl(const Factory& _factory) {
+      statusSet = false;
+      done = false;
+      newKey = NULL;
+      factory = &_factory;
+      jobConf = NULL;
+      inputKeyClass = NULL;
+      inputValueClass = NULL;
+      inputSplit = NULL;
+      mapper = NULL;
+      reducer = NULL;
+      reader = NULL;
+      writer = NULL;
+      partitioner = NULL;
+      protocol = NULL;
+      isNewKey = false;
+      isNewValue = false;
+      lastProgress = 0;
+      progressFloat = 0.0f;
+      hasTask = false;
+      pthread_mutex_init(&mutexDone, NULL);
+    }
+
+    void setProtocol(Protocol* _protocol, UpwardProtocol* _uplink) {
+
+      protocol = _protocol;
+      uplink = _uplink;
+    }
+
+    virtual void start(int protocol) {
+      if (protocol != 0) {
+        throw Error("Protocol version " + toString(protocol) + 
+                    " not supported");
+      }
+    }
+
+    virtual void setJobConf(vector<string> values) {
+      int len = values.size();
+      JobConfImpl* result = new JobConfImpl();
+      HADOOP_ASSERT(len % 2 == 0, "Odd length of job conf values");
+      for(int i=0; i < len; i += 2) {
+        result->set(values[i], values[i+1]);
+      }
+      jobConf = result;
+    }
+
+    virtual void setInputTypes(string keyType, string valueType) {
+      inputKeyClass = new string(keyType);
+      inputValueClass = new string(valueType);
+    }
+
+    virtual void runMap(string _inputSplit, int _numReduces, bool pipedInput) {
+      inputSplit = new string(_inputSplit);
+      reader = factory->createRecordReader(*this);
+      HADOOP_ASSERT((reader == NULL) == pipedInput,
+                    pipedInput ? "RecordReader defined when not needed.":
+                    "RecordReader not defined");
+      if (reader != NULL) {
+        value = new string();
+      }
+      mapper = factory->createMapper(*this);
+      numReduces = _numReduces;
+      if (numReduces != 0) { 
+        reducer = factory->createCombiner(*this);
+        partitioner = factory->createPartitioner(*this);
+      }
+      if (reducer != NULL) {
+        int64_t spillSize = 100;
+        if (jobConf->hasKey("mapreduce.task.io.sort.mb")) {
+          spillSize = jobConf->getInt("mapreduce.task.io.sort.mb");
+        }
+        writer = new CombineRunner(spillSize * 1024 * 1024, this, reducer, 
+                                   uplink, partitioner, numReduces);
+      }
+      hasTask = true;
+    }
+
+    virtual void mapItem(const string& _key, const string& _value) {
+      newKey = &_key;
+      value = &_value;
+      isNewKey = true;
+    }
+
+    virtual void runReduce(int reduce, bool pipedOutput) {
+      reducer = factory->createReducer(*this);
+      writer = factory->createRecordWriter(*this);
+      HADOOP_ASSERT((writer == NULL) == pipedOutput,
+                    pipedOutput ? "RecordWriter defined when not needed.":
+                    "RecordWriter not defined");
+      hasTask = true;
+    }
+
+    virtual void reduceKey(const string& _key) {
+      isNewKey = true;
+      newKey = &_key;
+    }
+
+    virtual void reduceValue(const string& _value) {
+      isNewValue = true;
+      value = &_value;
+    }
+    
+    virtual bool isDone() {
+      pthread_mutex_lock(&mutexDone);
+      bool doneCopy = done;
+      pthread_mutex_unlock(&mutexDone);
+      return doneCopy;
+    }
+
+    virtual void close() {
+      pthread_mutex_lock(&mutexDone);
+      done = true;
+      pthread_mutex_unlock(&mutexDone);
+    }
+
+    virtual void abort() {
+      throw Error("Aborted by driver");
+    }
+
+    void waitForTask() {
+      while (!done && !hasTask) {
+        protocol->nextEvent();
+      }
+    }
+
+    bool nextKey() {
+      if (reader == NULL) {
+        while (!isNewKey) {
+          nextValue();
+          if (done) {
+            return false;
+          }
+        }
+        key = *newKey;
+      } else {
+        if (!reader->next(key, const_cast<string&>(*value))) {
+          pthread_mutex_lock(&mutexDone);
+          done = true;
+          pthread_mutex_unlock(&mutexDone);
+          return false;
+        }
+        progressFloat = reader->getProgress();
+      }
+      isNewKey = false;
+      if (mapper != NULL) {
+        mapper->map(*this);
+      } else {
+        reducer->reduce(*this);
+      }
+      return true;
+    }
+
+    /**
+     * Advance to the next value.
+     */
+    virtual bool nextValue() {
+      if (isNewKey || done) {
+        return false;
+      }
+      isNewValue = false;
+      progress();
+      protocol->nextEvent();
+      return isNewValue;
+    }
+
+    /**
+     * Get the JobConf for the current task.
+     */
+    virtual JobConf* getJobConf() {
+      return jobConf;
+    }
+
+    /**
+     * Get the current key. 
+     * @return the current key or NULL if called before the first map or reduce
+     */
+    virtual const string& getInputKey() {
+      return key;
+    }
+
+    /**
+     * Get the current value. 
+     * @return the current value or NULL if called before the first map or 
+     *    reduce
+     */
+    virtual const string& getInputValue() {
+      return *value;
+    }
+
+    /**
+     * Mark your task as having made progress without changing the status 
+     * message.
+     */
+    virtual void progress() {
+      if (uplink != 0) {
+        uint64_t now = getCurrentMillis();
+        if (now - lastProgress > 1000) {
+          lastProgress = now;
+          if (statusSet) {
+            uplink->status(status);
+            statusSet = false;
+          }
+          uplink->progress(progressFloat);
+        }
+      }
+    }
+
+    /**
+     * Set the status message and call progress.
+     */
+    virtual void setStatus(const string& status) {
+      this->status = status;
+      statusSet = true;
+      progress();
+    }
+
+    /**
+     * Get the name of the key class of the input to this task.
+     */
+    virtual const string& getInputKeyClass() {
+      return *inputKeyClass;
+    }
+
+    /**
+     * Get the name of the value class of the input to this task.
+     */
+    virtual const string& getInputValueClass() {
+      return *inputValueClass;
+    }
+
+    /**
+     * Access the InputSplit of the mapper.
+     */
+    virtual const std::string& getInputSplit() {
+      return *inputSplit;
+    }
+
+    virtual void emit(const string& key, const string& value) {
+      progress();
+      if (writer != NULL) {
+        writer->emit(key, value);
+      } else if (partitioner != NULL) {
+        int part = partitioner->partition(key, numReduces);
+        uplink->partitionedOutput(part, key, value);
+      } else {
+        uplink->output(key, value);
+      }
+    }
+
+    /**
+     * Register a counter with the given group and name.
+     */
+    virtual Counter* getCounter(const std::string& group, 
+                               const std::string& name) {
+      int id = registeredCounterIds.size();
+      registeredCounterIds.push_back(id);
+      uplink->registerCounter(id, group, name);
+      return new Counter(id);
+    }
+
+    /**
+     * Increment the value of the counter with the given amount.
+     */
+    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
+      uplink->incrementCounter(counter, amount); 
+    }
+
+    void closeAll() {
+      if (reader) {
+        reader->close();
+      }
+      if (mapper) {
+        mapper->close();
+      }
+      if (reducer) {
+        reducer->close();
+      }
+      if (writer) {
+        writer->close();
+      }
+    }
+
+    virtual ~TaskContextImpl() {
+      delete jobConf;
+      delete inputKeyClass;
+      delete inputValueClass;
+      delete inputSplit;
+      if (reader) {
+        delete value;
+      }
+      delete reader;
+      delete mapper;
+      delete reducer;
+      delete writer;
+      delete partitioner;
+      pthread_mutex_destroy(&mutexDone);
+    }
+  };
+
+  /**
+   * Ping the parent every 5 seconds to know if it is alive 
+   */
+  void* ping(void* ptr) {
+    TaskContextImpl* context = (TaskContextImpl*) ptr;
+    char* portStr = getenv("mapreduce.pipes.command.port");
+    int MAX_RETRIES = 3;
+    int remaining_retries = MAX_RETRIES;
+    while (!context->isDone()) {
+      try{
+        sleep(5);
+        int sock = -1;
+        if (portStr) {
+          sock = socket(PF_INET, SOCK_STREAM, 0);
+          HADOOP_ASSERT(sock != - 1,
+                        string("problem creating socket: ") + strerror(errno));
+          sockaddr_in addr;
+          addr.sin_family = AF_INET;
+          addr.sin_port = htons(toInt(portStr));
+          addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
+          HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
+                        string("problem connecting command socket: ") +
+                        strerror(errno));
+
+        }
+        if (sock != -1) {
+          int result = shutdown(sock, SHUT_RDWR);
+          HADOOP_ASSERT(result == 0, "problem shutting socket");
+          result = close(sock);
+          HADOOP_ASSERT(result == 0, "problem closing socket");
+        }
+        remaining_retries = MAX_RETRIES;
+      } catch (Error& err) {
+        if (!context->isDone()) {
+          fprintf(stderr, "Hadoop Pipes Exception: in ping %s\n", 
+                err.getMessage().c_str());
+          remaining_retries -= 1;
+          if (remaining_retries == 0) {
+            exit(1);
+          }
+        } else {
+          return NULL;
+        }
+      }
+    }
+    return NULL;
+  }
+
+  /**
+   * Run the assigned task in the framework.
+   * The user's main function should set the various functions using the 
+   * set* functions above and then call this.
+   * @return true, if the task succeeded.
+   */
+  bool runTask(const Factory& factory) {
+    try {
+      TaskContextImpl* context = new TaskContextImpl(factory);
+      Protocol* connection;
+      char* portStr = getenv("mapreduce.pipes.command.port");
+      int sock = -1;
+      FILE* stream = NULL;
+      FILE* outStream = NULL;
+      char *bufin = NULL;
+      char *bufout = NULL;
+      if (portStr) {
+        sock = socket(PF_INET, SOCK_STREAM, 0);
+        HADOOP_ASSERT(sock != - 1,
+                      string("problem creating socket: ") + strerror(errno));
+        sockaddr_in addr;
+        addr.sin_family = AF_INET;
+        addr.sin_port = htons(toInt(portStr));
+        addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
+        HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
+                      string("problem connecting command socket: ") +
+                      strerror(errno));
+
+        stream = fdopen(sock, "r");
+        outStream = fdopen(sock, "w");
+
+        // increase buffer size
+        int bufsize = 128*1024;
+        int setbuf;
+        bufin = new char[bufsize];
+        bufout = new char[bufsize];
+        setbuf = setvbuf(stream, bufin, _IOFBF, bufsize);
+        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for inStream: ")
+                                     + strerror(errno));
+        setbuf = setvbuf(outStream, bufout, _IOFBF, bufsize);
+        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for outStream: ")
+                                     + strerror(errno));
+        connection = new BinaryProtocol(stream, context, outStream);
+      } else if (getenv("mapreduce.pipes.commandfile")) {
+        char* filename = getenv("mapreduce.pipes.commandfile");
+        string outFilename = filename;
+        outFilename += ".out";
+        stream = fopen(filename, "r");
+        outStream = fopen(outFilename.c_str(), "w");
+        connection = new BinaryProtocol(stream, context, outStream);
+      } else {
+        connection = new TextProtocol(stdin, context, stdout);
+      }
+      context->setProtocol(connection, connection->getUplink());
+      pthread_t pingThread;
+      pthread_create(&pingThread, NULL, ping, (void*)(context));
+      context->waitForTask();
+      while (!context->isDone()) {
+        context->nextKey();
+      }
+      context->closeAll();
+      connection->getUplink()->done();
+      pthread_join(pingThread,NULL);
+      delete context;
+      delete connection;
+      if (stream != NULL) {
+        fflush(stream);
+      }
+      if (outStream != NULL) {
+        fflush(outStream);
+      }
+      fflush(stdout);
+      if (sock != -1) {
+        int result = shutdown(sock, SHUT_RDWR);
+        HADOOP_ASSERT(result == 0, "problem shutting socket");
+        result = close(sock);
+        HADOOP_ASSERT(result == 0, "problem closing socket");
+      }
+      if (stream != NULL) {
+        //fclose(stream);
+      }
+      if (outStream != NULL) {
+        //fclose(outStream);
+      } 
+      delete bufin;
+      delete bufout;
+      return true;
+    } catch (Error& err) {
+      fprintf(stderr, "Hadoop Pipes Exception: %s\n", 
+              err.getMessage().c_str());
+      return false;
+    }
+  }
+}
+
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh b/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh
new file mode 100644
index 0000000..cadfd76
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh
@@ -0,0 +1,170 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef HADOOP_SERIAL_UTILS_HH
+#define HADOOP_SERIAL_UTILS_HH
+
+#include <string>
+#include <stdint.h>
+
+namespace HadoopUtils {
+
+  /**
+   * A simple exception class that records a message for the user.
+   */
+  class Error {
+  private:
+    std::string error;
+  public:
+
+    /**
+     * Create an error object with the given message.
+     */
+    Error(const std::string& msg);
+
+    /**
+     * Construct an error object with the given message that was created on
+     * the given file, line, and functino.
+     */
+    Error(const std::string& msg, 
+          const std::string& file, int line, const std::string& function);
+
+    /**
+     * Get the error message.
+     */
+    const std::string& getMessage() const;
+  };
+
+  /**
+   * Check to make sure that the condition is true, and throw an exception
+   * if it is not. The exception will contain the message and a description
+   * of the source location.
+   */
+  #define HADOOP_ASSERT(CONDITION, MESSAGE) \
+    { \
+      if (!(CONDITION)) { \
+        throw HadoopUtils::Error((MESSAGE), __FILE__, __LINE__, \
+                                    __func__); \
+      } \
+    }
+
+  /**
+   * An interface for an input stream.
+   */
+  class InStream {
+  public:
+    /**
+     * Reads len bytes from the stream into the buffer.
+     * @param buf the buffer to read into
+     * @param buflen the length of the buffer
+     * @throws Error if there are problems reading
+     */
+    virtual void read(void *buf, size_t len) = 0;
+    virtual ~InStream() {}
+  };
+
+  /**
+   * An interface for an output stream.
+   */
+  class OutStream {
+  public:
+    /**
+     * Write the given buffer to the stream.
+     * @param buf the data to write
+     * @param len the number of bytes to write
+     * @throws Error if there are problems writing
+     */
+    virtual void write(const void *buf, size_t len) = 0;
+    /**
+     * Flush the data to the underlying store.
+     */
+    virtual void flush() = 0;
+    virtual ~OutStream() {}
+  };
+
+  /**
+   * A class to read a file as a stream.
+   */
+  class FileInStream : public InStream {
+  public:
+    FileInStream();
+    bool open(const std::string& name);
+    bool open(FILE* file);
+    void read(void *buf, size_t buflen);
+    bool skip(size_t nbytes);
+    bool close();
+    virtual ~FileInStream();
+  private:
+    /**
+     * The file to write to.
+     */
+    FILE *mFile;
+    /**
+     * Does is this class responsible for closing the FILE*?
+     */
+    bool isOwned;
+  };
+
+  /**
+   * A class to write a stream to a file.
+   */
+  class FileOutStream: public OutStream {
+  public:
+
+    /**
+     * Create a stream that isn't bound to anything.
+     */
+    FileOutStream();
+
+    /**
+     * Create the given file, potentially overwriting an existing file.
+     */
+    bool open(const std::string& name, bool overwrite);
+    bool open(FILE* file);
+    void write(const void* buf, size_t len);
+    bool advance(size_t nbytes);
+    void flush();
+    bool close();
+    virtual ~FileOutStream();
+  private:
+    FILE *mFile;
+    bool isOwned;
+  };
+
+  /**
+   * A stream that reads from a string.
+   */
+  class StringInStream: public InStream {
+  public:
+    StringInStream(const std::string& str);
+    virtual void read(void *buf, size_t buflen);
+  private:
+    const std::string& buffer;
+    std::string::const_iterator itr;
+  };
+
+  void serializeInt(int32_t t, OutStream& stream);
+  int32_t deserializeInt(InStream& stream);
+  void serializeLong(int64_t t, OutStream& stream);
+  int64_t deserializeLong(InStream& stream);
+  void serializeFloat(float t, OutStream& stream);
+  float deserializeFloat(InStream& stream);
+  void serializeString(const std::string& t, OutStream& stream);
+  void deserializeString(std::string& t, InStream& stream);
+}
+
+#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh b/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh
new file mode 100644
index 0000000..4720172
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef HADOOP_STRING_UTILS_HH
+#define HADOOP_STRING_UTILS_HH
+
+#include <stdint.h>
+#include <string>
+#include <vector>
+
+namespace HadoopUtils {
+
+  /**
+   * Convert an integer to a string.
+   */
+  std::string toString(int32_t x);
+
+  /**
+   * Convert a string to an integer.
+   * @throws Error if the string is not a valid integer
+   */
+  int32_t toInt(const std::string& val);
+
+  /**
+   * Convert the string to a float.
+   * @throws Error if the string is not a valid float
+   */
+  float toFloat(const std::string& val);
+
+  /**
+   * Convert the string to a boolean.
+   * @throws Error if the string is not a valid boolean value
+   */
+  bool toBool(const std::string& val);
+
+  /**
+   * Get the current time in the number of milliseconds since 1970.
+   */
+  uint64_t getCurrentMillis();
+
+  /**
+   * Split a string into "words". Multiple deliminators are treated as a single
+   * word break, so no zero-length words are returned.
+   * @param str the string to split
+   * @param separator a list of characters that divide words
+   */
+  std::vector<std::string> splitString(const std::string& str,
+                                       const char* separator);
+
+  /**
+   * Quote a string to avoid "\", non-printable characters, and the 
+   * deliminators.
+   * @param str the string to quote
+   * @param deliminators the set of characters to always quote
+   */
+  std::string quoteString(const std::string& str,
+                          const char* deliminators);
+
+  /**
+   * Unquote the given string to return the original string.
+   * @param str the string to unquote
+   */
+  std::string unquoteString(const std::string& str);
+
+}
+
+#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc b/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc
new file mode 100644
index 0000000..03d009b
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc
@@ -0,0 +1,294 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "hadoop/SerialUtils.hh"
+#include "hadoop/StringUtils.hh"
+
+#include <errno.h>
+#include <rpc/types.h>
+#include <rpc/xdr.h>
+#include <string>
+#include <string.h>
+
+using std::string;
+
+namespace HadoopUtils {
+
+  Error::Error(const std::string& msg): error(msg) {
+  }
+
+  Error::Error(const std::string& msg, 
+               const std::string& file, int line, 
+               const std::string& function) {
+    error = msg + " at " + file + ":" + toString(line) + 
+            " in " + function;
+  }
+
+  const std::string& Error::getMessage() const {
+    return error;
+  }
+
+  FileInStream::FileInStream()
+  {
+    mFile = NULL;
+    isOwned = false;
+  }
+
+  bool FileInStream::open(const std::string& name)
+  {
+    mFile = fopen(name.c_str(), "rb");
+    isOwned = true;
+    return (mFile != NULL);
+  }
+
+  bool FileInStream::open(FILE* file)
+  {
+    mFile = file;
+    isOwned = false;
+    return (mFile != NULL);
+  }
+
+  void FileInStream::read(void *buf, size_t len)
+  {
+    size_t result = fread(buf, len, 1, mFile);
+    if (result == 0) {
+      if (feof(mFile)) {
+        HADOOP_ASSERT(false, "end of file");
+      } else {
+        HADOOP_ASSERT(false, string("read error on file: ") + strerror(errno));
+      }
+    }
+  }
+
+  bool FileInStream::skip(size_t nbytes)
+  {
+    return (0==fseek(mFile, nbytes, SEEK_CUR));
+  }
+
+  bool FileInStream::close()
+  {
+    int ret = 0;
+    if (mFile != NULL && isOwned) {
+      ret = fclose(mFile);
+    }
+    mFile = NULL;
+    return (ret==0);
+  }
+
+  FileInStream::~FileInStream()
+  {
+    if (mFile != NULL) {
+      close();
+    }
+  }
+
+  FileOutStream::FileOutStream()
+  {
+    mFile = NULL;
+    isOwned = false;
+  }
+
+  bool FileOutStream::open(const std::string& name, bool overwrite)
+  {
+    if (!overwrite) {
+      mFile = fopen(name.c_str(), "rb");
+      if (mFile != NULL) {
+        fclose(mFile);
+        return false;
+      }
+    }
+    mFile = fopen(name.c_str(), "wb");
+    isOwned = true;
+    return (mFile != NULL);
+  }
+
+  bool FileOutStream::open(FILE* file)
+  {
+    mFile = file;
+    isOwned = false;
+    return (mFile != NULL);
+  }
+
+  void FileOutStream::write(const void* buf, size_t len)
+  {
+    size_t result = fwrite(buf, len, 1, mFile);
+    HADOOP_ASSERT(result == 1,
+                  string("write error to file: ") + strerror(errno));
+  }
+
+  bool FileOutStream::advance(size_t nbytes)
+  {
+    return (0==fseek(mFile, nbytes, SEEK_CUR));
+  }
+
+  bool FileOutStream::close()
+  {
+    int ret = 0;
+    if (mFile != NULL && isOwned) {
+      ret = fclose(mFile);
+    }
+    mFile = NULL;
+    return (ret == 0);
+  }
+
+  void FileOutStream::flush()
+  {
+    fflush(mFile);
+  }
+
+  FileOutStream::~FileOutStream()
+  {
+    if (mFile != NULL) {
+      close();
+    }
+  }
+
+  StringInStream::StringInStream(const std::string& str): buffer(str) {
+    itr = buffer.begin();
+  }
+
+  void StringInStream::read(void *buf, size_t buflen) {
+    size_t bytes = 0;
+    char* output = (char*) buf;
+    std::string::const_iterator end = buffer.end();
+    while (bytes < buflen) {
+      output[bytes++] = *itr;
+      ++itr;
+      if (itr == end) {
+        break;
+      }
+    }
+    HADOOP_ASSERT(bytes == buflen, "unexpected end of string reached");
+  }
+
+  void serializeInt(int32_t t, OutStream& stream) {
+    serializeLong(t,stream);
+  }
+
+  void serializeLong(int64_t t, OutStream& stream)
+  {
+    if (t >= -112 && t <= 127) {
+      int8_t b = t;
+      stream.write(&b, 1);
+      return;
+    }
+        
+    int8_t len = -112;
+    if (t < 0) {
+      t ^= -1ll; // reset the sign bit
+      len = -120;
+    }
+        
+    uint64_t tmp = t;
+    while (tmp != 0) {
+      tmp = tmp >> 8;
+      len--;
+    }
+  
+    stream.write(&len, 1);      
+    len = (len < -120) ? -(len + 120) : -(len + 112);
+        
+    for (uint32_t idx = len; idx != 0; idx--) {
+      uint32_t shiftbits = (idx - 1) * 8;
+      uint64_t mask = 0xFFll << shiftbits;
+      uint8_t b = (t & mask) >> shiftbits;
+      stream.write(&b, 1);
+    }
+  }
+
+  int32_t deserializeInt(InStream& stream) {
+    return deserializeLong(stream);
+  }
+
+  int64_t deserializeLong(InStream& stream)
+  {
+    int8_t b;
+    stream.read(&b, 1);
+    if (b >= -112) {
+      return b;
+    }
+    bool negative;
+    int len;
+    if (b < -120) {
+      negative = true;
+      len = -120 - b;
+    } else {
+      negative = false;
+      len = -112 - b;
+    }
+    uint8_t barr[len];
+    stream.read(barr, len);
+    int64_t t = 0;
+    for (int idx = 0; idx < len; idx++) {
+      t = t << 8;
+      t |= (barr[idx] & 0xFF);
+    }
+    if (negative) {
+      t ^= -1ll;
+    }
+    return t;
+  }
+
+  void serializeFloat(float t, OutStream& stream)
+  {
+    char buf[sizeof(float)];
+    XDR xdrs;
+    xdrmem_create(&xdrs, buf, sizeof(float), XDR_ENCODE);
+    xdr_float(&xdrs, &t);
+    stream.write(buf, sizeof(float));
+  }
+
+  void deserializeFloat(float& t, InStream& stream)
+  {
+    char buf[sizeof(float)];
+    stream.read(buf, sizeof(float));
+    XDR xdrs;
+    xdrmem_create(&xdrs, buf, sizeof(float), XDR_DECODE);
+    xdr_float(&xdrs, &t);
+  }
+
+  void serializeString(const std::string& t, OutStream& stream)
+  {
+    serializeInt(t.length(), stream);
+    if (t.length() > 0) {
+      stream.write(t.data(), t.length());
+    }
+  }
+
+  void deserializeString(std::string& t, InStream& stream)
+  {
+    int32_t len = deserializeInt(stream);
+    if (len > 0) {
+      // resize the string to the right length
+      t.resize(len);
+      // read into the string in 64k chunks
+      const int bufSize = 65536;
+      int offset = 0;
+      char buf[bufSize];
+      while (len > 0) {
+        int chunkLength = len > bufSize ? bufSize : len;
+        stream.read(buf, chunkLength);
+        t.replace(offset, chunkLength, buf, chunkLength);
+        offset += chunkLength;
+        len -= chunkLength;
+      }
+    } else {
+      t.clear();
+    }
+  }
+
+}
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc b/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc
new file mode 100644
index 0000000..0d2a878
--- /dev/null
+++ b/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc
@@ -0,0 +1,180 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "hadoop/StringUtils.hh"
+#include "hadoop/SerialUtils.hh"
+
+#include <errno.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <strings.h>
+#include <sys/time.h>
+
+using std::string;
+using std::vector;
+
+namespace HadoopUtils {
+
+  string toString(int32_t x) {
+    char str[100];
+    sprintf(str, "%d", x);
+    return str;
+  }
+
+  int toInt(const string& val) {
+    int result;
+    char trash;
+    int num = sscanf(val.c_str(), "%d%c", &result, &trash);
+    HADOOP_ASSERT(num == 1,
+                  "Problem converting " + val + " to integer.");
+    return result;
+  }
+
+  float toFloat(const string& val) {
+    float result;
+    char trash;
+    int num = sscanf(val.c_str(), "%f%c", &result, &trash);
+    HADOOP_ASSERT(num == 1,
+                  "Problem converting " + val + " to float.");
+    return result;
+  }
+
+  bool toBool(const string& val) {
+    if (val == "true") {
+      return true;
+    } else if (val == "false") {
+      return false;
+    } else {
+      HADOOP_ASSERT(false,
+                    "Problem converting " + val + " to boolean.");
+    }
+  }
+
+  /**
+   * Get the current time in the number of milliseconds since 1970.
+   */
+  uint64_t getCurrentMillis() {
+    struct timeval tv;
+    struct timezone tz;
+    int sys = gettimeofday(&tv, &tz);
+    HADOOP_ASSERT(sys != -1, strerror(errno));
+    return tv.tv_sec * 1000 + tv.tv_usec / 1000;
+  }
+
+  vector<string> splitString(const std::string& str,
+			     const char* separator) {
+    vector<string> result;
+    string::size_type prev_pos=0;
+    string::size_type pos=0;
+    while ((pos = str.find_first_of(separator, prev_pos)) != string::npos) {
+      if (prev_pos < pos) {
+	result.push_back(str.substr(prev_pos, pos-prev_pos));
+      }
+      prev_pos = pos + 1;
+    }
+    if (prev_pos < str.size()) {
+      result.push_back(str.substr(prev_pos));
+    }
+    return result;
+  }
+
+  string quoteString(const string& str,
+                     const char* deliminators) {
+    
+    string result(str);
+    for(int i=result.length() -1; i >= 0; --i) {
+      char ch = result[i];
+      if (!isprint(ch) ||
+          ch == '\\' || 
+          strchr(deliminators, ch)) {
+        switch (ch) {
+        case '\\':
+          result.replace(i, 1, "\\\\");
+          break;
+        case '\t':
+          result.replace(i, 1, "\\t");
+          break;
+        case '\n':
+          result.replace(i, 1, "\\n");
+          break;
+        case ' ':
+          result.replace(i, 1, "\\s");
+          break;
+        default:
+          char buff[4];
+          sprintf(buff, "\\%02x", static_cast<unsigned char>(result[i]));
+          result.replace(i, 1, buff);
+        }
+      }
+    }
+    return result;
+  }
+
+  string unquoteString(const string& str) {
+    string result(str);
+    string::size_type current = result.find('\\');
+    while (current != string::npos) {
+      if (current + 1 < result.size()) {
+        char new_ch;
+        int num_chars;
+        if (isxdigit(result[current+1])) {
+          num_chars = 2;
+          HADOOP_ASSERT(current + num_chars < result.size(),
+                     "escape pattern \\<hex><hex> is missing second digit in '"
+                     + str + "'");
+          char sub_str[3];
+          sub_str[0] = result[current+1];
+          sub_str[1] = result[current+2];
+          sub_str[2] = '\0';
+          char* end_ptr = NULL;
+          long int int_val = strtol(sub_str, &end_ptr, 16);
+          HADOOP_ASSERT(*end_ptr == '\0' && int_val >= 0,
+                     "escape pattern \\<hex><hex> is broken in '" + str + "'");
+          new_ch = static_cast<char>(int_val);
+        } else {
+          num_chars = 1;
+          switch(result[current+1]) {
+          case '\\':
+            new_ch = '\\';
+            break;
+          case 't':
+            new_ch = '\t';
+            break;
+          case 'n':
+            new_ch = '\n';
+            break;
+          case 's':
+            new_ch = ' ';
+            break;
+          default:
+            string msg("unknow n escape character '");
+            msg += result[current+1];
+            HADOOP_ASSERT(false, msg + "' found in '" + str + "'");
+          }
+        }
+        result.replace(current, 1 + num_chars, 1, new_ch);
+        current = result.find('\\', current+1);
+      } else {
+        HADOOP_ASSERT(false, "trailing \\ in '" + str + "'");
+      }
+    }
+    return result;
+  }
+
+}
diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index 5d99dea..2fb23c7 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -57,6 +57,13 @@
       <artifactId>hadoop-extras</artifactId>
       <scope>compile</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-pipes</artifactId>
+      <scope>compile</scope>
+      <type>pom</type>
+      <version>${project.version}</version>
+    </dependency>
   </dependencies>
 
   <build>
@@ -79,4 +86,44 @@
     </plugins>
   </build>
 
+  <profiles>
+    <profile>
+      <id>dist</id>
+      <activation>
+        <activeByDefault>false</activeByDefault>
+      </activation>
+      <build>
+        <plugins>
+         <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-assembly-plugin</artifactId>
+            <dependencies>
+              <dependency>
+                <groupId>org.apache.hadoop</groupId>
+                <artifactId>hadoop-assemblies</artifactId>
+                <version>${project.version}</version>
+              </dependency>
+            </dependencies>
+            <executions>
+              <execution>
+                <id>dist</id>
+                <phase>prepare-package</phase>
+                <goals>
+                  <goal>single</goal>
+                </goals>
+                <configuration>
+                  <appendAssemblyId>false</appendAssemblyId>
+                  <attach>false</attach>
+                  <finalName>${project.artifactId}-${project.version}</finalName>
+                  <descriptorRefs>
+                    <descriptorRef>hadoop-tools</descriptorRef>
+                  </descriptorRefs>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+  </profiles>
 </project>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index e83d6c8..1a92ef6 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -34,6 +34,7 @@
     <module>hadoop-rumen</module>
     <module>hadoop-tools-dist</module>
     <module>hadoop-extras</module>
+    <module>hadoop-pipes</module>
   </modules>
 
   <build>
-- 
1.7.0.4

