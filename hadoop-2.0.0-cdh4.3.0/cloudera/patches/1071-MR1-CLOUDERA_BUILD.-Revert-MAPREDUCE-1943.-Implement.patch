From 88d6e697320762341664649e50040a906680772a Mon Sep 17 00:00:00 2001
From: Tom White <tom@cloudera.com>
Date: Thu, 15 Dec 2011 16:05:36 -0800
Subject: [PATCH 1071/1357] MR1: CLOUDERA_BUILD. Revert "MAPREDUCE-1943. Implement limits on per-job JobConf, Counters, StatusReport, Split-Sizes"

This reverts commit e3f8dc3926d119ce3b765325114e5b8ada01120f.
---
 src/mapred/mapred-default.xml                      |    7 -
 .../hadoop/mapred/CompletedJobStatusStore.java     |    7 +-
 src/mapred/org/apache/hadoop/mapred/JobClient.java |    8 +-
 .../org/apache/hadoop/mapred/JobInProgress.java    |  104 +++---------
 .../org/apache/hadoop/mapred/JobTracker.java       |   22 +--
 src/mapred/org/apache/hadoop/mapred/Task.java      |    8 -
 .../org/apache/hadoop/mapred/TaskTracker.java      |   20 --
 .../apache/hadoop/mapreduce/split/JobSplit.java    |    1 -
 .../hadoop/mapreduce/split/JobSplitWriter.java     |   34 +---
 .../mapreduce/split/SplitMetaInfoReader.java       |    7 -
 .../org/apache/hadoop/mapred/TestJobHistory.java   |    9 +-
 .../hadoop/mapred/TestUserDefinedCounters.java     |   57 +------
 .../hadoop/mapreduce/split/TestBlockLimits.java    |  185 --------------------
 .../hadoop/mapreduce/split/TestJobSplitWriter.java |  138 ---------------
 src/webapps/job/jobdetails.jsp                     |   15 +--
 15 files changed, 45 insertions(+), 577 deletions(-)
 delete mode 100644 src/test/org/apache/hadoop/mapreduce/split/TestBlockLimits.java
 delete mode 100644 src/test/org/apache/hadoop/mapreduce/split/TestJobSplitWriter.java

diff --git a/src/mapred/mapred-default.xml b/src/mapred/mapred-default.xml
index 4db4fe4..18ac940 100644
--- a/src/mapred/mapred-default.xml
+++ b/src/mapred/mapred-default.xml
@@ -1221,13 +1221,6 @@
   </description>
 </property>
 
-<property>
-  <name>mapreduce.job.counters.limit</name>
-  <value>120</value>
-  <description>Limit on the number of counters allowed per job.
-  </description>
-</property>
-
 <!--  end of node health script variables -->
 
 </configuration>
diff --git a/src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java b/src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java
index c297d7c..ad63f73 100644
--- a/src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java
+++ b/src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java
@@ -178,11 +178,8 @@ class CompletedJobStatusStore implements Runnable {
         job.getStatus().write(dataOut);
 
         job.getProfile().write(dataOut);
-        
-        Counters counters = new Counters();
-        boolean isFine = job.getCounters(counters);
-        counters = (isFine? counters: new Counters());
-        counters.write(dataOut);
+
+        job.getCounters().write(dataOut);
 
         TaskCompletionEvent[] events = 
                 job.getTaskCompletionEvents(0, Integer.MAX_VALUE);
diff --git a/src/mapred/org/apache/hadoop/mapred/JobClient.java b/src/mapred/org/apache/hadoop/mapred/JobClient.java
index f7592bb..0a9a1e5 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobClient.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobClient.java
@@ -1383,13 +1383,7 @@ public class JobClient extends Configured implements MRConstants, Tool  {
       }
     }
     LOG.info("Job complete: " + jobId);
-    Counters counters = null;
-    try{
-       counters = job.getCounters();
-    } catch(IOException ie) {
-      counters = null;
-      LOG.info(ie.getMessage());
-    }
+    Counters counters = job.getCounters();
     if (counters != null) {
       counters.log(LOG);
     }
diff --git a/src/mapred/org/apache/hadoop/mapred/JobInProgress.java b/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
index 638b8ab..cf09579 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
@@ -36,6 +36,7 @@ import java.util.SortedSet;
 import java.util.TreeMap;
 import java.util.TreeSet;
 import java.util.Vector;
+import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -44,10 +45,8 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.AuditLogger;
 import org.apache.hadoop.mapred.CleanupQueue.PathDeletionContext;
-import org.apache.hadoop.mapred.Counters.CountersExceededException;
-import org.apache.hadoop.mapred.Counters.Group;
+import org.apache.hadoop.mapred.AuditLogger;
 import org.apache.hadoop.mapred.JobHistory.Values;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.JobSubmissionFiles;
@@ -122,7 +121,7 @@ public class JobInProgress {
   long reduce_input_limit = -1L;
   private static float DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART = 0.05f;
   int completedMapsForReduceSlowstart = 0;
-    
+  
   // runningMapTasks include speculative tasks, so we need to capture 
   // speculative tasks separately 
   int speculativeMapTasks = 0;
@@ -506,9 +505,7 @@ public class JobInProgress {
    * this job.
    */
   public void updateMetrics() {
-    Counters counters = new Counters();
-    boolean isFine = getCounters(counters);
-    counters = (isFine? counters: new Counters());
+    Counters counters = getCounters();
     for (Counters.Group group : counters) {
       jobMetrics.setTag("group", group.getDisplayName());
       for (Counters.Counter counter : group) {
@@ -816,8 +813,7 @@ public class JobInProgress {
   TaskSplitMetaInfo[] createSplits(org.apache.hadoop.mapreduce.JobID jobId)
   throws IOException {
     TaskSplitMetaInfo[] allTaskSplitMetaInfo =
-      SplitMetaInfoReader.readSplitMetaInfo(jobId, fs, jobtracker.getConf(),
-          jobSubmitDir);
+      SplitMetaInfoReader.readSplitMetaInfo(jobId, fs, conf, jobSubmitDir);
     return allTaskSplitMetaInfo;
   }
 
@@ -1053,7 +1049,6 @@ public class JobInProgress {
     }
     return results;
   }
-  
 
   ////////////////////////////////////////////////////
   // Status update methods
@@ -1258,47 +1253,27 @@ public class JobInProgress {
   
   /**
    *  Returns map phase counters by summing over all map tasks in progress.
-   *  This method returns true if counters are within limit or false.
    */
-  public synchronized boolean getMapCounters(Counters counters) {
-    try {
-      counters = incrementTaskCounters(counters, maps);
-    } catch(CountersExceededException ce) {
-      LOG.info("Counters Exceeded for Job: " + jobId, ce);
-      return false;
-    }
-    return true;
+  public synchronized Counters getMapCounters() {
+    return incrementTaskCounters(new Counters(), maps);
   }
     
   /**
    *  Returns map phase counters by summing over all map tasks in progress.
-   *  This method returns true if counters are within limits and false otherwise.
    */
-  public synchronized boolean getReduceCounters(Counters counters) {
-    try {
-      counters = incrementTaskCounters(counters, reduces);
-    } catch(CountersExceededException ce) {
-      LOG.info("Counters Exceeded for Job: " + jobId, ce);
-      return false;
-    }
-    return true;
+  public synchronized Counters getReduceCounters() {
+    return incrementTaskCounters(new Counters(), reduces);
   }
     
   /**
    *  Returns the total job counters, by adding together the job, 
-   *  the map and the reduce counters. This method returns true if
-   *  counters are within limits and false otherwise.
+   *  the map and the reduce counters.
    */
-  public synchronized boolean getCounters(Counters result) {
-    try {
-      result.incrAllCounters(getJobCounters());
-      incrementTaskCounters(result, maps);
-      incrementTaskCounters(result, reduces);
-    } catch(CountersExceededException ce) {
-      LOG.info("Counters Exceeded for Job: " + jobId, ce);
-      return false;
-    }
-    return true;
+  public synchronized Counters getCounters() {
+    Counters result = new Counters();
+    result.incrAllCounters(getJobCounters());
+    incrementTaskCounters(result, maps);
+    return incrementTaskCounters(result, reduces);
   }
     
   /**
@@ -2610,9 +2585,6 @@ public class JobInProgress {
       retireMap(tip);
       if ((finishedMapTasks + failedMapTIPs) == (numMapTasks)) {
         this.status.setMapProgress(1.0f);
-        if (canLaunchJobCleanupTask()) {
-          checkCounterLimitsAndFail();
-        }
       }
     } else {
       runningReduceTasks -= 1;
@@ -2625,33 +2597,12 @@ public class JobInProgress {
       retireReduce(tip);
       if ((finishedReduceTasks + failedReduceTIPs) == (numReduceTasks)) {
         this.status.setReduceProgress(1.0f);
-        if (canLaunchJobCleanupTask()) {
-          checkCounterLimitsAndFail();
-        }
       }
     }
+    
     return true;
   }
-  
-  /**
-   * add up the counters and fail the job
-   * if it exceeds the counters. Make sure we do not
-   * recalculate the coutners after we fail the job. Currently
-   * this is taken care by terminateJob() since it does not 
-   * calculate the counters.
-   */
-  private void checkCounterLimitsAndFail() {
-    boolean mapIsFine, reduceIsFine, jobIsFine = true;
-    mapIsFine = getMapCounters(new Counters());
-    reduceIsFine = getReduceCounters(new Counters());
-    jobIsFine = getCounters(new Counters());
-    if (!(mapIsFine && reduceIsFine && jobIsFine)) {
-      status.setFailureInfo("Counters Exceeded: Limit: " + 
-          Counters.MAX_COUNTER_LIMIT);
-      jobtracker.failJob(this);
-    }
-  }
-  
+
   /**
    * Job state change must happen thru this call
    */
@@ -2701,33 +2652,20 @@ public class JobInProgress {
           checkCountersLimitsOrFail();
         }
       }
-     
       this.finishTime = jobtracker.getClock().getTime();
       LOG.info("Job " + this.status.getJobID() + 
-      " has completed successfully.");
-
+               " has completed successfully.");
+      
       // Log the job summary (this should be done prior to logging to 
       // job-history to ensure job-counters are in-sync 
       JobSummary.logJobSummary(this, jobtracker.getClusterStatus(false));
       
-      Counters mapCounters = new Counters();
-      boolean isFine = getMapCounters(mapCounters);
-      mapCounters = (isFine ? mapCounters: new Counters());
-      Counters reduceCounters = new Counters();
-      isFine = getReduceCounters(reduceCounters);;
-      reduceCounters = (isFine ? reduceCounters: new Counters());
-      Counters jobCounters = new Counters();
-      isFine = getCounters(jobCounters);
-      jobCounters = (isFine? jobCounters: new Counters());
-      
       // Log job-history
       JobHistory.JobInfo.logFinished(this.status.getJobID(), finishTime, 
                                      this.finishedMapTasks, 
                                      this.finishedReduceTasks, failedMapTasks, 
-                                     failedReduceTasks, mapCounters,
-                                     reduceCounters, jobCounters);
-      
-      
+                                     failedReduceTasks, getMapCounters(),
+                                     getReduceCounters(), getCounters());
       // Note that finalize will close the job history handles which garbage collect
       // might try to finalize
       garbageCollect();
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTracker.java b/src/mapred/org/apache/hadoop/mapred/JobTracker.java
index a45eef4..7ccadf7 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobTracker.java
@@ -76,7 +76,6 @@ import org.apache.hadoop.ipc.RPC;
 import org.apache.hadoop.ipc.Server;
 import org.apache.hadoop.ipc.RPC.VersionMismatch;
 import org.apache.hadoop.mapred.AuditLogger.Constants;
-import org.apache.hadoop.mapred.Counters.CountersExceededException;
 import org.apache.hadoop.mapred.JobHistory.Keys;
 import org.apache.hadoop.mapred.JobHistory.Listener;
 import org.apache.hadoop.mapred.JobHistory.Values;
@@ -134,7 +133,7 @@ public class JobTracker implements MRConstants, JTProtocols, JobTrackerMXBean {
   static long TASKTRACKER_EXPIRY_INTERVAL = 10 * 60 * 1000;
   static long RETIRE_JOB_INTERVAL;
   static long RETIRE_JOB_CHECK_INTERVAL;
-  
+
   private final long DELEGATION_TOKEN_GC_INTERVAL = 3600000; // 1 hour
   private final DelegationTokenSecretManager secretManager;
 
@@ -568,10 +567,7 @@ public class JobTracker implements MRConstants, JTProtocols, JobTrackerMXBean {
     }
 
     synchronized void addToCache(JobInProgress job) {
-      Counters counters = new Counters();
-      boolean isFine = job.getCounters(counters);
-      counters = (isFine? counters: new Counters());
-      RetireJobInfo info = new RetireJobInfo(counters, job.getStatus(),
+      RetireJobInfo info = new RetireJobInfo(job.getCounters(), job.getStatus(),
           job.getProfile(), job.getFinishTime(), job.getHistoryFile());
       jobRetireInfoQ.add(info);
       jobIDStatusMap.put(info.status.getJobID(), info);
@@ -4168,18 +4164,8 @@ public class JobTracker implements MRConstants, JTProtocols, JobTrackerMXBean {
 
         // check the job-access
         aclsManager.checkAccess(job, callerUGI, Operation.VIEW_JOB_COUNTERS);
-        Counters counters = new Counters();
-        if (isJobInited(job)) {
-          boolean isFine = job.getCounters(counters);
-          if (!isFine) {
-            throw new IOException("Counters Exceeded limit: " + 
-                Counters.MAX_COUNTER_LIMIT);
-          }
-          return counters;
-        }
-        else {
-          return EMPTY_COUNTERS;
-        }
+
+        return isJobInited(job) ? job.getCounters() : EMPTY_COUNTERS;
       } else {
         RetireJobInfo info = retireJobs.get(jobid);
         if (info != null) {
diff --git a/src/mapred/org/apache/hadoop/mapred/Task.java b/src/mapred/org/apache/hadoop/mapred/Task.java
index 50e2be6..518d742 100644
--- a/src/mapred/org/apache/hadoop/mapred/Task.java
+++ b/src/mapred/org/apache/hadoop/mapred/Task.java
@@ -545,8 +545,6 @@ abstract public class Task implements Writable, Configurable {
     private Progress taskProgress;
     private JvmContext jvmContext;
     private Thread pingThread = null;
-    private static final int PROGRESS_STATUS_LEN_LIMIT = 512;
-    
     /**
      * flag that indicates whether progress update needs to be sent to parent.
      * If true, it has been set. If false, it has been reset. 
@@ -568,12 +566,6 @@ abstract public class Task implements Writable, Configurable {
       return progressFlag.getAndSet(false);
     }
     public void setStatus(String status) {
-      //Check to see if the status string 
-      // is too long and just concatenate it
-      // to progress limit characters.
-      if (status.length() > PROGRESS_STATUS_LEN_LIMIT) {
-        status = status.substring(0, PROGRESS_STATUS_LEN_LIMIT);
-      }
       taskProgress.setStatus(status);
       // indicate that progress update needs to be sent
       setProgressFlag();
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
index dcbfed9..9a6430c 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
@@ -2851,26 +2851,6 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
         return;
       }
       
-      /** check for counter limits and fail the task in case limits are exceeded **/
-      Counters taskCounters = taskStatus.getCounters();
-      if (taskCounters.size() > Counters.MAX_COUNTER_LIMIT ||
-          taskCounters.getGroupNames().size() > Counters.MAX_GROUP_LIMIT) {
-        LOG.warn("Killing task " + task.getTaskID() + ": " +
-        		"Exceeded limit on counters.");
-        try { 
-          reportDiagnosticInfo("Error: Exceeded counter limits - " +
-          		"Counters=" + taskCounters.size() + " Limit=" 
-              + Counters.MAX_COUNTER_LIMIT  + ". " + 
-              "Groups=" + taskCounters.getGroupNames().size() + " Limit=" +
-              Counters.MAX_GROUP_LIMIT);
-          kill(true);
-        } catch(IOException ie) {
-          LOG.error("Error killing task " + task.getTaskID(), ie);
-        } catch (InterruptedException e) {
-          LOG.error("Error killing task " + task.getTaskID(), e);
-        }
-      }
-      
       this.taskStatus.statusUpdate(taskStatus);
       this.lastProgressReport = System.currentTimeMillis();
     }
diff --git a/src/mapred/org/apache/hadoop/mapreduce/split/JobSplit.java b/src/mapred/org/apache/hadoop/mapreduce/split/JobSplit.java
index 7a36a8b..30777ec 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/split/JobSplit.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/split/JobSplit.java
@@ -44,7 +44,6 @@ import org.apache.hadoop.mapreduce.InputSplit;
 public class JobSplit {
   static final int META_SPLIT_VERSION = 1;
   static final byte[] META_SPLIT_FILE_HEADER;
-  
   static {
     try {
       META_SPLIT_FILE_HEADER = "META-SPL".getBytes("UTF-8");
diff --git a/src/mapred/org/apache/hadoop/mapreduce/split/JobSplitWriter.java b/src/mapred/org/apache/hadoop/mapreduce/split/JobSplitWriter.java
index d2aa3d8..9b2327c 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/split/JobSplitWriter.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/split/JobSplitWriter.java
@@ -20,7 +20,6 @@ package org.apache.hadoop.mapreduce.split;
 
 import java.io.IOException;
 import java.io.UnsupportedEncodingException;
-import java.util.Arrays;
 import java.util.List;
 
 import org.apache.hadoop.conf.Configuration;
@@ -33,23 +32,18 @@ import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.serializer.SerializationFactory;
 import org.apache.hadoop.io.serializer.Serializer;
 import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobSubmissionFiles;
 import org.apache.hadoop.mapreduce.split.JobSplit.SplitMetaInfo;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
 /**
  * The class that is used by the Job clients to write splits (both the meta
  * and the raw bytes parts)
  */
 public class JobSplitWriter {
 
-  private static final Log LOG = LogFactory.getLog(JobSplitWriter.class);
   private static final int splitVersion = JobSplit.META_SPLIT_VERSION;
   private static final byte[] SPLIT_FILE_HEADER;
-  static final String MAX_SPLIT_LOCATIONS = "mapreduce.job.max.split.locations";
-  
   static {
     try {
       SPLIT_FILE_HEADER = "SPL".getBytes("UTF-8");
@@ -79,12 +73,12 @@ public class JobSplitWriter {
   }
   
   public static void createSplitFiles(Path jobSubmitDir, 
-      Configuration conf, FileSystem   fs, 
+      Configuration conf, FileSystem fs, 
       org.apache.hadoop.mapred.InputSplit[] splits) 
   throws IOException {
     FSDataOutputStream out = createFile(fs, 
         JobSubmissionFiles.getJobSplitFile(jobSubmitDir), conf);
-    SplitMetaInfo[] info = writeOldSplits(splits, out, conf);
+    SplitMetaInfo[] info = writeOldSplits(splits, out);
     out.close();
     writeJobSplitMetaInfo(fs,JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir), 
         new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION), splitVersion,
@@ -125,17 +119,9 @@ public class JobSplitWriter {
         serializer.open(out);
         serializer.serialize(split);
         int currCount = out.size();
-        String[] locations = split.getLocations();
-        final int max_loc = conf.getInt(MAX_SPLIT_LOCATIONS, 10);
-        if (locations.length > max_loc) {
-          LOG.warn("Max block location exceeded for split: "
-              + split + " splitsize: " + locations.length +
-              " maxsize: " + max_loc);
-          locations = Arrays.copyOf(locations, max_loc);
-        }
         info[i++] = 
           new JobSplit.SplitMetaInfo( 
-              locations, offset,
+              split.getLocations(), offset,
               split.getLength());
         offset += currCount - prevCount;
       }
@@ -145,7 +131,7 @@ public class JobSplitWriter {
   
   private static SplitMetaInfo[] writeOldSplits(
       org.apache.hadoop.mapred.InputSplit[] splits,
-      FSDataOutputStream out, Configuration conf) throws IOException {
+      FSDataOutputStream out) throws IOException {
     SplitMetaInfo[] info = new SplitMetaInfo[splits.length];
     if (splits.length != 0) {
       int i = 0;
@@ -155,16 +141,8 @@ public class JobSplitWriter {
         Text.writeString(out, split.getClass().getName());
         split.write(out);
         int currLen = out.size();
-        String[] locations = split.getLocations();
-        final int max_loc = conf.getInt(MAX_SPLIT_LOCATIONS, 10);
-        if (locations.length > max_loc) {
-          LOG.warn("Max block location exceeded for split: "
-              + split + " splitsize: " + locations.length +
-              " maxsize: " + max_loc);
-          locations = Arrays.copyOf(locations, max_loc);
-        }
         info[i++] = new JobSplit.SplitMetaInfo( 
-            locations, offset,
+            split.getLocations(), offset,
             split.getLength());
         offset += currLen - prevLen;
       }
diff --git a/src/mapred/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java b/src/mapred/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java
index 700ee4b..09e5cd4 100644
--- a/src/mapred/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java
+++ b/src/mapred/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java
@@ -62,16 +62,9 @@ public class SplitMetaInfoReader {
     int numSplits = WritableUtils.readVInt(in); //TODO: check for insane values
     JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo = 
       new JobSplit.TaskSplitMetaInfo[numSplits];
-    final int maxLocations =
-      conf.getInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, Integer.MAX_VALUE);
     for (int i = 0; i < numSplits; i++) {
       JobSplit.SplitMetaInfo splitMetaInfo = new JobSplit.SplitMetaInfo();
       splitMetaInfo.readFields(in);
-      final int numLocations = splitMetaInfo.getLocations().length;
-      if (numLocations > maxLocations) {
-        throw new IOException("Max block location exceeded for split: #"  + i +
-              " splitsize: " + numLocations + " maxsize: " + maxLocations);
-      }
       JobSplit.TaskSplitIndex splitIndex = new JobSplit.TaskSplitIndex(
           JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), 
           splitMetaInfo.getStartOffset());
diff --git a/src/test/org/apache/hadoop/mapred/TestJobHistory.java b/src/test/org/apache/hadoop/mapred/TestJobHistory.java
index efa6e7f..202f17a 100644
--- a/src/test/org/apache/hadoop/mapred/TestJobHistory.java
+++ b/src/test/org/apache/hadoop/mapred/TestJobHistory.java
@@ -555,18 +555,15 @@ public class TestJobHistory extends TestCase {
                values.get(Keys.USER)));
 
     // Validate job counters
-    Counters c = new Counters();
-    jip.getCounters(c);
+    Counters c = jip.getCounters();
     assertTrue("Counters of job obtained from history file did not " +
                "match the expected value",
                c.makeEscapedCompactString().equals(values.get(Keys.COUNTERS)));
-    Counters m = new Counters();
-    jip.getMapCounters(m);
+    Counters m = jip.getMapCounters();
     assertTrue("Map Counters of job obtained from history file did not " +
                "match the expected value", m.makeEscapedCompactString().
                equals(values.get(Keys.MAP_COUNTERS)));
-    Counters r = new Counters();
-    jip.getReduceCounters(r);
+    Counters r = jip.getReduceCounters();
     assertTrue("Reduce Counters of job obtained from history file did not " +
                "match the expected value", r.makeEscapedCompactString().
                equals(values.get(Keys.REDUCE_COUNTERS)));
diff --git a/src/test/org/apache/hadoop/mapred/TestUserDefinedCounters.java b/src/test/org/apache/hadoop/mapred/TestUserDefinedCounters.java
index 44d96cc..9c7737a 100644
--- a/src/test/org/apache/hadoop/mapred/TestUserDefinedCounters.java
+++ b/src/test/org/apache/hadoop/mapred/TestUserDefinedCounters.java
@@ -24,57 +24,28 @@ import java.io.InputStreamReader;
 import java.io.OutputStream;
 import java.io.OutputStreamWriter;
 import java.io.Writer;
-import java.util.Iterator;
-import java.util.Properties;
 
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.Counters.Counter;
 import org.apache.hadoop.mapred.lib.IdentityMapper;
 import org.apache.hadoop.mapred.lib.IdentityReducer;
-import org.apache.hadoop.util.StringUtils;
-import org.hsqldb.lib.StringUtil;
 
 public class TestUserDefinedCounters extends ClusterMapReduceTestCase {
-  protected void setUp() throws Exception {
-    super.setUp();
-    Properties prop = new Properties();
-    prop.put("mapred.job.tracker.persist.jobstatus.active", "true");
-    prop.put("mapred.job.tracker.persist.jobstatus.hours", "1");
-    startCluster(true, prop);
-  }
   
   enum EnumCounter { MAP_RECORDS }
   
   static class CountingMapper<K, V> extends IdentityMapper<K, V> {
-    private JobConf jconf;
-    boolean generateUniqueCounters = false;
-    
-    @Override
-    public void configure(JobConf jconf) {
-      this.jconf = jconf;
-      this.generateUniqueCounters = 
-        jconf.getBoolean("task.generate.unique.counters", false);
-    }
-    
+
     public void map(K key, V value,
         OutputCollector<K, V> output, Reporter reporter)
         throws IOException {
       output.collect(key, value);
       reporter.incrCounter(EnumCounter.MAP_RECORDS, 1);
       reporter.incrCounter("StringCounter", "MapRecords", 1);
-      for (int i =0; i < 50; i++) {
-        if (generateUniqueCounters) {
-          reporter.incrCounter("StringCounter", "countername_" + 
-              jconf.get("mapred.task.id") + "_"+ i, 1);
-        } else {
-          reporter.incrCounter("StringCounter", "countername_" + 
-              i, 1);
-        }
-      }    
     }
+
   }
   
   public void testMapReduceJob() throws Exception {
@@ -123,32 +94,12 @@ public class TestUserDefinedCounters extends ClusterMapReduceTestCase {
       reader.close();
       assertEquals(4, counter);
     }
-
+    
     assertEquals(4,
         runningJob.getCounters().getCounter(EnumCounter.MAP_RECORDS));
-    Counters counters = runningJob.getCounters();
     assertEquals(4,
         runningJob.getCounters().getGroup("StringCounter")
         .getCounter("MapRecords"));
-    assertTrue(counters.getGroupNames().size() <= 51);
-    int i = 0;
-    while (counters.size() < Counters.MAX_COUNTER_LIMIT) {
-      counters.incrCounter("IncrCounter", "limit " + i, 2);
-      i++;
-    }
-    try {
-      counters.incrCounter("IncrCountertest", "test", 2);
-      assertTrue(false);
-    } catch(RuntimeException re) {
-      System.out.println("Exceeded counter " + 
-          StringUtils.stringifyException(re));
-    }
-    conf.setBoolean("task.generate.unique.counters", true);
-    FileOutputFormat.setOutputPath(conf, new Path("output-fail"));
-    try {
-      runningJob = JobClient.runJob(conf);
-    } catch(Exception ie) {
-      System.out.println(StringUtils.stringifyException(ie));
-    }
   }
+
 }
diff --git a/src/test/org/apache/hadoop/mapreduce/split/TestBlockLimits.java b/src/test/org/apache/hadoop/mapreduce/split/TestBlockLimits.java
deleted file mode 100644
index 0b932e1..0000000
--- a/src/test/org/apache/hadoop/mapreduce/split/TestBlockLimits.java
+++ /dev/null
@@ -1,185 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapreduce.split;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Iterator;
-
-import junit.framework.TestCase;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.*;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- * A JUnit test to test limits on block locations
- */
-public class TestBlockLimits extends TestCase {
-  private static String TEST_ROOT_DIR =
-    new File(System.getProperty("test.build.data","/tmp"))
-    .toURI().toString().replace(' ', '+');
-    
-  public void testWithLimits()
-      throws IOException, InterruptedException, ClassNotFoundException {
-    MiniMRCluster mr = null;
-    try {
-      mr = new MiniMRCluster(2, "file:///", 3);
-      Configuration conf = new Configuration();
-      conf.setInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, 10);
-      mr = new MiniMRCluster(2, "file:///", 3, null, null, new JobConf(conf));
-      runCustomFormat(mr);
-    } finally {
-      if (mr != null) { mr.shutdown(); }
-    }
-  }
-  
-  private void runCustomFormat(MiniMRCluster mr) throws IOException {
-    JobConf job = new JobConf(mr.createJobConf());
-    job.setInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, 100);
-    FileSystem fileSys = FileSystem.get(job);
-    Path testDir = new Path(TEST_ROOT_DIR + "/test_mini_mr_local");
-    Path outDir = new Path(testDir, "out");
-    System.out.println("testDir= " + testDir);
-    fileSys.delete(testDir, true);
-    job.setInputFormat(MyInputFormat.class);
-    job.setOutputFormat(MyOutputFormat.class);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(Text.class);
-    
-    job.setMapperClass(MyMapper.class);        
-    job.setNumReduceTasks(0);
-    job.set("non.std.out", outDir.toString());
-    try {
-      JobClient.runJob(job);
-      fail("JobTracker neglected to fail misconfigured job");
-    } catch(IOException ie) {
-      System.out.println("Failed job " + StringUtils.stringifyException(ie));
-    } finally {
-      fileSys.delete(testDir, true);
-    }
-    
-  }
-  
-  static class MyMapper extends MapReduceBase
-    implements Mapper<WritableComparable, Writable,
-                    WritableComparable, Writable> {
-
-    public void map(WritableComparable key, Writable value,
-                  OutputCollector<WritableComparable, Writable> out,
-                  Reporter reporter) throws IOException {
-    }
-  }
-
-  private static class MyInputFormat
-    implements InputFormat<Text, Text> {
-    
-    private static class MySplit implements InputSplit {
-      int first;
-      int length;
-
-      public MySplit() { }
-
-      public MySplit(int first, int length) {
-        this.first = first;
-        this.length = length;
-      }
-
-      public String[] getLocations() {
-        final String[] ret = new String[200];
-        Arrays.fill(ret, "SPLIT");
-        return ret;
-      }
-
-      public long getLength() {
-        return length;
-      }
-
-      public void write(DataOutput out) throws IOException {
-        WritableUtils.writeVInt(out, first);
-        WritableUtils.writeVInt(out, length);
-      }
-
-      public void readFields(DataInput in) throws IOException {
-        first = WritableUtils.readVInt(in);
-        length = WritableUtils.readVInt(in);
-      }
-    }
-    
-    public InputSplit[] getSplits(JobConf job, 
-                                  int numSplits) throws IOException {
-      return new MySplit[]{new MySplit(0, 1), new MySplit(1, 3),
-                           new MySplit(4, 2)};
-    }
-
-    public RecordReader<Text, Text> getRecordReader(InputSplit split,
-                                                           JobConf job, 
-                                                           Reporter reporter)
-                                                           throws IOException {
-      MySplit sp = (MySplit) split;
-      return new RecordReader<Text,Text>() {
-        @Override public boolean next(Text key, Text value) { return false; }
-        @Override public Text createKey() { return new Text(); }
-        @Override public Text createValue() { return new Text(); }
-        @Override public long getPos() throws IOException { return 0; }
-        @Override public void close() throws IOException { }
-        @Override public float getProgress() throws IOException { return 1.0f; }
-      };
-    }
-    
-  }
-  
-
-  static class MyOutputFormat implements OutputFormat {
-    static class MyRecordWriter implements RecordWriter<Object, Object> {
-      private DataOutputStream out;
-      
-      public MyRecordWriter(Path outputFile, JobConf job) throws IOException {
-      }
-      
-      public void write(Object key, Object value) throws IOException {
-        return;
-      }
-
-      public void close(Reporter reporter) throws IOException {
-      }
-    }
-    
-    public RecordWriter getRecordWriter(FileSystem ignored, JobConf job, 
-                                        String name,
-                                        Progressable progress
-                                        ) throws IOException {
-      return new MyRecordWriter(new Path(job.get("non.std.out")), job);
-    }
-
-    public void checkOutputSpecs(FileSystem ignored, 
-                                 JobConf job) throws IOException {
-    }
-  }
-
-}
diff --git a/src/test/org/apache/hadoop/mapreduce/split/TestJobSplitWriter.java b/src/test/org/apache/hadoop/mapreduce/split/TestJobSplitWriter.java
deleted file mode 100644
index 2d3fd48..0000000
--- a/src/test/org/apache/hadoop/mapreduce/split/TestJobSplitWriter.java
+++ /dev/null
@@ -1,138 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapreduce.split;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.InputSplit;
-
-public class TestJobSplitWriter {
-
-  static final String TEST_ROOT = System.getProperty("test.build.data", "/tmp");
-  static final Path TEST_DIR =
-    new Path(TEST_ROOT, TestJobSplitWriter.class.getSimpleName());
-
-  @AfterClass
-  public static void cleanup() throws IOException {
-    final FileSystem fs = FileSystem.getLocal(new Configuration()).getRaw();
-    fs.delete(TEST_DIR, true);
-  }
-
-  static abstract class NewSplit extends InputSplit implements Writable {
-    @Override public long getLength() { return 42L; }
-    @Override public void readFields(DataInput in) throws IOException { }
-    @Override public void write(DataOutput in) throws IOException { }
-  }
-
-  @Test
-  public void testSplitLocationLimit()
-      throws IOException, InterruptedException  {
-    final int SPLITS = 5;
-    final int MAX_LOC = 10;
-    final Path outdir = new Path(TEST_DIR, "testSplitLocationLimit");
-    final String[] locs = getLoc(MAX_LOC + 5);
-    final Configuration conf = new Configuration();
-    final FileSystem rfs = FileSystem.getLocal(conf).getRaw();
-    final InputSplit split = new NewSplit() {
-      @Override public String[] getLocations() { return locs; }
-    };
-    List<InputSplit> splits = Collections.nCopies(SPLITS, split);
-
-    conf.setInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, MAX_LOC);
-    JobSplitWriter.createSplitFiles(outdir, conf,
-        FileSystem.getLocal(conf).getRaw(), splits);
-
-    checkMeta(MAX_LOC,
-        SplitMetaInfoReader.readSplitMetaInfo(null, rfs, conf, outdir),
-        Arrays.copyOf(locs, MAX_LOC));
-
-    conf.setInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, MAX_LOC / 2);
-    try {
-      SplitMetaInfoReader.readSplitMetaInfo(null, rfs, conf, outdir);
-      fail("Reader failed to detect location limit");
-    } catch (IOException e) { }
-  }
-
-  static abstract class OldSplit
-      implements org.apache.hadoop.mapred.InputSplit {
-    @Override public long getLength() { return 42L; }
-    @Override public void readFields(DataInput in) throws IOException { }
-    @Override public void write(DataOutput in) throws IOException { }
-  }
-
-  @Test
-  public void testSplitLocationLimitOldApi() throws IOException {
-    final int SPLITS = 5;
-    final int MAX_LOC = 10;
-    final Path outdir = new Path(TEST_DIR, "testSplitLocationLimitOldApi");
-    final String[] locs = getLoc(MAX_LOC + 5);
-    final Configuration conf = new Configuration();
-    final FileSystem rfs = FileSystem.getLocal(conf).getRaw();
-    final org.apache.hadoop.mapred.InputSplit split = new OldSplit() {
-      @Override public String[] getLocations() { return locs; }
-    };
-    org.apache.hadoop.mapred.InputSplit[] splits =
-      new org.apache.hadoop.mapred.InputSplit[SPLITS];
-    Arrays.fill(splits, split);
-
-    conf.setInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, MAX_LOC);
-    JobSplitWriter.createSplitFiles(outdir, conf,
-        FileSystem.getLocal(conf).getRaw(), splits);
-    checkMeta(MAX_LOC,
-        SplitMetaInfoReader.readSplitMetaInfo(null, rfs, conf, outdir),
-        Arrays.copyOf(locs, MAX_LOC));
-
-    conf.setInt(JobSplitWriter.MAX_SPLIT_LOCATIONS, MAX_LOC / 2);
-    try {
-      SplitMetaInfoReader.readSplitMetaInfo(null, rfs, conf, outdir);
-      fail("Reader failed to detect location limit");
-    } catch (IOException e) { }
-  }
-
-  private static void checkMeta(int MAX_LOC,
-      JobSplit.TaskSplitMetaInfo[] metaSplits, String[] chk_locs) {
-    for (JobSplit.TaskSplitMetaInfo meta : metaSplits) {
-      final String[] meta_locs = meta.getLocations();
-      assertEquals(MAX_LOC, meta_locs.length);
-      assertArrayEquals(chk_locs, meta_locs);
-    }
-  }
-
-  private static String[] getLoc(int locations) {
-    final String ret[] = new String[locations];
-    for (int i = 0; i < locations; ++i) {
-      ret[i] = "LOC" + i;
-    }
-    return ret;
-  }
-
-}
diff --git a/src/webapps/job/jobdetails.jsp b/src/webapps/job/jobdetails.jsp
index 50ae596..b89077f 100644
--- a/src/webapps/job/jobdetails.jsp
+++ b/src/webapps/job/jobdetails.jsp
@@ -365,17 +365,10 @@
       <th>Total</th>
     </tr>
     <%
-    boolean isFine = true;
-    Counters mapCounters = new Counters();
-    isFine = job.getMapCounters(mapCounters);
-    mapCounters = (isFine? mapCounters: new Counters());
-    Counters reduceCounters = new Counters();
-    isFine = job.getReduceCounters(reduceCounters);
-    reduceCounters = (isFine? reduceCounters: new Counters());
-    Counters totalCounters = new Counters();
-    isFine = job.getCounters(totalCounters);
-    totalCounters = (isFine? totalCounters: new Counters());
-        
+    Counters mapCounters = job.getMapCounters();
+    Counters reduceCounters = job.getReduceCounters();
+    Counters totalCounters = job.getCounters();
+    
     for (String groupName : totalCounters.getGroupNames()) {
       Counters.Group totalGroup = totalCounters.getGroup(groupName);
       Counters.Group mapGroup = mapCounters.getGroup(groupName);
-- 
1.7.0.4

