From 4a161db6f714c4e6e28c1efdf62a94938e3f8691 Mon Sep 17 00:00:00 2001
From: Roman Shaposhnik <rvs@cloudera.com>
Date: Wed, 1 May 2013 19:26:14 -0700
Subject: [PATCH 1307/1357] MR1: KITCHEN-BUILD. Grafting MR1 code base on top of Hadoop 2.X

---
 .eclipse.templates/.classpath                      |   53 ---
 .../Hadoop_Ant_Builder.launch                      |   22 --
 .eclipse.templates/.project                        |   27 --
 .eclipse.templates/README.txt                      |    6 -
 conf/task-log4j.properties                         |  123 -------
 .../.eclipse.templates/.classpath                  |   53 +++
 .../Hadoop_Ant_Builder.launch                      |   22 ++
 .../.eclipse.templates/.project                    |   27 ++
 .../.eclipse.templates/README.txt                  |    6 +
 .../conf/task-log4j.properties                     |  123 +++++++
 .../documentation/content/xdocs/Pluggable_Sort.xml |   93 +++++
 .../org/apache/hadoop/mapred/IndexRecord.java      |   37 ++
 .../apache/hadoop/mapred/MapOutputCollector.java   |   65 ++++
 .../hadoop/mapred/ShuffleConsumerPlugin.java       |   89 +++++
 .../hadoop/mapred/TestCombineOutputCollector.java  |   77 ++++
 .../org/apache/hadoop/mapred/TestJobLocalizer.java |   44 +++
 .../test/org/apache/hadoop/mapred/TestMerger.java  |  135 ++++++++
 .../apache/hadoop/mapreduce/TestLocalRunner.java   |  363 ++++++++++++++++++++
 .../documentation/content/xdocs/Pluggable_Sort.xml |   93 -----
 .../org/apache/hadoop/mapred/IndexRecord.java      |   37 --
 .../apache/hadoop/mapred/MapOutputCollector.java   |   65 ----
 .../hadoop/mapred/ShuffleConsumerPlugin.java       |   89 -----
 .../hadoop/mapred/TestCombineOutputCollector.java  |   77 ----
 .../org/apache/hadoop/mapred/TestJobLocalizer.java |   44 ---
 src/test/org/apache/hadoop/mapred/TestMerger.java  |  135 --------
 .../apache/hadoop/mapreduce/TestLocalRunner.java   |  363 --------------------
 26 files changed, 1134 insertions(+), 1134 deletions(-)
 delete mode 100644 .eclipse.templates/.classpath
 delete mode 100644 .eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch
 delete mode 100644 .eclipse.templates/.project
 delete mode 100644 .eclipse.templates/README.txt
 delete mode 100644 conf/task-log4j.properties
 create mode 100644 hadoop-mapreduce1-project/.eclipse.templates/.classpath
 create mode 100644 hadoop-mapreduce1-project/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch
 create mode 100644 hadoop-mapreduce1-project/.eclipse.templates/.project
 create mode 100644 hadoop-mapreduce1-project/.eclipse.templates/README.txt
 create mode 100644 hadoop-mapreduce1-project/conf/task-log4j.properties
 create mode 100644 hadoop-mapreduce1-project/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml
 create mode 100644 hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/IndexRecord.java
 create mode 100644 hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java
 create mode 100644 hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
 create mode 100644 hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java
 create mode 100644 hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java
 create mode 100644 hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestMerger.java
 create mode 100644 hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java
 delete mode 100644 src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml
 delete mode 100644 src/mapred/org/apache/hadoop/mapred/IndexRecord.java
 delete mode 100644 src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java
 delete mode 100644 src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
 delete mode 100644 src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java
 delete mode 100644 src/test/org/apache/hadoop/mapred/TestJobLocalizer.java
 delete mode 100644 src/test/org/apache/hadoop/mapred/TestMerger.java
 delete mode 100644 src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java

diff --git a/.eclipse.templates/.classpath b/.eclipse.templates/.classpath
deleted file mode 100644
index 12c3be5..0000000
--- a/.eclipse.templates/.classpath
+++ /dev/null
@@ -1,53 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<classpath>
-	<classpathentry kind="src" path="src/benchmarks"/>
-	<classpathentry kind="src" path="src/core"/>
-	<classpathentry kind="src" path="src/examples"/>
-	<classpathentry kind="src" path="src/hdfs"/>
-	<classpathentry kind="src" path="src/mapred"/>
-	<classpathentry kind="src" path="src/test"/>
-	<classpathentry kind="src" path="src/tools"/>
-	<classpathentry kind="src" path="src/contrib/data_join/src/java"/>
-	<classpathentry kind="src" path="src/contrib/data_join/src/examples"/>
-	<classpathentry kind="src" path="src/contrib/streaming/src/java"/>
-	<classpathentry kind="src" path="src/contrib/streaming/src/test"/>
-	<classpathentry kind="con" path="org.eclipse.jdt.launching.JRE_CONTAINER"/>
-	<classpathentry kind="var" path="ANT_HOME/lib/ant.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-cli-1.2.jar"/>
-  <classpathentry kind="lib" path="lib/hsqldb-1.8.0.10.jar"/>
-	<classpathentry kind="lib" path="lib/kfs-0.2.2.jar"/>
-  	<classpathentry kind="lib" path="lib/jsp-2.1/jsp-2.1.jar"/>
-  	<classpathentry kind="lib" path="lib/jsp-2.1/jsp-api-2.1.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-codec-1.4.jar"/>
-  <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-daemon-1.0.1.jar" />
-  <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-httpclient-3.1.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-el-1.0.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jasper-compiler-5.5.12.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jasper-runtime-5.5.12.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-logging-1.0.4.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-logging-api-1.0.4.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-net-1.4.1.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/guava-r09-jarjar.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jets3t-0.6.1.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/junit-4.5.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/log4j-1.2.15.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/mockito-all-1.8.2.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/oro-2.0.8.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jetty-6.1.26.cloudera.2.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jetty-util-6.1.26.cloudera.2.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jackson-core-asl-1.5.2.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jackson-mapper-asl-1.5.2.jar"/>
-  	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/servlet-api-2.5-6.1.14.jar"/>
-  	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/core-3.1.1.jar"/>
-	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/slf4j-api-1.4.3.jar"/>
-    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/slf4j-log4j12-1.4.3.jar"/>
-    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/xmlenc-0.52.jar"/>
-    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/aspectjrt-1.6.5.jar"/>
-    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/aspectjtools-1.6.5.jar"/>
-	<classpathentry kind="lib" path="src/test/lib/ftplet-api-1.0.0-SNAPSHOT.jar"/>
-	<classpathentry kind="lib" path="src/test/lib/ftpserver-core-1.0.0-SNAPSHOT.jar"/>
-	<classpathentry kind="lib" path="src/test/lib/ftpserver-server-1.0.0-SNAPSHOT.jar"/>
-    <classpathentry kind="lib" path="src/test/lib/mina-core-2.0.0-M2-20080407.124109-12.jar"/>
-	<classpathentry kind="lib" path="build/test/classes"/>
-	<classpathentry kind="output" path="build/eclipse-classes"/>
-</classpath>
diff --git a/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch b/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch
deleted file mode 100644
index 1b944aa..0000000
--- a/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch
+++ /dev/null
@@ -1,22 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<launchConfiguration type="org.eclipse.ant.AntBuilderLaunchConfigurationType">
-<stringAttribute key="org.eclipse.ant.ui.ATTR_ANT_AFTER_CLEAN_TARGETS" value="clean,"/>
-<stringAttribute key="org.eclipse.ant.ui.ATTR_ANT_MANUAL_TARGETS" value="compile,compile-core-test,"/>
-<booleanAttribute key="org.eclipse.ant.ui.ATTR_TARGETS_UPDATED" value="true"/>
-<booleanAttribute key="org.eclipse.ant.ui.DEFAULT_VM_INSTALL" value="false"/>
-<stringAttribute key="org.eclipse.debug.core.ATTR_REFRESH_SCOPE" value="${project}"/>
-<listAttribute key="org.eclipse.debug.core.MAPPED_RESOURCE_PATHS">
-<listEntry value="/@PROJECT@/build.xml"/>
-</listAttribute>
-<listAttribute key="org.eclipse.debug.core.MAPPED_RESOURCE_TYPES">
-<listEntry value="1"/>
-</listAttribute>
-<booleanAttribute key="org.eclipse.debug.core.appendEnvironmentVariables" value="true"/>
-<booleanAttribute key="org.eclipse.debug.ui.ATTR_LAUNCH_IN_BACKGROUND" value="false"/>
-<stringAttribute key="org.eclipse.jdt.launching.CLASSPATH_PROVIDER" value="org.eclipse.ant.ui.AntClasspathProvider"/>
-<booleanAttribute key="org.eclipse.jdt.launching.DEFAULT_CLASSPATH" value="true"/>
-<stringAttribute key="org.eclipse.jdt.launching.PROJECT_ATTR" value="@PROJECT@"/>
-<stringAttribute key="org.eclipse.ui.externaltools.ATTR_LOCATION" value="${workspace_loc:/@PROJECT@/build.xml}"/>
-<stringAttribute key="org.eclipse.ui.externaltools.ATTR_RUN_BUILD_KINDS" value="full,incremental,"/>
-<booleanAttribute key="org.eclipse.ui.externaltools.ATTR_TRIGGERS_CONFIGURED" value="true"/>
-</launchConfiguration>
diff --git a/.eclipse.templates/.project b/.eclipse.templates/.project
deleted file mode 100644
index 8356099..0000000
--- a/.eclipse.templates/.project
+++ /dev/null
@@ -1,27 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<projectDescription>
-	<name>@PROJECT@</name>
-	<comment></comment>
-	<projects>
-	</projects>
-	<buildSpec>
-		<buildCommand>
-			<name>org.eclipse.jdt.core.javabuilder</name>
-			<arguments>
-			</arguments>
-		</buildCommand>
-		<buildCommand>
-			<name>org.eclipse.ui.externaltools.ExternalToolBuilder</name>
-			<triggers>full,incremental,</triggers>
-			<arguments>
-				<dictionary>
-					<key>LaunchConfigHandle</key>
-					<value>&lt;project&gt;/.externalToolBuilders/Hadoop_Ant_Builder.launch</value>
-				</dictionary>
-			</arguments>
-		</buildCommand>
-	</buildSpec>
-	<natures>
-		<nature>org.eclipse.jdt.core.javanature</nature>
-	</natures>
-</projectDescription>
diff --git a/.eclipse.templates/README.txt b/.eclipse.templates/README.txt
deleted file mode 100644
index 1905042..0000000
--- a/.eclipse.templates/README.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-This directory contains templates for generating Eclipse files to configure
-Eclipse for Hadoop development.
-
-For further information please consult
-
-http://wiki.apache.org/hadoop/EclipseEnvironment 
diff --git a/conf/task-log4j.properties b/conf/task-log4j.properties
deleted file mode 100644
index a60a667..0000000
--- a/conf/task-log4j.properties
+++ /dev/null
@@ -1,123 +0,0 @@
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-
-
-# Define some default values that can be overridden by system properties
-hadoop.root.logger=INFO,console
-hadoop.log.dir=.
-hadoop.log.file=hadoop.log
-
-#
-# Job Summary Appender 
-#
-# Use following logger to send summary to separate file defined by 
-# hadoop.mapreduce.jobsummary.log.file rolled daily:
-# hadoop.mapreduce.jobsummary.logger=INFO,JSA
-# 
-hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
-hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
-
-# Define the root logger to the system property "hadoop.root.logger".
-log4j.rootLogger=${hadoop.root.logger}, EventCounter
-
-# Logging Threshold
-log4j.threshhold=ALL
-
-#
-# Daily Rolling File Appender
-#
-
-log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
-log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
-
-# Rollver at midnight
-log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
-
-# 30-day backup
-#log4j.appender.DRFA.MaxBackupIndex=30
-log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
-
-# Pattern format: Date LogLevel LoggerName LogMessage
-log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
-# Debugging Pattern format
-#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
-
-
-#
-# console
-# Add "console" to rootlogger above if you want to use this 
-#
-
-log4j.appender.console=org.apache.log4j.ConsoleAppender
-log4j.appender.console.target=System.err
-log4j.appender.console.layout=org.apache.log4j.PatternLayout
-log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
-
-#
-# TaskLog Appender
-#
-
-#Default values
-hadoop.tasklog.taskid=null
-hadoop.tasklog.iscleanup=false
-hadoop.tasklog.noKeepSplits=4
-hadoop.tasklog.totalLogFileSize=100
-hadoop.tasklog.purgeLogSplits=true
-hadoop.tasklog.logsRetainHours=12
-
-log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
-log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
-log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
-log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
-
-log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
-log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
-
-#
-# Rolling File Appender
-#
-
-#log4j.appender.RFA=org.apache.log4j.RollingFileAppender
-#log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
-
-# Logfile size and and 30-day backups
-#log4j.appender.RFA.MaxFileSize=1MB
-#log4j.appender.RFA.MaxBackupIndex=30
-
-#log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
-#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n
-#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
-
-
-# Custom Logging levels
-
-hadoop.metrics.log.level=INFO
-#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
-#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
-#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
-log4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}
-
-# Jets3t library
-log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR
-
-#
-# Null Appender
-# Trap security logger on the hadoop client side
-#
-log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
-
-#
-# Event Counter Appender
-# Sends counts of logging messages at different severity levels to Hadoop Metrics.
-#
-log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
-
diff --git a/hadoop-mapreduce1-project/.eclipse.templates/.classpath b/hadoop-mapreduce1-project/.eclipse.templates/.classpath
new file mode 100644
index 0000000..12c3be5
--- /dev/null
+++ b/hadoop-mapreduce1-project/.eclipse.templates/.classpath
@@ -0,0 +1,53 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<classpath>
+	<classpathentry kind="src" path="src/benchmarks"/>
+	<classpathentry kind="src" path="src/core"/>
+	<classpathentry kind="src" path="src/examples"/>
+	<classpathentry kind="src" path="src/hdfs"/>
+	<classpathentry kind="src" path="src/mapred"/>
+	<classpathentry kind="src" path="src/test"/>
+	<classpathentry kind="src" path="src/tools"/>
+	<classpathentry kind="src" path="src/contrib/data_join/src/java"/>
+	<classpathentry kind="src" path="src/contrib/data_join/src/examples"/>
+	<classpathentry kind="src" path="src/contrib/streaming/src/java"/>
+	<classpathentry kind="src" path="src/contrib/streaming/src/test"/>
+	<classpathentry kind="con" path="org.eclipse.jdt.launching.JRE_CONTAINER"/>
+	<classpathentry kind="var" path="ANT_HOME/lib/ant.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-cli-1.2.jar"/>
+  <classpathentry kind="lib" path="lib/hsqldb-1.8.0.10.jar"/>
+	<classpathentry kind="lib" path="lib/kfs-0.2.2.jar"/>
+  	<classpathentry kind="lib" path="lib/jsp-2.1/jsp-2.1.jar"/>
+  	<classpathentry kind="lib" path="lib/jsp-2.1/jsp-api-2.1.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-codec-1.4.jar"/>
+  <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-daemon-1.0.1.jar" />
+  <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-httpclient-3.1.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-el-1.0.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jasper-compiler-5.5.12.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jasper-runtime-5.5.12.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-logging-1.0.4.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-logging-api-1.0.4.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/commons-net-1.4.1.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/guava-r09-jarjar.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jets3t-0.6.1.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/junit-4.5.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/log4j-1.2.15.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/mockito-all-1.8.2.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/oro-2.0.8.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jetty-6.1.26.cloudera.2.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jetty-util-6.1.26.cloudera.2.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jackson-core-asl-1.5.2.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/jackson-mapper-asl-1.5.2.jar"/>
+  	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/servlet-api-2.5-6.1.14.jar"/>
+  	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/core-3.1.1.jar"/>
+	<classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/slf4j-api-1.4.3.jar"/>
+    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/slf4j-log4j12-1.4.3.jar"/>
+    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/xmlenc-0.52.jar"/>
+    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/aspectjrt-1.6.5.jar"/>
+    <classpathentry kind="lib" path="build/ivy/lib/Hadoop/common/aspectjtools-1.6.5.jar"/>
+	<classpathentry kind="lib" path="src/test/lib/ftplet-api-1.0.0-SNAPSHOT.jar"/>
+	<classpathentry kind="lib" path="src/test/lib/ftpserver-core-1.0.0-SNAPSHOT.jar"/>
+	<classpathentry kind="lib" path="src/test/lib/ftpserver-server-1.0.0-SNAPSHOT.jar"/>
+    <classpathentry kind="lib" path="src/test/lib/mina-core-2.0.0-M2-20080407.124109-12.jar"/>
+	<classpathentry kind="lib" path="build/test/classes"/>
+	<classpathentry kind="output" path="build/eclipse-classes"/>
+</classpath>
diff --git a/hadoop-mapreduce1-project/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch b/hadoop-mapreduce1-project/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch
new file mode 100644
index 0000000..1b944aa
--- /dev/null
+++ b/hadoop-mapreduce1-project/.eclipse.templates/.externalToolBuilders/Hadoop_Ant_Builder.launch
@@ -0,0 +1,22 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<launchConfiguration type="org.eclipse.ant.AntBuilderLaunchConfigurationType">
+<stringAttribute key="org.eclipse.ant.ui.ATTR_ANT_AFTER_CLEAN_TARGETS" value="clean,"/>
+<stringAttribute key="org.eclipse.ant.ui.ATTR_ANT_MANUAL_TARGETS" value="compile,compile-core-test,"/>
+<booleanAttribute key="org.eclipse.ant.ui.ATTR_TARGETS_UPDATED" value="true"/>
+<booleanAttribute key="org.eclipse.ant.ui.DEFAULT_VM_INSTALL" value="false"/>
+<stringAttribute key="org.eclipse.debug.core.ATTR_REFRESH_SCOPE" value="${project}"/>
+<listAttribute key="org.eclipse.debug.core.MAPPED_RESOURCE_PATHS">
+<listEntry value="/@PROJECT@/build.xml"/>
+</listAttribute>
+<listAttribute key="org.eclipse.debug.core.MAPPED_RESOURCE_TYPES">
+<listEntry value="1"/>
+</listAttribute>
+<booleanAttribute key="org.eclipse.debug.core.appendEnvironmentVariables" value="true"/>
+<booleanAttribute key="org.eclipse.debug.ui.ATTR_LAUNCH_IN_BACKGROUND" value="false"/>
+<stringAttribute key="org.eclipse.jdt.launching.CLASSPATH_PROVIDER" value="org.eclipse.ant.ui.AntClasspathProvider"/>
+<booleanAttribute key="org.eclipse.jdt.launching.DEFAULT_CLASSPATH" value="true"/>
+<stringAttribute key="org.eclipse.jdt.launching.PROJECT_ATTR" value="@PROJECT@"/>
+<stringAttribute key="org.eclipse.ui.externaltools.ATTR_LOCATION" value="${workspace_loc:/@PROJECT@/build.xml}"/>
+<stringAttribute key="org.eclipse.ui.externaltools.ATTR_RUN_BUILD_KINDS" value="full,incremental,"/>
+<booleanAttribute key="org.eclipse.ui.externaltools.ATTR_TRIGGERS_CONFIGURED" value="true"/>
+</launchConfiguration>
diff --git a/hadoop-mapreduce1-project/.eclipse.templates/.project b/hadoop-mapreduce1-project/.eclipse.templates/.project
new file mode 100644
index 0000000..8356099
--- /dev/null
+++ b/hadoop-mapreduce1-project/.eclipse.templates/.project
@@ -0,0 +1,27 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<projectDescription>
+	<name>@PROJECT@</name>
+	<comment></comment>
+	<projects>
+	</projects>
+	<buildSpec>
+		<buildCommand>
+			<name>org.eclipse.jdt.core.javabuilder</name>
+			<arguments>
+			</arguments>
+		</buildCommand>
+		<buildCommand>
+			<name>org.eclipse.ui.externaltools.ExternalToolBuilder</name>
+			<triggers>full,incremental,</triggers>
+			<arguments>
+				<dictionary>
+					<key>LaunchConfigHandle</key>
+					<value>&lt;project&gt;/.externalToolBuilders/Hadoop_Ant_Builder.launch</value>
+				</dictionary>
+			</arguments>
+		</buildCommand>
+	</buildSpec>
+	<natures>
+		<nature>org.eclipse.jdt.core.javanature</nature>
+	</natures>
+</projectDescription>
diff --git a/hadoop-mapreduce1-project/.eclipse.templates/README.txt b/hadoop-mapreduce1-project/.eclipse.templates/README.txt
new file mode 100644
index 0000000..1905042
--- /dev/null
+++ b/hadoop-mapreduce1-project/.eclipse.templates/README.txt
@@ -0,0 +1,6 @@
+This directory contains templates for generating Eclipse files to configure
+Eclipse for Hadoop development.
+
+For further information please consult
+
+http://wiki.apache.org/hadoop/EclipseEnvironment 
diff --git a/hadoop-mapreduce1-project/conf/task-log4j.properties b/hadoop-mapreduce1-project/conf/task-log4j.properties
new file mode 100644
index 0000000..a60a667
--- /dev/null
+++ b/hadoop-mapreduce1-project/conf/task-log4j.properties
@@ -0,0 +1,123 @@
+#   Licensed under the Apache License, Version 2.0 (the "License");
+#   you may not use this file except in compliance with the License.
+#   You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#   Unless required by applicable law or agreed to in writing, software
+#   distributed under the License is distributed on an "AS IS" BASIS,
+#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#   See the License for the specific language governing permissions and
+#   limitations under the License.
+
+
+# Define some default values that can be overridden by system properties
+hadoop.root.logger=INFO,console
+hadoop.log.dir=.
+hadoop.log.file=hadoop.log
+
+#
+# Job Summary Appender 
+#
+# Use following logger to send summary to separate file defined by 
+# hadoop.mapreduce.jobsummary.log.file rolled daily:
+# hadoop.mapreduce.jobsummary.logger=INFO,JSA
+# 
+hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
+hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
+
+# Define the root logger to the system property "hadoop.root.logger".
+log4j.rootLogger=${hadoop.root.logger}, EventCounter
+
+# Logging Threshold
+log4j.threshhold=ALL
+
+#
+# Daily Rolling File Appender
+#
+
+log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+# Rollver at midnight
+log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
+
+# 30-day backup
+#log4j.appender.DRFA.MaxBackupIndex=30
+log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+
+#
+# console
+# Add "console" to rootlogger above if you want to use this 
+#
+
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+
+#
+# TaskLog Appender
+#
+
+#Default values
+hadoop.tasklog.taskid=null
+hadoop.tasklog.iscleanup=false
+hadoop.tasklog.noKeepSplits=4
+hadoop.tasklog.totalLogFileSize=100
+hadoop.tasklog.purgeLogSplits=true
+hadoop.tasklog.logsRetainHours=12
+
+log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
+log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
+log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
+log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
+
+log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
+log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+
+#
+# Rolling File Appender
+#
+
+#log4j.appender.RFA=org.apache.log4j.RollingFileAppender
+#log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+# Logfile size and and 30-day backups
+#log4j.appender.RFA.MaxFileSize=1MB
+#log4j.appender.RFA.MaxBackupIndex=30
+
+#log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
+#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n
+#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+
+# Custom Logging levels
+
+hadoop.metrics.log.level=INFO
+#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
+#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
+#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
+log4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}
+
+# Jets3t library
+log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR
+
+#
+# Null Appender
+# Trap security logger on the hadoop client side
+#
+log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
+
+#
+# Event Counter Appender
+# Sends counts of logging messages at different severity levels to Hadoop Metrics.
+#
+log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
+
diff --git a/hadoop-mapreduce1-project/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml b/hadoop-mapreduce1-project/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml
new file mode 100644
index 0000000..721ef03
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml
@@ -0,0 +1,93 @@
+<?xml version="1.0"?>
+<!--
+  Copyright 2002-2004 The Apache Software Foundation
+
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
+    "http://forrest.apache.org/dtd/document-v20.dtd">
+
+
+<document>
+
+  <header>
+    <title>
+      Hadoop Pluggable Sort
+    </title>
+  </header>
+
+  <body>
+    <section>
+      <title>Introduction</title>
+      <p>
+        The pluggable sort capability allows replacing the built in sort logic with 
+        alternate implementations. Example use cases for this are replacing the sort 
+        logic with custom algorithms that enable Hash aggregation and Limit-N query.
+      </p>
+      <p>
+        <strong>IMPORTANT:</strong> The pluggable sort capability is experimental 
+        and unstable. This means the provided APIs may change and break compatibility 
+        in future versions of Hadoop.
+      </p>
+    </section>
+
+    <section>
+      <title>Implementing a Custom Sort</title>
+      <p>
+        A custom sort implementation requires a <strong>org.apache.hadoop.mapred.MapOutputCollector</strong>
+        implementation class running in the Mapper tasks and (optionally, depending
+        on the sort implementation) a <strong>org.apache.hadoop.mapred.ShuffleConsumerPlugin</strong>
+        implementation class running in the Reducer tasks.
+      </p>
+      <p>
+        The default implementations provided by Hadoop can be used as references:
+      </p>
+      <ul>
+        <li><strong>org.apache.hadoop.mapred.MapTask$MapOutputBuffer</strong></li>
+        <li><strong>org.apache.hadoop.mapreduce.task.reduce.Shuffle</strong></li>
+      </ul>
+    </section>
+
+    <section>
+      <title>Configuration</title>
+
+      <p>
+        All the pluggable components run in the job tasks. This means, they can be 
+        configured on per job basis.        
+      </p>
+      <p>
+        <strong>Job Configuration Properties</strong>
+      </p>
+
+      <ul>
+        <li>
+            <strong>mapreduce.job.reduce.shuffle.consumer.plugin.class</strong>. 
+            Default:<strong>org.apache.hadoop.mapreduce.task.reduce.Shuffle</strong>.
+            The <strong>ShuffleConsumerPlugin</strong> implementation to use
+        </li>
+        <li>
+          <strong>mapreduce.job.map.output.collector.class</strong>.
+          Default:<strong>org.apache.hadoop.mapred.MapTask$MapOutputBuffer</strong>.
+          The <strong>MapOutputCollector</strong> implementation to use
+        </li>
+      </ul>
+
+      <p>
+        These properties can also be set in the <strong>mapred-site.xml</strong> to 
+        change the default values for all jobs.        
+      </p>
+    </section>
+
+  </body>
+</document>
diff --git a/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/IndexRecord.java b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/IndexRecord.java
new file mode 100644
index 0000000..3996534
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/IndexRecord.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+@InterfaceAudience.LimitedPrivate({"MapReduce"})
+@InterfaceStability.Unstable
+public class IndexRecord {
+  public long startOffset;
+  public long rawLength;
+  public long partLength;
+
+  public IndexRecord() { }
+
+  public IndexRecord(long startOffset, long rawLength, long partLength) {
+    this.startOffset = startOffset;
+    this.rawLength = rawLength;
+    this.partLength = partLength;
+  }
+}
diff --git a/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java
new file mode 100644
index 0000000..368c016
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+import org.apache.hadoop.mapred.Task.TaskReporter;
+
+@InterfaceAudience.LimitedPrivate({"MapReduce"})
+@InterfaceStability.Unstable
+public interface MapOutputCollector<K, V> {
+  public void init(Context context
+                  ) throws IOException, ClassNotFoundException;
+  public void collect(K key, V value, int partition
+                     ) throws IOException, InterruptedException;
+  public void close() throws IOException, InterruptedException;
+    
+  public void flush() throws IOException, InterruptedException, 
+                             ClassNotFoundException;
+
+  @InterfaceAudience.LimitedPrivate({"MapReduce"})
+  @InterfaceStability.Unstable
+  public static class Context {
+    private final MapTask mapTask;
+    private final JobConf jobConf;
+    private final TaskReporter reporter;
+
+    public Context(MapTask mapTask, JobConf jobConf, TaskReporter reporter) {
+      this.mapTask = mapTask;
+      this.jobConf = jobConf;
+      this.reporter = reporter;
+    }
+
+    public MapTask getMapTask() {
+      return mapTask;
+    }
+
+    public JobConf getJobConf() {
+      return jobConf;
+    }
+
+    public TaskReporter getReporter() {
+      return reporter;
+    }
+  }
+}
diff --git a/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
new file mode 100644
index 0000000..657e1db
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.mapred.Task.TaskReporter;
+
+@InterfaceAudience.LimitedPrivate("MapReduce")
+@InterfaceStability.Unstable
+public interface ShuffleConsumerPlugin {
+  /**
+   * To initialize the reduce copier plugin.
+   * @param context reduce copier context.
+   */
+  public void init(Context context)
+    throws ClassNotFoundException, IOException;
+
+  /**
+   * To fetch the map outputs.
+   * @return true if the fetch was successful; false otherwise.
+   */
+  public boolean fetchOutputs() throws IOException;
+
+  /**
+   * To create a key-value iterator to read the merged output.
+   * @return an iterator for merged key-value pairs.
+   */
+  public RawKeyValueIterator createKVIterator() throws IOException;
+
+  /**
+   * close and clean any resource associated with this object.
+   */
+  public void close();
+  
+  /**
+   * To get any exception from merge.
+   */
+  public Throwable getMergeThrowable();
+
+  public static class Context {
+    private final TaskUmbilicalProtocol umbilical;
+    private final JobConf jobConf;
+    private final TaskReporter reporter;
+    private final ReduceTask reduceTask;
+
+    public Context(TaskUmbilicalProtocol umbilical, JobConf conf,
+                   TaskReporter reporter, ReduceTask reduceTask) {
+      this.umbilical = umbilical;
+      this.jobConf = conf;
+      this.reporter = reporter;
+      this.reduceTask = reduceTask;
+    }
+
+    public TaskUmbilicalProtocol getUmbilical() {
+      return umbilical;
+    }
+
+    public JobConf getJobConf() {
+      return jobConf;
+    }
+
+    public TaskReporter getReporter() {
+      return reporter;
+    }
+
+    public ReduceTask getReduceTask() {
+      return reduceTask;
+    }
+  }
+}
diff --git a/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java
new file mode 100644
index 0000000..9399267
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.IFile.Writer;
+import org.apache.hadoop.mapred.Task.CombineOutputCollector;
+import org.apache.hadoop.mapred.Task.TaskReporter;
+import org.junit.Test;
+
+public class TestCombineOutputCollector {
+  private CombineOutputCollector<String, Integer> coc;
+
+  @Test
+  public void testCustomCollect() throws Throwable {
+    //mock creation
+    TaskReporter mockTaskReporter = mock(TaskReporter.class);
+    Counters.Counter outCounter = new Counters.Counter();
+    Writer<String, Integer> mockWriter = mock(Writer.class);
+
+    Configuration conf = new Configuration();
+    conf.set("mapred.combine.recordsBeforeProgress", "2");
+    
+    coc = new CombineOutputCollector<String, Integer>(outCounter, mockTaskReporter, conf);
+    coc.setWriter(mockWriter);
+    verify(mockTaskReporter, never()).progress();
+
+    coc.collect("dummy", 1);
+    verify(mockTaskReporter, never()).progress();
+    
+    coc.collect("dummy", 2);
+    verify(mockTaskReporter, times(1)).progress();
+  }
+  
+  @Test
+  public void testDefaultCollect() throws Throwable {
+    //mock creation
+    TaskReporter mockTaskReporter = mock(TaskReporter.class);
+    Counters.Counter outCounter = new Counters.Counter();
+    Writer<String, Integer> mockWriter = mock(Writer.class);
+
+    Configuration conf = new Configuration();
+    
+    coc = new CombineOutputCollector<String, Integer>(outCounter, mockTaskReporter, conf);
+    coc.setWriter(mockWriter);
+    verify(mockTaskReporter, never()).progress();
+
+    for(int i = 0; i < Task.DEFAULT_MR_COMBINE_RECORDS_BEFORE_PROGRESS; i++) {
+    	coc.collect("dummy", i);
+    }
+    verify(mockTaskReporter, times(1)).progress();
+    for(int i = 0; i < Task.DEFAULT_MR_COMBINE_RECORDS_BEFORE_PROGRESS; i++) {
+    	coc.collect("dummy", i);
+    }
+    verify(mockTaskReporter, times(2)).progress();
+  }
+}
diff --git a/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java
new file mode 100644
index 0000000..149fcb7
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.junit.Test;
+
+public class TestJobLocalizer {
+
+  @Test(timeout = 1000)
+  public void testConcurrentJobLocalizers() throws IOException {
+    final String LOCAL_DIR = "/tmp/mapred/local";
+    JobConf conf = new JobConf(new Configuration());
+    
+    JobLocalizer localizer1 = new JobLocalizer(conf, "user1", "jobid1",
+        LOCAL_DIR);
+    JobLocalizer localizer2 = new JobLocalizer(conf, "user2", "jobid2",
+        LOCAL_DIR);
+    assertTrue("Localizer 1 job local dirs should have user1",
+        localizer1.ttConf.get(JobLocalizer.JOB_LOCAL_CTXT).contains("user1"));
+    assertTrue("Localizer 2 job local dirs should have user2",
+        localizer2.ttConf.get(JobLocalizer.JOB_LOCAL_CTXT).contains("user2"));
+  }
+}
diff --git a/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestMerger.java b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestMerger.java
new file mode 100644
index 0000000..501a826
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapred/TestMerger.java
@@ -0,0 +1,135 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import static org.mockito.Matchers.any;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.Counters.Counter;
+import org.apache.hadoop.mapred.IFile.Reader;
+import org.apache.hadoop.mapred.Merger.Segment;
+import org.apache.hadoop.util.Progressable;
+import org.junit.Test;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+public class TestMerger {
+
+  @Test
+  public void testUncompressed() throws IOException {
+    testMergeShouldReturnProperProgress(getUncompressedSegments());
+  }
+  
+  @Test
+  public void testCompressed() throws IOException {
+    testMergeShouldReturnProperProgress(getCompressedSegments());
+  }
+  
+  @SuppressWarnings( { "deprecation", "unchecked" })
+  public void testMergeShouldReturnProperProgress(
+      List<Segment<Text, Text>> segments) throws IOException {
+    Configuration conf = new Configuration();
+    JobConf jobConf = new JobConf();
+    FileSystem fs = FileSystem.getLocal(conf);
+    Path tmpDir = new Path("localpath");
+    Class<Text> keyClass = (Class<Text>) jobConf.getMapOutputKeyClass();
+    Class<Text> valueClass = (Class<Text>) jobConf.getMapOutputValueClass();
+    RawComparator<Text> comparator = jobConf.getOutputKeyComparator();
+    Counter readsCounter = new Counter();
+    Counter writesCounter = new Counter();
+    RawKeyValueIterator mergeQueue = Merger.merge(conf, fs, keyClass,
+        valueClass, segments, 2, tmpDir, comparator, getReporter(),
+        readsCounter, writesCounter);
+    Assert.assertEquals(1.0f, mergeQueue.getProgress().get());
+  }
+
+  private Progressable getReporter() {
+    Progressable reporter = new Progressable() {
+      @Override
+      public void progress() {
+      }
+    };
+    return reporter;
+  }
+
+  private List<Segment<Text, Text>> getUncompressedSegments() throws IOException {
+    List<Segment<Text, Text>> segments = new ArrayList<Segment<Text, Text>>();
+    for (int i = 1; i < 10; i++) {
+      segments.add(getUncompressedSegment(i));
+    }
+    return segments;
+  }
+
+  private List<Segment<Text, Text>> getCompressedSegments() throws IOException {
+    List<Segment<Text, Text>> segments = new ArrayList<Segment<Text, Text>>();
+    for (int i = 1; i < 10; i++) {
+      segments.add(getCompressedSegment(i));
+    }
+    return segments;
+  }
+  
+  private Segment<Text, Text> getUncompressedSegment(int i) throws IOException {
+    return new Segment<Text, Text>(getReader(i), false);
+  }
+  
+  private Segment<Text, Text> getCompressedSegment(int i) throws IOException {
+    return new Segment<Text, Text>(getReader(i), false, 3000l);
+  }
+
+  @SuppressWarnings("unchecked")
+  private Reader<Text, Text> getReader(int i) throws IOException {
+    Reader<Text, Text> readerMock = mock(Reader.class);
+    when(readerMock.getPosition()).thenReturn(0l).thenReturn(10l).thenReturn(
+        20l);
+    when(
+        readerMock.next(any(DataInputBuffer.class), any(DataInputBuffer.class)))
+        .thenAnswer(getAnswer("Segment" + i));
+    return readerMock;
+  }
+
+  private Answer<?> getAnswer(final String segmentName) {
+    return new Answer<Object>() {
+      int i = 0;
+
+      public Boolean answer(InvocationOnMock invocation) {
+        Object[] args = invocation.getArguments();
+        DataInputBuffer key = (DataInputBuffer) args[0];
+        DataInputBuffer value = (DataInputBuffer) args[1];
+        if (i++ == 2) {
+          return false;
+        }
+        key.reset(("Segement Key " + segmentName + i).getBytes(), 20);
+        value.reset(("Segement Value" + segmentName + i).getBytes(), 20);
+        return true;
+      }
+    };
+  }
+}
diff --git a/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java
new file mode 100644
index 0000000..d3cd1ab
--- /dev/null
+++ b/hadoop-mapreduce1-project/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java
@@ -0,0 +1,363 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapreduce;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.LocalJobRunner;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+
+import org.junit.Test;
+import junit.framework.TestCase;
+
+/**
+ * Stress tests for the LocalJobRunner
+ */
+public class TestLocalRunner extends TestCase {
+
+  private static final Log LOG = LogFactory.getLog(TestLocalRunner.class);
+
+  private static class StressMapper
+      extends Mapper<LongWritable, Text, LongWritable, Text> {
+
+    // Different map tasks operate at different speeds.
+    // We define behavior for 6 threads.
+    private int threadId;
+
+    // Used to ensure that the compiler doesn't optimize away
+    // some code.
+    public long exposedState;
+
+    protected void setup(Context context) {
+      // Get the thread num from the file number.
+      FileSplit split = (FileSplit) context.getInputSplit();
+      Path filePath = split.getPath();
+      String name = filePath.getName();
+      this.threadId = Integer.valueOf(name);
+
+      LOG.info("Thread " + threadId + " : "
+          + context.getInputSplit());
+    }
+
+    /** Map method with different behavior based on the thread id */
+    public void map(LongWritable key, Text val, Context c)
+        throws IOException, InterruptedException {
+
+      switch(threadId) {
+      case 0:
+        // Write a single value and be done.
+        c.write(new LongWritable(0), val);
+        break;
+      case 1:
+      case 2:
+        // Write many values quickly.
+        for (int i = 0; i < 500; i++) {
+          c.write(new LongWritable(0), val);
+        }
+        break;
+      case 3:
+        // Write many values, using thread sleeps to delay this.
+        for (int i = 0; i < 50; i++) {
+          for (int j = 0; j < 10; j++) {
+            c.write(new LongWritable(0), val);
+          }
+          Thread.sleep(1);
+        }
+        break;
+      case 4:
+        // Write many values, using busy-loops to delay this.
+        for (int i = 0; i < 500; i++) {
+          for (int j = 0; j < 10000; j++) {
+            this.exposedState++;
+          }
+          c.write(new LongWritable(0), val);
+        }
+        break;
+      case 5:
+        // Write many values, using very slow busy-loops to delay this.
+        for (int i = 0; i < 500; i++) {
+          for (int j = 0; j < 100000; j++) {
+            this.exposedState++;
+          }
+          c.write(new LongWritable(0), val);
+        }
+        break;
+      default:
+        // Write a single value and be done.
+        c.write(new LongWritable(0), val);
+        break;
+      }
+    }
+
+    protected void cleanup(Context context) {
+      // Output this here, to ensure that the incrementing done in map()
+      // cannot be optimized away.
+      LOG.debug("Busy loop counter: " + this.exposedState);
+    }
+  }
+
+  private static class CountingReducer
+      extends Reducer<LongWritable, Text, LongWritable, LongWritable> {
+
+    public void reduce(LongWritable key, Iterable<Text> vals, Context context)
+        throws IOException, InterruptedException {
+      long out = 0;
+      for (Text val : vals) {
+        out++;
+      }
+
+      context.write(key, new LongWritable(out));
+    }
+  }
+
+  /**
+   * Create a single input file in the input directory.
+   * @param dirPath the directory in which the file resides
+   * @param id the file id number
+   * @param numRecords how many records to write to each file.
+   */
+  private void createInputFile(Path dirPath, int id, int numRecords)
+      throws IOException {
+    final String MESSAGE = "This is a line in a file: ";
+
+    Path filePath = new Path(dirPath, "" + id);
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.getLocal(conf);
+
+    OutputStream os = fs.create(filePath);
+    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os));
+
+    for (int i = 0; i < numRecords; i++) {
+      w.write(MESSAGE + id + " " + i + "\n");
+    }
+
+    w.close();
+  }
+
+  // This is the total number of map output records we expect to generate,
+  // based on input file sizes (see createMultiMapsInput()) and the behavior
+  // of the different StressMapper threads.
+  private final static int TOTAL_RECORDS = 50000
+      + (500 * 500)
+      + (500 * 500)
+      + (20 * 500)
+      + (5000 * 500)
+      + (500 * 500);
+
+  private final String INPUT_DIR = "multiMapInput";
+  private final String OUTPUT_DIR = "multiMapOutput";
+
+  private Path getInputPath() {
+    String dataDir = System.getProperty("test.build.data");
+    if (null == dataDir) {
+      return new Path(INPUT_DIR);
+    } else {
+      return new Path(new Path(dataDir), INPUT_DIR);
+    }
+  }
+
+  private Path getOutputPath() {
+    String dataDir = System.getProperty("test.build.data");
+    if (null == dataDir) {
+      return new Path(OUTPUT_DIR);
+    } else {
+      return new Path(new Path(dataDir), OUTPUT_DIR);
+    }
+  }
+
+  /**
+   * Create the inputs for the MultiMaps test.
+   * @return the path to the input directory.
+   */
+  private Path createMultiMapsInput() throws IOException {
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.getLocal(conf);
+    Path inputPath = getInputPath();
+
+    // Clear the input directory if it exists, first.
+    if (fs.exists(inputPath)) {
+      fs.delete(inputPath, true);
+    }
+
+    // Create input files, with sizes calibrated based on
+    // the amount of work done in each mapper.
+    createInputFile(inputPath, 0, 50000);
+    createInputFile(inputPath, 1, 500);
+    createInputFile(inputPath, 2, 500);
+    createInputFile(inputPath, 3, 20);
+    createInputFile(inputPath, 4, 5000);
+    createInputFile(inputPath, 5, 500);
+
+    return inputPath;
+  }
+
+  /**
+   * Verify that we got the correct amount of output.
+   */
+  private void verifyOutput(Path outputPath) throws IOException {
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.getLocal(conf);
+
+    Path outputFile = new Path(outputPath, "part-r-00000");
+    InputStream is = fs.open(outputFile);
+    BufferedReader r = new BufferedReader(new InputStreamReader(is));
+
+    // Should get a single line of the form "0\t(count)"
+    String line = r.readLine().trim();
+    assertTrue("Line does not have correct key", line.startsWith("0\t"));
+    int count = Integer.valueOf(line.substring(2));
+    assertEquals("Incorrect count generated!", TOTAL_RECORDS, count);
+
+    r.close();
+
+  }
+
+  /**
+   * Run a test with several mappers in parallel, operating at different
+   * speeds. Verify that the correct amount of output is created.
+   */
+  @Test
+  public void testMultiMaps() throws Exception {
+    Job job = new Job();
+
+    Path inputPath = createMultiMapsInput();
+    Path outputPath = getOutputPath();
+
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.getLocal(conf);
+
+    if (fs.exists(outputPath)) {
+      fs.delete(outputPath, true);
+    }
+
+    job.setMapperClass(StressMapper.class);
+    job.setReducerClass(CountingReducer.class);
+    job.setNumReduceTasks(1);
+    LocalJobRunner.setLocalMaxRunningMaps(job, 6);
+    job.getConfiguration().set("io.sort.record.pct", "0.50");
+    job.getConfiguration().set("io.sort.mb", "25");
+    FileInputFormat.addInputPath(job, inputPath);
+    FileOutputFormat.setOutputPath(job, outputPath);
+
+    job.waitForCompletion(true);
+
+    verifyOutput(outputPath);
+  }
+
+  /**
+   * Run a test with a misconfigured number of mappers.
+   * Expect failure.
+   */
+  @Test
+  public void testInvalidMultiMapParallelism() throws Exception {
+    Job job = new Job();
+
+    Path inputPath = createMultiMapsInput();
+    Path outputPath = getOutputPath();
+
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.getLocal(conf);
+
+    if (fs.exists(outputPath)) {
+      fs.delete(outputPath, true);
+    }
+
+    job.setMapperClass(StressMapper.class);
+    job.setReducerClass(CountingReducer.class);
+    job.setNumReduceTasks(1);
+    LocalJobRunner.setLocalMaxRunningMaps(job, -6);
+    FileInputFormat.addInputPath(job, inputPath);
+    FileOutputFormat.setOutputPath(job, outputPath);
+
+    boolean success = job.waitForCompletion(true);
+    assertFalse("Job succeeded somehow", success);
+  }
+
+  /** An IF that creates no splits */
+  private static class EmptyInputFormat extends InputFormat<Object, Object> {
+    public List<InputSplit> getSplits(JobContext context) {
+      return new ArrayList<InputSplit>();
+    }
+
+    public RecordReader<Object, Object> createRecordReader(InputSplit split,
+        TaskAttemptContext context) {
+      return new EmptyRecordReader();
+    }
+  }
+
+  private static class EmptyRecordReader extends RecordReader<Object, Object> {
+    public void initialize(InputSplit split, TaskAttemptContext context) {
+    }
+
+    public Object getCurrentKey() {
+      return new Object();
+    }
+
+    public Object getCurrentValue() {
+      return new Object();
+    }
+
+    public float getProgress() {
+      return 0.0f;
+    }
+
+    public void close() {
+    }
+
+    public boolean nextKeyValue() {
+      return false;
+    }
+  }
+
+  /** Test case for zero mappers */
+  public void testEmptyMaps() throws Exception {
+    Job job = new Job();
+    Path outputPath = getOutputPath();
+
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.getLocal(conf);
+
+    if (fs.exists(outputPath)) {
+      fs.delete(outputPath, true);
+    }
+
+    job.setInputFormatClass(EmptyInputFormat.class);
+    job.setNumReduceTasks(1);
+    FileOutputFormat.setOutputPath(job, outputPath);
+
+    boolean success = job.waitForCompletion(true);
+    assertTrue("Empty job should work", success);
+  }
+}
\ No newline at end of file
diff --git a/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml b/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml
deleted file mode 100644
index 721ef03..0000000
--- a/src/docs/src/documentation/content/xdocs/Pluggable_Sort.xml
+++ /dev/null
@@ -1,93 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Copyright 2002-2004 The Apache Software Foundation
-
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-
-<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
-    "http://forrest.apache.org/dtd/document-v20.dtd">
-
-
-<document>
-
-  <header>
-    <title>
-      Hadoop Pluggable Sort
-    </title>
-  </header>
-
-  <body>
-    <section>
-      <title>Introduction</title>
-      <p>
-        The pluggable sort capability allows replacing the built in sort logic with 
-        alternate implementations. Example use cases for this are replacing the sort 
-        logic with custom algorithms that enable Hash aggregation and Limit-N query.
-      </p>
-      <p>
-        <strong>IMPORTANT:</strong> The pluggable sort capability is experimental 
-        and unstable. This means the provided APIs may change and break compatibility 
-        in future versions of Hadoop.
-      </p>
-    </section>
-
-    <section>
-      <title>Implementing a Custom Sort</title>
-      <p>
-        A custom sort implementation requires a <strong>org.apache.hadoop.mapred.MapOutputCollector</strong>
-        implementation class running in the Mapper tasks and (optionally, depending
-        on the sort implementation) a <strong>org.apache.hadoop.mapred.ShuffleConsumerPlugin</strong>
-        implementation class running in the Reducer tasks.
-      </p>
-      <p>
-        The default implementations provided by Hadoop can be used as references:
-      </p>
-      <ul>
-        <li><strong>org.apache.hadoop.mapred.MapTask$MapOutputBuffer</strong></li>
-        <li><strong>org.apache.hadoop.mapreduce.task.reduce.Shuffle</strong></li>
-      </ul>
-    </section>
-
-    <section>
-      <title>Configuration</title>
-
-      <p>
-        All the pluggable components run in the job tasks. This means, they can be 
-        configured on per job basis.        
-      </p>
-      <p>
-        <strong>Job Configuration Properties</strong>
-      </p>
-
-      <ul>
-        <li>
-            <strong>mapreduce.job.reduce.shuffle.consumer.plugin.class</strong>. 
-            Default:<strong>org.apache.hadoop.mapreduce.task.reduce.Shuffle</strong>.
-            The <strong>ShuffleConsumerPlugin</strong> implementation to use
-        </li>
-        <li>
-          <strong>mapreduce.job.map.output.collector.class</strong>.
-          Default:<strong>org.apache.hadoop.mapred.MapTask$MapOutputBuffer</strong>.
-          The <strong>MapOutputCollector</strong> implementation to use
-        </li>
-      </ul>
-
-      <p>
-        These properties can also be set in the <strong>mapred-site.xml</strong> to 
-        change the default values for all jobs.        
-      </p>
-    </section>
-
-  </body>
-</document>
diff --git a/src/mapred/org/apache/hadoop/mapred/IndexRecord.java b/src/mapred/org/apache/hadoop/mapred/IndexRecord.java
deleted file mode 100644
index 3996534..0000000
--- a/src/mapred/org/apache/hadoop/mapred/IndexRecord.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-@InterfaceAudience.LimitedPrivate({"MapReduce"})
-@InterfaceStability.Unstable
-public class IndexRecord {
-  public long startOffset;
-  public long rawLength;
-  public long partLength;
-
-  public IndexRecord() { }
-
-  public IndexRecord(long startOffset, long rawLength, long partLength) {
-    this.startOffset = startOffset;
-    this.rawLength = rawLength;
-    this.partLength = partLength;
-  }
-}
diff --git a/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java b/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java
deleted file mode 100644
index 368c016..0000000
--- a/src/mapred/org/apache/hadoop/mapred/MapOutputCollector.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.IOException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-import org.apache.hadoop.mapred.Task.TaskReporter;
-
-@InterfaceAudience.LimitedPrivate({"MapReduce"})
-@InterfaceStability.Unstable
-public interface MapOutputCollector<K, V> {
-  public void init(Context context
-                  ) throws IOException, ClassNotFoundException;
-  public void collect(K key, V value, int partition
-                     ) throws IOException, InterruptedException;
-  public void close() throws IOException, InterruptedException;
-    
-  public void flush() throws IOException, InterruptedException, 
-                             ClassNotFoundException;
-
-  @InterfaceAudience.LimitedPrivate({"MapReduce"})
-  @InterfaceStability.Unstable
-  public static class Context {
-    private final MapTask mapTask;
-    private final JobConf jobConf;
-    private final TaskReporter reporter;
-
-    public Context(MapTask mapTask, JobConf jobConf, TaskReporter reporter) {
-      this.mapTask = mapTask;
-      this.jobConf = jobConf;
-      this.reporter = reporter;
-    }
-
-    public MapTask getMapTask() {
-      return mapTask;
-    }
-
-    public JobConf getJobConf() {
-      return jobConf;
-    }
-
-    public TaskReporter getReporter() {
-      return reporter;
-    }
-  }
-}
diff --git a/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java b/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
deleted file mode 100644
index 657e1db..0000000
--- a/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.IOException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.mapred.Task.TaskReporter;
-
-@InterfaceAudience.LimitedPrivate("MapReduce")
-@InterfaceStability.Unstable
-public interface ShuffleConsumerPlugin {
-  /**
-   * To initialize the reduce copier plugin.
-   * @param context reduce copier context.
-   */
-  public void init(Context context)
-    throws ClassNotFoundException, IOException;
-
-  /**
-   * To fetch the map outputs.
-   * @return true if the fetch was successful; false otherwise.
-   */
-  public boolean fetchOutputs() throws IOException;
-
-  /**
-   * To create a key-value iterator to read the merged output.
-   * @return an iterator for merged key-value pairs.
-   */
-  public RawKeyValueIterator createKVIterator() throws IOException;
-
-  /**
-   * close and clean any resource associated with this object.
-   */
-  public void close();
-  
-  /**
-   * To get any exception from merge.
-   */
-  public Throwable getMergeThrowable();
-
-  public static class Context {
-    private final TaskUmbilicalProtocol umbilical;
-    private final JobConf jobConf;
-    private final TaskReporter reporter;
-    private final ReduceTask reduceTask;
-
-    public Context(TaskUmbilicalProtocol umbilical, JobConf conf,
-                   TaskReporter reporter, ReduceTask reduceTask) {
-      this.umbilical = umbilical;
-      this.jobConf = conf;
-      this.reporter = reporter;
-      this.reduceTask = reduceTask;
-    }
-
-    public TaskUmbilicalProtocol getUmbilical() {
-      return umbilical;
-    }
-
-    public JobConf getJobConf() {
-      return jobConf;
-    }
-
-    public TaskReporter getReporter() {
-      return reporter;
-    }
-
-    public ReduceTask getReduceTask() {
-      return reduceTask;
-    }
-  }
-}
diff --git a/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java b/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java
deleted file mode 100644
index 9399267..0000000
--- a/src/test/org/apache/hadoop/mapred/TestCombineOutputCollector.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.never;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.IFile.Writer;
-import org.apache.hadoop.mapred.Task.CombineOutputCollector;
-import org.apache.hadoop.mapred.Task.TaskReporter;
-import org.junit.Test;
-
-public class TestCombineOutputCollector {
-  private CombineOutputCollector<String, Integer> coc;
-
-  @Test
-  public void testCustomCollect() throws Throwable {
-    //mock creation
-    TaskReporter mockTaskReporter = mock(TaskReporter.class);
-    Counters.Counter outCounter = new Counters.Counter();
-    Writer<String, Integer> mockWriter = mock(Writer.class);
-
-    Configuration conf = new Configuration();
-    conf.set("mapred.combine.recordsBeforeProgress", "2");
-    
-    coc = new CombineOutputCollector<String, Integer>(outCounter, mockTaskReporter, conf);
-    coc.setWriter(mockWriter);
-    verify(mockTaskReporter, never()).progress();
-
-    coc.collect("dummy", 1);
-    verify(mockTaskReporter, never()).progress();
-    
-    coc.collect("dummy", 2);
-    verify(mockTaskReporter, times(1)).progress();
-  }
-  
-  @Test
-  public void testDefaultCollect() throws Throwable {
-    //mock creation
-    TaskReporter mockTaskReporter = mock(TaskReporter.class);
-    Counters.Counter outCounter = new Counters.Counter();
-    Writer<String, Integer> mockWriter = mock(Writer.class);
-
-    Configuration conf = new Configuration();
-    
-    coc = new CombineOutputCollector<String, Integer>(outCounter, mockTaskReporter, conf);
-    coc.setWriter(mockWriter);
-    verify(mockTaskReporter, never()).progress();
-
-    for(int i = 0; i < Task.DEFAULT_MR_COMBINE_RECORDS_BEFORE_PROGRESS; i++) {
-    	coc.collect("dummy", i);
-    }
-    verify(mockTaskReporter, times(1)).progress();
-    for(int i = 0; i < Task.DEFAULT_MR_COMBINE_RECORDS_BEFORE_PROGRESS; i++) {
-    	coc.collect("dummy", i);
-    }
-    verify(mockTaskReporter, times(2)).progress();
-  }
-}
diff --git a/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java b/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java
deleted file mode 100644
index 149fcb7..0000000
--- a/src/test/org/apache/hadoop/mapred/TestJobLocalizer.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import static org.junit.Assert.assertTrue;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.junit.Test;
-
-public class TestJobLocalizer {
-
-  @Test(timeout = 1000)
-  public void testConcurrentJobLocalizers() throws IOException {
-    final String LOCAL_DIR = "/tmp/mapred/local";
-    JobConf conf = new JobConf(new Configuration());
-    
-    JobLocalizer localizer1 = new JobLocalizer(conf, "user1", "jobid1",
-        LOCAL_DIR);
-    JobLocalizer localizer2 = new JobLocalizer(conf, "user2", "jobid2",
-        LOCAL_DIR);
-    assertTrue("Localizer 1 job local dirs should have user1",
-        localizer1.ttConf.get(JobLocalizer.JOB_LOCAL_CTXT).contains("user1"));
-    assertTrue("Localizer 2 job local dirs should have user2",
-        localizer2.ttConf.get(JobLocalizer.JOB_LOCAL_CTXT).contains("user2"));
-  }
-}
diff --git a/src/test/org/apache/hadoop/mapred/TestMerger.java b/src/test/org/apache/hadoop/mapred/TestMerger.java
deleted file mode 100644
index 501a826..0000000
--- a/src/test/org/apache/hadoop/mapred/TestMerger.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import static org.mockito.Matchers.any;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.Counters.Counter;
-import org.apache.hadoop.mapred.IFile.Reader;
-import org.apache.hadoop.mapred.Merger.Segment;
-import org.apache.hadoop.util.Progressable;
-import org.junit.Test;
-import org.mockito.invocation.InvocationOnMock;
-import org.mockito.stubbing.Answer;
-
-public class TestMerger {
-
-  @Test
-  public void testUncompressed() throws IOException {
-    testMergeShouldReturnProperProgress(getUncompressedSegments());
-  }
-  
-  @Test
-  public void testCompressed() throws IOException {
-    testMergeShouldReturnProperProgress(getCompressedSegments());
-  }
-  
-  @SuppressWarnings( { "deprecation", "unchecked" })
-  public void testMergeShouldReturnProperProgress(
-      List<Segment<Text, Text>> segments) throws IOException {
-    Configuration conf = new Configuration();
-    JobConf jobConf = new JobConf();
-    FileSystem fs = FileSystem.getLocal(conf);
-    Path tmpDir = new Path("localpath");
-    Class<Text> keyClass = (Class<Text>) jobConf.getMapOutputKeyClass();
-    Class<Text> valueClass = (Class<Text>) jobConf.getMapOutputValueClass();
-    RawComparator<Text> comparator = jobConf.getOutputKeyComparator();
-    Counter readsCounter = new Counter();
-    Counter writesCounter = new Counter();
-    RawKeyValueIterator mergeQueue = Merger.merge(conf, fs, keyClass,
-        valueClass, segments, 2, tmpDir, comparator, getReporter(),
-        readsCounter, writesCounter);
-    Assert.assertEquals(1.0f, mergeQueue.getProgress().get());
-  }
-
-  private Progressable getReporter() {
-    Progressable reporter = new Progressable() {
-      @Override
-      public void progress() {
-      }
-    };
-    return reporter;
-  }
-
-  private List<Segment<Text, Text>> getUncompressedSegments() throws IOException {
-    List<Segment<Text, Text>> segments = new ArrayList<Segment<Text, Text>>();
-    for (int i = 1; i < 10; i++) {
-      segments.add(getUncompressedSegment(i));
-    }
-    return segments;
-  }
-
-  private List<Segment<Text, Text>> getCompressedSegments() throws IOException {
-    List<Segment<Text, Text>> segments = new ArrayList<Segment<Text, Text>>();
-    for (int i = 1; i < 10; i++) {
-      segments.add(getCompressedSegment(i));
-    }
-    return segments;
-  }
-  
-  private Segment<Text, Text> getUncompressedSegment(int i) throws IOException {
-    return new Segment<Text, Text>(getReader(i), false);
-  }
-  
-  private Segment<Text, Text> getCompressedSegment(int i) throws IOException {
-    return new Segment<Text, Text>(getReader(i), false, 3000l);
-  }
-
-  @SuppressWarnings("unchecked")
-  private Reader<Text, Text> getReader(int i) throws IOException {
-    Reader<Text, Text> readerMock = mock(Reader.class);
-    when(readerMock.getPosition()).thenReturn(0l).thenReturn(10l).thenReturn(
-        20l);
-    when(
-        readerMock.next(any(DataInputBuffer.class), any(DataInputBuffer.class)))
-        .thenAnswer(getAnswer("Segment" + i));
-    return readerMock;
-  }
-
-  private Answer<?> getAnswer(final String segmentName) {
-    return new Answer<Object>() {
-      int i = 0;
-
-      public Boolean answer(InvocationOnMock invocation) {
-        Object[] args = invocation.getArguments();
-        DataInputBuffer key = (DataInputBuffer) args[0];
-        DataInputBuffer value = (DataInputBuffer) args[1];
-        if (i++ == 2) {
-          return false;
-        }
-        key.reset(("Segement Key " + segmentName + i).getBytes(), 20);
-        value.reset(("Segement Value" + segmentName + i).getBytes(), 20);
-        return true;
-      }
-    };
-  }
-}
diff --git a/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java b/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java
deleted file mode 100644
index d3cd1ab..0000000
--- a/src/test/org/apache/hadoop/mapreduce/TestLocalRunner.java
+++ /dev/null
@@ -1,363 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapreduce;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.LocalJobRunner;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-
-import org.junit.Test;
-import junit.framework.TestCase;
-
-/**
- * Stress tests for the LocalJobRunner
- */
-public class TestLocalRunner extends TestCase {
-
-  private static final Log LOG = LogFactory.getLog(TestLocalRunner.class);
-
-  private static class StressMapper
-      extends Mapper<LongWritable, Text, LongWritable, Text> {
-
-    // Different map tasks operate at different speeds.
-    // We define behavior for 6 threads.
-    private int threadId;
-
-    // Used to ensure that the compiler doesn't optimize away
-    // some code.
-    public long exposedState;
-
-    protected void setup(Context context) {
-      // Get the thread num from the file number.
-      FileSplit split = (FileSplit) context.getInputSplit();
-      Path filePath = split.getPath();
-      String name = filePath.getName();
-      this.threadId = Integer.valueOf(name);
-
-      LOG.info("Thread " + threadId + " : "
-          + context.getInputSplit());
-    }
-
-    /** Map method with different behavior based on the thread id */
-    public void map(LongWritable key, Text val, Context c)
-        throws IOException, InterruptedException {
-
-      switch(threadId) {
-      case 0:
-        // Write a single value and be done.
-        c.write(new LongWritable(0), val);
-        break;
-      case 1:
-      case 2:
-        // Write many values quickly.
-        for (int i = 0; i < 500; i++) {
-          c.write(new LongWritable(0), val);
-        }
-        break;
-      case 3:
-        // Write many values, using thread sleeps to delay this.
-        for (int i = 0; i < 50; i++) {
-          for (int j = 0; j < 10; j++) {
-            c.write(new LongWritable(0), val);
-          }
-          Thread.sleep(1);
-        }
-        break;
-      case 4:
-        // Write many values, using busy-loops to delay this.
-        for (int i = 0; i < 500; i++) {
-          for (int j = 0; j < 10000; j++) {
-            this.exposedState++;
-          }
-          c.write(new LongWritable(0), val);
-        }
-        break;
-      case 5:
-        // Write many values, using very slow busy-loops to delay this.
-        for (int i = 0; i < 500; i++) {
-          for (int j = 0; j < 100000; j++) {
-            this.exposedState++;
-          }
-          c.write(new LongWritable(0), val);
-        }
-        break;
-      default:
-        // Write a single value and be done.
-        c.write(new LongWritable(0), val);
-        break;
-      }
-    }
-
-    protected void cleanup(Context context) {
-      // Output this here, to ensure that the incrementing done in map()
-      // cannot be optimized away.
-      LOG.debug("Busy loop counter: " + this.exposedState);
-    }
-  }
-
-  private static class CountingReducer
-      extends Reducer<LongWritable, Text, LongWritable, LongWritable> {
-
-    public void reduce(LongWritable key, Iterable<Text> vals, Context context)
-        throws IOException, InterruptedException {
-      long out = 0;
-      for (Text val : vals) {
-        out++;
-      }
-
-      context.write(key, new LongWritable(out));
-    }
-  }
-
-  /**
-   * Create a single input file in the input directory.
-   * @param dirPath the directory in which the file resides
-   * @param id the file id number
-   * @param numRecords how many records to write to each file.
-   */
-  private void createInputFile(Path dirPath, int id, int numRecords)
-      throws IOException {
-    final String MESSAGE = "This is a line in a file: ";
-
-    Path filePath = new Path(dirPath, "" + id);
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.getLocal(conf);
-
-    OutputStream os = fs.create(filePath);
-    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os));
-
-    for (int i = 0; i < numRecords; i++) {
-      w.write(MESSAGE + id + " " + i + "\n");
-    }
-
-    w.close();
-  }
-
-  // This is the total number of map output records we expect to generate,
-  // based on input file sizes (see createMultiMapsInput()) and the behavior
-  // of the different StressMapper threads.
-  private final static int TOTAL_RECORDS = 50000
-      + (500 * 500)
-      + (500 * 500)
-      + (20 * 500)
-      + (5000 * 500)
-      + (500 * 500);
-
-  private final String INPUT_DIR = "multiMapInput";
-  private final String OUTPUT_DIR = "multiMapOutput";
-
-  private Path getInputPath() {
-    String dataDir = System.getProperty("test.build.data");
-    if (null == dataDir) {
-      return new Path(INPUT_DIR);
-    } else {
-      return new Path(new Path(dataDir), INPUT_DIR);
-    }
-  }
-
-  private Path getOutputPath() {
-    String dataDir = System.getProperty("test.build.data");
-    if (null == dataDir) {
-      return new Path(OUTPUT_DIR);
-    } else {
-      return new Path(new Path(dataDir), OUTPUT_DIR);
-    }
-  }
-
-  /**
-   * Create the inputs for the MultiMaps test.
-   * @return the path to the input directory.
-   */
-  private Path createMultiMapsInput() throws IOException {
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.getLocal(conf);
-    Path inputPath = getInputPath();
-
-    // Clear the input directory if it exists, first.
-    if (fs.exists(inputPath)) {
-      fs.delete(inputPath, true);
-    }
-
-    // Create input files, with sizes calibrated based on
-    // the amount of work done in each mapper.
-    createInputFile(inputPath, 0, 50000);
-    createInputFile(inputPath, 1, 500);
-    createInputFile(inputPath, 2, 500);
-    createInputFile(inputPath, 3, 20);
-    createInputFile(inputPath, 4, 5000);
-    createInputFile(inputPath, 5, 500);
-
-    return inputPath;
-  }
-
-  /**
-   * Verify that we got the correct amount of output.
-   */
-  private void verifyOutput(Path outputPath) throws IOException {
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.getLocal(conf);
-
-    Path outputFile = new Path(outputPath, "part-r-00000");
-    InputStream is = fs.open(outputFile);
-    BufferedReader r = new BufferedReader(new InputStreamReader(is));
-
-    // Should get a single line of the form "0\t(count)"
-    String line = r.readLine().trim();
-    assertTrue("Line does not have correct key", line.startsWith("0\t"));
-    int count = Integer.valueOf(line.substring(2));
-    assertEquals("Incorrect count generated!", TOTAL_RECORDS, count);
-
-    r.close();
-
-  }
-
-  /**
-   * Run a test with several mappers in parallel, operating at different
-   * speeds. Verify that the correct amount of output is created.
-   */
-  @Test
-  public void testMultiMaps() throws Exception {
-    Job job = new Job();
-
-    Path inputPath = createMultiMapsInput();
-    Path outputPath = getOutputPath();
-
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.getLocal(conf);
-
-    if (fs.exists(outputPath)) {
-      fs.delete(outputPath, true);
-    }
-
-    job.setMapperClass(StressMapper.class);
-    job.setReducerClass(CountingReducer.class);
-    job.setNumReduceTasks(1);
-    LocalJobRunner.setLocalMaxRunningMaps(job, 6);
-    job.getConfiguration().set("io.sort.record.pct", "0.50");
-    job.getConfiguration().set("io.sort.mb", "25");
-    FileInputFormat.addInputPath(job, inputPath);
-    FileOutputFormat.setOutputPath(job, outputPath);
-
-    job.waitForCompletion(true);
-
-    verifyOutput(outputPath);
-  }
-
-  /**
-   * Run a test with a misconfigured number of mappers.
-   * Expect failure.
-   */
-  @Test
-  public void testInvalidMultiMapParallelism() throws Exception {
-    Job job = new Job();
-
-    Path inputPath = createMultiMapsInput();
-    Path outputPath = getOutputPath();
-
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.getLocal(conf);
-
-    if (fs.exists(outputPath)) {
-      fs.delete(outputPath, true);
-    }
-
-    job.setMapperClass(StressMapper.class);
-    job.setReducerClass(CountingReducer.class);
-    job.setNumReduceTasks(1);
-    LocalJobRunner.setLocalMaxRunningMaps(job, -6);
-    FileInputFormat.addInputPath(job, inputPath);
-    FileOutputFormat.setOutputPath(job, outputPath);
-
-    boolean success = job.waitForCompletion(true);
-    assertFalse("Job succeeded somehow", success);
-  }
-
-  /** An IF that creates no splits */
-  private static class EmptyInputFormat extends InputFormat<Object, Object> {
-    public List<InputSplit> getSplits(JobContext context) {
-      return new ArrayList<InputSplit>();
-    }
-
-    public RecordReader<Object, Object> createRecordReader(InputSplit split,
-        TaskAttemptContext context) {
-      return new EmptyRecordReader();
-    }
-  }
-
-  private static class EmptyRecordReader extends RecordReader<Object, Object> {
-    public void initialize(InputSplit split, TaskAttemptContext context) {
-    }
-
-    public Object getCurrentKey() {
-      return new Object();
-    }
-
-    public Object getCurrentValue() {
-      return new Object();
-    }
-
-    public float getProgress() {
-      return 0.0f;
-    }
-
-    public void close() {
-    }
-
-    public boolean nextKeyValue() {
-      return false;
-    }
-  }
-
-  /** Test case for zero mappers */
-  public void testEmptyMaps() throws Exception {
-    Job job = new Job();
-    Path outputPath = getOutputPath();
-
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.getLocal(conf);
-
-    if (fs.exists(outputPath)) {
-      fs.delete(outputPath, true);
-    }
-
-    job.setInputFormatClass(EmptyInputFormat.class);
-    job.setNumReduceTasks(1);
-    FileOutputFormat.setOutputPath(job, outputPath);
-
-    boolean success = job.waitForCompletion(true);
-    assertTrue("Empty job should work", success);
-  }
-}
\ No newline at end of file
-- 
1.7.0.4

