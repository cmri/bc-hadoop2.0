<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Hadoop 0.23.1 Release Notes</title>
<STYLE type="text/css">
		H1 {font-family: sans-serif}
		H2 {font-family: sans-serif; margin-left: 7mm}
		TABLE {margin-left: 7mm}
	</STYLE>
</head>
<body>
<h1>Hadoop 0.23.1 Release Notes</h1>
		These release notes include new developer and user-facing incompatibilities, features, and major improvements. 

<a name="changes"/>
<h2>Changes since Hadoop 0.23.0</h2>

<h3>Jiras with Release Notes (describe major or incompatible changes)</h3>
<ul>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7348">HADOOP-7348</a>.
     Major improvement reported by xiexianshan and fixed by xiexianshan (fs)<br>
     <b>Modify the option of FsShell getmerge from [addnl] to [-nl] for more comprehensive</b><br>
     <blockquote>                                              The &#39;fs -getmerge&#39; tool now uses a -nl flag to determine if adding a newline at end of each file is required, in favor of the &#39;addnl&#39; boolean flag that was used earlier.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7802">HADOOP-7802</a>.
     Major bug reported by bmahe and fixed by bmahe <br>
     <b>Hadoop scripts unconditionally source &quot;$bin&quot;/../libexec/hadoop-config.sh.</b><br>
     <blockquote>                    Here is a patch to enable this behavior<br/>


</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7963">HADOOP-7963</a>.
     Blocker bug reported by tgraves and fixed by sseth <br>
     <b>test failures: TestViewFileSystemWithAuthorityLocalFileSystem and TestViewFileSystemLocalFileSystem</b><br>
     <blockquote>                                              Fix ViewFS to catch a null canonical service-name and pass tests TestViewFileSystem*

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7986">HADOOP-7986</a>.
     Major bug reported by mahadev and fixed by mahadev <br>
     <b>Add config for History Server protocol in hadoop-policy for service level authorization.</b><br>
     <blockquote>                                              Adding config for MapReduce History Server protocol in hadoop-policy.xml for service level authorization.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1314">HDFS-1314</a>.
     Minor bug reported by karims and fixed by sho.shimauchi <br>
     <b>dfs.blocksize accepts only absolute value</b><br>
     <blockquote>                                              The default blocksize property &#39;dfs.blocksize&#39; now accepts unit symbols to be used instead of byte length. Values such as &quot;10k&quot;, &quot;128m&quot;, &quot;1g&quot; are now OK to provide instead of just no. of bytes as was before.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2129">HDFS-2129</a>.
     Major sub-task reported by tlipcon and fixed by tlipcon (hdfs client, performance)<br>
     <b>Simplify BlockReader to not inherit from FSInputChecker</b><br>
     <blockquote>                                              BlockReader has been reimplemented to use direct byte buffers. If you use a custom socket factory, it must generate sockets that have associated Channels.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2130">HDFS-2130</a>.
     Major sub-task reported by tlipcon and fixed by tlipcon (hdfs client)<br>
     <b>Switch default checksum to CRC32C</b><br>
     <blockquote>                                              The default checksum algorithm used on HDFS is now CRC32C. Data from previous versions of Hadoop can still be read backwards-compatibly.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2246">HDFS-2246</a>.
     Major improvement reported by sanjay.radia and fixed by jnp <br>
     <b>Shortcut a local client reads to a Datanodes files directly</b><br>
     <blockquote>                    1. New configurations<br/>

a. dfs.block.local-path-access.user is the key in datanode configuration to specify the user allowed to do short circuit read.<br/>

b. dfs.client.read.shortcircuit is the key to enable short circuit read at the client side configuration.<br/>

c. dfs.client.read.shortcircuit.skip.checksum is the key to bypass checksum check at the client side.<br/>

2. By default none of the above are enabled and short circuit read will not kick in.<br/>

3. If security is on, the feature can be used only for user that has kerberos credentials at the client, therefore map reduce tasks cannot benefit from it in general.<br/>


</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2316">HDFS-2316</a>.
     Major new feature reported by szetszwo and fixed by szetszwo <br>
     <b>[umbrella] WebHDFS: a complete FileSystem implementation for accessing HDFS over HTTP</b><br>
     <blockquote>                    Provide WebHDFS as a complete FileSystem implementation for accessing HDFS over HTTP.<br/>

Previous hftp feature was a read-only FileSystem and does not provide &quot;write&quot; accesses.
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-778">MAPREDUCE-778</a>.
     Major new feature reported by hong.tang and fixed by amar_kamat (tools/rumen)<br>
     <b>[Rumen] Need a standalone JobHistory log anonymizer</b><br>
     <blockquote>                                              Added an anonymizer tool to Rumen. Anonymizer takes a Rumen trace file and/or topology as input. It supports persistence and plugins to override the default behavior.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2733">MAPREDUCE-2733</a>.
     Major task reported by vinaythota and fixed by vinaythota <br>
     <b>Gridmix v3 cpu emulation system tests.</b><br>
     <blockquote>                                              Adds system tests for the CPU emulation feature in Gridmix3.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2765">MAPREDUCE-2765</a>.
     Major new feature reported by mithun and fixed by mithun (distcp, mrv2)<br>
     <b>DistCp Rewrite</b><br>
     <blockquote>                                              DistCpV2 added to hadoop-tools.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2784">MAPREDUCE-2784</a>.
     Major bug reported by amar_kamat and fixed by amar_kamat (contrib/gridmix)<br>
     <b>[Gridmix] TestGridmixSummary fails with NPE when run in DEBUG mode.</b><br>
     <blockquote>                                              Fixed bugs in ExecutionSummarizer and ResourceUsageMatcher.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2863">MAPREDUCE-2863</a>.
     Blocker improvement reported by acmurthy and fixed by tgraves (mrv2, nodemanager, resourcemanager)<br>
     <b>Support web-services for RM &amp; NM</b><br>
     <blockquote>                                              Support for web-services in YARN and MR components.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2950">MAPREDUCE-2950</a>.
     Major bug reported by amar_kamat and fixed by ravidotg (contrib/gridmix)<br>
     <b>[Gridmix] TestUserResolve fails in trunk</b><br>
     <blockquote>                                              Fixes bug in TestUserResolve.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3102">MAPREDUCE-3102</a>.
     Major sub-task reported by vinodkv and fixed by hitesh (mrv2, security)<br>
     <b>NodeManager should fail fast with wrong configuration or permissions for LinuxContainerExecutor</b><br>
     <blockquote>                                              Changed NodeManager to fail fast when LinuxContainerExecutor has wrong configuration or permissions.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3215">MAPREDUCE-3215</a>.
     Minor sub-task reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>org.apache.hadoop.mapreduce.TestNoJobSetupCleanup failing on trunk</b><br>
     <blockquote>                    Reneabled and fixed bugs in the failing test TestNoJobSetupCleanup.<br/>


</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3217">MAPREDUCE-3217</a>.
     Minor sub-task reported by hitesh and fixed by devaraj.k (mrv2, test)<br>
     <b>ant test TestAuditLogger fails on trunk</b><br>
     <blockquote>                    Reenabled and fixed bugs in the failing ant test TestAuditLogger.<br/>


</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3219">MAPREDUCE-3219</a>.
     Minor sub-task reported by hitesh and fixed by hitesh (mrv2, test)<br>
     <b>ant test TestDelegationToken failing on trunk</b><br>
     <blockquote>                    Reenabled and fixed bugs in the failing test TestDelegationToken.<br/>


</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3221">MAPREDUCE-3221</a>.
     Minor sub-task reported by hitesh and fixed by devaraj.k (mrv2, test)<br>
     <b>ant test TestSubmitJob failing on trunk</b><br>
     <blockquote>                                              Fixed a bug in TestSubmitJob.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3280">MAPREDUCE-3280</a>.
     Major bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>MR AM should not read the username from configuration</b><br>
     <blockquote>                                              Removed the unnecessary job user-name configuration in mapred-site.xml.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3297">MAPREDUCE-3297</a>.
     Major task reported by sseth and fixed by sseth (mrv2)<br>
     <b>Move Log Related components from yarn-server-nodemanager to yarn-common</b><br>
     <blockquote>                    Moved log related components into yarn-common so that HistoryServer and clients can use them without depending on the yarn-server-nodemanager module.<br/>


</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3299">MAPREDUCE-3299</a>.
     Minor improvement reported by sseth and fixed by jeagles (mrv2)<br>
     <b>Add AMInfo table to the AM job page</b><br>
     <blockquote>                                              Added AMInfo table to the MR AM job pages to list all the job-attempts when AM restarts and recovers.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3312">MAPREDUCE-3312</a>.
     Major bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>Make MR AM not send a stopContainer w/o corresponding start container</b><br>
     <blockquote>                                              Modified MR AM to not send a stop-container request for a container that isn&#39;t launched at all.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3325">MAPREDUCE-3325</a>.
     Major improvement reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>Improvements to CapacityScheduler doc</b><br>
     <blockquote>                                              document changes only.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3333">MAPREDUCE-3333</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>MR AM for sort-job going out of memory</b><br>
     <blockquote>                                              Fixed bugs in ContainerLauncher of MR AppMaster due to which per-container connections to NodeManager were lingering long enough to hit the ulimits on number of processes.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3339">MAPREDUCE-3339</a>.
     Blocker bug reported by ramgopalnaali and fixed by sseth (mrv2)<br>
     <b>Job is getting hanged indefinitely,if the child processes are killed on the NM.  KILL_CONTAINER eventtype is continuosly sent to the containers that are not existing</b><br>
     <blockquote>                                              Fixed MR AM to stop considering node blacklisting after the number of nodes blacklisted crosses a threshold.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3342">MAPREDUCE-3342</a>.
     Critical bug reported by tgraves and fixed by jeagles (jobhistoryserver, mrv2)<br>
     <b>JobHistoryServer doesn&apos;t show job queue</b><br>
     <blockquote>                                              Fixed JobHistoryServer to also show the job&#39;s queue name.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3345">MAPREDUCE-3345</a>.
     Major bug reported by vinodkv and fixed by hitesh (mrv2, resourcemanager)<br>
     <b>Race condition in ResourceManager causing TestContainerManagerSecurity to fail sometimes</b><br>
     <blockquote>                                              Fixed a race condition in ResourceManager that was causing TestContainerManagerSecurity to fail sometimes.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3349">MAPREDUCE-3349</a>.
     Blocker bug reported by vinodkv and fixed by amar_kamat (mrv2)<br>
     <b>No rack-name logged in JobHistory for unsuccessful tasks</b><br>
     <blockquote>                                              Unsuccessful tasks now log hostname and rackname to job history. 

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3355">MAPREDUCE-3355</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>AM scheduling hangs frequently with sort job on 350 nodes</b><br>
     <blockquote>                                              Fixed MR AM&#39;s ContainerLauncher to handle node-command timeouts correctly.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3360">MAPREDUCE-3360</a>.
     Critical improvement reported by kam_iitkgp and fixed by kamesh (mrv2)<br>
     <b>Provide information about lost nodes in the UI.</b><br>
     <blockquote>                                              Added information about lost/rebooted/decommissioned nodes on the webapps.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3368">MAPREDUCE-3368</a>.
     Critical bug reported by rramya and fixed by hitesh (build, mrv2)<br>
     <b>compile-mapred-test fails</b><br>
     <blockquote>                                              Fixed ant test compilation.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3375">MAPREDUCE-3375</a>.
     Major task reported by vinaythota and fixed by vinaythota <br>
     <b>Memory Emulation system tests.</b><br>
     <blockquote>                                              Added system tests to test the memory emulation feature in Gridmix.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3379">MAPREDUCE-3379</a>.
     Major bug reported by sseth and fixed by sseth (mrv2, nodemanager)<br>
     <b>LocalResourceTracker should not tracking deleted cache entries</b><br>
     <blockquote>                                              Fixed LocalResourceTracker in NodeManager to remove deleted cache entries correctly.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3382">MAPREDUCE-3382</a>.
     Critical bug reported by vinodkv and fixed by raviprak (applicationmaster, mrv2)<br>
     <b>Network ACLs can prevent AMs to ping the Job-end notification URL</b><br>
     <blockquote>                                              Enhanced MR AM to use a proxy to ping the job-end notification URL.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3387">MAPREDUCE-3387</a>.
     Critical bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>A tracking URL of N/A before the app master is launched breaks oozie</b><br>
     <blockquote>                                              Fixed AM&#39;s tracking URL to always go through the proxy, even before the job started, so that it works properly with oozie throughout the job execution.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3392">MAPREDUCE-3392</a>.
     Blocker sub-task reported by johnvijoe and fixed by johnvijoe <br>
     <b>Cluster.getDelegationToken() throws NPE if client.getDelegationToken() returns null.</b><br>
     <blockquote>                                              Fixed Cluster&#39;s getDelegationToken&#39;s API to return null when there isn&#39;t a supported token.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3398">MAPREDUCE-3398</a>.
     Blocker bug reported by sseth and fixed by sseth (mrv2, nodemanager)<br>
     <b>Log Aggregation broken in Secure Mode</b><br>
     <blockquote>                                              Fixed log aggregation to work correctly in secure mode. Contributed by Siddharth Seth.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3399">MAPREDUCE-3399</a>.
     Blocker sub-task reported by sseth and fixed by sseth (mrv2, nodemanager)<br>
     <b>ContainerLocalizer should request new resources after completing the current one</b><br>
     <blockquote>                                              Modified ContainerLocalizer to send a heartbeat to NM immediately after downloading a resource instead of always waiting for a second.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3404">MAPREDUCE-3404</a>.
     Critical bug reported by patwhitey2007 and fixed by eepayne (job submission, mrv2)<br>
     <b>Speculative Execution: speculative map tasks launched even if -Dmapreduce.map.speculative=false</b><br>
     <blockquote>                                              Corrected MR AM to honor speculative configuration and enable speculating either maps or reduces.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3407">MAPREDUCE-3407</a>.
     Minor bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Wrong jar getting used in TestMR*Jobs* for MiniMRYarnCluster</b><br>
     <blockquote>                                              Fixed pom files to refer to the correct MR app-jar needed by the integration tests.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3412">MAPREDUCE-3412</a>.
     Major bug reported by amar_kamat and fixed by amar_kamat <br>
     <b>&apos;ant docs&apos; is broken</b><br>
     <blockquote>                                              Fixes &#39;ant docs&#39; by removing stale references to capacity-scheduler docs.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3417">MAPREDUCE-3417</a>.
     Blocker bug reported by tgraves and fixed by jeagles (mrv2)<br>
     <b>job access controls not working app master and job history UI&apos;s</b><br>
     <blockquote>                                              Fixed job-access-controls to work with MR AM and JobHistoryServer web-apps.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3426">MAPREDUCE-3426</a>.
     Blocker sub-task reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>uber-jobs tried to write outputs into wrong dir</b><br>
     <blockquote>                                              Fixed MR AM in uber mode to write map intermediate outputs in the correct directory to work properly in secure mode.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3462">MAPREDUCE-3462</a>.
     Blocker bug reported by amar_kamat and fixed by raviprak (mrv2, test)<br>
     <b>Job submission failing in JUnit tests</b><br>
     <blockquote>                                              Fixed failing JUnit tests in Gridmix.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3481">MAPREDUCE-3481</a>.
     Major improvement reported by amar_kamat and fixed by amar_kamat (contrib/gridmix)<br>
     <b>[Gridmix] Improve STRESS mode locking</b><br>
     <blockquote>                                              Modified Gridmix STRESS mode locking structure. The submitted thread and the polling thread now run simultaneously without blocking each other. 

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3484">MAPREDUCE-3484</a>.
     Major bug reported by raviprak and fixed by raviprak (mr-am, mrv2)<br>
     <b>JobEndNotifier is getting interrupted before completing all its retries.</b><br>
     <blockquote>                                              Fixed JobEndNotifier to not get interrupted before completing all its retries.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3487">MAPREDUCE-3487</a>.
     Critical bug reported by tgraves and fixed by jlowe (mrv2)<br>
     <b>jobhistory web ui task counters no longer links to singletakecounter page</b><br>
     <blockquote>                                              Fixed JobHistory web-UI to display links to single task&#39;s counters&#39; page.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3490">MAPREDUCE-3490</a>.
     Blocker bug reported by sseth and fixed by sharadag (mr-am, mrv2)<br>
     <b>RMContainerAllocator counts failed maps towards Reduce ramp up</b><br>
     <blockquote>                                              Fixed MapReduce AM to count failed maps also towards Reduce ramp up.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3511">MAPREDUCE-3511</a>.
     Blocker sub-task reported by sseth and fixed by vinodkv (mr-am, mrv2)<br>
     <b>Counters occupy a good part of AM heap</b><br>
     <blockquote>                                              Removed a multitude of cloned/duplicate counters in the AM thereby reducing the AM heap size and preventing full GCs.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3512">MAPREDUCE-3512</a>.
     Blocker sub-task reported by sseth and fixed by sseth (mr-am, mrv2)<br>
     <b>Batch jobHistory disk flushes</b><br>
     <blockquote>                                              Batching JobHistory flushing to DFS so that we don&#39;t flush for every event slowing down AM.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3519">MAPREDUCE-3519</a>.
     Blocker sub-task reported by ravidotg and fixed by ravidotg (mrv2, nodemanager)<br>
     <b>Deadlock in LocalDirsHandlerService and ShuffleHandler</b><br>
     <blockquote>                                              Fixed a deadlock in NodeManager LocalDirectories&#39;s handling service.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3528">MAPREDUCE-3528</a>.
     Major bug reported by sseth and fixed by sseth (mr-am, mrv2)<br>
     <b>The task timeout check interval should be configurable independent of mapreduce.task.timeout</b><br>
     <blockquote>                                              Fixed TaskHeartBeatHandler to use a new configuration for the thread loop interval separate from task-timeout configuration property.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3530">MAPREDUCE-3530</a>.
     Blocker bug reported by karams and fixed by acmurthy (mrv2, resourcemanager, scheduler)<br>
     <b>Sometimes NODE_UPDATE to the scheduler throws an NPE causing the scheduling to stop</b><br>
     <blockquote>                                              Fixed an NPE occuring during scheduling in the ResourceManager.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3532">MAPREDUCE-3532</a>.
     Critical bug reported by karams and fixed by kamesh (mrv2, nodemanager)<br>
     <b>When 0 is provided as port number in yarn.nodemanager.webapp.address, NMs webserver component picks up random port, NM keeps on Reporting 0 port to RM</b><br>
     <blockquote>                                              Modified NM to report correct http address when an ephemeral web port is configured.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3549">MAPREDUCE-3549</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>write api documentation for web service apis for RM, NM, mapreduce app master, and job history server</b><br>
     <blockquote>                    new files added: A      hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebServicesIntro.apt.vm<br/>

A      hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/NodeManagerRest.apt.vm<br/>

A      hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/ResourceManagerRest.apt.vm<br/>

A      hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/MapredAppMasterRest.apt.vm<br/>

A      hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/HistoryServerRest.apt.vm<br/>

<br/>

The hadoop-project/src/site/site.xml is split into separate patch.
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3564">MAPREDUCE-3564</a>.
     Blocker bug reported by mahadev and fixed by sseth (mrv2)<br>
     <b>TestStagingCleanup and TestJobEndNotifier are failing on trunk.</b><br>
     <blockquote>                                              Fixed failures in TestStagingCleanup and TestJobEndNotifier tests.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3568">MAPREDUCE-3568</a>.
     Critical sub-task reported by vinodkv and fixed by vinodkv (mr-am, mrv2, performance)<br>
     <b>Optimize Job&apos;s progress calculations in MR AM</b><br>
     <blockquote>                                              Optimized Job&#39;s progress calculations in MR AM.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3586">MAPREDUCE-3586</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (mr-am, mrv2)<br>
     <b>Lots of AMs hanging around in PIG testing</b><br>
     <blockquote>                                              Modified CompositeService to avoid duplicate stop operations thereby solving race conditions in MR AM shutdown.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3597">MAPREDUCE-3597</a>.
     Major improvement reported by ravidotg and fixed by ravidotg (tools/rumen)<br>
     <b>Provide a way to access other info of history file from Rumentool</b><br>
     <blockquote>                                              Rumen now provides {{Parsed*}} objects. These objects provide extra information that are not provided by {{Logged*}} objects.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3618">MAPREDUCE-3618</a>.
     Major sub-task reported by sseth and fixed by sseth (mrv2, performance)<br>
     <b>TaskHeartbeatHandler holds a global lock for all task-updates</b><br>
     <blockquote>                                              Fixed TaskHeartbeatHandler to not hold a global lock for all task-updates.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3630">MAPREDUCE-3630</a>.
     Critical task reported by amolkekre and fixed by mahadev (mrv2)<br>
     <b>NullPointerException running teragen</b><br>
     <blockquote>                                              Committed to trunk and branch-0.23. Thanks Mahadev.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3639">MAPREDUCE-3639</a>.
     Blocker bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>TokenCache likely broken for FileSystems which don&apos;t issue delegation tokens</b><br>
     <blockquote>                                              Fixed TokenCache to work with absent FileSystem canonical service-names.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3641">MAPREDUCE-3641</a>.
     Blocker sub-task reported by acmurthy and fixed by acmurthy (mrv2, scheduler)<br>
     <b>CapacityScheduler should be more conservative assigning off-switch requests</b><br>
     <blockquote>                                              Making CapacityScheduler more conservative so as to assign only one off-switch container in a single scheduling iteration.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3656">MAPREDUCE-3656</a>.
     Blocker bug reported by karams and fixed by sseth (applicationmaster, mrv2, resourcemanager)<br>
     <b>Sort job on 350 scale is consistently failing with latest MRV2 code </b><br>
     <blockquote>                                              Fixed a race condition in MR AM which is failing the sort benchmark consistently.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3699">MAPREDUCE-3699</a>.
     Major bug reported by vinodkv and fixed by hitesh (mrv2)<br>
     <b>Default RPC handlers are very low for YARN servers</b><br>
     <blockquote>                                              Increased RPC handlers for all YARN servers to reasonable values for working at scale.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3703">MAPREDUCE-3703</a>.
     Critical bug reported by eepayne and fixed by eepayne (mrv2, resourcemanager)<br>
     <b>ResourceManager should provide node lists in JMX output</b><br>
     <blockquote>                    New JMX Bean in ResourceManager to provide list of live node managers:<br/>

<br/>

Hadoop:service=ResourceManager,name=RMNMInfo LiveNodeManagers
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3710">MAPREDUCE-3710</a>.
     Major bug reported by sseth and fixed by sseth (mrv1, mrv2)<br>
     <b>last split generated by FileInputFormat.getSplits may not have the best locality</b><br>
     <blockquote>                                              Improved FileInputFormat to return better locality for the last split.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3711">MAPREDUCE-3711</a>.
     Blocker sub-task reported by sseth and fixed by revans2 (mrv2)<br>
     <b>AppMaster recovery for Medium to large jobs take long time</b><br>
     <blockquote>                                              Fixed MR AM recovery so that only single selected task output is recovered and thus reduce the unnecessarily bloated recovery time.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3713">MAPREDUCE-3713</a>.
     Blocker bug reported by sseth and fixed by acmurthy (mrv2, resourcemanager)<br>
     <b>Incorrect headroom reported to jobs</b><br>
     <blockquote>                                              Fixed the way head-room is allocated to applications by CapacityScheduler so that it deducts current-usage per user and not per-application.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3714">MAPREDUCE-3714</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (mrv2, task)<br>
     <b>Reduce hangs in a corner case</b><br>
     <blockquote>                                              Fixed EventFetcher and Fetcher threads to shut-down properly so that reducers don&#39;t hang in corner cases.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3716">MAPREDUCE-3716</a>.
     Blocker bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>java.io.File.createTempFile fails in map/reduce tasks</b><br>
     <blockquote>                                              Fixing YARN+MR to allow MR jobs to be able to use java.io.File.createTempFile to create temporary files as part of their tasks.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3720">MAPREDUCE-3720</a>.
     Major bug reported by vinodkv and fixed by vinodkv (client, mrv2)<br>
     <b>Command line listJobs should not visit each AM</b><br>
     <blockquote>                    Changed bin/mapred job -list to not print job-specific information not available at RM.<br/>

<br/>

Very minor incompatibility in cmd-line output, inevitable due to MRv2 architecture.
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3732">MAPREDUCE-3732</a>.
     Blocker bug reported by acmurthy and fixed by acmurthy (mrv2, resourcemanager, scheduler)<br>
     <b>CS should only use &apos;activeUsers with pending requests&apos; for computing user-limits</b><br>
     <blockquote>                                              Modified CapacityScheduler to use only users with pending requests for computing user-limits.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3752">MAPREDUCE-3752</a>.
     Blocker bug reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>Headroom should be capped by queue max-cap</b><br>
     <blockquote>                                              Modified application limits to include queue max-capacities besides the usual user limits.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3754">MAPREDUCE-3754</a>.
     Major bug reported by vinodkv and fixed by vinodkv (mrv2, webapps)<br>
     <b>RM webapp should have pages filtered based on App-state</b><br>
     <blockquote>                                              Modified RM UI to filter applications based on state of the applications.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3760">MAPREDUCE-3760</a>.
     Major bug reported by rramya and fixed by vinodkv (mrv2)<br>
     <b>Blacklisted NMs should not appear in Active nodes list</b><br>
     <blockquote>                                              Changed active nodes list to not contain unhealthy nodes on the webUI and metrics.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3774">MAPREDUCE-3774</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>yarn-default.xml should be moved to hadoop-yarn-common.</b><br>
     <blockquote>      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3784">MAPREDUCE-3784</a>.
     Major bug reported by rramya and fixed by acmurthy (mrv2)<br>
     <b>maxActiveApplications(|PerUser) per queue is too low for small clusters</b><br>
     <blockquote>                                              Fixed CapacityScheduler so that maxActiveApplication and maxActiveApplicationsPerUser per queue are not too low for small clusters. 

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3804">MAPREDUCE-3804</a>.
     Major bug reported by davet and fixed by davet (jobhistoryserver, mrv2, resourcemanager)<br>
     <b>yarn webapp interface vulnerable to cross scripting attacks</b><br>
     <blockquote>                                              fix cross scripting attacks vulnerability through webapp interface.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3808">MAPREDUCE-3808</a>.
     Blocker bug reported by sseth and fixed by revans2 (mrv2)<br>
     <b>NPE in FileOutputCommitter when running a 0 reduce job</b><br>
     <blockquote>                                              Fixed an NPE in FileOutputCommitter for jobs with maps but no reduces.

      
</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3815">MAPREDUCE-3815</a>.
     Critical sub-task reported by sseth and fixed by sseth (mrv2)<br>
     <b>Data Locality suffers if the AM asks for containers using IPs instead of hostnames</b><br>
     <blockquote>                                              Fixed MR AM to always use hostnames and never IPs when requesting containers so that scheduler can give off data local containers correctly.

      
</blockquote></li>

</ul>

<h3>Other Jiras (describe bug fixes and minor changes)</h3>
<ul>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-4515">HADOOP-4515</a>.
     Minor improvement reported by abagri and fixed by sho.shimauchi <br>
     <b>conf.getBoolean must be case insensitive</b><br>
     <blockquote>Currently, if xx is set to &quot;TRUE&quot;, conf.getBoolean(&quot;xx&quot;, false) would return false. <br><br>conf.getBoolean should do an equalsIgnoreCase() instead of equals()<br><br>I am marking the change as incompatible because it does change semantics as pointed by Steve in HADOOP-4416</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6490">HADOOP-6490</a>.
     Minor bug reported by zshao and fixed by umamaheswararao (fs)<br>
     <b>Path.normalize should use StringUtils.replace in favor of String.replace</b><br>
     <blockquote>in our environment, we are seeing that the JobClient is going out of memory because Path.normalizePath(String) is called several tens of thousands of times, and each time it calls &quot;String.replace&quot; twice.<br><br>java.lang.String.replace compiles a regex to do the job which is very costly.<br>We should use org.apache.commons.lang.StringUtils.replace which is much faster and consumes almost no extra memory.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6614">HADOOP-6614</a>.
     Minor improvement reported by stevel@apache.org and fixed by jmhsieh (util)<br>
     <b>RunJar should provide more diags when it can&apos;t create a temp file</b><br>
     <blockquote>When you see a stack trace about permissions, it is better if the trace included the file/directory at fault:<br>{code}<br>Exception in thread &quot;main&quot; java.io.IOException: Permission denied<br>	at java.io.UnixFileSystem.createFileExclusively(Native Method)<br>	at java.io.File.checkAndCreate(File.java:1704)<br>	at java.io.File.createTempFile(File.java:1792)<br>	at org.apache.hadoop.util.RunJar.main(RunJar.java:147)<br>{code}<br><br>As it is, you need to go into the code, discover that it&apos;s {{${hadoop.tmp.dir}/hadoop-unja...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6840">HADOOP-6840</a>.
     Minor improvement reported by nspiegelberg and fixed by nspiegelberg (fs, io)<br>
     <b>Support non-recursive create() in FileSystem &amp; SequenceFile.Writer</b><br>
     <blockquote>The proposed solution for HBASE-2312 requires the sequence file to handle a non-recursive create.  This is already supported by HDFS, but needs to have an equivalent FileSystem &amp; SequenceFile.Writer API.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6886">HADOOP-6886</a>.
     Minor improvement reported by nspiegelberg and fixed by nspiegelberg (fs)<br>
     <b>LocalFileSystem Needs createNonRecursive API</b><br>
     <blockquote>While running sanity check tests for HBASE-2312, I noticed that HDFS-617 did not include createNonRecursive() support for the LocalFileSystem.  This is a problem for HBase, which allows the user to run over the LocalFS instead of HDFS for local cluster testing.  I think this only affects 0.20-append, but may affect the trunk based upon how exactly FileContext handles non-recursive creates.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7424">HADOOP-7424</a>.
     Major improvement reported by eli and fixed by umamaheswararao <br>
     <b>Log an error if the topology script doesn&apos;t handle multiple args</b><br>
     <blockquote>ScriptBasedMapping#resolve currently warns and returns null if it passes n arguments to the topology script and gets back a different number of resolutions. This indicates a bug in the topology script (or it&apos;s input) and therefore should be an error.<br><br>{code}<br>// invalid number of entries returned by the script<br>LOG.warn(&quot;Script &quot; + scriptName + &quot; returned &quot;<br>   + Integer.toString(m.size()) + &quot; values when &quot;<br>   + Integer.toString(names.size()) + &quot; were expected.&quot;);<br>return null;<br>{code}<br><br>There&apos;s on...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7470">HADOOP-7470</a>.
     Minor improvement reported by stevel@apache.org and fixed by enis (util)<br>
     <b>move up to Jackson 1.8.8</b><br>
     <blockquote>I see that hadoop-core still depends on Jackson 1.0.1 -but that project is now up to 1.8.2 in releases. Upgrading will make it easier for other Jackson-using apps that are more up to date to keep their classpath consistent.<br><br>The patch would be updating the ivy file to pull in the later version; no test</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7504">HADOOP-7504</a>.
     Trivial improvement reported by eli and fixed by qwertymaniac (metrics)<br>
     <b>hadoop-metrics.properties missing some Ganglia31 options </b><br>
     <blockquote>The &quot;jvm&quot;, &quot;rpc&quot;, and &quot;ugi&quot; sections of hadoop-metrics.properties should have Ganglia31 options like &quot;dfs&quot; and &quot;mapred&quot;</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7574">HADOOP-7574</a>.
     Trivial improvement reported by xiexianshan and fixed by xiexianshan (fs)<br>
     <b>Improvement for FSshell -stat</b><br>
     <blockquote>Add two optional formats for FSshell -stat, one is %G for group name of owner and the other is %U for user name.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7590">HADOOP-7590</a>.
     Major sub-task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Mavenize streaming and MR examples</b><br>
     <blockquote>MR1 code is still available in MR2 for testing contribs.<br><br>While this is a temporary until contribs tests are ported to MR2.<br><br>As a follow up the contrib projects themselves should be mavenized.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7657">HADOOP-7657</a>.
     Major improvement reported by mrbsd and fixed by decster <br>
     <b>Add support for LZ4 compression</b><br>
     <blockquote>According to several benchmark sites, LZ4 seems to overtake other fast compression algorithms, especially in the decompression speed area. The interface is also trivial to integrate (http://code.google.com/p/lz4/source/browse/trunk/lz4.h) and there is no license issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7736">HADOOP-7736</a>.
     Trivial improvement reported by qwertymaniac and fixed by qwertymaniac (fs)<br>
     <b>Remove duplicate call of Path#normalizePath during initialization.</b><br>
     <blockquote>Found during code reading on HADOOP-6490, there seems to be an unnecessary call of {{normalizePath(...)}} being made in the constructor {{Path(Path, Path)}}. Since {{initialize(...)}} normalizes its received path string already, its unnecessary to do it to the path parameter in the constructor&apos;s call of the same.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7758">HADOOP-7758</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (fs)<br>
     <b>Make GlobFilter class public</b><br>
     <blockquote>Currently the GlobFilter class is package private.<br><br>As a generic filter it is quite useful (and I&apos;ve found myself doing cut&amp;paste of it a few times)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7761">HADOOP-7761</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (io, performance, util)<br>
     <b>Improve performance of raw comparisons</b><br>
     <blockquote>Guava has a nice implementation of lexicographical byte-array comparison that uses sun.misc.Unsafe to compare unsigned byte arrays long-at-a-time. Their benchmarks show it as being 2x more CPU-efficient than the equivalent pure-Java implementation. We can easily integrate this into WritableComparator.compareBytes to improve CPU performance in the shuffle.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7777">HADOOP-7777</a>.
     Major improvement reported by stevel@apache.org and fixed by stevel@apache.org (util)<br>
     <b>Implement a base class for DNSToSwitchMapping implementations that can offer extra topology information</b><br>
     <blockquote>HDFS-2492 has identified a need for DNSToSwitchMapping implementations to provide a bit more topology information (e.g. whether or not there are multiple switches). This could be done by writing an extended interface, querying its methods if present and coming up with a default action if there is no extended interface. <br><br>Alternatively, we have a base class that all the standard mappings implement, with a boolean isMultiRack() method; all the standard subclasses would extend this, as could any...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7787">HADOOP-7787</a>.
     Major bug reported by bmahe and fixed by bmahe (build)<br>
     <b>Make source tarball use conventional name.</b><br>
     <blockquote>When building binary and source tarballs, I get the following artifacts:<br>Binary tarball: hadoop-0.23.0-SNAPSHOT.tar.gz <br>Source tarball: hadoop-dist-0.23.0-SNAPSHOT-src.tar.gz<br><br>Notice the &quot;-dist&quot; right between &quot;hadoop&quot; and the version in the source tarball name.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7801">HADOOP-7801</a>.
     Major bug reported by bmahe and fixed by bmahe (build)<br>
     <b>HADOOP_PREFIX cannot be overriden</b><br>
     <blockquote>hadoop-config.sh forces HADOOP_prefix to a specific value:<br>export HADOOP_PREFIX=`dirname &quot;$this&quot;`/..<br><br>It would be nice to make this overridable.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7804">HADOOP-7804</a>.
     Major improvement reported by arpitgupta and fixed by arpitgupta (conf)<br>
     <b>enable hadoop config generator to set dfs.block.local-path-access.user to enable short circuit read</b><br>
     <blockquote>we have a new config that allows to select which user can have access for short circuit read. We should make that configurable through the config generator scripts.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7808">HADOOP-7808</a>.
     Major new feature reported by daryn and fixed by daryn (fs, security)<br>
     <b>Port token service changes from 205</b><br>
     <blockquote>Need to merge the 205 token bug fixes and the feature to enable hostname-based tokens.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7810">HADOOP-7810</a>.
     Blocker bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>move hadoop archive to core from tools</b><br>
     <blockquote>&quot;The HadoopArchieves classes are included in the $HADOOP_HOME/hadoop_tools.jar, but this file is not found in `hadoop classpath`.<br><br>A Pig script using HCatalog&apos;s dynamic partitioning with HAR enabled will therefore fail if a jar with HAR is not included in the pig call&apos;s &apos;-cp&apos; and &apos;-Dpig.additional.jars&apos; arguments.&quot;<br><br>I am not aware of any reason to not include hadoop-tools.jar in &apos;hadoop classpath&apos;. Will attach a patch soon.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7811">HADOOP-7811</a>.
     Major bug reported by jeagles and fixed by jeagles (security, test)<br>
     <b>TestUserGroupInformation#testGetServerSideGroups test fails in chroot</b><br>
     <blockquote>It is common when running in chroot to have root&apos;s group vector preserved when running as your self.<br><br>For example<br><br># Enter chroot<br>$ sudo chroot /myroot<br><br># still root<br>$ whoami<br>root<br><br># switch to user preserving root&apos;s group vector<br>$ sudo -u user -P -s<br><br># root&apos;s groups<br>$ groups root<br>a b c<br><br># user&apos;s real groups<br>$ groups user<br>d e f<br><br># user&apos;s effective groups<br>$ groups<br>a b c d e f<br>-------------------------------<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7837">HADOOP-7837</a>.
     Major bug reported by stevel@apache.org and fixed by eli (conf)<br>
     <b>no NullAppender in the log4j config</b><br>
     <blockquote>running sbin/start-dfs.sh gives me a telling off about no null appender -should one be in the log4j config file.<br><br>Full trace (failure expected, but full output not as expected)<br>{code}<br>./start-dfs.sh <br>log4j:ERROR Could not find value for key log4j.appender.NullAppender<br>log4j:ERROR Could not instantiate appender named &quot;NullAppender&quot;.<br>Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured.<br>Starting namenodes on []<br>cat: /Users/slo/J...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7843">HADOOP-7843</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>compilation failing because workDir not initialized in RunJar.java</b><br>
     <blockquote>Compilation is failing on 0.23 and trunk because workDir is not initialized in RunJar.java</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7853">HADOOP-7853</a>.
     Blocker bug reported by daryn and fixed by daryn (security)<br>
     <b>multiple javax security configurations cause conflicts</b><br>
     <blockquote>Both UGI and the SPNEGO KerberosAuthenticator set the global javax security configuration.  SPNEGO stomps on UGI&apos;s security config which leads to kerberos/SASL authentication errors.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7854">HADOOP-7854</a>.
     Critical bug reported by daryn and fixed by daryn (security)<br>
     <b>UGI getCurrentUser is not synchronized</b><br>
     <blockquote>Sporadic {{ConcurrentModificationExceptions}} are originating from {{UGI.getCurrentUser}} when it needs to create a new instance.  The problem was specifically observed in a JT under heavy load when a post-job cleanup is accessing the UGI while a new job is being processed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7858">HADOOP-7858</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Drop some info logging to DEBUG level in IPC, metrics, and HTTP</b><br>
     <blockquote>Our info level logs have gotten noisier and noisier over time, which is annoying both for users and when looking at unit tests. I&apos;d like to drop a few of the less useful INFO level messages down to DEBUG.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7859">HADOOP-7859</a>.
     Major bug reported by eli and fixed by eli (fs)<br>
     <b>TestViewFsHdfs.testgetFileLinkStatus is failing an assert</b><br>
     <blockquote>Probably introduced by HADOOP-7783. I&apos;ll fix it.<br><br>{noformat}<br>java.lang.AssertionError<br>	at org.apache.hadoop.fs.FileContext.qualifySymlinkTarget(FileContext.java:1111)<br>	at org.apache.hadoop.fs.FileContext.access$000(FileContext.java:170)<br>	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1142)<br>	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1137)<br>	at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2327)<br>	at org.apache.hadoop.fs.FileContext.getF...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7864">HADOOP-7864</a>.
     Major bug reported by abayer and fixed by abayer (build)<br>
     <b>Building mvn site with Maven &lt; 3.0.2 causes OOM errors</b><br>
     <blockquote>If you try to run mvn site with Maven 3.0.0 (and possibly 3.0.1 - haven&apos;t actually tested that), you get hit with unavoidable OOM errors. Switching to Maven 3.0.2 or later fixes this. The enforcer should require 3.0.2 for builds.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7870">HADOOP-7870</a>.
     Major bug reported by jmhsieh and fixed by jmhsieh <br>
     <b>fix SequenceFile#createWriter with boolean createParent arg to respect createParent.</b><br>
     <blockquote>After HBASE-6840, one set of calls to createNonRecursive(...) seems fishy - the new boolean createParent variable from the signature isn&apos;t used at all.  <br><br>{code}<br>+  public static Writer<br>+    createWriter(FileSystem fs, Configuration conf, Path name,<br>+                 Class keyClass, Class valClass, int bufferSize,<br>+                 short replication, long blockSize, boolean createParent,<br>+                 CompressionType compressionType, CompressionCodec codec,<br>+                 Metadata meta...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7874">HADOOP-7874</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>native libs should be under lib/native/ dir</b><br>
     <blockquote>Currently common and hdfs SO files end up under lib/ dir with all JARs, they should end up under lib/native.<br><br>In addition, the hadoop-config.sh script needs some cleanup when comes to native lib handling:<br><br>* it is using lib/native/${JAVA_PLATFORM} for the java.library.path, when it should use lib/native.<br>* it is looking for build/lib/native, this is from the old ant build, not applicable anymore.<br>* it is looking for the libhdfs.a and adding to the java.librar.path, this is not correct.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7877">HADOOP-7877</a>.
     Major task reported by szetszwo and fixed by szetszwo (documentation)<br>
     <b>Federation: update Balancer documentation</b><br>
     <blockquote>Update Balancer documentation for the new balancing policy and CLI.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7878">HADOOP-7878</a>.
     Minor bug reported by stevel@apache.org and fixed by stevel@apache.org (util)<br>
     <b>Regression HADOOP-7777 switch changes break HDFS tests when the isSingleSwitch() predicate is used</b><br>
     <blockquote>This doesn&apos;t show up until you apply the HDFS-2492 patch, but the attempt to make the {{StaticMapping}} topology clever by deciding if it is single rack or multi rack based on its rack-&gt;node mapping breaks the HDFS {{TestBlocksWithNotEnoughRacks}} test. Why? Because the racks go in after the switch topology is cached by the {{BlockManager}}, which assumes the system is always single-switch.<br><br>Fix: default to assuming multi-switch; remove the intelligence, add a setter for anyone who really wan...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7887">HADOOP-7887</a>.
     Critical bug reported by tucu00 and fixed by tucu00 (security)<br>
     <b>KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration</b><br>
     <blockquote>While the KerberosAuthenticatorHandler defines the name rules property, it does not set it in KerberosName.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7890">HADOOP-7890</a>.
     Trivial improvement reported by knoguchi and fixed by knoguchi (scripts)<br>
     <b>Redirect hadoop script&apos;s deprecation message to stderr</b><br>
     <blockquote>$ hadoop dfs -ls<br>DEPRECATED: Use of this script to execute hdfs command is deprecated.<br>Instead use the hdfs command for it.<br>...<br><br>If we&apos;re still letting the command run, I think we should redirect the deprecation message to stderr in case users have a script taking the output from stdout.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7898">HADOOP-7898</a>.
     Minor bug reported by sureshms and fixed by sureshms (security)<br>
     <b>Fix javadoc warnings in AuthenticationToken.java</b><br>
     <blockquote>Fix the following javadoc warning:<br>[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest<br>[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7902">HADOOP-7902</a>.
     Major bug reported by szetszwo and fixed by tucu00 <br>
     <b>skipping name rules setting (if already set) should be done on UGI initialization only </b><br>
     <blockquote>Both TestDelegationToken and TestOfflineEditsViewer are currently failing.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7907">HADOOP-7907</a>.
     Blocker bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>hadoop-tools JARs are not part of the distro</b><br>
     <blockquote>After mavenizing streaming, the hadoop-streaming JAR is not part of the final tar.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7910">HADOOP-7910</a>.
     Minor improvement reported by sho.shimauchi and fixed by sho.shimauchi (conf)<br>
     <b>add configuration methods to handle human readable size values</b><br>
     <blockquote>It&apos;s better to have a new configuration methods which handle human readable size values.<br>For example, see HDFS-1314.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7912">HADOOP-7912</a>.
     Major bug reported by revans2 and fixed by revans2 (build)<br>
     <b>test-patch should run eclipse:eclipse to verify that it does not break again</b><br>
     <blockquote>Recently the eclipse:eclipse build was broken.  If we are going to document this on the wiki and have many developers use it we should verify that it always works.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7914">HADOOP-7914</a>.
     Major bug reported by szetszwo and fixed by szetszwo (build)<br>
     <b>duplicate declaration of hadoop-hdfs test-jar</b><br>
     <blockquote>[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-common-project:pom:0.24.0-SNAPSHOT<br>[WARNING] &apos;dependencyManagement.dependencies.dependency.(groupId:artifactId:type:classifier)&apos; must be unique: org.apache.hadoop:hadoop-hdfs:test-jar -&gt; duplicate declaration of version ${project.version} @ org.apache.hadoop:hadoop-project:0.24.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-project/pom.xml, line 140, ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7917">HADOOP-7917</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>compilation of protobuf files fails in windows/cygwin</b><br>
     <blockquote>HADOOP-7899 &amp; HDFS-2511 introduced compilation of proto files as part of the build.<br><br>Such compilation is failing in windows/cygwin</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7919">HADOOP-7919</a>.
     Trivial improvement reported by qwertymaniac and fixed by qwertymaniac (documentation)<br>
     <b>[Doc] Remove hadoop.logfile.* properties.</b><br>
     <blockquote>The following only resides in core-default.xml and doesn&apos;t look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all.<br><br>These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.<br><br>{code}<br>137 &lt;!--- logging properties --&gt;<br>138 <br>139 &lt;property&gt;<br>140   &lt;name&gt;hadoop.logfile.size&lt;/name&gt;<br>141   &lt;value&gt;10000000&lt;/value&gt;<br>142   &lt;description&gt;The max size of each log file&lt;/description&gt;<br>...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7933">HADOOP-7933</a>.
     Critical bug reported by sseth and fixed by sseth <br>
     <b>Viewfs changes for MAPREDUCE-3529</b><br>
     <blockquote>ViewFs.getDelegationTokens returns a list of tokens for the associated namenodes. Credentials serializes these tokens using the service name for the actual namenodes. Effectively, tokens are not cached for viewfs (some more details in MR 3529). Affects any job which uses the TokenCache in tasks along with viewfs (some Pig jobs).<br><br>Talk to Jitendra about this, some options<br>1. Change Credentials.getAllTokens to return the key, instead of just a token list (associate the viewfs canonical name wit...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7934">HADOOP-7934</a>.
     Critical improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Normalize dependencies versions across all modules</b><br>
     <blockquote>Move all dependencies versions to the dependencyManagement section in the hadoop-project POM<br><br>Move all plugin versions to the dependencyManagement section in the hadoop-project POM</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7936">HADOOP-7936</a>.
     Major bug reported by eli and fixed by tucu00 (build)<br>
     <b>There&apos;s a Hoop README in the root dir of the tarball</b><br>
     <blockquote>The Hoop README.txt is now in the root dir of the tarball.<br><br>{noformat}<br>hadoop-trunk1 $ tar xvzf hadoop-dist/target/hadoop-0.24.0-SNAPSHOT.tar.gz  -C /tmp/<br>..<br>hadoop-trunk1 $ head -n3 /tmp/hadoop-0.24.0-SNAPSHOT/README.txt <br>-----------------------------------------------------------------------------<br>HttpFS - Hadoop HDFS over HTTP<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7939">HADOOP-7939</a>.
     Major improvement reported by rvs and fixed by rvs (build, conf, documentation, scripts)<br>
     <b>Improve Hadoop subcomponent integration in Hadoop 0.23</b><br>
     <blockquote>h1. Introduction<br><br>For the rest of this proposal it is assumed that the current set<br>of Hadoop subcomponents is:<br> * hadoop-common<br> * hadoop-hdfs<br> * hadoop-yarn<br> * hadoop-mapreduce<br><br>It must be noted that this is an open ended list, though. For example,<br>implementations of additional frameworks on top of yarn (e.g. MPI) would<br>also be considered a subcomponent.<br><br>h1. Problem statement<br><br>Currently there&apos;s an unfortunate coupling and hard-coding present at the<br>level of launcher scripts, configuration s...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7948">HADOOP-7948</a>.
     Minor bug reported by cim_michajlomatijkiw and fixed by cim_michajlomatijkiw (build)<br>
     <b>Shell scripts created by hadoop-dist/pom.xml to build tar do not properly propagate failure</b><br>
     <blockquote>The run() function, as defined in dist-layout-stitching.sh and dist-tar-stitching, created in hadoop-dist/pom.xml, does not properly propagate the error code of a failing command.  See the following:<br>{code}<br>    ...<br>    &quot;${@}&quot;                 # call fails with non-zero exit code<br>    if [ $? != 0 ]; then   <br>        echo               <br>        echo &quot;Failed!&quot;     <br>        echo               <br>        exit $?            # $?=result of echo above, likely 0, thus exit with code 0<br>    ...<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7949">HADOOP-7949</a>.
     Trivial bug reported by eli and fixed by eli (ipc)<br>
     <b>Updated maxIdleTime default in the code to match core-default.xml</b><br>
     <blockquote>HADOOP-2909 intended to set the server max idle time for a connection to twice the client value. (&quot;The server-side max idle time should be greater than the client-side max idle time, for example, twice of the client-side max idle time.&quot;) This way when a server times out a connection it&apos;s due a crashed client and not an inactive client so we don&apos;t close client connections with outstanding requests (by setting 2x the client value on the server side the client should time out the connection firs...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7964">HADOOP-7964</a>.
     Blocker bug reported by kihwal and fixed by daryn (security, util)<br>
     <b>Deadlock in class init.</b><br>
     <blockquote>After HADOOP-7808, client-side commands hang occasionally. There are cyclic dependencies in NetUtils and SecurityUtil class initialization. Upon initial look at the stack trace, two threads deadlock when they hit the either of class init the same time.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7971">HADOOP-7971</a>.
     Blocker bug reported by tgraves and fixed by prashant_ <br>
     <b>hadoop &lt;job/queue/pipes&gt; removed - should be added back, but deprecated</b><br>
     <blockquote>The mapred subcommands (mradmin|jobtracker|tasktracker|pipes|job|queue)<br> were removed from the /bin/hadoop command. I believe for backwards compatibility at least some of these should have stayed along with the deprecated warnings.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7974">HADOOP-7974</a>.
     Major bug reported by eli and fixed by qwertymaniac (fs)<br>
     <b>TestViewFsTrash incorrectly determines the user&apos;s home directory</b><br>
     <blockquote>HADOOP-7284 added a test called TestViewFsTrash which contains the following code to determine the user&apos;s home directory. It only works if the user&apos;s directory is one level deep, and breaks if the home directory is more than one level deep (eg user hudson, who&apos;s home dir might be /usr/lib/hudson instead of /home/hudson).<br><br>{code}<br>    // create a link for home directory so that trash path works<br>    // set up viewfs&apos;s home dir root to point to home dir root on target<br>    // But home dir is diffe...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7975">HADOOP-7975</a>.
     Minor bug reported by qwertymaniac and fixed by qwertymaniac <br>
     <b>Add entry to XML defaults for new LZ4 codec</b><br>
     <blockquote>HADOOP-7657 added in a new LZ4 codec, but failed to extend the io.compression.codecs list which MR/etc. use up to load codecs.<br><br>We should add an entry to the core-default XML for this new codec, just as we did with Snappy.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7981">HADOOP-7981</a>.
     Major bug reported by jeagles and fixed by jeagles (io)<br>
     <b>Improve documentation for org.apache.hadoop.io.compress.Decompressor.getRemaining</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7982">HADOOP-7982</a>.
     Major bug reported by tlipcon and fixed by tlipcon (security)<br>
     <b>UserGroupInformation fails to login if thread&apos;s context classloader can&apos;t load HadoopLoginModule</b><br>
     <blockquote>In a few hard-to-reproduce situations, we&apos;ve seen a problem where the UGI login call causes a failure to login exception with the following cause:<br><br>Caused by: javax.security.auth.login.LoginException: unable to find <br>LoginModule class: org.apache.hadoop.security.UserGroupInformation <br>$HadoopLoginModule<br><br>After a bunch of debugging, I determined that this happens when the login occurs in a thread whose Context ClassLoader has been set to null.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7987">HADOOP-7987</a>.
     Major improvement reported by devaraj and fixed by jnp (security)<br>
     <b>Support setting the run-as user in unsecure mode</b><br>
     <blockquote>Some applications need to be able to perform actions (such as launch MR jobs) from map or reduce tasks. In earlier unsecure versions of hadoop (20.x), it was possible to do this by setting user.name in the configuration. But in 20.205 and 1.0, when running in unsecure mode, this does not work. (In secure mode, you can do this using the kerberos credentials).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7988">HADOOP-7988</a>.
     Major bug reported by jnp and fixed by jnp <br>
     <b>Upper case in hostname part of the principals doesn&apos;t work with kerberos.</b><br>
     <blockquote>Kerberos doesn&apos;t like upper case in the hostname part of the principals.<br>This issue has been seen in 23 as well as 1.0.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7993">HADOOP-7993</a>.
     Major bug reported by anupamseth and fixed by anupamseth (conf)<br>
     <b>Hadoop ignores old-style config options for enabling compressed output</b><br>
     <blockquote>Hadoop seems to ignore the config options even though they are printed as deprecation warnings in the log: mapred.output.compress and<br>mapred.output.compression.codec<br><br>- settings that work on 0.20 but not on 0.23<br>mapred.output.compress=true<br>mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec<br><br>- settings that work on 0.23<br>mapreduce.output.fileoutputformat.compress=true<br>mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec<br><br>This breaks bac...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7997">HADOOP-7997</a>.
     Major bug reported by gchanan and fixed by gchanan (io)<br>
     <b>SequenceFile.createWriter(...createParent...) no longer works on existing file</b><br>
     <blockquote>SequenceFile.createWriter no longer works on an existing file, because old version specified OVEWRITE by default and new version does not.  This breaks some HBase tests.<br><br>Tested against trunk.<br><br>Patch with test to follow.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7998">HADOOP-7998</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>CheckFileSystem does not correctly honor setVerifyChecksum</b><br>
     <blockquote>Regardless of the verify checksum flag, {{ChecksumFileSystem#open}} will instantiate a {{ChecksumFSInputChecker}} instead of a normal stream.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7999">HADOOP-7999</a>.
     Critical bug reported by jlowe and fixed by jlowe (scripts)<br>
     <b>&quot;hadoop archive&quot; fails with ClassNotFoundException</b><br>
     <blockquote>Running &quot;hadoop archive&quot; from a command prompt results in this error:<br><br>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/tools/HadoopArchives<br>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tools.HadoopArchives<br>	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)<br>	at java.security.AccessController.doPrivileged(Native Method)<br>	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br>	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)<br>	...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8000">HADOOP-8000</a>.
     Critical bug reported by arpitgupta and fixed by arpitgupta <br>
     <b>fetchdt command not available in bin/hadoop</b><br>
     <blockquote>fetchdt command needs to be added to bin/hadoop to allow for backwards compatibility.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8001">HADOOP-8001</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>ChecksumFileSystem&apos;s rename doesn&apos;t correctly handle checksum files</b><br>
     <blockquote>Rename will move the src file and its crc *if present* to the destination.  If the src file has no crc, but the destination already exists with a crc, then src will be associated with the old file&apos;s crc.  Subsequent access to the file will fail with checksum errors.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8002">HADOOP-8002</a>.
     Major bug reported by arpitgupta and fixed by arpitgupta <br>
     <b>SecurityUtil acquired token message should be a debug rather than info</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8006">HADOOP-8006</a>.
     Major bug reported by umamaheswararao and fixed by daryn (fs)<br>
     <b>TestFSInputChecker is failing in trunk.</b><br>
     <blockquote>Trunk build number 939 failed with TestFSInputChecker.<br>https://builds.apache.org/job/Hadoop-Hdfs-trunk/939/<br><br>junit.framework.AssertionFailedError: expected:&lt;10&gt; but was:&lt;0&gt;<br>	at junit.framework.Assert.fail(Assert.java:47)<br>	at junit.framework.Assert.failNotEquals(Assert.java:283)<br>	at junit.framework.Assert.assertEquals(Assert.java:64)<br>	at junit.framework.Assert.assertEquals(Assert.java:130)<br>	at junit.framework.Assert.assertEquals(Assert.java:136)<br>	at org.apache.hadoop.hdfs.TestFSInputChecker.ch...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8009">HADOOP-8009</a>.
     Critical improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Create hadoop-client and hadoop-minicluster artifacts for downstream projects </b><br>
     <blockquote>Using Hadoop from projects like Pig/Hive/Sqoop/Flume/Oozie or any in-house system that interacts with Hadoop is quite challenging for the following reasons:<br><br>* *Different versions of Hadoop produce different artifacts:* Before Hadoop 0.23 there was a single artifact hadoop-core, starting with Hadoop 0.23 there are several (common, hdfs, mapred*, yarn*)<br><br>* *There are no &apos;client&apos; artifacts:* Current artifacts include all JARs needed to run the services, thus bringing into clients several JARs t...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8012">HADOOP-8012</a>.
     Minor bug reported by rvs and fixed by rvs (scripts)<br>
     <b>hadoop-daemon.sh and yarn-daemon.sh are trying to mkdir and chow log/pid dirs which can fail</b><br>
     <blockquote>Here&apos;s what I see when using Hadoop in Bigtop:<br><br>{noformat}<br>$ sudo /sbin/service hadoop-hdfs-namenode start<br>Starting Hadoop namenode daemon (hadoop-namenode): chown: changing ownership of `/var/log/hadoop&apos;: Operation not permitted<br>starting namenode, logging to /var/log/hadoop/hadoop-hdfs-namenode-centos5.out<br>{noformat}<br><br>This is a cosmetic issue, but it would be nice to fix it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8013">HADOOP-8013</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>ViewFileSystem does not honor setVerifyChecksum</b><br>
     <blockquote>{{ViewFileSystem#setVerifyChecksum}} is a no-op.  It should call {{setVerifyChecksum}} on the mount points.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8015">HADOOP-8015</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>ChRootFileSystem should extend FilterFileSystem</b><br>
     <blockquote>{{ChRootFileSystem}} simply extends {{FileSystem}}, and attempts to delegate some methods to the underlying mount point.  It is essentially the same as {{FilterFileSystem}} but it mangles the paths to include the chroot path.  Unfortunately {{ChRootFileSystem}} is not delegating some methods that should be delegated.  Changing the inheritance will prevent a copy-n-paste of code for HADOOP-8013 and HADOOP-8014 into both {{ChRootFileSystem}} and {{FilterFileSystem}}.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8018">HADOOP-8018</a>.
     Major bug reported by mattf and fixed by jeagles (build, test)<br>
     <b>Hudson auto test for HDFS has started throwing javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list</b><br>
     <blockquote>Hudson automated testing has started failing with one javadoc warning message, consisting of<br>javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list<br><br>This may be due to Oracle&apos;s decommissioning of the sun.com domain.  If one tries to access it manually, it is redirected to <br>http://download.oracle.com/javase/6/docs/api/package-list<br><br>So it looks like a build script needs to be updated.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-8027">HADOOP-8027</a>.
     Minor improvement reported by qwertymaniac and fixed by atm (metrics)<br>
     <b>Visiting /jmx on the daemon web interfaces may print unnecessary error in logs</b><br>
     <blockquote>Logs that follow a {{/jmx}} servlet visit:<br><br>{code}<br>11/11/22 12:09:52 ERROR jmx.JMXJsonServlet: getting attribute UsageThreshold of java.lang:type=MemoryPool,name=Par Eden Space threw an exception<br>javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported<br>	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:856)<br>...<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-69">HDFS-69</a>.
     Minor bug reported by raviphulari and fixed by qwertymaniac <br>
     <b>Improve dfsadmin command line help </b><br>
     <blockquote>Enhance dfsadmin command line help informing &quot;A quota of one forces a directory to remain empty&quot; </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-362">HDFS-362</a>.
     Major improvement reported by szetszwo and fixed by umamaheswararao (name-node)<br>
     <b>FSEditLog should not writes long and short as UTF8 and should not use ArrayWritable for writing non-array items</b><br>
     <blockquote>In FSEditLog, <br><br>- long and short are first converted to String and are further converted to UTF8<br><br>- For some non-array items, it first create an ArrayWritable object to hold all the items and then writes the ArrayWritable object.<br><br>These result creating many intermediate objects which affects Namenode CPU performance and Namenode restart.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-442">HDFS-442</a>.
     Minor bug reported by rramya and fixed by qwertymaniac (test)<br>
     <b>dfsthroughput in test.jar throws NPE</b><br>
     <blockquote>On running hadoop jar hadoop-test.jar dfsthroughput OR hadoop org.apache.hadoop.hdfs.BenchmarkThroughput, we get NullPointerException. Below is the stacktrace:<br>{noformat}<br>Exception in thread &quot;main&quot; java.lang.NullPointerException<br>        at java.util.Hashtable.put(Hashtable.java:394)<br>        at java.util.Properties.setProperty(Properties.java:143)<br>        at java.lang.System.setProperty(System.java:731)<br>        at org.apache.hadoop.hdfs.BenchmarkThroughput.run(BenchmarkThroughput.java:198)<br>   ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-554">HDFS-554</a>.
     Minor improvement reported by stevel@apache.org and fixed by qwertymaniac (name-node)<br>
     <b>BlockInfo.ensureCapacity may get a speedup from System.arraycopy()</b><br>
     <blockquote>BlockInfo.ensureCapacity() uses a for() loop to copy the old array data into the expanded array.  {{System.arraycopy()}} is generally much faster for this, as it can do a bulk memory copy. There is also the typesafe Java6 {{Arrays.copyOf()}} to consider, though here it offers no tangible benefit.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2178">HDFS-2178</a>.
     Major improvement reported by tucu00 and fixed by tucu00 <br>
     <b>HttpFS - a read/write Hadoop file system proxy</b><br>
     <blockquote>We&apos;d like to contribute Hoop to Hadoop HDFS as a replacement (an improvement) for HDFS Proxy.<br><br>Hoop provides access to all Hadoop Distributed File System (HDFS) operations (read and write) over HTTP/S.<br><br>The Hoop server component is a REST HTTP gateway to HDFS supporting all file system operations. It can be accessed using standard HTTP tools (i.e. curl and wget), HTTP libraries from different programing languages (i.e. Perl, Java Script) as well as using the Hoop client. The Hoop server compo...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2335">HDFS-2335</a>.
     Major improvement reported by eli and fixed by umamaheswararao (data-node, name-node)<br>
     <b>DataNodeCluster and NNStorage always pull fresh entropy</b><br>
     <blockquote>Jira for giving DataNodeCluster and NNStorage the same treatment as HDFS-1835. They&apos;re not truly cryptographic uses as well. We should also factor this out to a utility method, seems like the three uses are slightly different, eg one uses DFSUtil.getRandom and the other creates a new Random object.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2349">HDFS-2349</a>.
     Trivial improvement reported by qwertymaniac and fixed by qwertymaniac (data-node)<br>
     <b>DN should log a WARN, not an INFO when it detects a corruption during block transfer</b><br>
     <blockquote>Currently, in DataNode.java, we have:<br><br>{code}<br><br>      LOG.info(&quot;Can&apos;t replicate block &quot; + block<br>          + &quot; because on-disk length &quot; + onDiskLength <br>          + &quot; is shorter than NameNode recorded length &quot; + block.getNumBytes());<br><br>{code}<br><br>This log is better off as a WARN as it indicates (and also reports) a corruption.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2397">HDFS-2397</a>.
     Major improvement reported by tlipcon and fixed by eli (name-node)<br>
     <b>Undeprecate SecondaryNameNode</b><br>
     <blockquote>I would like to consider un-deprecating the SecondaryNameNode for 0.23, and amending the documentation to indicate that it is still the most trust-worthy way to run checkpoints, and while CN/BN may have some advantages, they&apos;re not battle hardened as of yet. The test coverage for the 2NN is far superior to the CheckpointNode or BackupNode, and people have a lot more production experience. Indicating that it is deprecated before we have expanded test coverage of the CN/BN won&apos;t send the right ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2454">HDFS-2454</a>.
     Minor improvement reported by umamaheswararao and fixed by qwertymaniac (data-node)<br>
     <b>Move maxXceiverCount check to before starting the thread in dataXceiver</b><br>
     <blockquote>We can hoist the maxXceiverCount out of DataXceiverServer#run, there&apos;s no need to check each time we accept a connection, we can accept when we create a thread.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2502">HDFS-2502</a>.
     Minor improvement reported by eli and fixed by qwertymaniac (documentation)<br>
     <b>hdfs-default.xml should include dfs.name.dir.restore</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2511">HDFS-2511</a>.
     Minor improvement reported by tlipcon and fixed by tucu00 (build)<br>
     <b>Add dev script to generate HDFS protobufs</b><br>
     <blockquote>Would like to add a simple shell script to re-generate the protobuf code in HDFS -- just easier than remembering the right syntax.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2533">HDFS-2533</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (data-node, performance)<br>
     <b>Remove needless synchronization on FSDataSet.getBlockFile</b><br>
     <blockquote>HDFS-1148 discusses lock contention issues in FSDataset. It provides a more comprehensive fix, converting it all to RWLocks, etc. This JIRA is for one very specific fix which gives a decent performance improvement for TestParallelRead: getBlockFile() currently holds the lock which is completely unnecessary.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2536">HDFS-2536</a>.
     Trivial improvement reported by atm and fixed by qwertymaniac (name-node)<br>
     <b>Remove unused imports</b><br>
     <blockquote>Looks like it has 11 unused imports by my count.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2541">HDFS-2541</a>.
     Major bug reported by qwertymaniac and fixed by qwertymaniac (data-node)<br>
     <b>For a sufficiently large value of blocks, the DN Scanner may request a random number with a negative seed value.</b><br>
     <blockquote>Running off 0.20-security, I noticed that one could get the following exception when scanners are used:<br><br>{code}<br>DataXceiver <br>java.lang.IllegalArgumentException: n must be positive <br>at java.util.Random.nextInt(Random.java:250) <br>at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.getNewBlockScanTime(DataBlockScanner.java:251) <br>at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.addBlock(DataBlockScanner.java:268) <br>at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(Da...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2543">HDFS-2543</a>.
     Major bug reported by bmahe and fixed by bmahe (scripts)<br>
     <b>HADOOP_PREFIX cannot be overriden</b><br>
     <blockquote>hadoop-config.sh forces HADOOP_prefix to a specific value:<br>export HADOOP_PREFIX=`dirname &quot;$this&quot;`/..<br><br>It would be nice to make this overridable.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2544">HDFS-2544</a>.
     Major bug reported by bmahe and fixed by bmahe (scripts)<br>
     <b>Hadoop scripts unconditionally source &quot;$bin&quot;/../libexec/hadoop-config.sh.</b><br>
     <blockquote>It would be nice to be able to specify some other location for hadoop-config.sh</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2545">HDFS-2545</a>.
     Major bug reported by szetszwo and fixed by szetszwo <br>
     <b>Webhdfs: Support multiple namenodes in federation</b><br>
     <blockquote>DatanodeWebHdfsMethods only talks to the default namenode.  It won&apos;t work if there are multiple namenodes in federation.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2552">HDFS-2552</a>.
     Major task reported by szetszwo and fixed by szetszwo (documentation)<br>
     <b>Add WebHdfs Forrest doc</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2553">HDFS-2553</a>.
     Critical bug reported by tlipcon and fixed by umamaheswararao (data-node)<br>
     <b>BlockPoolSliceScanner spinning in loop</b><br>
     <blockquote>Playing with trunk, I managed to get a DataNode in a situation where the BlockPoolSliceScanner is spinning in the following loop, using 100% CPU:<br>        at org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.isAlive(DataNode.java:820)<br>        at org.apache.hadoop.hdfs.server.datanode.DataNode.isBPServiceAlive(DataNode.java:2962)<br>        at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner.scan(BlockPoolSliceScanner.java:625)<br>        at org.apache.hadoop.hdfs.server.data...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2560">HDFS-2560</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Refactor BPOfferService to be a static inner class</b><br>
     <blockquote>Currently BPOfferService is a non-static inner class of DataNode. For HA we are adding another inner class inside of this, which makes the scope very hard to understand when reading the code (and has resulted in subtle bugs like HDFS-2529 where a variable is referenced from the wrong scope. Making it a static inner class with a reference to the DN has two advantages: a) scope is now explicit, and b) enables unit testing of the BPOS against a mocked-out DN.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2562">HDFS-2562</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Refactor DN configuration variables out of DataNode class</b><br>
     <blockquote>Right now there are many member variables in DataNode.java which are just read from configuration when the DN is started. Similar to what we did with DFSClient, we should refactor them into a new DNConf class which can be passed around - the motivation is to remove the many references we have throughout the code that read package-protected members of DataNode and reduce the number of members in DataNode itself.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2563">HDFS-2563</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Some cleanup in BPOfferService</b><br>
     <blockquote>BPOfferService is currently rather difficult to follow and not really commented. This JIRA is to clean up the code a bit, add javadocs/comments where necessary, and improve the formatting of the log messages.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2566">HDFS-2566</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Move BPOfferService to be a non-inner class</b><br>
     <blockquote>Rounding out the cleanup of BPOfferService, it would be good to move it to its own file, so it&apos;s no longer an inner class. DataNode.java is really large and hard to navigate. BPOfferService itself is ~700 lines, so seems like a large enough unit to merit its own file.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2567">HDFS-2567</a>.
     Major bug reported by qwertymaniac and fixed by qwertymaniac (name-node)<br>
     <b>When 0 DNs are available, show a proper error when trying to browse DFS via web UI</b><br>
     <blockquote>Trace:<br><br>{code}<br>HTTP ERROR 500<br><br>Problem accessing /nn_browsedfscontent.jsp. Reason:<br><br>    n must be positive<br>Caused by:<br><br>java.lang.IllegalArgumentException: n must be positive<br>	at java.util.Random.nextInt(Random.java:250)<br>	at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:556)<br>	at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:524)<br>	at org.apache.hadoop.hdfs.server.namenode.NamenodeJspHelper.getRandomDatanode(NamenodeJspHelper.java:372)<br>	at org....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2568">HDFS-2568</a>.
     Trivial improvement reported by qwertymaniac and fixed by qwertymaniac (data-node)<br>
     <b>Use a set to manage child sockets in XceiverServer</b><br>
     <blockquote>Found while reading up for HDFS-2454, currently we maintain childSockets in a DataXceiverServer as a Map&lt;Socket,Socket&gt;. This can very well be a Set&lt;Socket&gt; data structure -- since the goal is easy removals.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2570">HDFS-2570</a>.
     Trivial improvement reported by eli and fixed by eli (documentation)<br>
     <b>Add descriptions for dfs.*.https.address in hdfs-default.xml</b><br>
     <blockquote>Let&apos;s add descriptions for dfs.*.https.address in hdfs-default.xml.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2572">HDFS-2572</a>.
     Trivial improvement reported by qwertymaniac and fixed by qwertymaniac (data-node)<br>
     <b>Unnecessary double-check in DN#getHostName</b><br>
     <blockquote>We do a double config.get unnecessarily inside DN#getHostName(...). Can be removed by this patch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2574">HDFS-2574</a>.
     Trivial task reported by joecrobak and fixed by joecrobak (documentation)<br>
     <b>remove references to deprecated properties in hdfs-site.xml template and hdfs-default.xml</b><br>
     <blockquote>Some examples: hadoop-hdfs/src/main/packages/templates/conf/hdfs-site.xml contains an entry for dfs.name.dir rather than dfs.namenode.name.dir and hdfs-default.xml references dfs.name.dir twice in &lt;description&gt; tags rather than using dfs.namenode.name.dir.<br><br>List of deprecated properties is here: http://hadoop.apache.org/common/docs/r0.23.0/hadoop-project-dist/hadoop-common/DeprecatedProperties.html</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2575">HDFS-2575</a>.
     Minor bug reported by tlipcon and fixed by tlipcon (test)<br>
     <b>DFSTestUtil may create empty files</b><br>
     <blockquote>DFSTestUtil creates files with random sizes, but there is no minimum size. So, sometimes, it can make a file with length 0. This will cause tests that use this functionality to fail - for example, TestListCorruptFileBlocks assumes that each of the created files has at least one block. We should add a minSize parameter to prevent this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2587">HDFS-2587</a>.
     Major task reported by szetszwo and fixed by szetszwo (documentation)<br>
     <b>Add WebHDFS apt doc</b><br>
     <blockquote>This issue is to add a WebHDFS doc in apt format in additional to the forrest doc (HDFS-2552).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2588">HDFS-2588</a>.
     Trivial bug reported by davevr and fixed by davevr (scripts)<br>
     <b>hdfs jsp pages missing DOCTYPE [post-split branches]</b><br>
     <blockquote>Some jsp pages in the UI are missing a DOCTYPE declaration. This causes the pages to render incorrectly on some browsers, such as IE9.  Please see parent bug HADOOP-7827 for details and patch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2590">HDFS-2590</a>.
     Major bug reported by szetszwo and fixed by szetszwo (documentation)<br>
     <b>Some links in WebHDFS forrest doc do not work</b><br>
     <blockquote>Some links are pointing to DistributedFileSystem javadoc but the javadoc of DistributedFileSystem is not generated by default.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2594">HDFS-2594</a>.
     Critical bug reported by tucu00 and fixed by szetszwo <br>
     <b>webhdfs HTTP API should implement getDelegationTokens() instead getDelegationToken()</b><br>
     <blockquote>The current API returns a single delegation token, that method from the FileSystem API is deprecated in favor of the one that returns a list of tokens. The HTTP API should implement the new/undeprecated signature getDelegationTokens().</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2596">HDFS-2596</a>.
     Major bug reported by eli and fixed by eli (data-node, test)<br>
     <b>TestDirectoryScanner doesn&apos;t test parallel scans</b><br>
     <blockquote>The code from HDFS-854 below doesn&apos;t run the test with parallel scanning. They probably intended &quot;parallelism &lt; 3&quot;.<br><br>{code}<br>+  public void testDirectoryScanner() throws Exception {<br>+    // Run the test with and without parallel scanning<br>+    for (int parallelism = 1; parallelism &lt; 2; parallelism++) {<br>+      runTest(parallelism);<br>+    }<br>+  }<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2604">HDFS-2604</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (data-node, documentation, name-node)<br>
     <b>Add a log message to show if WebHDFS is enabled</b><br>
     <blockquote>WebHDFS can be enabled/disabled by the conf key {{dfs.webhdfs.enabled}}.  Let&apos;s add a log message to show if it is enabled.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2606">HDFS-2606</a>.
     Critical bug reported by tucu00 and fixed by tucu00 (hdfs client)<br>
     <b>webhdfs client filesystem impl must set the content-type header for create/append</b><br>
     <blockquote>Currently the content-type header is not being set and for some reason for append it is being set to the form encoded content type making jersey parameter parsing fail.<br><br>For this and to avoid any kind of proxy transcoding the content-type should be set to binary.<br><br>{code}<br>  conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/octet-stream&quot;);<br>{code}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2614">HDFS-2614</a>.
     Major bug reported by bmahe and fixed by tucu00 (build)<br>
     <b>hadoop dist tarball is missing hdfs headers</b><br>
     <blockquote>It would be nice to provide hdfs header so one could easily write programs to be linked against that library and access HDFS</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2640">HDFS-2640</a>.
     Major bug reported by tomwhite and fixed by tomwhite <br>
     <b>Javadoc generation hangs</b><br>
     <blockquote>Typing &apos;mvn javadoc:javadoc&apos; causes the build to hang.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2646">HDFS-2646</a>.
     Major bug reported by umamaheswararao and fixed by tucu00 <br>
     <b>Hadoop HttpFS introduced 4 findbug warnings.</b><br>
     <blockquote>https://builds.apache.org/job/PreCommit-HDFS-Build/1665//artifact/trunk/hadoop-hdfs-project/patchprocess/newPatchFindbugsWarningshadoop-hdfs-httpfs.html</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2649">HDFS-2649</a>.
     Major bug reported by jlowe and fixed by jlowe (build)<br>
     <b>eclipse:eclipse build fails for hadoop-hdfs-httpfs</b><br>
     <blockquote>Building the eclipse:eclipse target fails in the hadoop-hdfs-httpfs project with this error:<br><br>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-eclipse-plugin:2.8:eclipse (default-cli) on project hadoop-hdfs-httpfs: Request to merge when &apos;filtering&apos; is not identical. Original=resource src/main/resources: output=target/classes, include=[httpfs.properties], exclude=[**/*.java], test=false, filtering=true, merging with=resource src/main/resources: output=target/classes, include=[], e...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2653">HDFS-2653</a>.
     Major improvement reported by eli and fixed by eli (data-node)<br>
     <b>DFSClient should cache whether addrs are non-local when short-circuiting is enabled</b><br>
     <blockquote>Something Todd mentioned to me off-line.. currently DFSClient doesn&apos;t cache the fact that non-local reads are non-local, so if short-circuiting is enabled every time we create a block reader we&apos;ll go through the isLocalAddress code path. We should cache the fact that an addr is non-local as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2654">HDFS-2654</a>.
     Major improvement reported by eli and fixed by eli (data-node)<br>
     <b>Make BlockReaderLocal not extend RemoteBlockReader2</b><br>
     <blockquote>The BlockReaderLocal code paths are easier to understand (especially true on branch-1 where BlockReaderLocal inherits code from BlockerReader and FSInputChecker) if the local and remote block reader implementations are independent, and they&apos;re not really sharing much code anyway. If for some reason they start to share significant code we can make the BlockReader interface an abstract class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2657">HDFS-2657</a>.
     Major bug reported by eli and fixed by tucu00 <br>
     <b>TestHttpFSServer and TestServerWebApp are failing on trunk</b><br>
     <blockquote>&gt;&gt;&gt; org.apache.hadoop.fs.http.server.TestHttpFSServer.instrumentation<br>&gt;&gt;&gt; org.apache.hadoop.lib.servlet.TestServerWebApp.lifecycle</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2658">HDFS-2658</a>.
     Major bug reported by eli and fixed by tucu00 <br>
     <b>HttpFS introduced 70 javadoc warnings</b><br>
     <blockquote>{noformat}<br>hadoop1 (trunk)$ grep warning javadoc.txt |grep -c httpfs<br>70<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2675">HDFS-2675</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Reduce verbosity when double-closing edit logs</b><br>
     <blockquote>Currently the edit logs log at WARN level when they&apos;re double-closed. But this happens in the normal flow of things, so we may as well reduce it to DEBUG to reduce log spam in unit tests, etc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2705">HDFS-2705</a>.
     Major bug reported by tucu00 and fixed by tucu00 <br>
     <b>HttpFS server should check that upload requests have correct content-type</b><br>
     <blockquote>The append/create requests should require &apos;application/octet-stream&apos; as content-type when uploading data. This is to prevent the default content-type form-encoded (used as default by some HTTP libraries) to be used or text based content-types to be used.<br><br>If the form-encoded content type is used, then Jersey tries to process the upload stream as parameters<br>If a test base content-type is used, HTTP proxies/gateways could do attempt some transcoding on the stream thus corrupting the data.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2706">HDFS-2706</a>.
     Major bug reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Use configuration for blockInvalidateLimit if it is set</b><br>
     <blockquote>HDFS-2191 accidentally removed the following code.<br>{code}<br>- this.blockInvalidateLimit = conf.getInt(<br>-        DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY, this.blockInvalidateLimit);<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2707">HDFS-2707</a>.
     Major bug reported by tucu00 and fixed by tucu00 (security)<br>
     <b>HttpFS should read the hadoop-auth secret from a file instead inline from the configuration</b><br>
     <blockquote>Similar to HADOOP-7621, the secret should be in a file other than the configuration file.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2710">HDFS-2710</a>.
     Critical bug reported by sseth and fixed by  <br>
     <b>HDFS part of MAPREDUCE-3529, HADOOP-7933</b><br>
     <blockquote>viewfs implementation of getDelegationTokens(String, Credentials)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2722">HDFS-2722</a>.
     Major bug reported by qwertymaniac and fixed by qwertymaniac (hdfs client)<br>
     <b>HttpFs shouldn&apos;t be using an int for block size</b><br>
     <blockquote>{{./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java: blockSize = fs.getConf().getInt(&quot;dfs.block.size&quot;, 67108864);}}<br><br>Should instead be using dfs.blocksize and should instead be long.<br><br>I&apos;ll post a patch for this after HDFS-1314 is resolved -- which changes the internal behavior a bit (should be getLongBytes, and not just getLong, to gain formatting advantages).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2726">HDFS-2726</a>.
     Major improvement reported by bien and fixed by qwertymaniac <br>
     <b>&quot;Exception in createBlockOutputStream&quot; shouldn&apos;t delete exception stack trace</b><br>
     <blockquote>I&apos;m occasionally (1/5000 times) getting this error after upgrading everything to hadoop-0.18:<br><br>08/09/09 03:28:36 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream<br>08/09/09 03:28:36 INFO dfs.DFSClient: Abandoning block blk_624229997631234952_8205908<br><br>DFSClient contains the logging code:<br><br>        LOG.info(&quot;Exception in createBlockOutputStream &quot; + ie);<br><br>This would be better written with ie as the second argument to LOG.info, so that the stac...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2729">HDFS-2729</a>.
     Minor improvement reported by qwertymaniac and fixed by qwertymaniac (name-node)<br>
     <b>Update BlockManager&apos;s comments regarding the invalid block set</b><br>
     <blockquote>Looks like after HDFS-82 was covered at some point, the comments and logs still carry presence of two sets when there really is just one set.<br><br>This patch changes the logs and comments to be more accurate about that.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2751">HDFS-2751</a>.
     Major bug reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Datanode drops OS cache behind reads even for short reads</b><br>
     <blockquote>HDFS-2465 has some code which attempts to disable the &quot;drop cache behind reads&quot; functionality when the reads are &lt;256KB (eg HBase random access). But this check was missing in the {{close()}} function, so it always drops cache behind reads regardless of the size of the read. This hurts HBase random read performance when this patch is enabled.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2784">HDFS-2784</a>.
     Major sub-task reported by daryn and fixed by kihwal (hdfs client, name-node, security)<br>
     <b>Update hftp and hdfs for host-based token support</b><br>
     <blockquote>Need to port 205 token changes and update any new related code dealing with tokens in these filesystems.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2785">HDFS-2785</a>.
     Major sub-task reported by daryn and fixed by revans2 (name-node, security)<br>
     <b>Update webhdfs and httpfs for host-based token support</b><br>
     <blockquote>Need to port 205 tokens into these filesystems.  Will mainly involve ensuring code duplicated from hftp is updated accordingly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2788">HDFS-2788</a>.
     Major improvement reported by eli and fixed by eli (data-node)<br>
     <b>HdfsServerConstants#DN_KEEPALIVE_TIMEOUT is dead code</b><br>
     <blockquote>HDFS-941 introduced HdfsServerConstants#DN_KEEPALIVE_TIMEOUT but its never used. Perhaps was renamed to DFSConfigKeys#DFS_DATANODE_SOCKET_REUSE_KEEPALIVE_DEFAULT while the patch was written and the old one wasn&apos;t deleted.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2790">HDFS-2790</a>.
     Minor bug reported by arpitgupta and fixed by arpitgupta <br>
     <b>FSNamesystem.setTimes throws exception with wrong configuration name in the message</b><br>
     <blockquote>the api throws this message when hdfs is not configured for accessTime<br><br>&quot;Access time for hdfs is not configured.  Please set dfs.support.accessTime configuration parameter.&quot;<br><br><br>The property name should be dfs.access.time.precision</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2791">HDFS-2791</a>.
     Major bug reported by tlipcon and fixed by tlipcon (data-node, name-node)<br>
     <b>If block report races with closing of file, replica is incorrectly marked corrupt</b><br>
     <blockquote>The following sequence of events results in a replica mistakenly marked corrupt:<br>1. Pipeline is open with 2 replicas<br>2. DN1 generates a block report but is slow in sending to the NN (eg some flaky network). It gets &quot;stuck&quot; right before the block report RPC.<br>3. Client closes the file.<br>4. DN2 is fast and sends blockReceived to the NN. NN marks the block as COMPLETE<br>5. DN1&apos;s block report proceeds, and includes the block in an RBW state.<br>6. (x) NN incorrectly marks the replica as corrupt, since i...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2803">HDFS-2803</a>.
     Minor improvement reported by jxiang and fixed by jxiang (name-node)<br>
     <b>Adding logging to LeaseRenewer for better lease expiration triage.</b><br>
     <blockquote>It will be helpful to add some logging to LeaseRenewer when the daemon is terminated (Info level),<br>and when the lease is renewed (Debug level).  Since lacking logging, it is hard to know<br>if a DFS client doesn&apos;t renew the lease because it hangs, or the lease renewer daemon is gone somehow.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2810">HDFS-2810</a>.
     Critical bug reported by tlipcon and fixed by tlipcon (hdfs client)<br>
     <b>Leases not properly getting renewed by clients</b><br>
     <blockquote>We&apos;ve been testing HBase on clusters running trunk and seen an issue where they seem to lose their HDFS leases after a couple of hours of runtime. We don&apos;t quite have enough data to understand what&apos;s happening, but the NN is expiring them, claiming the hard lease period has elapsed. The clients report no error until their output stream gets killed underneath them.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2814">HDFS-2814</a>.
     Minor improvement reported by hitesh and fixed by hitesh <br>
     <b>NamenodeMXBean does not account for svn revision in the version information</b><br>
     <blockquote>Unlike the jobtracker where both the UI and jmx information report the version as &quot;x.y.z, r&lt;svn revision&quot;, in case of the namenode, the UI displays x.y.z and svn revision info but the jmx output only contains the x.y.z version.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2816">HDFS-2816</a>.
     Trivial bug reported by hitesh and fixed by hitesh <br>
     <b>Fix missing license header in hadoop-hdfs-project/hadoop-hdfs-httpfs/dev-support/findbugsExcludeFile.xml</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2817">HDFS-2817</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (test)<br>
     <b>Combine the two TestSafeMode test suites</b><br>
     <blockquote>We currently have two tests by the same name. We should combine them. Also adding a new test for safemode extension, which wasn&apos;t previously covered.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2818">HDFS-2818</a>.
     Trivial bug reported by tlipcon and fixed by  (name-node)<br>
     <b>dfshealth.jsp missing space between role and node name</b><br>
     <blockquote>There seems to be a missing space in the titles of our webpages. EG: &lt;title&gt;Hadoop NameNodestyx01.sf.cloudera.com:8021&lt;/title&gt;. It seems like the JSP compiler is doing something to the space which is in the .jsp. Probably a simple fix if you know something about JSP :)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2822">HDFS-2822</a>.
     Major bug reported by tlipcon and fixed by tlipcon (ha, name-node)<br>
     <b>processMisReplicatedBlock incorrectly identifies under-construction blocks as under-replicated</b><br>
     <blockquote>When the NN processes mis-replicated blocks while exiting safemode, it considers under-construction blocks as under-replicated, inserting them into the neededReplicationsQueue. This makes them show up as corrupt in the metrics and UI momentarily, until they&apos;re pulled off the queue. At that point, it realizes that they aren&apos;t in fact under-replicated, correctly. This affects both the HA branch and trunk/23, best I can tell.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2825">HDFS-2825</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Add test hook to turn off the writer preferring its local DN</b><br>
     <blockquote>Currently, the default block placement policy always places the first replica in the pipeline on the local node if there is a valid DN running there. In some network designs, within-rack bandwidth is never constrained so this doesn&apos;t give much of an advantage. It would also be really useful to disable this for MiniDFSCluster tests, since currently if you start a multi-DN cluster and write with replication level 1, all of the replicas go to the same DN.<br>_[per discussion below, this was changed...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2826">HDFS-2826</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (name-node, test)<br>
     <b>Test case for HDFS-1476 (safemode can initialize repl queues before exiting)</b><br>
     <blockquote>HDFS-1476 introduced a feature whereby SafeMode can trigger the initialization of replication queues before the safemode exit threshold has been reached. But, it didn&apos;t include a test for this new behavior. This JIRA is to contribute such a test</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2827">HDFS-2827</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (name-node)<br>
     <b>Cannot save namespace after renaming a directory above a file with an open lease</b><br>
     <blockquote>When i execute the following operations and wait for checkpoint to complete.<br><br>fs.mkdirs(new Path(&quot;/test1&quot;));<br>FSDataOutputStream create = fs.create(new Path(&quot;/test/abc.txt&quot;)); //dont close<br>fs.rename(new Path(&quot;/test/&quot;), new Path(&quot;/test1/&quot;));<br><br>Check-pointing is failing with the following exception.<br><br>2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\HDFS-1623\hadoop-hdfs-project\hadoop-hdfs\build\test\data\dfs\name3<br>java.io.IOException: saveLease...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2835">HDFS-2835</a>.
     Major bug reported by revans2 and fixed by revans2 (tools)<br>
     <b>Fix org.apache.hadoop.hdfs.tools.GetConf$Command Findbug issue</b><br>
     <blockquote>https://builds.apache.org/job/PreCommit-HDFS-Build/1804//artifact/trunk/hadoop-hdfs-project/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html shows a findbugs warning.  It is unrelated to the patch being tested, and has shown up on a few other JIRAS as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2836">HDFS-2836</a>.
     Major bug reported by revans2 and fixed by revans2 <br>
     <b>HttpFSServer still has 2 javadoc warnings in trunk</b><br>
     <blockquote>{noformat}<br>[WARNING] hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java:241: warning - @param argument &quot;override,&quot; is not a parameter name.<br>[WARNING] hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java:450: warning - @param argument &quot;override,&quot; is not a parameter name.<br>{noformat}<br><br>These are causing other patches to get a -1 in automated testing.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2837">HDFS-2837</a>.
     Major bug reported by revans2 and fixed by revans2 <br>
     <b>mvn javadoc:javadoc not seeing LimitedPrivate class </b><br>
     <blockquote>mvn javadoc:javadoc not seeing LimitedPrivate class <br>{noformat}<br>[WARNING] org/apache/hadoop/fs/FileSystem.class(org/apache/hadoop/fs:FileSystem.class): warning: Cannot find annotation method &apos;value()&apos; in type &apos;org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate&apos;: class file for org.apache.hadoop.classification.InterfaceAudience not found<br>[WARNING] org/apache/hadoop/fs/FileSystem.class(org/apache/hadoop/fs:FileSystem.class): warning: Cannot find annotation method &apos;value()&apos; in typ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2840">HDFS-2840</a>.
     Major bug reported by eli and fixed by tucu00 (test)<br>
     <b>TestHostnameFilter should work with localhost or localhost.localdomain </b><br>
     <blockquote>TestHostnameFilter may currently fail with the following:<br><br>{noformat}<br>Error Message<br><br>null expected:&lt;localhost[.localdomain]&gt; but was:&lt;localhost[]&gt;<br>Stacktrace<br><br>junit.framework.ComparisonFailure: null expected:&lt;localhost[.localdomain]&gt; but was:&lt;localhost[]&gt;<br>	at junit.framework.Assert.assertEquals(Assert.java:81)<br>	at junit.framework.Assert.assertEquals(Assert.java:87)<br>	at org.apache.hadoop.lib.servlet.TestHostnameFilter$1.doFilter(TestHostnameFilter.java:50)<br>	at org.apache.hadoop.lib.servlet.Hos...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2864">HDFS-2864</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node)<br>
     <b>Remove redundant methods and a constant from FSDataset</b><br>
     <blockquote>- METADATA_VERSION is declared in both FSDataset and BlockMetadataHeader.<br><br>- In FSDataset, the methods findBlockFile(..), getBlockFile(..) and getFile(..) are very similar. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2868">HDFS-2868</a>.
     Minor improvement reported by qwertymaniac and fixed by qwertymaniac (data-node)<br>
     <b>Add number of active transfer threads to the DataNode status</b><br>
     <blockquote>Presently, we do not provide any stats from the DN that specifically indicates the total number of active transfer threads (xceivers). Having such a metric can be very helpful as well, over plain num-ops(type) form of metrics, which already exist.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2879">HDFS-2879</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node)<br>
     <b>Change FSDataset to package private</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2889">HDFS-2889</a>.
     Major bug reported by gchanan and fixed by gchanan (hdfs client)<br>
     <b>getNumCurrentReplicas is package private but should be public on 0.23 (see HDFS-2408)</b><br>
     <blockquote>See https://issues.apache.org/jira/browse/HDFS-2408<br>HDFS-2408 was not committed to 0.23 (or trunk it looks like).<br><br>This is breaking HBase unit tests with &quot;-Dhadoop.profile=23&quot;</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2893">HDFS-2893</a>.
     Minor bug reported by eli2 and fixed by eli2 <br>
     <b>The start/stop scripts don&apos;t start/stop the 2NN when using the default configuration</b><br>
     <blockquote>HDFS-1703 changed the behavior of the start/stop scripts so that the masters file is no longer used to indicate which hosts to start the 2NN on. The 2NN is now started, when using start-dfs.sh, on hosts only when dfs.namenode.secondary.http-address is configured with a non-wildcard IP. This means you can not start a NN using an http-address specified using a wildcard IP. We should allow a 2NN to be started with the default config, ie start-dfs.sh should start a NN, 2NN and DN. The packaging a...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1744">MAPREDUCE-1744</a>.
     Major bug reported by dking and fixed by dking <br>
     <b>DistributedCache creates its own FileSytem instance when adding a file/archive to the path</b><br>
     <blockquote>According to the contract of {{UserGroupInformation.doAs()}} the only required operations within the {{doAs()}} block are the<br>creation of a {{JobClient}} or getting a {{FileSystem}} .<br><br>The {{DistributedCache.add(File/Archive)ToClasspath()}} methods create a {{FileSystem}} instance outside of the {{doAs()}} block,<br>this {{FileSystem}} instance is not in the scope of the proxy user but of the superuser and permissions may make the method<br>fail.<br><br>One option is to overload the methods above to rece...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2450">MAPREDUCE-2450</a>.
     Major bug reported by matei and fixed by rajesh.balamohan <br>
     <b>Calls from running tasks to TaskTracker methods sometimes fail and incur a 60s timeout</b><br>
     <blockquote>I&apos;m seeing some map tasks in my jobs take 1 minute to commit after they finish the map computation. On the map side, the output looks like this:<br><br>&lt;code&gt;<br>2009-03-02 21:30:54,384 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=MAP, sessionId= - already initialized<br>2009-03-02 21:30:54,437 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 800<br>2009-03-02 21:30:54,437 INFO org.apache.hadoop.mapred.MapTask: io.sort.mb = 300<br>2009-03-02 21:30:55,493 I...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3045">MAPREDUCE-3045</a>.
     Minor bug reported by rramya and fixed by jeagles (jobhistoryserver, mrv2)<br>
     <b>Elapsed time filter on jobhistory server displays incorrect table entries</b><br>
     <blockquote>The elapsed time filter on the jobhistory server filters incorrect information. <br>For e.g. on a cluster where the elapsed time of all the tasks is either 7 or 8sec, the filter displays non null table entries for 1sec or 3sec</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3121">MAPREDUCE-3121</a>.
     Blocker bug reported by vinodkv and fixed by ravidotg (mrv2, nodemanager)<br>
     <b>DFIP aka &apos;NodeManager should handle Disk-Failures In Place&apos;</b><br>
     <blockquote>This is akin to MAPREDUCE-2413 but for YARN&apos;s NodeManager. We want to minimize the impact of transient/permanent disk failures on containers. With larger number of disks per node, the ability to continue to run containers on other disks is crucial.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3147">MAPREDUCE-3147</a>.
     Major improvement reported by raviprak and fixed by raviprak (mrv2)<br>
     <b>Handle leaf queues with the same name properly</b><br>
     <blockquote>If there are two leaf queues with the same name, there is ambiguity while submitting jobs, displaying queue info. When such ambiguity exists, the system should ask for clarification / show disambiguated information.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3169">MAPREDUCE-3169</a>.
     Major improvement reported by tlipcon and fixed by ahmed.radwan (mrv1, mrv2, test)<br>
     <b>Create a new MiniMRCluster equivalent which only provides client APIs cross MR1 and MR2</b><br>
     <blockquote>Many dependent projects like HBase, Hive, Pig, etc, depend on MiniMRCluster for writing tests. Many users do as well. MiniMRCluster, however, exposes MR implementation details like the existence of TaskTrackers, JobTrackers, etc, since it was used by MR1 for testing the server implementations as well.<br><br>This JIRA is to create a new interface which could be implemented either by MR1 or MR2 that exposes only the client-side portions of the MR framework. Ideally it would be &quot;recompile-compatible&quot;...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3194">MAPREDUCE-3194</a>.
     Major bug reported by sseth and fixed by jlowe (mrv2)<br>
     <b>&quot;mapred mradmin&quot; command is broken in mrv2</b><br>
     <blockquote>$mapred  mradmin  <br>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/tools/MRAdmin<br>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.tools.MRAdmin<br>        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)<br>        at java.security.AccessController.doPrivileged(Native Method)<br>        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)<br>        at sun.misc.Launc...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3238">MAPREDUCE-3238</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>Small cleanup in SchedulerApp</b><br>
     <blockquote>While reading this code, I did a little bit of cleanup:<br>- added some javadoc<br>- rather than using a Map&lt;Priority, Integer&gt; for keeping counts, switched to Guava&apos;s HashMultiset, which makes a simpler API.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3243">MAPREDUCE-3243</a>.
     Major bug reported by rramya and fixed by jeagles (contrib/streaming, mrv2)<br>
     <b>Invalid tracking URL for streaming jobs</b><br>
     <blockquote>The tracking URL for streaming jobs currently display &quot;http://N/A&quot;<br><br>{noformat}<br>INFO streaming.StreamJob: To kill this job, run:<br>INFO streaming.StreamJob: hadoop job -kill &lt;jobID&gt;<br>INFO streaming.StreamJob: Tracking URL: http://N/A<br>INFO mapreduce.Job: Running job: &lt;jobID&gt;<br>INFO mapreduce.Job:  map 0% reduce 0%<br>INFO mapred.ClientServiceDelegate: Tracking Url of JOB is &lt;host:port&gt;<br><br>{noformat}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3251">MAPREDUCE-3251</a>.
     Critical task reported by anupamseth and fixed by anupamseth (mrv2)<br>
     <b>Network ACLs can prevent some clients to talk to MR ApplicationMaster</b><br>
     <blockquote>In 0.20.xxx, the JobClient while polling goes to JT to get the job status. With YARN, AM can be launched on any port and the client will have to have ACL open to that port to talk to AM and get the job status. When the client is within the same grid network access to AM is not a problem. But some applications may have one installation per set of clusters and may launch jobs even across such sets (on job trackers in another set of clusters). For that to work only the JT port needs to be open c...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3265">MAPREDUCE-3265</a>.
     Blocker improvement reported by tlipcon and fixed by acmurthy (mrv2)<br>
     <b>Reduce log level on MR2 IPC construction, etc</b><br>
     <blockquote>Currently MR&apos;s IPC logging is very verbose. For example, I see a lot of:<br><br>11/10/25 12:14:06 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC<br>11/10/25 12:14:06 INFO mapred.ResourceMgrDelegate: Connecting to ResourceManager at c0309.hal.cloudera.com/172.29.81.91:40012<br>11/10/25 12:14:06 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ClientRMProtocol<br>11/10/25 12:14:07 INFO mapred.ResourceMgrDelegate...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3291">MAPREDUCE-3291</a>.
     Blocker bug reported by rramya and fixed by revans2 (mrv2)<br>
     <b>App fail to launch due to delegation token not found in cache</b><br>
     <blockquote>In secure mode, saw an app failure due to &quot;org.apache.hadoop.security.token.SecretManager$InvalidToken: token (HDFS_DELEGATION_TOKEN token &lt;id&gt; for &lt;user&gt;) can&apos;t be found in cache&quot; Exception in the next comment.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3324">MAPREDUCE-3324</a>.
     Critical bug reported by jeagles and fixed by jeagles (jobhistoryserver, mrv2, nodemanager)<br>
     <b>Not All HttpServer tools links (stacks,logs,config,metrics) are accessible through all UI servers</b><br>
     <blockquote>Nodemanager has no tools listed under tools UI.<br>Jobhistory server has no logs tool listed under tools UI.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3326">MAPREDUCE-3326</a>.
     Critical bug reported by tgraves and fixed by jlowe (mrv2)<br>
     <b>RM web UI scheduler link not as useful as should be</b><br>
     <blockquote>The resource manager web ui page for scheduler doesn&apos;t have all the information about the configuration like the jobtracker page used to have.  The things it seems to show you are the current queues - each queues used, set, and max percent and then what apps are running in that queue.  <br><br>It doesn&apos;t list any of yarn.scheduler.capacity.maximum-applications, yarn.scheduler.capacity.maximum-am-resource-percent, yarn.scheduler.capacity.&lt;queue-path&gt;.user-limit-factor, yarn.scheduler.capacity.&lt;queue...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3327">MAPREDUCE-3327</a>.
     Critical bug reported by tgraves and fixed by anupamseth (mrv2)<br>
     <b>RM web ui scheduler link doesn&apos;t show correct max value for queues</b><br>
     <blockquote>Configure a cluster to use the capacity scheduler and then specifying a maximum-capacity &lt; 100% for a queue.  If you go to the RM Web UI and hover over the queue, it always shows the max at 100%.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3328">MAPREDUCE-3328</a>.
     Critical bug reported by tgraves and fixed by raviprak (mrv2)<br>
     <b>mapred queue -list output inconsistent and missing child queues</b><br>
     <blockquote>When running mapred queue -list on a 0.23.0 cluster with capacity scheduler configured with child queues.  In my case I have queues default, test1, and test2.  test1 has subqueues of a1, a2.  test2 has subqueues of a3 and a4.<br><br>- the child queues do not show up<br>- The output of maximum capacity doesn&apos;t match the format of the current capacity and capacity.  the latter two use float while the maximum is specified as int:<br><br>Queue Name : default <br>Queue State : running <br>Scheduling Info : queueName: ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3329">MAPREDUCE-3329</a>.
     Blocker bug reported by tgraves and fixed by acmurthy (mrv2)<br>
     <b>capacity schedule maximum-capacity allowed to be less then capacity</b><br>
     <blockquote>When configuring the capacity scheduler capacity and maximum-capacity, it allows the maximum-capacity to be less then the capacity.  I did not test to see what true limit is, I assume maximum capacity.<br><br>output from mapred queue -list where capacity = 10%, max capacity = 5%.<br><br>Queue Name : test2 <br>Queue State : running <br>Scheduling Info : queueName: &quot;test2&quot;, capacity: 0.1, maximumCapacity: 5.0, currentCapacity: 0.0, state: Q_RUNNING,  <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3331">MAPREDUCE-3331</a>.
     Minor improvement reported by anupamseth and fixed by anupamseth (mrv2)<br>
     <b>Improvement to single node cluster setup documentation for 0.23</b><br>
     <blockquote>This JIRA is to track some minor corrections and suggestions for improvement for the documentation for the setup of a single node cluster using 0.23 currently available at http://people.apache.org/~acmurthy/hadoop-0.23/hadoop-yarn/hadoop-yarn-site/SingleCluster.html<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3336">MAPREDUCE-3336</a>.
     Critical bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>com.google.inject.internal.Preconditions not public api - shouldn&apos;t be using it</b><br>
     <blockquote>com.google.inject.internal.Preconditions does not exist in guice 3.0 and from in guice 2.0 it was an internal api and shouldn&apos;t have been used.   We should use com.google.common.base.Preconditions instead.<br><br>This is currently being used in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3341">MAPREDUCE-3341</a>.
     Major improvement reported by anupamseth and fixed by anupamseth (mrv2)<br>
     <b>Enhance logging of initalized queue limit values</b><br>
     <blockquote>Currently the RM log shows only a partial set of the limits that are configured when a queue is initialized / reinitialized.<br><br>For example, this is what is currently shown in the RM log for an initialized queue:<br># &lt;datestamp&gt; INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing<br>default, capacity=0.25, asboluteCapacity=0.25, maxCapacity=25.0, asboluteMaxCapacity=0.25, userLimit=100,<br>userLimitFactor=20.0, maxApplications=2500, maxApplicationsPerUser=50000...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3344">MAPREDUCE-3344</a>.
     Major bug reported by brocknoland and fixed by brocknoland <br>
     <b>o.a.h.mapreduce.Reducer since 0.21 blindly casts to ReduceContext.ValueIterator</b><br>
     <blockquote>0.21 mapreduce.Reducer introduced a blind cast to ReduceContext.ValueIterator. There should an instanceof check around this block to ensure we don&apos;t throw a CastClassException:<br>{code}<br>       // If a back up store is used, reset it<br>      ((ReduceContext.ValueIterator)<br>          (context.getValues().iterator())).resetBackupStore();<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3346">MAPREDUCE-3346</a>.
     Blocker bug reported by karams and fixed by amar_kamat (tools/rumen)<br>
     <b>Rumen LoggedTaskAttempt  getHostName call returns hostname as null</b><br>
     <blockquote>After MAPREDUCE-3035 and MAPREDUCE-3317<br>Now MRV2 job history contains hostName and rackName.<br>when rumen trace builder is ran on jobhistory, its generated trace contains hostname in form of <br>hostName : /raclname/hostname<br><br>But getHostName for LoggedTaskAttempt returns hostname as null<br>Seems that TraceBuilder is setting hostName properly but JobTraceReader is not able read it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3354">MAPREDUCE-3354</a>.
     Blocker bug reported by vinodkv and fixed by jeagles (jobhistoryserver, mrv2)<br>
     <b>JobHistoryServer should be started by bin/mapred and not by bin/yarn</b><br>
     <blockquote>JobHistoryServer belongs to mapreduce land.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3366">MAPREDUCE-3366</a>.
     Major bug reported by eyang and fixed by eyang (mrv2)<br>
     <b>Mapreduce component should use consistent directory structure layout as HDFS/common</b><br>
     <blockquote>Directory structure for MRv2 layout looks like:<br><br>{noformat}<br>hadoop-mapreduce-0.23.0-SNAPSHOT/bin<br>                                /conf<br>                                /lib<br>                                /modules<br>{noformat}<br><br>The directory structure layout should be updated to reflect changes implemented in HADOOP-6255.<br><br>{noformat}<br>hadoop-mapreduce-0.23.0-SNAPSHOT/bin<br>                                /etc/hadoop<br>                                /lib<br>                                /libexec<br>     ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3369">MAPREDUCE-3369</a>.
     Major improvement reported by ahmed.radwan and fixed by ahmed.radwan (mrv1, mrv2, test)<br>
     <b>Migrate MR1 tests to run on MR2 using the new interfaces introduced in MAPREDUCE-3169</b><br>
     <blockquote>This ticket tracks the migration of MR1 tests (currently residing in &quot;hadoop-mapreduce-project/src/test/&quot;) to run on MR2. The migration is using the new interfaces introduced in MAPREDUCE-3169.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3370">MAPREDUCE-3370</a>.
     Major bug reported by ahmed.radwan and fixed by ahmed.radwan (mrv2, test)<br>
     <b>MiniMRYarnCluster uses a hard coded path location for the MapReduce application jar</b><br>
     <blockquote>MiniMRYarnCluster uses a hard coded relative path location for the MapReduce application jar. It is better to have this location as a system property so tests can pick the application jar regardless of their working directory.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3371">MAPREDUCE-3371</a>.
     Minor improvement reported by raviprak and fixed by raviprak (documentation, mrv2)<br>
     <b>Review and improve the yarn-api javadocs.</b><br>
     <blockquote>Review and improve the yarn-api javadocs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3372">MAPREDUCE-3372</a>.
     Major bug reported by bmahe and fixed by bmahe <br>
     <b>HADOOP_PREFIX cannot be overriden</b><br>
     <blockquote>hadoop-config.sh forces HADOOP_prefix to a specific value:<br>export HADOOP_PREFIX=`dirname &quot;$this&quot;`/..<br><br>It would be nice to make this overridable.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3373">MAPREDUCE-3373</a>.
     Major bug reported by bmahe and fixed by bmahe <br>
     <b>Hadoop scripts unconditionally source &quot;$bin&quot;/../libexec/hadoop-config.sh.</b><br>
     <blockquote>It would be nice to be able to specify some other location for hadoop-config.sh</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3376">MAPREDUCE-3376</a>.
     Major bug reported by revans2 and fixed by subrotosanyal (mrv1, mrv2)<br>
     <b>Old mapred API combiner uses NULL reporter</b><br>
     <blockquote>The OldCombinerRunner class inside Task.java uses a NULL Reporter.  If the combiner code runs for an extended period of time, even with reporting progress as it should, the map task can timeout and be killed.  It appears that the NewCombinerRunner class uses a valid reporter and as such is not impacted by this bug.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3380">MAPREDUCE-3380</a>.
     Blocker sub-task reported by tucu00 and fixed by mahadev (mr-am, mrv2)<br>
     <b>Token infrastructure for running clients which are not kerberos authenticated</b><br>
     <blockquote>The JobClient.getDelegationToken() method is returning NULL, this makes Oozie fail when trying to get the delegation token to use it for starting a job.<br><br>What is seems to be happing is that Jobclient.getDelegationToken() calls Cluster.getDelegationToken() that calls YarnRunner.getDelegationToken() that calls ResourceMgrDelegate.getDelegationToken(). And the last one is not implemented. (Thanks Ahmed for tracing this in MR2 code)<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3389">MAPREDUCE-3389</a>.
     Critical bug reported by tucu00 and fixed by tucu00 (mrv2)<br>
     <b>MRApps loads the &apos;mrapp-generated-classpath&apos; file with classpath from the build machine</b><br>
     <blockquote>The &apos;mrapp-generated-classpath&apos; file contains the classpath from where Hadoop was build. This classpath is not useful under any circumstances.<br><br>For example the content of the &apos;mrapp-generated-classpath&apos; in my dev environment is:<br><br>/Users/tucu/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/tucu/.m2/repository/asm/asm/3.2/asm-3.2.jar:/Users/tucu/.m2/repository/com/cenqua/clover/clover/3.0.2/clover-3.0.2.jar:/Users/tucu/.m2/repository/com/google/guava/guava/r09/guava-r09.ja...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3391">MAPREDUCE-3391</a>.
     Minor bug reported by subrotosanyal and fixed by subrotosanyal (applicationmaster)<br>
     <b>Connecting to CM is logged as Connecting to RM</b><br>
     <blockquote>In class *org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster*<br>{code}<br>private void connectToCM() {<br>      String cmIpPortStr = container.getNodeId().getHost() + &quot;:&quot; <br>          + container.getNodeId().getPort();		<br>      InetSocketAddress cmAddress = NetUtils.createSocketAddr(cmIpPortStr);		<br>      LOG.info(&quot;Connecting to ResourceManager at &quot; + cmIpPortStr);<br>      this.cm = ((ContainerManager) rpc.getProxy(ContainerManager.class, cmAddress, conf));<br>    }<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3408">MAPREDUCE-3408</a>.
     Major bug reported by bmahe and fixed by bmahe (mrv2, nodemanager, resourcemanager)<br>
     <b>yarn-daemon.sh unconditionnaly sets yarn.root.logger</b><br>
     <blockquote>yarn-daemon.sh unconditionnaly sets yarn.root.logger which then prevent any override from happening.<br>From ./hadoop-mapreduce-project/hadoop-yarn/bin/yarn-daemon.sh:<br>&gt; export YARN_ROOT_LOGGER=&quot;INFO,DRFA&quot;<br>&gt; export YARN_JHS_LOGGER=&quot;INFO,JSA&quot;<br><br>and then yarn-daemon.sh will call &quot;$YARN_HOME&quot;/bin/yarn which does the following:<br>&gt; YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}&quot;<br>&gt; YARN_OPTS=&quot;$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}&quot;<br><br>This has at leas...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3411">MAPREDUCE-3411</a>.
     Minor improvement reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>Performance Upgrade for jQuery</b><br>
     <blockquote>jQuery 1.6.4 is almost twice as fast as current version 1.4.4 on modern browsers on some operations. There are also many modern browser compatibility fixes<br><br>http://jsperf.com/jquery-15-unique-traversal/15</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3413">MAPREDUCE-3413</a>.
     Minor bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>RM web ui applications not sorted in any order by default</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3422">MAPREDUCE-3422</a>.
     Major bug reported by tomwhite and fixed by jeagles (mrv2)<br>
     <b>Counter display names are not being picked up</b><br>
     <blockquote>When running a job I see &quot;MAP_INPUT_RECORDS&quot; rather than &quot;Map input records&quot; for the counter name. To fix this the resource bundle properties files need to be moved to the src/main/resources tree. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3427">MAPREDUCE-3427</a>.
     Blocker bug reported by tucu00 and fixed by hitesh (contrib/streaming, mrv2)<br>
     <b>streaming tests fail with MR2</b><br>
     <blockquote>After Mavenizing streaming and getting its testcases to use the MiniMRCluster wrapper (MAPREDUCE-3169), 4 testcases fail to pass.<br><br>Following is an assessment of those failures. Note that the testcases have been tweaked only to set the streaming JAR and yarn as the  framework.<br> <br>(If these issues are unrelated we should create sub-tasks for each one of them).<br><br>*TestStreamingCombiner*, fails because returned counters don&apos;t match assertion. However, counters printed in the test output indicate va...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3433">MAPREDUCE-3433</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite (client, mrv2)<br>
     <b>Finding counters by legacy group name returns empty counters</b><br>
     <blockquote>Attempting to find counters with a legacy group name (e.g. org.apache.hadoop.mapred.Task$Counter rather than the new org.apache.hadoop.mapreduce.TaskCounter) returns empty counters. This causes TestStreamingCombiner to fail when run with YARN.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3434">MAPREDUCE-3434</a>.
     Blocker bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Nightly build broken </b><br>
     <blockquote>https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk/901/<br><br>Results :<br><br>Failed tests:   testSleepJob(org.apache.hadoop.mapreduce.v2.TestMRJobs)<br>  testRandomWriter(org.apache.hadoop.mapreduce.v2.TestMRJobs)<br>  testDistributedCache(org.apache.hadoop.mapreduce.v2.TestMRJobs)<br><br>Tests in error: <br>  org.apache.hadoop.mapreduce.v2.TestMROldApiJobs: Failed to Start org.apache.hadoop.mapreduce.v2.TestMROldApiJobs<br>  org.apache.hadoop.mapreduce.v2.TestUberAM: Failed to Start org.apache.h...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3436">MAPREDUCE-3436</a>.
     Major bug reported by bmahe and fixed by ahmed.radwan (mrv2, webapps)<br>
     <b>JobHistory webapp address should use the host from the jobhistory address</b><br>
     <blockquote>On the following page : http://&lt;RESOURCE_MANAGER&gt;:8088/cluster/apps<br>There are links to the history for each application. None of them can be reached since they all point to the ip 0.0.0.0. For instance:<br>http://0.0.0.0:8088/proxy/application_1321658790349_0002/jobhistory/job/job_1321658790349_2_2<br><br>Am I missing something?<br><br>[root@bigtop-fedora-15 ~]# jps<br>9968 ResourceManager<br>1495 NameNode<br>1645 DataNode<br>12935 Jps<br>11140 -- process information unavailable<br>5309 JobHistoryServer<br>10237 NodeManager<br><br>[r...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3437">MAPREDUCE-3437</a>.
     Blocker bug reported by jeagles and fixed by jeagles (build, mrv2)<br>
     <b>Branch 23 fails to build with Failure to find org.apache.hadoop:hadoop-project:pom:0.24.0-SNAPSHOT</b><br>
     <blockquote>[INFO] Scanning for projects...<br>[ERROR] The build could not read 1 project -&gt; [Help 1]<br>[ERROR]   <br>[ERROR]   The project org.apache.hadoop:hadoop-mapreduce-examples:0.24.0-SNAPSHOT (/home/jeagles/hadoop/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/pom.xml) has 1 error<br>[ERROR]     Non-resolvable parent POM: Failure to find org.apache.hadoop:hadoop-project:pom:0.24.0-SNAPSHOT in http://stormwalk.champ.corp.yahoo.com:8081/nexus/content/groups/public was cached in the local repository,...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3443">MAPREDUCE-3443</a>.
     Blocker bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Oozie jobs are running as oozie user even though they create the jobclient as doAs.</b><br>
     <blockquote>Oozie is having issues with job submission, since it does the following:<br><br>{code}<br>doAs(userwhosubmittedjob) {<br> jobclient = new JobClient(jobconf);<br>}<br><br>jobclient.submitjob()<br><br>{code}<br><br>In 0.20.2** this works because the JT proxy is created as soon as we call new JobClient(). But in 0.23 this is no longer true since the client has to talk to multiple servers (AM/RM/JHS). To keep this behavior we will have to store the ugi in new JobClient() and make sure all the calls are run with a doAs() inside t...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3444">MAPREDUCE-3444</a>.
     Blocker bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>trunk/0.23 builds broken </b><br>
     <blockquote>https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/208/ <br>https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1310/<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3447">MAPREDUCE-3447</a>.
     Blocker bug reported by tgraves and fixed by mahadev (mrv2)<br>
     <b>mapreduce examples not working</b><br>
     <blockquote>Since the mavenization went in the mapreduce examples jar no longer works.  <br><br>$ hadoop jar ./hadoop-0.23.0-SNAPSHOT/modules/hadoop-mapreduce-examples-0.23.0-SNAPSHOT.jar  wordcount input output<br>Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: wordcount<br>        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)<br>        at java.security.AccessController.doPrivileged(Native Method)<br>        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)<br>        at java.lang.Class...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3448">MAPREDUCE-3448</a>.
     Minor bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>TestCombineOutputCollector javac unchecked warning on mocked generics</b><br>
     <blockquote>  [javac] found   : org.apache.hadoop.mapred.IFile.Writer<br>    [javac] required: org.apache.hadoop.mapred.IFile.Writer&lt;java.lang.String,java.lang.Integer&gt;<br>    [javac]     Writer&lt;String, Integer&gt; mockWriter = mock(Writer.class);<br>    [javac]                                              ^<br>    [javac] /home/jeagles/hadoop/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestCombineOutputCollector.java:125: warning: [unchecked] unchecked conversion<br>    [javac] found   : org.a...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3450">MAPREDUCE-3450</a>.
     Major bug reported by sseth and fixed by sseth (mr-am, mrv2)<br>
     <b>NM port info no longer available in JobHistory</b><br>
     <blockquote>The NM RPC port used to be part of the hostname field in JobHistory. That seems to have gone missing. Required for the task log link on the history server.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3452">MAPREDUCE-3452</a>.
     Major bug reported by tgraves and fixed by jeagles (mrv2)<br>
     <b>fifoscheduler web ui page always shows 0% used for the queue</b><br>
     <blockquote>When the fifo scheduler is configured to be on, go to the RM web ui page and click the scheduler link.  Hover over the default queue to see the used%.  It always shows used% as 0.0% even when jobs are running.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3453">MAPREDUCE-3453</a>.
     Major bug reported by tgraves and fixed by jeagles (mrv2)<br>
     <b>RM web ui application details page shows RM cluster about information</b><br>
     <blockquote>Go to the RM Web ui page.  Click on the Applications link, then click on a particular application. The applications details page inadvertently includes the RM about page information after the application details:<br><br>Cluster ID: 	1321943597242<br>ResourceManager state: 	STARTED<br>ResourceManager started on: 	22-Nov-2011 06:33:17<br>ResourceManager version: 	0.23.0-SNAPSHOT from 1203458 by user source checksum 0c288fc0971ed28c970272a62f547eae on Tue Nov 22 06:31:09 UTC 2011<br>Hadoop version: 	0.23.0-SNAPSH...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3454">MAPREDUCE-3454</a>.
     Major bug reported by amar_kamat and fixed by hitesh (contrib/gridmix)<br>
     <b>[Gridmix] TestDistCacheEmulation is broken</b><br>
     <blockquote>TestDistCacheEmulation is broken as &apos;MapReduceTestUtil&apos; no longer exists.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3456">MAPREDUCE-3456</a>.
     Blocker bug reported by eepayne and fixed by eepayne (mrv2)<br>
     <b>$HADOOP_PREFIX/bin/yarn should set defaults for $HADOOP_*_HOME</b><br>
     <blockquote>If the $HADOOP_PREFIX/hadoop-dist/target/hadoop-0.23.0-SNAPSHOT.tar.gz tarball is used to distribute hadoop, all of the HADOOP components (HDFS, MAPRED, COMMON) are all under one directory. In this use case, HADOOP_PREFIX should be set and should point to the root directory for all components, and it should not be necessary to set HADOOP_HDFS_HOME, HADOOP_COMMON_HOME, and HADOOP_MAPRED_HOME. However, the $HADOOP_PREFIX/bin/yarn script requires these 3 to be set explicitly in the calling envir...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3458">MAPREDUCE-3458</a>.
     Major bug reported by acmurthy and fixed by devaraj.k (mrv2)<br>
     <b>Fix findbugs warnings in hadoop-examples</b><br>
     <blockquote>I see 12 findbugs warnings in hadoop-examples: <br>https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1336//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-examples.html</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3460">MAPREDUCE-3460</a>.
     Blocker bug reported by sseth and fixed by revans2 (mr-am, mrv2)<br>
     <b>MR AM can hang if containers are allocated on a node blacklisted by the AM</b><br>
     <blockquote>When an AM is assigned a FAILED_MAP (priority = 5) container on a nodemanager which it has blacklisted - it tries to<br>find a corresponding container request.<br>This uses the hostname to find the matching container request - and can end up returning any of the ContainerRequests which may have requested a container on this node. This container request is cleaned to remove the bad node - and then added back to the RM &apos;ask&apos; list.<br>The AM cleans the &apos;ask&apos; list after each heartbeat - The RM Allocator i...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3463">MAPREDUCE-3463</a>.
     Blocker bug reported by karams and fixed by sseth (applicationmaster, mrv2)<br>
     <b>Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job</b><br>
     <blockquote>Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml. Started yarn 4 Node cluster.<br>First Ran Randowriter/Sort/Sort-validate successfully<br>Then again sort, when job was 50% complete<br>Login node running AppMaster, and killed AppMaster with kill -9<br>On Client side failed with following:<br>{code}<br>11/11/23 10:57:27 INFO mapreduce.Job:  map 58% reduce 8%<br>11/11/23 10:57:27 INFO mapred.ClientServiceDelegate: Failed to contact AM/History for job job_1322040898409_0005 retrying..<br>11/11/23 10:57:28 INF...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3464">MAPREDUCE-3464</a>.
     Trivial bug reported by davevr and fixed by davevr <br>
     <b>mapreduce jsp pages missing DOCTYPE [post-split branches]</b><br>
     <blockquote>Some jsp pages in the UI are missing a DOCTYPE declaration. This causes the pages to render incorrectly on some browsers, such as IE9. Please see parent bug HADOOP-7827 for details and patch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3465">MAPREDUCE-3465</a>.
     Minor bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin fails on 0.23 </b><br>
     <blockquote>Running org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin<br>Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.121 sec &lt;&lt;&lt; FAILURE!<br>Tests in error: <br>  testParsingProcStatAndCpuFile(org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin): /homes/hortonhi/dev/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/target/test-dir/CPUINFO_943711651 (No such file or directory)<br>  testParsingProcMemFile(org.apache.hadoop.yarn.util.TestLinuxResourceCalcu...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3468">MAPREDUCE-3468</a>.
     Major task reported by sseth and fixed by sseth <br>
     <b>Change version to 0.23.1 for ant builds on the 23 branch</b><br>
     <blockquote>Maven version has been changed to 0.23.1-SNAPSHOT. The ant build files need to change as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3477">MAPREDUCE-3477</a>.
     Major bug reported by bmahe and fixed by jeagles (documentation, mrv2)<br>
     <b>Hadoop site documentation cannot be built anymore on trunk and branch-0.23</b><br>
     <blockquote>Maven fails and here is the issue I get:<br><br>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.0:site (default-site) on project hadoop-yarn-site: Error during page generation: Error parsing &apos;/home/bruno/freesoftware/bigtop/build/hadoop/rpm/BUILD/apache-hadoop-common-e127450/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/SingleCluster.apt.vm&apos;: line [23] Unable to execute macro in the APT document: ParseException: expected SECTION2, found SECTION3 -&gt; [...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3478">MAPREDUCE-3478</a>.
     Minor bug reported by abayer and fixed by tomwhite (mrv2)<br>
     <b>Cannot build against ZooKeeper 3.4.0</b><br>
     <blockquote>I tried to see if one could build Hadoop 0.23.0 against ZooKeeper 3.4.0, rather than 3.3.1 (3.3.3 does work, fwiw) and hit compilation errors:<br><br>{quote}<br>[INFO] ------------------------------------------------------------------------<br>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:testCompile (default-testCompile) on project hadoop-yarn-server-common: Compilation failure: Compilation failure:<br>[ERROR] /Volumes/EssEssDee/abayer/src/asf-git/hadoop-common/hadoop-...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3479">MAPREDUCE-3479</a>.
     Major bug reported by tomwhite and fixed by tomwhite (client)<br>
     <b>JobClient#getJob cannot find local jobs</b><br>
     <blockquote>The problem is that JobClient#submitJob doesn&apos;t pass the Cluster object to Job for the submission process, which means that two Cluster objects and two LocalJobRunner objects are created. LocalJobRunner keeps an instance map of job IDs to Jobs, and when JobClient#getJob is called the LocalJobRunner with the unpopulated map is used which results in the job not being found.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3485">MAPREDUCE-3485</a>.
     Major sub-task reported by hitesh and fixed by ravidotg (mrv2)<br>
     <b>DISKS_FAILED -101 error code should be defined in same location as ABORTED_CONTAINER_EXIT_STATUS</b><br>
     <blockquote>With MAPREDUCE-3121, it is defined in ContainerExecutor as part of yarn-nodemanager which would be a problem for client-side code if it needs to understand the exit code. <br><br>A short term fix would be to move it into YarnConfiguration where ABORTED_CONTAINER_EXIT_STATUS is defined. A longer term fix would be to find a more formal and extensible approach for new yarn framework error codes to be added and be easily accessible to client-side code or other AMs. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3488">MAPREDUCE-3488</a>.
     Blocker bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Streaming jobs are failing because the main class isnt set in the pom files.</b><br>
     <blockquote>Streaming jobs are failing since the main MANIFEST file isnt being set in the pom files.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3496">MAPREDUCE-3496</a>.
     Major bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>Yarn initializes ACL operations from capacity scheduler config in a non-deterministic order</b><br>
     <blockquote>&apos;mapred queue -showacls&apos; does not output put acls in a predictable manner. This is a regression from previous versions.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3499">MAPREDUCE-3499</a>.
     Blocker bug reported by tucu00 and fixed by johnvijoe (mrv2, test)<br>
     <b>New MiniMR does not setup proxyuser configuration correctly, thus tests using doAs do not work</b><br>
     <blockquote>The new MiniMR implementation is not taking proxyuser settings.<br><br>Because of this, testcases using/testing doAs functionality fail.<br><br>This affects all Oozie testcases that use MiniMR.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3500">MAPREDUCE-3500</a>.
     Major bug reported by tucu00 and fixed by tucu00 (mrv2)<br>
     <b>MRJobConfig creates an LD_LIBRARY_PATH using the platform ARCH</b><br>
     <blockquote>With HADOOP-7874 we are removing the arch from the java.library.path.<br><br>The LD_LIBRARY_PATH being set should not include the ARCH.<br><br>{code}<br>  public static final String DEFAULT_MAPRED_ADMIN_USER_ENV =<br>      &quot;LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native/&quot; + PlatformName.getPlatformName();<br>{code}<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3505">MAPREDUCE-3505</a>.
     Major bug reported by bmahe and fixed by ahmed.radwan (mrv2)<br>
     <b>yarn APPLICATION_CLASSPATH needs to be overridable</b><br>
     <blockquote>Right now MRApps sets the classpath to just being mrapp-generated-classpath, its content and a hardcoded list of directories.<br>If I understand correctly mrapp-generated-classpath is only there for testing and may change or disappear at any time<br><br>The list of hardcoded directories is defined in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationConstants.java at line 92.<br>For convenience, here is its current content:<br>{noformat}<br>  /**<br>   * Clas...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3510">MAPREDUCE-3510</a>.
     Major bug reported by jeagles and fixed by jeagles (contrib/capacity-sched, mrv2)<br>
     <b>Capacity Scheduler inherited ACLs not displayed by mapred queue -showacls</b><br>
     <blockquote>mapred queue -showacls does not show inherited acls</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3513">MAPREDUCE-3513</a>.
     Trivial bug reported by mahadev and fixed by chaku88 (mrv2)<br>
     <b>Capacity Scheduler web UI has a spelling mistake for Memory.</b><br>
     <blockquote>The web page for capacity scheduler has a column named &quot;Memopry Total&quot;, a spelling mistake which needs to be fixed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3518">MAPREDUCE-3518</a>.
     Critical bug reported by jeagles and fixed by jeagles (client, mrv2)<br>
     <b>mapred queue -info &lt;queue&gt; -showJobs throws NPE</b><br>
     <blockquote>mapred queue -info default -showJobs<br><br>Exception in thread &quot;main&quot; java.lang.NullPointerException<br>        at org.apache.hadoop.mapreduce.tools.CLI.displayJobList(CLI.java:572)<br>        at org.apache.hadoop.mapred.JobQueueClient.displayQueueInfo(JobQueueClient.java:190)<br>        at org.apache.hadoop.mapred.JobQueueClient.run(JobQueueClient.java:103)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)<br>        at o...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3521">MAPREDUCE-3521</a>.
     Minor bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>Hadoop Streaming ignores unknown parameters</b><br>
     <blockquote>The hadoop streaming command will ignore any command line arguments to it.<br><br>{code}<br>hadoop jar streaming.jar -input input -output output -mapper cat -reducer cat ThisIsABadArgument<br>{code}<br><br>Works just fine.  This can mask issues where quotes were mistakenly missed like<br><br>{code}<br>hadoop jar streaming.jar -input input -output output -mapper xargs cat -reducer cat -archive someArchive.tgz<br>{code}<br><br>Streaming should fail if it encounters an unexpected command line parameter</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3522">MAPREDUCE-3522</a>.
     Major bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>Capacity Scheduler ACLs not inherited by default</b><br>
     <blockquote>Hierarchical Queues do not inherit parent ACLs correctly by default. Instead, if no value is specified for submit or administer acls, then all access is granted.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3527">MAPREDUCE-3527</a>.
     Major bug reported by tomwhite and fixed by tomwhite <br>
     <b>Fix minor API incompatibilities between 1.0 and 0.23</b><br>
     <blockquote>There are a few minor incompatibilities that were found in HADOOP-7738 and are straightforward to fix.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3529">MAPREDUCE-3529</a>.
     Critical bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>TokenCache does not cache viewfs credentials correctly</b><br>
     <blockquote>viewfs returns a list of delegation tokens for the actual namenodes. TokenCache caches these based on the actual service name - subsequent calls to TokenCache end up trying to get a new set of tokens.<br><br>Tasks which happen to access TokenCache fail when using viewfs - since they end up trying to get a new set of tokens even though the tokens are already available.<br><br>{noformat}<br>Error: java.io.IOException: Delegation Token can be issued only with kerberos or web authentication<br>        at org.apach...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3531">MAPREDUCE-3531</a>.
     Blocker bug reported by karams and fixed by revans2 (mrv2, resourcemanager, scheduler)<br>
     <b>Sometimes java.lang.IllegalArgumentException: Invalid key to HMAC computation in NODE_UPDATE also causing RM to stop scheduling </b><br>
     <blockquote>Filling this Jira a bit late<br>Started 350 cluster<br>sbummited large sleep job.<br>Foud that job was not running as RM has not allocated resouces to it.<br>{code}<br>2011-12-01 11:56:25,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: &lt;NMHost&gt;:48490 clusterResources: memory: 3225600<br>2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event<br>type NODE_UPDATE to the scheduler<br>java.lang.IllegalAr...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3537">MAPREDUCE-3537</a>.
     Blocker bug reported by acmurthy and fixed by acmurthy <br>
     <b>DefaultContainerExecutor has a race condn. with multiple concurrent containers</b><br>
     <blockquote>DCE relies cwd before calling ContainerLocalizer.runLocalization. However, with multiple containers setting cwd on same localFS reference leads to race. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3541">MAPREDUCE-3541</a>.
     Blocker bug reported by raviprak and fixed by raviprak (mrv2)<br>
     <b>Fix broken TestJobQueueClient test</b><br>
     <blockquote>Ant build complains <br>    [javac] /hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java&gt;:80: printJobQueueInfo(org.apache.hadoop.mapred.JobQueueInfo,java.io.Writer,java.lang.String) in org.apache.hadoop.mapred.JobQueueClient cannot be applied to (org.apache.hadoop.mapred.JobQueueInfo,java.io.StringWriter)<br>    [javac]     client.printJobQueueInfo(root, writer);<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3542">MAPREDUCE-3542</a>.
     Major bug reported by tomwhite and fixed by tomwhite <br>
     <b>Support &quot;FileSystemCounter&quot; legacy counter group name for compatibility</b><br>
     <blockquote>The group name changed from &quot;FileSystemCounter&quot; to &quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;, but we should support the old one for compatibility&apos;s sake. This came up in PIG-2347. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3544">MAPREDUCE-3544</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build, tools/rumen)<br>
     <b>gridmix build is broken, requires hadoop-archives to be added as ivy dependency</b><br>
     <blockquote>Having moved HAR/HadoopArchives to common/tools makes gridmix to fail as HadoopArchives is not in the mr1 classpath anymore.<br><br>hadoop-archives artifact should be added to gridmix dependencies<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3547">MAPREDUCE-3547</a>.
     Critical sub-task reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>finish unit tests for web services for RM and NM</b><br>
     <blockquote>Write more unit tests for the web services added for rm and nm.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3548">MAPREDUCE-3548</a>.
     Critical sub-task reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>write unit tests for web services for mapreduce app master and job history server</b><br>
     <blockquote>write more unit tests for mapreduce application master and job history server web services added in MAPREDUCE-2863</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3553">MAPREDUCE-3553</a>.
     Minor sub-task reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>Add support for data returned when exceptions thrown from web service apis to be in either xml or in JSON</b><br>
     <blockquote>When the web services apis for rm, nm, app master, and job history server throw an exception - like bad request, not found, they always return the data in JSON format.  It would be nice to return based on what they requested - xml or JSON.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3557">MAPREDUCE-3557</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>MR1 test fail to compile because of missing hadoop-archives dependency</b><br>
     <blockquote>MAPREDUCE-3544 added hadoop-archives as dependency to gridmix and raid, but missed to add it to the main ivy.xml for the MR1 testcases thus the ant target &apos;compile-mapred-test&apos; fails.<br><br>I was under the impression that this stuff was not used anymore but trunk is failing on that target.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3560">MAPREDUCE-3560</a>.
     Blocker bug reported by vinodkv and fixed by sseth (mrv2, resourcemanager, test)<br>
     <b>TestRMNodeTransitions is failing on trunk</b><br>
     <blockquote>Apparently Jenkins is screwed up. It is happily blessing patches, even though tests are failing.<br><br>Link to logs: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1454//testReport/org.apache.hadoop.yarn.server.resourcemanager/TestRMNodeTransitions/testExpiredContainer/</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3563">MAPREDUCE-3563</a>.
     Major bug reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>LocalJobRunner doesn&apos;t handle Jobs using o.a.h.mapreduce.OutputCommitter</b><br>
     <blockquote>LocalJobRunner doesn&apos;t handle Jobs using o.a.h.mapreduce.OutputCommitter, ran into this debugging PIG-2347.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3566">MAPREDUCE-3566</a>.
     Critical sub-task reported by vinodkv and fixed by vinodkv (mr-am, mrv2)<br>
     <b>MR AM slows down due to repeatedly constructing ContainerLaunchContext</b><br>
     <blockquote>The construction of the context is expensive, includes per-task trips to NameNode for obtaining the information about job.jar, job splits etc which is redundant across all tasks.<br><br>We should have a common job-level context and a task-specific context inheriting from the common job-level context.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3567">MAPREDUCE-3567</a>.
     Major sub-task reported by vinodkv and fixed by vinodkv (mr-am, mrv2, performance)<br>
     <b>Extraneous JobConf objects in AM heap</b><br>
     <blockquote>MR AM creates new JobConf objects unnecessarily in a couple of places in JobImpl and TaskImpl which occupy non-trivial amount of heap.<br><br>While working with a 64 bit JVM on 100K maps jobs, with uncompressed pointers, removing those extraneous objects helped in addressing OOM with 2GB AM heap size.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3569">MAPREDUCE-3569</a>.
     Critical sub-task reported by vinodkv and fixed by vinodkv (mr-am, mrv2, performance)<br>
     <b>TaskAttemptListener holds a global lock for all task-updates</b><br>
     <blockquote>This got added via MAPREDUCE-3274. We really don&apos;t need the lock if we just implement what I mentioned on that ticket [here|https://issues.apache.org/jira/browse/MAPREDUCE-3274?focusedCommentId=13137214&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13137214].<br><br>This has performance implications on MR AM with lots of tasks.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3572">MAPREDUCE-3572</a>.
     Critical sub-task reported by vinodkv and fixed by vinodkv (mr-am, mrv2, performance)<br>
     <b>MR AM&apos;s dispatcher is blocked by heartbeats to ResourceManager</b><br>
     <blockquote>All the heartbeat processing is done in {{RMContainerAllocator}} locking the object. The event processing is also locked on this, causing the dispatcher to be blocked and the rest of the AM getting stalled.<br><br>The event processing should be in a separate thread.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3579">MAPREDUCE-3579</a>.
     Major bug reported by atm and fixed by atm (mrv2)<br>
     <b>ConverterUtils should not include a port in a path for a URL with no port</b><br>
     <blockquote>In {{ConverterUtils#getPathFromYarnURL}}, it&apos;s incorrectly assumed that if a URL includes a valid host it must also include a valid port.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3582">MAPREDUCE-3582</a>.
     Major bug reported by ahmed.radwan and fixed by ahmed.radwan (mrv2, test)<br>
     <b>Move successfully passing MR1 tests to MR2 maven tree.</b><br>
     <blockquote>This ticket will track moving mr1 tests that are passing successfully to mr2 maven tree.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3588">MAPREDUCE-3588</a>.
     Blocker bug reported by acmurthy and fixed by acmurthy <br>
     <b>bin/yarn broken after MAPREDUCE-3366</b><br>
     <blockquote>bin/yarn broken after MAPREDUCE-3366, doesn&apos;t add yarn jars to classpath. As a result no servers can be started.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3595">MAPREDUCE-3595</a>.
     Major test reported by tomwhite and fixed by tomwhite (test)<br>
     <b>Add missing TestCounters#testCounterValue test from branch 1 to 0.23</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3596">MAPREDUCE-3596</a>.
     Blocker bug reported by raviprak and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>Sort benchmark got hang after completion of 99% map phase</b><br>
     <blockquote>Courtesy [~vinaythota]<br>{quote}<br>Ran sort benchmark couple of times and every time the job got hang after completion 99% map phase. There are some map tasks failed. Also it&apos;s not scheduled some of the pending map tasks.<br>Cluster size is 350 nodes.<br><br>Build Details:<br>==============<br><br>Compiled:       Fri Dec 9 16:25:27 PST 2011 by someone from branches/branch-0.23/hadoop-common-project/hadoop-common <br>ResourceManager version:        revision 1212681 by someone source checksum on Fri Dec 9 16:52:07 PST ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3604">MAPREDUCE-3604</a>.
     Blocker bug reported by acmurthy and fixed by acmurthy (contrib/streaming)<br>
     <b>Streaming&apos;s check for local mode is broken</b><br>
     <blockquote>Streaming isn&apos;t checking for mapreduce.framework.name as part of check for &apos;local&apos; mode.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3608">MAPREDUCE-3608</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>MAPREDUCE-3522 commit causes compilation to fail</b><br>
     <blockquote>There are compilation errors after MAPREDUCE-3522 was committed. Some more changes were need to webapps to fix the compilation issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3610">MAPREDUCE-3610</a>.
     Minor improvement reported by sho.shimauchi and fixed by sho.shimauchi <br>
     <b>Some parts in MR use old property dfs.block.size</b><br>
     <blockquote>Some parts in MR use old property dfs.block.size.<br>dfs.blocksize should be used instead.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3615">MAPREDUCE-3615</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>mapred ant test failures</b><br>
     <blockquote>The following mapred ant tests are failing.  This started on December 22nd.<br><br><br>    [junit] Running org.apache.hadoop.mapred.TestTrackerBlacklistAcrossJobs<br>    [junit] Running org.apache.hadoop.mapred.TestMiniMRDFSSort<br>    [junit] Running org.apache.hadoop.mapred.TestBadRecords<br>    [junit] Running org.apache.hadoop.mapred.TestClusterMRNotification<br>    [junit] Running org.apache.hadoop.mapred.TestDebugScript<br>    [junit] Running org.apache.hadoop.mapred.TestJobCleanup<br>    [junit] Running org.apac...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3616">MAPREDUCE-3616</a>.
     Major sub-task reported by vinodkv and fixed by vinodkv (mr-am, performance)<br>
     <b>Thread pool for launching containers in MR AM not expanding as expected</b><br>
     <blockquote>Found this while running some benchmarks on 350 nodes. The thread pool stays at 60 for a long time and only expands to 350 towards the fag end of the job.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3617">MAPREDUCE-3617</a>.
     Major bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>Remove yarn default values for resource manager and nodemanager principal</b><br>
     <blockquote>Default values should be empty since no use can be made of them without correct values defined.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3624">MAPREDUCE-3624</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>bin/yarn script adds jdk tools.jar to the classpath.</b><br>
     <blockquote>Thanks to Roman for pointing it out. Looks like we have the following lines in bin/yarn:<br><br>{code}<br>CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar<br>{code}<br><br>We dont really have a dependency on the tools jar. We should remove this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3625">MAPREDUCE-3625</a>.
     Critical bug reported by acmurthy and fixed by jlowe (mrv2)<br>
     <b>CapacityScheduler web-ui display of queue&apos;s used capacity is broken</b><br>
     <blockquote>The display of the queue&apos;s used capacity at runtime is broken because it display&apos;s &apos;used&apos; relative to the queue&apos;s capacity and not the parent&apos;s capacity as shown in the above attachment.<br><br>The display should be relative to parent&apos;s capacity and not leaf queues as everything else in the display is relative to parent&apos;s capacity.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3640">MAPREDUCE-3640</a>.
     Blocker sub-task reported by sseth and fixed by acmurthy (mrv2)<br>
     <b>AMRecovery should pick completed task form partial JobHistory files</b><br>
     <blockquote>Currently, if the JobHistory file has a partial record, AMRecovery will start from scratch. This will become more relevant after MAPREDUCE-3512.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3645">MAPREDUCE-3645</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv1)<br>
     <b>TestJobHistory fails</b><br>
     <blockquote>TestJobHistory fails.<br><br>&gt;&gt;&gt; org.apache.hadoop.mapred.TestJobHistory.testDoneFolderOnHDFS 	<br>&gt;&gt;&gt; org.apache.hadoop.mapred.TestJobHistory.testDoneFolderNotOnDefaultFileSystem 	<br>&gt;&gt;&gt; org.apache.hadoop.mapred.TestJobHistory.testHistoryFolderOnHDFS 	<br>&gt;&gt;&gt; org.apache.hadoop.mapred.TestJobHistory.testJobHistoryFile <br><br>It looks like this was introduced by MAPREDUCE-3349 and the issue is that the test expects the hostname to be in the format rackname/hostname, but with 3349 it split those apart into 2 diff...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3646">MAPREDUCE-3646</a>.
     Major bug reported by rramya and fixed by jeagles (client, mrv2)<br>
     <b>Remove redundant URL info from &quot;mapred job&quot; output</b><br>
     <blockquote>The URL information to track the job is printed for all the &quot;mapred job&quot;mrv2 commands. This information is redundant and has to be removed.<br><br>E.g:<br>{noformat}<br>-bash-3.2$ mapred job -list <br><br>Total jobs:3<br>JobId   State   StartTime       UserName        Queue   Priority        Maps    Reduces UsedContainers  RsvdContainers  UsedMem RsvdMem NeededMem       AM info<br>12/01/09 22:20:15 INFO mapred.ClientServiceDelegate: The url to track the job: &lt;RM host&gt;:8088/proxy/&lt;application ID 1&gt;/<br>&lt;job ID 1&gt;  RUNNI...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3648">MAPREDUCE-3648</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>TestJobConf failing</b><br>
     <blockquote>TestJobConf is failing:<br><br><br><br>testFindContainingJar <br>testFindContainingJarWithPlus <br><br>java.lang.ClassNotFoundException: ClassWithNoPackage<br>	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)<br>	at java.security.AccessController.doPrivileged(Native Method)<br>	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br>	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)<br>	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)<br>	at java.lang.Class.forName0(Native Method)<br>	at java.lang.Cla...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3649">MAPREDUCE-3649</a>.
     Blocker bug reported by mahadev and fixed by raviprak (mrv2)<br>
     <b>Job End notification gives an error on calling back.</b><br>
     <blockquote>When calling job end notification for oozie the AM fails with the following trace:<br><br>{noformat}<br>2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&amp;status=SUCCEEDED&amp; failed<br>java.net.UnknownServiceException: no content-type<br>	at java.net.URLConnection.getContentHandler(URLConnection.java:1192)<br>	at java.net.URLConnection.getContent(URLConnection.java:689)<br>	at org.a...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3651">MAPREDUCE-3651</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>TestQueueManagerRefresh fails</b><br>
     <blockquote>The following tests fail:<br>org.apache.hadoop.mapred.TestQueueManagerRefresh.testRefreshWithRemovedQueues <br>org.apache.hadoop.mapred.TestQueueManagerRefresh.testRefreshOfSchedulerProperties <br><br>It looks like its simply trying to remove one of the queues but the remove is failing.It looks like MAPREDUCE-3328. mapred queue -list output inconsistent and missing child queues - change the getChilren routine to do a new JobQueueInfo on each one when returning it which is making the remove routine fail s...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3652">MAPREDUCE-3652</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>org.apache.hadoop.mapred.TestWebUIAuthorization.testWebUIAuthorization fails</b><br>
     <blockquote>org.apache.hadoop.mapred.TestWebUIAuthorization.testWebUIAuthorization fails.<br><br>This is testing the old jsp web interfaces.  I think this test should just be removed.<br><br><br>Any objections?</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3657">MAPREDUCE-3657</a>.
     Minor bug reported by jlowe and fixed by jlowe (build, mrv2)<br>
     <b>State machine visualize build fails</b><br>
     <blockquote>Attempting to build the state machine graphs with {{mvn -Pvisualize compile}} fails for the resourcemanager and nodemanager projects.  The build fails because org.apache.commons.logging.LogFactory isn&apos;t in the classpath.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3664">MAPREDUCE-3664</a>.
     Minor bug reported by praveensripati and fixed by brandonli (documentation)<br>
     <b>HDFS Federation Documentation has incorrect configuration example</b><br>
     <blockquote>HDFS Federation documentation example (1) has the following<br><br>&lt;property&gt;<br>    &lt;name&gt;dfs.namenode.rpc-address.ns1&lt;/name&gt;<br>    &lt;value&gt;hdfs://nn-host1:rpc-port&lt;/value&gt;<br>&lt;/property&gt;<br><br>dfs.namenode.rpc-address.* should be set to hostname:port, hdfs:// should not be there.<br><br>(1) - http://hadoop.apache.org/common/docs/r0.23.0/hadoop-yarn/hadoop-yarn-site/Federation.html</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3669">MAPREDUCE-3669</a>.
     Blocker bug reported by tgraves and fixed by mahadev (mrv2)<br>
     <b>Getting a lot of PriviledgedActionException / SaslException when running a job</b><br>
     <blockquote>On a secure cluster, when running a job we are seeing a lot of PriviledgedActionException / SaslExceptions.  The job runs fine, its just the jobclient can&apos;t connect to the AM to get the progress information.<br><br>Its in a very tight loop retrying while getting the exceptions.<br><br>snip of the client log is:<br>12/01/13 15:33:45 INFO security.SecurityUtil: Acquired token Ident: 00 1c 68 61 64 6f 6f 70 71 61 40 44 45 56 2e 59 47<br>52 49 44 2e 59 41 48 4f 4f 2e 43 4f 4d 08 6d 61 70 72 65 64 71 61 00 8a 01 34...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3679">MAPREDUCE-3679</a>.
     Major improvement reported by mahadev and fixed by vinodkv (mrv2)<br>
     <b>AM logs and others should not automatically refresh after every 1 second.</b><br>
     <blockquote>If you are looking through the logs for AM or containers, the page is automatically refreshed after 1 second or so which makes it problematic to search through the page or debug using the content on the page. We should not refresh the logs page. There should be a button to manually refresh if the user needs to.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3681">MAPREDUCE-3681</a>.
     Critical bug reported by tgraves and fixed by acmurthy (mrv2)<br>
     <b>capacity scheduler LeafQueues calculate used capacity wrong</b><br>
     <blockquote>In the Capacity scheduler if you configure the queues to be hierarchical where you have root -&gt; parent queue -&gt; leaf queue, the leaf queue doesn&apos;t calculate the used capacity properly. It seems to be using the entire cluster memory rather then its parents memory capacity. <br><br>In updateResource in LeafQueue:<br>    setUsedCapacity(<br>        usedResources.getMemory() / (clusterResource.getMemory() * capacity));<br><br>I think the clusterResource.getMemory() should be something like getParentsMemory().</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3683">MAPREDUCE-3683</a>.
     Blocker bug reported by tgraves and fixed by acmurthy (mrv2)<br>
     <b>Capacity scheduler LeafQueues maximum capacity calculation issues</b><br>
     <blockquote>In the Capacity scheduler if you configure the queues to be hierarchical where you have root -&gt; parent queue -&gt; leaf queue, the leaf queue doesn&apos;t take into account its parents maximum capacity when calculate its own maximum capacity, instead it seems to use the parents capacity.  Looking at the code its using the parents absoluteCapacity and I think it should be using the parents absoluteMaximumCapacity.<br><br>It also seems to only use the parents capacity in the leaf queues max capacity calculat...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3684">MAPREDUCE-3684</a>.
     Major bug reported by tomwhite and fixed by tomwhite (client)<br>
     <b>LocalDistributedCacheManager does not shut down its thread pool</b><br>
     <blockquote>This was observed by running a Hive job in local mode. The job completed but the client process did not exit for 60 seconds.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3689">MAPREDUCE-3689</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>RM web UI doesn&apos;t handle newline in job name</b><br>
     <blockquote>a user submitted a mapreduce job with a newline (\n) in the job name. This caused the resource manager web ui to get a javascript exception when loading the application and scheduler pages and the pages were pretty well useless after that since they didn&apos;t load everything.  Note that this only happens when the data is returned in the JS_ARRAY, which is when you get over 100 applications.<br><br>errors:<br>Uncaught SyntaxError: Unexpected token ILLEGAL<br>Uncaught ReferenceError: appsData is not defined<br><br>...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3691">MAPREDUCE-3691</a>.
     Critical bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>webservices add support to compress response</b><br>
     <blockquote> The web services currently don&apos;t support header &apos;Accept-Encoding: gzip&apos;<br><br>Given that the responses have a lot of duplicate data like the property names in JSON or the tag names in XML, it should<br>compress very well, and would save on bandwidth and download time when fetching a potentially large response, like the<br>ones from ws/v1/cluster/apps and ws/v1/history/mapreduce/jobs</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3692">MAPREDUCE-3692</a>.
     Blocker improvement reported by eli and fixed by eli (mrv2)<br>
     <b>yarn-resourcemanager out and log files can get big</b><br>
     <blockquote>I&apos;m seeing 8gb resourcemanager out files and big log files, seeing lots of repeated logs (eg every rpc call or event) looks like we&apos;re being too verbose in  a couple of places.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3693">MAPREDUCE-3693</a>.
     Minor improvement reported by rvs and fixed by rvs (mrv2)<br>
     <b>Add admin env to mapred-default.xml</b><br>
     <blockquote>I have noticed that org.apache.hadoop.mapred.MapReduceChildJVM doesn&apos;t forward the value of -Djava.library.path= from the parent JVM to the child JVM. Thus if one wants to use native libraries for compression the only option seems to be to manually include relevant java.library.path settings into the mapred-site.xml (as mapred.[map|reduce].child.java.opts).<br><br>This seems to be a change in behavior compared to MR1 where TaskRunner.java used to do that:<br><br>{noformat}<br>String libraryPath = System.get...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3696">MAPREDUCE-3696</a>.
     Blocker bug reported by johnvijoe and fixed by johnvijoe (mrv2)<br>
     <b>MR job via oozie does not work on hadoop 23</b><br>
     <blockquote>NM throws an error on submitting an MR job via oozie on the latest Hadoop 23.<br>*Courtesy: Mona Chitnis (ooize)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3697">MAPREDUCE-3697</a>.
     Blocker bug reported by johnvijoe and fixed by mahadev (mrv2)<br>
     <b>Hadoop Counters API limits Oozie&apos;s working across different hadoop versions</b><br>
     <blockquote>Oozie uses Hadoop Counters API, by invoking Counters.getGroup(). However, in<br>hadoop 23, org.apache.hadoop.mapred.Counters does not implement getGroup(). Its<br>parent class AbstractCounters implements it. This is different from hadoop20X.<br>As a result, Oozie compiled with either hadoop version does not work with the<br>other version.<br>A specific scenario, Oozie compiled with .23 and run against 205, does not<br>update job status owing to a Counters API exception.<br><br>Will explicit re-compilation against th...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3698">MAPREDUCE-3698</a>.
     Blocker sub-task reported by sseth and fixed by mahadev (mrv2)<br>
     <b>Client cannot talk to the history server in secure mode</b><br>
     <blockquote>{noformat}<br>12/01/19 02:56:22 ERROR security.UserGroupInformation: PriviledgedActionException as:XXX@XXX(auth:KERBEROS) cause:java.io.IOException: Failed to specify server&apos;s Kerberos principal name<br>12/01/19 02:56:22 WARN security.UserGroupInformation: Not attempting to re-login since the last re-login was attempted less than 600 seconds before.<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3701">MAPREDUCE-3701</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Delete HadoopYarnRPC from 0.23 branch.</b><br>
     <blockquote>HadoopYarnRPC file exists in 0.23 (should have been removed with the new HadoopYarnProtoRPC). Trunk does not have this issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3702">MAPREDUCE-3702</a>.
     Critical bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>internal server error trying access application master via proxy with filter enabled</b><br>
     <blockquote>I had a hadoop.http.filter.initializers in place to do user authentication, but was purposely trying to let it bypass authentication on certain pages.  One of those was the proxy and the application master main page. When I then tried to go to the application master through the proxy it throws an internal server error:<br><br>Problem accessing /mapreduce. Reason:<br><br>    INTERNAL_SERVER_ERROR<br>Caused by:<br><br>java.lang.NullPointerException<br>	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFi...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3705">MAPREDUCE-3705</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>ant build fails on 0.23 branch </b><br>
     <blockquote>running the ant build in mapreduce on the latest 23 branch fails.  Looks like the ivy properties file still has 0.24.0 and then the gridmix dependencies need to have rumen as dependency.<br><br>The gridmix errors look like:<br>   [javac] /home/tgraves/anttest/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java:249: cannot find symbol<br>    [javac] symbol  : class JobStoryProducer<br>    [javac] location: class org.apache.hadoop.mapred.gridmix...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3708">MAPREDUCE-3708</a>.
     Major bug reported by kam_iitkgp and fixed by kamesh (mrv2)<br>
     <b>Metrics: Incorrect Apps Submitted Count</b><br>
     <blockquote>Submitted an application with the following configuration<br>{code:xml}<br>&lt;property&gt;<br> &lt;name&gt;yarn.resourcemanager.am.max-retries&lt;/name&gt;<br> &lt;value&gt;2&lt;/value&gt;<br>&lt;/property&gt;<br>{code}<br>In the above case, application had failed first time. So AM attempted the same application again. <br>While attempting the same application, *Apps Submitted* counter also has been incremented.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3709">MAPREDUCE-3709</a>.
     Major bug reported by eli and fixed by hitesh (mrv2, test)<br>
     <b>TestDistributedShell is failing</b><br>
     <blockquote>TestDistributedShell#testDSShell is failing the assert on line 90 on branch-23.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3712">MAPREDUCE-3712</a>.
     Blocker bug reported by raviprak and fixed by mahadev (mrv2)<br>
     <b>The mapreduce tar does not contain the hadoop-mapreduce-client-jobclient-tests.jar. </b><br>
     <blockquote>Working MRv1 tests were moved into the maven build as part of MAPREDUCE-3582. Some classes like MRBench, SleepJob, FailJob which are essential for QE got moved to jobclient-tests.jar. However the tar.gz file does not contain this jar.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3717">MAPREDUCE-3717</a>.
     Blocker bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>JobClient test jar has missing files to run all the test programs.</b><br>
     <blockquote>Looks like MAPREDUCE-3582 forgot to move couple of files from the ant builds. The current test jar from jobclient does not work. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3718">MAPREDUCE-3718</a>.
     Major sub-task reported by vinodkv and fixed by hitesh (mrv2, performance)<br>
     <b>Default AM heartbeat interval should be one second</b><br>
     <blockquote>Helps in improving app performance. RM should be able to handle this, as the heartbeats aren&apos;t really costly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3721">MAPREDUCE-3721</a>.
     Blocker bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>Race in shuffle can cause it to hang</b><br>
     <blockquote>If all current {{Fetcher}}s complete while an in-memory merge is in progress - shuffle could hang. <br>Specifically - if the memory freed by an in-memory merge does not bring {{MergeManager.usedMemory}} below {{MergeManager.memoryLimit}} and all current Fetchers complete before the in-memory merge completes, another in-memory merge will not be triggered - and shuffle will hang. (All new fetchers are asked to WAIT).<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3723">MAPREDUCE-3723</a>.
     Major bug reported by kam_iitkgp and fixed by kamesh (mrv2, test, webapps)<br>
     <b>TestAMWebServicesJobs &amp; TestHSWebServicesJobs incorrectly asserting tests</b><br>
     <blockquote>While testing a patch for one of the MR issues, I found TestAMWebServicesJobs &amp; TestHSWebServicesJobs incorrectly asserting tests. <br>Moreover tests may fail if<br>{noformat}<br>	index of counterGroups &gt; #counters in a particular counterGroup<br>{noformat}<br>{code:title=TestAMWebServicesJobs.java|borderStyle=solid}<br>for (int j = 0; j &lt; counters.length(); j++) {<br> JSONObject counter = counters.getJSONObject(i);<br>{code}<br><br>where is *i* is index of outer loop. It should be *j* instead of *i*.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3727">MAPREDUCE-3727</a>.
     Critical bug reported by tucu00 and fixed by tucu00 (security)<br>
     <b>jobtoken location property in jobconf refers to wrong jobtoken file</b><br>
     <blockquote>Oozie launcher job (for MR/Pig/Hive/Sqoop action) reads the location of the jobtoken file from the *HADOOP_TOKEN_FILE_LOCATION* ENV var and seeds it as the *mapreduce.job.credentials.binary* property in the jobconf that will be used to launch the real (MR/Pig/Hive/Sqoop) job.<br><br>The MR/Pig/Hive/Sqoop submission code (via Hadoop job submission) uses correctly the injected *mapreduce.job.credentials.binary* property to load the credentials and submit their MR jobs.<br><br>The problem is that the *mapre...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3733">MAPREDUCE-3733</a>.
     Major bug reported by mahadev and fixed by mahadev <br>
     <b>Add Apache License Header to hadoop-distcp/pom.xml</b><br>
     <blockquote>Looks like I missed the Apache Headers in the review. Adding it now.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3735">MAPREDUCE-3735</a>.
     Blocker bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Add distcp jar to the distribution (tar)</b><br>
     <blockquote>Distcp jar isnt getting added to the tarball as of now. We need to add it along with archives/streaming and others.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3737">MAPREDUCE-3737</a>.
     Critical bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>The Web Application Proxy&apos;s is not documented very well</b><br>
     <blockquote>The Web Application Proxy is a security feature, but there is no documentation for what it does, why it does it, and more importantly what attacks it is known not protect against.  This is so that anyone addopting Hadoop can know exactly what they potential security issues they may encounter.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3742">MAPREDUCE-3742</a>.
     Blocker bug reported by jlowe and fixed by jlowe (mrv2)<br>
     <b>&quot;yarn logs&quot; command fails with ClassNotFoundException</b><br>
     <blockquote>Executing &quot;yarn logs&quot; at a shell prompt fails with this error:<br><br>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogDumper<br>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogDumper<br>	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)<br>	at java.security.AccessController.doPrivileged(Native Method)<br>	at java.net.URLClassLoader.findClass(U...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3744">MAPREDUCE-3744</a>.
     Blocker bug reported by jlowe and fixed by jlowe (mrv2)<br>
     <b>Unable to retrieve application logs via &quot;yarn logs&quot; or &quot;mapred job -logs&quot;</b><br>
     <blockquote>Trying to retrieve application logs via the &quot;yarn logs&quot; shell command results in an error similar to this:<br><br>Exception in thread &quot;main&quot; java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.<br>	at org.apache.hadoop.fs.Hdfs$DirListingIterator.&lt;init&gt;(Hdfs.java:226)<br>	at org.apache.hadoop.fs.Hdfs$DirListingIterator.&lt;init&gt;(Hdfs.java:217)<br>	at org.apache.hadoop.fs.Hdfs$2.&lt;init&gt;(Hdfs.java:192)<br>	at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)<br>	at org.a...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3747">MAPREDUCE-3747</a>.
     Major bug reported by rramya and fixed by acmurthy (mrv2)<br>
     <b>Memory Total is not refreshed until an app is launched</b><br>
     <blockquote>Memory Total on the RM UI is not refreshed until an application is launched. This is a problem when the cluster is started for the first time or when there are any lost/decommissioned NMs.<br>When the cluster is started for the first time, Active Nodes is &gt; 0 but the Memory Total=0. Also when there are any lost/decommissioned nodes, Memory Total has wrong value.<br>This is a useful tool for cluster admins and has to be updated correctly without having the need to submit an app each time.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3748">MAPREDUCE-3748</a>.
     Minor bug reported by rramya and fixed by rramya (mrv2)<br>
     <b>Move CS related nodeUpdate log messages to DEBUG</b><br>
     <blockquote>Currently, the RM has nodeUpdate logs per NM per second such as the following:<br>2012-01-27 21:51:32,429 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: &lt;nodemanager1&gt;:&lt;port1&gt; clusterResources: memory: 57344<br>2012-01-27 21:51:32,510 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: &lt;nodemanager2&gt;:&lt;port2&gt; clusterResources: memory: 57344<br>2012-01-27 21:51:33,094 INFO org.apache.hadoop.yarn.server...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3749">MAPREDUCE-3749</a>.
     Blocker bug reported by tomwhite and fixed by tomwhite (mrv2)<br>
     <b>ConcurrentModificationException in counter groups</b><br>
     <blockquote>Iterating over a counter&apos;s groups while adding more groups will cause a ConcurrentModificationException.<br><br>This was found while running Hive unit tests against a recent 0.23 version.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3756">MAPREDUCE-3756</a>.
     Major improvement reported by acmurthy and fixed by hitesh (mrv2)<br>
     <b>Make single shuffle limit configurable</b><br>
     <blockquote>Make single shuffle limit configurable, currently it&apos;s hard-coded.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3759">MAPREDUCE-3759</a>.
     Major bug reported by rramya and fixed by vinodkv (mrv2)<br>
     <b>ClassCastException thrown in -list-active-trackers when there are a few unhealthy nodes</b><br>
     <blockquote>When there are a few blacklisted nodes in the cluster, &quot;bin/mapred job -list-active-trackers&quot; throws &quot;java.lang.ClassCastException: org.apache.hadoop.yarn.server.resourcemanager.resource.Resources$1 cannot be cast to org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl&quot;</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3762">MAPREDUCE-3762</a>.
     Critical bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Resource Manager fails to come up with default capacity scheduler configs.</b><br>
     <blockquote>Thanks to [~harip] for pointing out the issue. This is the stack trace for bringing up RM with default CS configs:<br><br>{code}<br>java.lang.IllegalArgumentException: Illegal value  of maximumCapacity -0.01 used in call to setMaxCapacity for queue default<br>        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils.checkMaxCapacity(CSQueueUtils.java:28)<br>        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs(LeafQueue.java:21...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3764">MAPREDUCE-3764</a>.
     Critical bug reported by sseth and fixed by acmurthy (mrv2)<br>
     <b>AllocatedGB etc metrics incorrect if min-allocation-mb isn&apos;t a multiple of 1GB</b><br>
     <blockquote>MutableGaugeInt incremented as {{allocatedGB.incr(res.getMemory() / GB * containers);}}<br><br>Setting yarn.scheduler.capacity.minimum-allocation-mb to 1536 - each increment is counted as 1GB.<br>Trying to analyze the metrics - looks like the cluster is never over 67-68% utilized, depending on high ram requests.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3765">MAPREDUCE-3765</a>.
     Minor bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>FifoScheduler does not respect yarn.scheduler.fifo.minimum-allocation-mb setting</b><br>
     <blockquote>FifoScheduler uses default min 1 GB regardless of the configuration value set for minimum memory allocation.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3771">MAPREDUCE-3771</a>.
     Major improvement reported by acmurthy and fixed by acmurthy <br>
     <b>Port MAPREDUCE-1735 to trunk/0.23</b><br>
     <blockquote>Per discussion in general@, we should port MAPREDUCE-1735 to 0.23 &amp; trunk to &apos;undeprecate&apos; old mapred api:<br>http://s.apache.org/undeprecate-mapred-apis</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3775">MAPREDUCE-3775</a>.
     Minor bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Change MiniYarnCluster to escape special chars in testname</b><br>
     <blockquote>When using MiniYarnCluster with the testname set to a nested classname, the &quot;$&quot; within the class name creates issues with the container launch scripts as they try to expand the $... within the paths/variables in use.  </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3780">MAPREDUCE-3780</a>.
     Blocker bug reported by rramya and fixed by hitesh (mrv2)<br>
     <b>RM assigns containers to killed applications</b><br>
     <blockquote>RM attempts to assign containers to killed applications. The applications were killed when they were inactive and waiting for AM allocation.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3791">MAPREDUCE-3791</a>.
     Major bug reported by rvs and fixed by mahadev (documentation, mrv2)<br>
     <b>can&apos;t build site in hadoop-yarn-server-common</b><br>
     <blockquote>Here&apos;s how to reproduce:<br><br>{noformat}<br>$ mvn site site:stage -DskipTests -DskipTest -DskipITs<br>....<br>main:<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Reactor Summary:<br>[INFO] <br>[INFO] Apache Hadoop Main ................................ SUCCESS [49.017s]<br>[INFO] Apache Hadoop Project POM ......................... SUCCESS [5.152s]<br>[INFO] Apache Hadoop Annotations ......................... SUCCESS [4.973s]<br>[INFO] Apache Hadoop Project Dist POM ..................</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3794">MAPREDUCE-3794</a>.
     Major bug reported by tomwhite and fixed by tomwhite (mrv2)<br>
     <b>Support mapred.Task.Counter and mapred.JobInProgress.Counter enums for compatibility</b><br>
     <blockquote>The new counters are mapreduce.TaskCounter and mapreduce.JobCounter, but we should support the old ones too since they are public in Hadoop 1.x.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3795">MAPREDUCE-3795</a>.
     Major bug reported by vinodkv and fixed by vinodkv (mrv2)<br>
     <b>&quot;job -status&quot; command line output is malformed</b><br>
     <blockquote>Misses new lines after numMaps and numReduces. Caused by MAPREDUCE-3720.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3803">MAPREDUCE-3803</a>.
     Major test reported by raviprak and fixed by raviprak (build)<br>
     <b>HDFS-2864 broke ant compilation</b><br>
     <blockquote>compile:<br>     [echo] contrib: raid<br>    [javac] &lt;somePath&gt;/hadoop-mapreduce-project/src/contrib/build-contrib.xml:194: warning: &apos;includeantruntime&apos; was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds<br>    [javac] Compiling 28 source files to &lt;somepath&gt;/hadoop-mapreduce-project/build/contrib/raid/classes<br>    [javac] &lt;somepath&gt;/hadoop-mapreduce-project/src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/datanode/RaidBlockSender.java:111: cannot find symbol<br> ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3809">MAPREDUCE-3809</a>.
     Blocker sub-task reported by sseth and fixed by sseth (mrv2)<br>
     <b>Tasks may take upto 3 seconds to exit after completion</b><br>
     <blockquote>Task.TaskReporter.stopCommunicationThread can end up waiting for a thread.sleep(3000) before stopping the thread.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3810">MAPREDUCE-3810</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (mrv2, performance)<br>
     <b>MR AM&apos;s ContainerAllocator is assigning the allocated containers very slowly</b><br>
     <blockquote>This is mostly due to logging and other not-so-cheap operations we are doing as part of the AM-&gt;RM heartbeat cycle.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3811">MAPREDUCE-3811</a>.
     Critical task reported by sseth and fixed by sseth (mrv2)<br>
     <b>Make the Client-AM IPC retry count configurable</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3813">MAPREDUCE-3813</a>.
     Major sub-task reported by vinodkv and fixed by vinodkv (mrv2, performance)<br>
     <b>RackResolver should maintain a cache to avoid repetitive lookups.</b><br>
     <blockquote>With the current code, during task creation, we repeatedly resolve hosts and RackResolver doesn&apos;t cache any of the results. Caching will improve performance.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3814">MAPREDUCE-3814</a>.
     Major bug reported by acmurthy and fixed by acmurthy (mrv1, mrv2)<br>
     <b>MR1 compile fails</b><br>
     <blockquote>$ ant veryclean all-jars -Dversion=0.23.1 -Dresolvers=internal<br><br><br>BUILD FAILED<br>/grid/0/dev/acm/hadoop-0.23/hadoop-mapreduce-project/build.xml:537: srcdir &quot;/grid/0/dev/acm/hadoop-0.23/hadoop-mapreduce-project/src/test/mapred/testjar&quot; does not exist!<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3817">MAPREDUCE-3817</a>.
     Major bug reported by arpitgupta and fixed by arpitgupta (mrv2)<br>
     <b>bin/mapred command cannot run distcp and archive jobs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3826">MAPREDUCE-3826</a>.
     Major bug reported by arpitgupta and fixed by jeagles (mrv2)<br>
     <b>RM UI when loaded throws a message stating Data Tables warning and then the column sorting stops working</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3833">MAPREDUCE-3833</a>.
     Major bug reported by jlowe and fixed by jlowe (mrv2)<br>
     <b>Capacity scheduler queue refresh doesn&apos;t recompute queue capacities properly</b><br>
     <blockquote>Refreshing the capacity scheduler configuration (e.g.: via yarn rmadmin -refreshQueues) can fail to compute the proper absolute capacity for leaf queues.</blockquote></li>


</ul>



<h2>Changes since Hadoop 0.22</h2>

<ul>
<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7778">HADOOP-7778</a>.
     Major bug reported by tomwhite and fixed by tomwhite <br>
     <b>FindBugs warning in Token.getKind()</b><br>
     <blockquote>From https://builds.apache.org/job/PreCommit-HADOOP-Build/330//artifact/trunk/hadoop-common-project/patchprocess/newPatchFindbugsWarningshadoop-common.html<br><br>bq. org.apache.hadoop.security.token.Token.getKind() is unsynchronized, org.apache.hadoop.security.token.Token.setKind(Text) is synchronized<br><br>Looks like this was introduced by MAPREDUCE-2764.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7772">HADOOP-7772</a>.
     Trivial improvement reported by stevel@apache.org and fixed by stevel@apache.org <br>
     <b>javadoc the topology classes</b><br>
     <blockquote>To help people understand and make changes to the Topology classes, their javadocs could be rounded off.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7771">HADOOP-7771</a>.
     Blocker bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>NPE when running hdfs dfs -copyToLocal, -get etc</b><br>
     <blockquote>NPE when running hdfs dfs -copyToLocal if the destination directory does not exist. The behavior in branch-0.20-security is to create the directory and copy/get the contents from source.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7770">HADOOP-7770</a>.
     Blocker bug reported by raviprak and fixed by raviprak (fs)<br>
     <b>ViewFS getFileChecksum throws FileNotFoundException for files in /tmp and /user</b><br>
     <blockquote>Thanks to Rohini Palaniswamy for discovering this bug. To quote<br>bq. When doing getFileChecksum for path /user/hadoopqa/somefile, it is trying to fetch checksum for /user/user/hadoopqa/somefile. If /tmp/file, it is trying /tmp/tmp/file. Works fine for other FS operations.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7766">HADOOP-7766</a>.
     Major bug reported by jnp and fixed by jnp <br>
     <b>The auth to local mappings are not being respected, with webhdfs and security enabled.</b><br>
     <blockquote>KerberosAuthenticationHandler reloads the KerberosName statically and overrides the auth to local mappings. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7764">HADOOP-7764</a>.
     Blocker bug reported by jeagles and fixed by jeagles <br>
     <b>Allow both ACL list and global path spec filters to HttpServer</b><br>
     <blockquote>HttpServer allows setting global path spec filters in one constructor and ACL list in another constructor. Having both set in HttpServer is not user settable either by public API or constructor.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7763">HADOOP-7763</a>.
     Major improvement reported by tomwhite and fixed by tomwhite (documentation)<br>
     <b>Add top-level navigation to APT docs</b><br>
     <blockquote>We need navigation menus for the APT docs that have been written so far.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7753">HADOOP-7753</a>.
     Major sub-task reported by tlipcon and fixed by tlipcon (io, native, performance)<br>
     <b>Support fadvise and sync_data_range in NativeIO, add ReadaheadPool class</b><br>
     <blockquote>This JIRA adds JNI wrappers for sync_data_range and posix_fadvise. It also implements a ReadaheadPool class for future use from HDFS and MapReduce.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7749">HADOOP-7749</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (util)<br>
     <b>Add NetUtils call which provides more help in exception messages</b><br>
     <blockquote>In setting up MR2, I accidentally had a bad configuration value specified for one of the IP configs. I was getting a NumberFormatException parsing this config, but no indication as to what config value was being fetched. This JIRA is to add an API to NetUtils.createSocketAddr which takes the configuration name, so that any exceptions thrown will point back to where the user needs to fix it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7745">HADOOP-7745</a>.
     Major bug reported by raviprak and fixed by raviprak <br>
     <b>I switched variable names in HADOOP-7509</b><br>
     <blockquote>As Aaron pointed out on https://issues.apache.org/jira/browse/HADOOP-7509?focusedCommentId=13126725&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13126725 I stupidly swapped CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION with CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7744">HADOOP-7744</a>.
     Major bug reported by jeagles and fixed by jeagles (test)<br>
     <b>Incorrect exit code for hadoop-core-test tests when exception thrown</b><br>
     <blockquote>Please see MAPREDUCE-3179 for a full description.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7743">HADOOP-7743</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Add Maven profile to create a full source tarball</b><br>
     <blockquote>Currently we are building binary distributions only.<br><br>We should also build a full source distribution from where Hadoop can be built.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7740">HADOOP-7740</a>.
     Minor bug reported by arpitgupta and fixed by arpitgupta (conf)<br>
     <b>security audit logger is not on by default, fix the log4j properties to enable the logger</b><br>
     <blockquote>                                              Fixed security audit logger configuration. (Arpit Gupta via Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7737">HADOOP-7737</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>normalize hadoop-mapreduce &amp; hadoop-dist dist/tar build with common/hdfs</b><br>
     <blockquote>Normalize the build fo hadoop-mapreduce and hadoop-dist with hadoop-common and hadoop-hdfs making the -Pdist and -Dtar maven options to be consistent.<br><br>* -Pdist should create the layout<br>* -Dtar should create the TAR<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7728">HADOOP-7728</a>.
     Major bug reported by rramya and fixed by rramya (conf)<br>
     <b>hadoop-setup-conf.sh should be modified to enable task memory manager</b><br>
     <blockquote>                                              Enable task memory management to be configurable via hadoop config setup script.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7724">HADOOP-7724</a>.
     Major bug reported by gkesavan and fixed by arpitgupta <br>
     <b>hadoop-setup-conf.sh should put proxy user info into the core-site.xml </b><br>
     <blockquote>                                              Fixed hadoop-setup-conf.sh to put proxy user in core-site.xml.  (Arpit Gupta via Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7721">HADOOP-7721</a>.
     Major bug reported by arpitgupta and fixed by jnp <br>
     <b>dfs.web.authentication.kerberos.principal expects the full hostname and does not replace _HOST with the hostname</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7720">HADOOP-7720</a>.
     Major improvement reported by arpitgupta and fixed by arpitgupta (conf)<br>
     <b>improve the hadoop-setup-conf.sh to read in the hbase user and setup the configs</b><br>
     <blockquote>                                              Added parameter for HBase user to setup config script. (Arpit Gupta via Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7709">HADOOP-7709</a>.
     Major improvement reported by jeagles and fixed by jeagles <br>
     <b>Running a set of methods in a Single Test Class</b><br>
     <blockquote>Instead of running every test method in a class, limit to specific testing methods as describe in the link below.<br><br>http://maven.apache.org/plugins/maven-surefire-plugin/examples/single-test.html<br><br>Upgrade to the latest version of maven-surefire-plugin that has this feature.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7708">HADOOP-7708</a>.
     Critical bug reported by arpitgupta and fixed by eyang (conf)<br>
     <b>config generator does not update the properties file if on exists already</b><br>
     <blockquote>                                              Fixed hadoop-setup-conf.sh to handle config file consistently.  (Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7707">HADOOP-7707</a>.
     Major improvement reported by arpitgupta and fixed by arpitgupta (conf)<br>
     <b>improve config generator to allow users to specify proxy user, turn append on or off, turn webhdfs on or off</b><br>
     <blockquote>                                              Added toggle for dfs.support.append, webhdfs and hadoop proxy user to setup config script. (Arpit Gupta via Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7705">HADOOP-7705</a>.
     Minor new feature reported by stevel@apache.org and fixed by stevel@apache.org (util)<br>
     <b>Add a log4j back end that can push out JSON data, one per line</b><br>
     <blockquote>If we had a back end for Log4j that pushed out log events in single line JSON content, we&apos;d have something that is fairly straightforward to machine parse. If: it may be harder to do than expected. Once working HADOOP-6244 could use it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7691">HADOOP-7691</a>.
     Major bug reported by gkesavan and fixed by eyang <br>
     <b>hadoop deb pkg should take a diff group id</b><br>
     <blockquote>                                              Fixed conflict uid for install packages. (Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7684">HADOOP-7684</a>.
     Major bug reported by eyang and fixed by eyang (scripts)<br>
     <b>jobhistory server and secondarynamenode should have init.d script</b><br>
     <blockquote>                                              Added init.d script for jobhistory server and secondary namenode. (Eric Yang)<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7681">HADOOP-7681</a>.
     Minor bug reported by arpitgupta and fixed by arpitgupta (conf)<br>
     <b>log4j.properties is missing properties for security audit and hdfs audit should be changed to info</b><br>
     <blockquote>(Arpit Gupta via Eric Yang)<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7671">HADOOP-7671</a>.
     Major bug reported by raviprak and fixed by raviprak <br>
     <b>Add license headers to hadoop-common/src/main/packages/templates/conf/</b><br>
     <blockquote>hadoop-common/src/main/packages/templates/conf/ not in the exclude list for apache-rat plugin . This causes 10 release audit warnings for missing license headers (in the properties and xml files like hdfs-site.xml)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7668">HADOOP-7668</a>.
     Minor improvement reported by sureshms and fixed by stevel@apache.org (util)<br>
     <b>Add a NetUtils method that can tell if an InetAddress belongs to local host</b><br>
     <blockquote>                                              closing again<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7664">HADOOP-7664</a>.
     Minor improvement reported by raviprak and fixed by raviprak (conf)<br>
     <b>o.a.h.conf.Configuration complains of overriding final parameter even if the value with which its attempting to override is the same. </b><br>
     <blockquote>o.a.h.conf.Configuration complains of overriding final parameter even if the value with which its attempting to override is the same. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7663">HADOOP-7663</a>.
     Major bug reported by mayank_bansal and fixed by mayank_bansal (test)<br>
     <b>TestHDFSTrash failing on 22</b><br>
     <blockquote>Seems to have started failing recently in many commit builds as well as the last two nightly builds of 22:<br>https://builds.apache.org/hudson/job/Hadoop-Hdfs-22-branch/51/testReport/org.apache.hadoop.hdfs/TestHDFSTrash/testTrashEmptier/<br><br>https://issues.apache.org/jira/browse/HDFS-1967</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7662">HADOOP-7662</a>.
     Major bug reported by tgraves and fixed by tgraves <br>
     <b>logs servlet should use pathspec of /*</b><br>
     <blockquote>The logs servlet in HttpServer should use a pathspec of /* instead of /.<br>      logContext.addServlet(AdminAuthorizedServlet.class, &quot;/*&quot;);<br><br>In making the changes for the yarn webapps (MAPREDUCE-2999), I registered a webapp to use &quot;/&quot;.  This blocked the /logs servlet from working.  because both had a pathSpec of &quot;/&quot; and the guice filter seemed to take precendence.  Changing the pathspec of the logs servlet to /* fixes the issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7658">HADOOP-7658</a>.
     Major bug reported by gkesavan and fixed by eyang <br>
     <b>to fix hadoop config template</b><br>
     <blockquote>hadoop rpm config template by default sets the HADOOP_SECURE_DN_USER, HADOOP_SECURE_DN_LOG_DIR &amp; HADOOP_SECURE_DN_PID_DIR <br>the above values should only be set for secured deployment ; <br># On secure datanodes, user to run the datanode as after dropping privileges<br>export HADOOP_SECURE_DN_USER=${HADOOP_HDFS_USER}<br><br># Where log files are stored.  $HADOOP_HOME/logs by default.<br>export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}/$USER<br><br># Where log files are stored in the secure data environment.<br>export HADOOP_SE...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7655">HADOOP-7655</a>.
     Major improvement reported by arpitgupta and fixed by arpitgupta <br>
     <b>provide a small validation script that smoke tests the installed cluster</b><br>
     <blockquote>                                              Committed to trunk and v23, since code reviewed by Eric.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7642">HADOOP-7642</a>.
     Major improvement reported by tucu00 and fixed by tomwhite (build)<br>
     <b>create hadoop-dist module where TAR stitching would happen</b><br>
     <blockquote>Instead having a post build script that stitches common&amp;hdfs&amp;mmr, this should be done as part of the build when running &apos;mvn package -Pdist -Dtar&apos;<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7639">HADOOP-7639</a>.
     Major bug reported by tgraves and fixed by tgraves <br>
     <b>yarn ui not properly filtered in HttpServer</b><br>
     <blockquote>Currently httpserver only has .html&quot;, &quot;.jsp as user facing urls when you add a filter. For the new web framework in yarn, the pages no longer have the *.html or *.jsp and thus they are not properly being filtered.  The yarn ui just uses paths - for it would be serve:port/yarn/*</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7637">HADOOP-7637</a>.
     Major bug reported by eyang and fixed by eyang (build)<br>
     <b>Fair scheduler configuration file is not bundled in RPM</b><br>
     <blockquote>205 build of tar is fine, but rpm failed with:<br><br>{noformat}<br>      [rpm] Processing files: hadoop-0.20.205.0-1<br>      [rpm] warning: File listed twice: /usr/libexec<br>      [rpm] warning: File listed twice: /usr/libexec/hadoop-config.sh<br>      [rpm] warning: File listed twice: /usr/libexec/jsvc.i386<br>      [rpm] Checking for unpackaged file(s): /usr/lib/rpm/check-files /tmp/hadoop_package_build_hortonfo/BUILD<br>      [rpm] error: Installed (but unpackaged) file(s) found:<br>      [rpm]    /etc/hadoop/fai...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7633">HADOOP-7633</a>.
     Major bug reported by arpitgupta and fixed by eyang (conf)<br>
     <b>log4j.properties should be added to the hadoop conf on deploy</b><br>
     <blockquote>currently the log4j properties are not present in the hadoop conf dir. We should add them so that log rotation happens appropriately and also define other logs that hadoop can generate for example the audit and the auth logs as well as the mapred summary logs etc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7631">HADOOP-7631</a>.
     Major bug reported by rramya and fixed by eyang (conf)<br>
     <b>In mapred-site.xml, stream.tmpdir is mapped to ${mapred.temp.dir} which is undeclared.</b><br>
     <blockquote>Streaming jobs seem to fail with the following exception:<br><br>{noformat}<br>Exception in thread &quot;main&quot; java.io.IOException: No such file or directory<br>        at java.io.UnixFileSystem.createFileExclusively(Native Method)<br>        at java.io.File.checkAndCreate(File.java:1704)<br>        at java.io.File.createTempFile(File.java:1792)<br>        at org.apache.hadoop.streaming.StreamJob.packageJobJar(StreamJob.java:603)<br>        at org.apache.hadoop.streaming.StreamJob.setJobConf(StreamJob.java:798)<br>        a...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7630">HADOOP-7630</a>.
     Major bug reported by arpitgupta and fixed by eyang (conf)<br>
     <b>hadoop-metrics2.properties should have a property *.period set to a default value foe metrics</b><br>
     <blockquote>currently the hadoop-metrics2.properties file does not have a value set for *.period<br><br>This property is useful for metrics to determine when the property will refresh. We should set it to default of 60</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7629">HADOOP-7629</a>.
     Major bug reported by phunt and fixed by tlipcon <br>
     <b>regression with MAPREDUCE-2289 - setPermission passed immutable FsPermission (rpc failure)</b><br>
     <blockquote>MAPREDUCE-2289 introduced the following change:<br><br>{noformat}<br>+        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);<br>{noformat}<br><br>JOB_DIR_PERMISSION is an immutable FsPermission which cannot be used in RPC calls, it results in the following exception:<br><br>{noformat}<br>2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1<br>java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.&lt;init&gt;()<br>   ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7627">HADOOP-7627</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (metrics, test)<br>
     <b>Improve MetricsAsserts to give more understandable output on failure</b><br>
     <blockquote>In developing a test case that uses MetricsAsserts, I had two issues:<br>1) the error output in the case that an assertion failed does not currently give any information as to the _actual_ value of the metric<br>2) there is no way to retrieve the metric variable (eg to assert that the sum of a metric over all DNs is equal to some value)<br><br>This JIRA is to improve this test class to fix the above issues.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7626">HADOOP-7626</a>.
     Major bug reported by eyang and fixed by eyang (scripts)<br>
     <b>Allow overwrite of HADOOP_CLASSPATH and HADOOP_OPTS</b><br>
     <blockquote>Quote email from Ashutosh Chauhan:<br><br>bq. There is a bug in hadoop-env.sh which prevents hcatalog server to start in secure settings. Instead of adding classpath, it overrides them. I was not able to verify where the bug belongs to, in HMS or in hadoop scripts. Looks like hadoop-env.sh is generated from hadoop-env.sh.template in installation process by HMS. Hand crafted patch follows:<br><br>bq. - export HADOOP_CLASSPATH=$f<br>bq. +export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:$f<br><br>bq. -export HADOOP_OPTS=...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7612">HADOOP-7612</a>.
     Major improvement reported by tomwhite and fixed by tomwhite (build)<br>
     <b>Change test-patch to run tests for all nested modules</b><br>
     <blockquote>HADOOP-7561 changed the behaviour of test-patch to run tests for changed modules, however this was assuming a flat structure. Given the nested maven hierarchy we should always run all the common tests for any common change, all the HDFS tests for any HDFS change, and all the MapReduce tests for any MapReduce change.<br><br>In addition, we should do a top-level build to test compilation after any change.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7610">HADOOP-7610</a>.
     Major bug reported by eyang and fixed by eyang (scripts)<br>
     <b>/etc/profile.d does not exist on Debian</b><br>
     <blockquote>As part of post installation script, there is a symlink created in /etc/profile.d/hadoop-env.sh to source /etc/hadoop/hadoop-env.sh.  Therefore, users do not need to configure HADOOP_* environment.  Unfortunately, /etc/profile.d only exists in Ubuntu.  [Section 9.9 of the Debian Policy|http://www.debian.org/doc/debian-policy/ch-opersys.html#s9.9] states:<br><br>{quote}<br>A program must not depend on environment variables to get reasonable defaults. (That&apos;s because these environment variables would ha...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7608">HADOOP-7608</a>.
     Major bug reported by tucu00 and fixed by tucu00 (io)<br>
     <b>SnappyCodec check for Hadoop native lib is wrong</b><br>
     <blockquote>Currently SnappyCodec is doing:<br><br>{code}<br>  public static boolean isNativeSnappyLoaded(Configuration conf) {<br>    return LoadSnappy.isLoaded() &amp;&amp; conf.getBoolean(<br>        CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY,<br>        CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_DEFAULT);<br>  }<br>{code}<br><br>But the conf check is wrong as it defaults to true. Instead it should use *NativeCodeLoader.isNativeCodeLoaded()*</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7606">HADOOP-7606</a>.
     Major bug reported by atm and fixed by tucu00 (test)<br>
     <b>Upgrade Jackson to version 1.7.1 to match the version required by Jersey</b><br>
     <blockquote>As of 2 days ago, 13 tests started failing, all with errors in Avro-related tests.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7604">HADOOP-7604</a>.
     Critical bug reported by mahadev and fixed by mahadev <br>
     <b>Hadoop Auth examples pom in 0.23 point to 0.24 versions.</b><br>
     <blockquote>hadoop-auth-examples/pom.xml has references to 0.24 in the 0.23 branch.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7603">HADOOP-7603</a>.
     Major bug reported by eyang and fixed by eyang <br>
     <b>Set default hdfs, mapred uid, and hadoop group gid for RPM packages</b><br>
     <blockquote>                                              Set hdfs uid, mapred uid, and hadoop gid to fixed numbers (201, 202, and 123, respectively).<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7599">HADOOP-7599</a>.
     Major bug reported by eyang and fixed by eyang (scripts)<br>
     <b>Improve hadoop setup conf script to setup secure Hadoop cluster</b><br>
     <blockquote>Setting up a secure Hadoop cluster requires a lot of manual setup.  The motivation of this jira is to provide setup scripts to automate setup secure Hadoop cluster.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7598">HADOOP-7598</a>.
     Major bug reported by revans2 and fixed by revans2 (build)<br>
     <b>smart-apply-patch.sh does not handle patching from a sub directory correctly.</b><br>
     <blockquote>smart-apply-patch.sh does not apply valid patches from trunk, or from git like it was designed to do in some situations.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7595">HADOOP-7595</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Upgrade dependency to Avro 1.5.3</b><br>
     <blockquote>Avro 1.5.3 depends on Snappy-Java 1.5.3 which enables the use of its SO file from the java.library.path</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7594">HADOOP-7594</a>.
     Major new feature reported by szetszwo and fixed by szetszwo <br>
     <b>Support HTTP REST in HttpServer</b><br>
     <blockquote>Provide an API in HttpServer for supporting HTTP REST.<br><br>This is a part of HDFS-2284.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7593">HADOOP-7593</a>.
     Major bug reported by szetszwo and fixed by umamaheswararao (test)<br>
     <b>AssertionError in TestHttpServer.testMaxThreads()</b><br>
     <blockquote>TestHttpServer passed but there were AssertionError in the output.<br>{noformat}<br>11/08/30 03:35:56 INFO http.TestHttpServer: HTTP server started: http://localhost:52974/<br>Exception in thread &quot;pool-1-thread-61&quot; java.lang.AssertionError: <br>	at org.junit.Assert.fail(Assert.java:91)<br>	at org.junit.Assert.assertTrue(Assert.java:43)<br>	at org.junit.Assert.assertTrue(Assert.java:54)<br>	at org.apache.hadoop.http.TestHttpServer$1.run(TestHttpServer.java:164)<br>	at java.util.concurrent.ThreadPoolExecutor$Worker.ru...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7589">HADOOP-7589</a>.
     Major bug reported by revans2 and fixed by revans2 (build)<br>
     <b>Prefer mvn test -DskipTests over mvn compile in test-patch.sh</b><br>
     <blockquote>I got a failure running test-patch with a clean .m2 directory.<br><br>To quote Alejandro:<br>{quote}<br>The reason for this failure is because of how Maven reactor/dependency<br>resolution works (IMO a bug).<br><br>Maven reactor/dependency resolution is smart enough to create the classpath<br>using the classes from all modules being built.<br><br>However, this smartness falls short just a bit. The dependencies are<br>resolved using the deepest maven phase used by current mvn invocation. If<br>you are doing &apos;mvn compile&apos; you don...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7580">HADOOP-7580</a>.
     Major bug reported by sseth and fixed by sseth <br>
     <b>Add a version of getLocalPathForWrite to LocalDirAllocator which doesn&apos;t create dirs</b><br>
     <blockquote>Required in MR where directories are created by ContainerExecutor (mrv2) / TaskController (0.20) as a specific user.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7579">HADOOP-7579</a>.
     Major task reported by tucu00 and fixed by tucu00 (security)<br>
     <b>Rename package names from alfredo to auth</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7578">HADOOP-7578</a>.
     Major bug reported by mahadev and fixed by mahadev <br>
     <b>Fix test-patch to be able to run on MR patches.</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7576">HADOOP-7576</a>.
     Major bug reported by tomwhite and fixed by szetszwo (security)<br>
     <b>Fix findbugs warnings in Hadoop Auth (Alfredo)</b><br>
     <blockquote>Found in HADOOP-7567: https://builds.apache.org/job/PreCommit-HADOOP-Build/65//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-alfredo.html</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7575">HADOOP-7575</a>.
     Minor bug reported by jeagles and fixed by jeagles (fs)<br>
     <b>Support fully qualified paths as part of LocalDirAllocator</b><br>
     <blockquote>Contexts with configuration path strings using fully qualified paths (e.g. file:///tmp instead of /tmp) mistakenly creates a directory named &apos;file:&apos; and sub-directories in the current local file system working directory.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7568">HADOOP-7568</a>.
     Major bug reported by shv and fixed by zero45 (io)<br>
     <b>SequenceFile should not print into stdout</b><br>
     <blockquote>The following line in {{SequenceFile.Reader.initialize()}} should be removed:<br>{code}<br>System.out.println(&quot;Setting end to &quot; + end);<br>{code}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7566">HADOOP-7566</a>.
     Major bug reported by mahadev and fixed by tucu00 <br>
     <b>MR tests are failing  webapps/hdfs not found in CLASSPATH</b><br>
     <blockquote>While running ant tests, the tests are failing with the following trace:<br><br>{noformat}<br><br>webapps/hdfs not found in CLASSPATH<br>java.io.FileNotFoundException: webapps/hdfs not found in CLASSPATH<br>        at org.apache.hadoop.http.HttpServer.getWebAppsPath(HttpServer.java:470)<br>        at org.apache.hadoop.http.HttpServer.&lt;init&gt;(HttpServer.java:186)<br>        at org.apache.hadoop.http.HttpServer.&lt;init&gt;(HttpServer.java:147)<br>        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.run(NameNo...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7564">HADOOP-7564</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite <br>
     <b>Remove test-patch SVN externals</b><br>
     <blockquote>With the new top-level test-patch script in dev-support, the SVN externals for the old test-patch scripts are no longer needed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7563">HADOOP-7563</a>.
     Major bug reported by eyang and fixed by eyang (scripts)<br>
     <b>hadoop-config.sh setup CLASSPATH, HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME incorrectly</b><br>
     <blockquote>HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME was set to HADOOP_PREFIX/share/hadoop/hdfs and HADOOP_PREFIX/share/hadoop/mapreduce.  This setup confuses the location of hdfs and mapred scripts.  Instead the script should look for hdfs and mapred script in HADOOP_PREFIX/bin.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7561">HADOOP-7561</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite <br>
     <b>Make test-patch only run tests for changed modules</b><br>
     <blockquote>By running test-patch from trunk we can check that a change in one project (e.g. common) doesn&apos;t cause compile errors in other projects (e.g. HDFS). To get this to work we only need to run tests for the modules that are affected by the patch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7560">HADOOP-7560</a>.
     Major sub-task reported by tucu00 and fixed by tucu00 <br>
     <b>Make hadoop-common a POM module with sub-modules (common &amp; alfredo)</b><br>
     <blockquote>Currently hadoop-common is a JAR module, thus it cannot aggregate sub-modules.<br><br>Changing it to POM module it makes it an aggregator module, all the code under hadoop-common must be moved to a sub-module.<br><br>I.e.:<br><br>mkdir hadoop-common-project<br><br>mv hadoop-common hadoop-common-project<br><br>mv hadoop-alfredo hadoop-common-project<br><br>hadoop-common-project/pom.xml is a POM module that aggregates common &amp; alfredo<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7555">HADOOP-7555</a>.
     Trivial improvement reported by atm and fixed by atm (build)<br>
     <b>Add a eclipse-generated files to .gitignore</b><br>
     <blockquote>The .gitignore file in the hadoop-mapreduce directory specifically excludes .classpath, .settings, and .project files/dirs. We should move these excludes to the top level .gitignore so that Common and HDFS have these files excluded as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7552">HADOOP-7552</a>.
     Minor improvement reported by eli and fixed by eli (fs)<br>
     <b>FileUtil#fullyDelete doesn&apos;t throw IOE but lists it in the throws clause</b><br>
     <blockquote>FileUtil#fullyDelete doesn&apos;t throw IOException so it shouldn&apos;t have IOException in its throws clause. Having it listed makes it easy to think you&apos;ll get an IOException eg trying to delete a non-existant file or on an IO error accessing the local file, but you don&apos;t.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7547">HADOOP-7547</a>.
     Minor bug reported by umamaheswararao and fixed by umamaheswararao (io)<br>
     <b>Fix the warning in writable classes.[ WritableComparable is a raw type. References to generic type WritableComparable&lt;T&gt; should be parameterized  ]</b><br>
     <blockquote>WritableComparable is a raw type. References to generic type WritableComparable&lt;T&gt; should be parameterized.<br><br>Also address the same in example implementation in WritableComparable interface&apos;s javadoc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7545">HADOOP-7545</a>.
     Critical bug reported by tlipcon and fixed by tlipcon (build, test)<br>
     <b>common -tests jar should not include properties and configs</b><br>
     <blockquote>This is the cause of HDFS-2242. The -tests jar generated from the common build should only include the test classes, and not the test resources.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7536">HADOOP-7536</a>.
     Major bug reported by kihwal and fixed by tucu00 (build)<br>
     <b>Correct the dependency version regressions introduced in HADOOP-6671</b><br>
     <blockquote>I just noticed the versions specified for dependencies have gone backward with HADOOP-6671.<br>To name a few,<br>* commons-logging  was 1.1.1, now 1.0.4<br>* commons-logging-api  was 1.1, now 1.0.4<br>* slf4 was 1.5.11, now 1.5.8<br><br>There might be more.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7533">HADOOP-7533</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite <br>
     <b>Allow test-patch to be run from any subproject directory </b><br>
     <blockquote>Currently dev-support/test-patch.sh can only be run from the top-level (and only for hadoop-common).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7531">HADOOP-7531</a>.
     Major improvement reported by eli and fixed by eli (util)<br>
     <b>Add servlet util methods for handling paths in requests </b><br>
     <blockquote>Common side of HDFS-2235.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7529">HADOOP-7529</a>.
     Critical bug reported by tlipcon and fixed by vicaya (metrics)<br>
     <b>Possible deadlock in metrics2</b><br>
     <blockquote>Lock cycle detected by jcarder between MetricsSystemImpl and DefaultMetricsSystem</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7528">HADOOP-7528</a>.
     Major sub-task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Maven build fails in Windows</b><br>
     <blockquote>Maven does not run in window for the following reasons:<br><br>* Enforcer plugin restricts build to Unix<br>* Ant run snippets to create TAR are not cygwin friendly</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7526">HADOOP-7526</a>.
     Minor test reported by eli and fixed by eli (fs)<br>
     <b>Add TestPath tests for URI conversion and reserved characters  </b><br>
     <blockquote>TestPath needs tests that cover URI conversion (eg places where Paths and URIs differ) and handling of URI reserved characters in paths. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7525">HADOOP-7525</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite (scripts)<br>
     <b>Make arguments to test-patch optional</b><br>
     <blockquote>Currently you have to specify all the arguments to test-patch.sh, which makes it cumbersome to use. We should make all arguments except the patch file optional. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7523">HADOOP-7523</a>.
     Blocker bug reported by jlee@mindset-media.com and fixed by jlee@mindset-media.com (test)<br>
     <b>Test org.apache.hadoop.fs.TestFilterFileSystem fails due to java.lang.NoSuchMethodException</b><br>
     <blockquote>Test org.apache.hadoop.fs.TestFilterFileSystem fails due to java.lang.NoSuchMethodException. Here is the error message:<br><br>-------------------------------------------------------------------------------<br>Test set: org.apache.hadoop.fs.TestFilterFileSystem<br>-------------------------------------------------------------------------------<br>Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.232 sec &lt;&lt;&lt; FAILURE!<br>testFilterFileSystem(org.apache.hadoop.fs.TestFilterFileSystem)  Time elapsed...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7520">HADOOP-7520</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>hadoop-main fails to deploy</b><br>
     <blockquote>Doing a Maven deployment hadoop-main (trunk/pom.xml) fails to deploy because it does not have the distribution management information.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7515">HADOOP-7515</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite (build)<br>
     <b>test-patch reports the wrong number of javadoc warnings</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7512">HADOOP-7512</a>.
     Trivial task reported by qwertymaniac and fixed by qwertymaniac (documentation)<br>
     <b>Fix example mistake in WritableComparable javadocs</b><br>
     <blockquote>From IRC, via uberj:<br><br>{code}<br>[9:58pm] uberj: http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/WritableComparable.html<br>[9:58pm] uberj: In the example it says &quot;int thatValue = ((IntWritable)o).value;&quot;<br>[9:59pm] uberj: should &apos;o&apos; be replaced with &apos;w&apos;?<br>[9:59pm] uberj: int thatValue = ((IntWritable)w).value;<br>{code}<br><br>Attaching patch for s/w/o.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7509">HADOOP-7509</a>.
     Trivial improvement reported by raviprak and fixed by raviprak <br>
     <b>Improve message when Authentication is required</b><br>
     <blockquote>                    Thanks Aaron and Suresh!<br/><br><br>Marking as resolved fixed since changes have gone in.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7508">HADOOP-7508</a>.
     Major sub-task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>compiled nativelib is in wrong directory and it is not picked up by surefire setup</b><br>
     <blockquote>The location of the compiled native libraries differs from the one surefire plugin (run testcases) is configured to use.<br><br>This makes testcases using nativelibs to fail loading them.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7507">HADOOP-7507</a>.
     Major bug reported by jwfbean and fixed by tucu00 (metrics)<br>
     <b>jvm metrics all use the same namespace</b><br>
     <blockquote>                                              JVM metrics published to Ganglia now include the process name as part of the gmetric name.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7502">HADOOP-7502</a>.
     Major sub-task reported by vicaya and fixed by vicaya <br>
     <b>Use canonical (IDE friendly) generated-sources directory for generated sources</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7501">HADOOP-7501</a>.
     Major sub-task reported by tucu00 and fixed by tomwhite (build)<br>
     <b>publish Hadoop Common artifacts (post HADOOP-6671) to Apache SNAPSHOTs repo</b><br>
     <blockquote>A *distributionManagement* section must be added to the hadoop-project POM with the SNAPSHOTs section, then &apos;mvn deploy&apos; will push the artifacts to it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7499">HADOOP-7499</a>.
     Major bug reported by naisbitt and fixed by naisbitt (util)<br>
     <b>Add method for doing a sanity check on hostnames in NetUtils</b><br>
     <blockquote>As part of MAPREDUCE-2489, we need a method in NetUtils to do a sanity check on hostnames</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7498">HADOOP-7498</a>.
     Major sub-task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Remove legacy TAR layout creation</b><br>
     <blockquote>Currently the build creates 2 different tarball layouts.<br><br>One is the legacy one, the layout used until 0.22 (ant tar &amp;  mvn package -Ptar)<br><br>The other is new new one, the layout used in trunk that mimics the Unix layout (ant binary &amp; mvn package -Pbintar).<br><br>The legacy layout is of not use as all the scripts have been modified to work with the new layout only.<br><br>We should thus remove the legacy layout generation.<br><br>In addition we could rename the current &apos;bintar&apos; to just &apos;tar&apos;<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7496">HADOOP-7496</a>.
     Major sub-task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>break Maven TAR &amp; bintar profiles into just LAYOUT &amp; TAR proper</b><br>
     <blockquote>Currently the tar &amp; bintar profile create the layout and create tarball.<br><br>For development it would be convenient to break them into layout and tar, thus not having to pay the overhead of TARing up.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7493">HADOOP-7493</a>.
     Major new feature reported by umamaheswararao and fixed by umamaheswararao (io)<br>
     <b>[HDFS-362] Provide ShortWritable class in hadoop.</b><br>
     <blockquote>As part of HDFS-362, Provide the ShortWritable class.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7491">HADOOP-7491</a>.
     Major improvement reported by eli and fixed by eli (scripts)<br>
     <b>hadoop command should respect HADOOP_OPTS when given a class name </b><br>
     <blockquote>When using the hadoop command HADOOP_OPTS and HADOOP_CLIENT_OPTS options are not passeed through.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7474">HADOOP-7474</a>.
     Major improvement reported by jnp and fixed by jnp <br>
     <b>Refactor ClientCache out of WritableRpcEngine.</b><br>
     <blockquote>This jira captures the changes in common corresponding to MAPREDUCE-2707.<br>Moving ClientCache out into its own class makes sense because it can be used by other RpcEngine implementations as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7472">HADOOP-7472</a>.
     Minor improvement reported by kihwal and fixed by kihwal (ipc)<br>
     <b>RPC client should deal with the IP address changes</b><br>
     <blockquote>The current RPC client implementation and the client-side callers assume that the hostname-address mappings of servers never change. The resolved address is stored in an immutable InetSocketAddress object above/outside RPC, and the reconnect logic in the RPC Connection implementation also trusts the resolved address that was passed down.<br><br>If the NN suffers a failure that requires migration, it may be started on a different node with a different IP address. In this case, even if the name-addre...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7471">HADOOP-7471</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>the saveVersion.sh script sometimes fails to extract SVN URL</b><br>
     <blockquote>When using an SVN checkout of the source, sometime the {{svn info}} command outputs a &apos;Copied from URL: ###&apos; line in addition to the &apos;URL: ###&apos;.<br><br>This breaks the saveVersion.sh script that assume there is only one line in the output of {{svn info}} that contains the word URL.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7469">HADOOP-7469</a>.
     Minor sub-task reported by stevel@apache.org and fixed by stevel@apache.org (util)<br>
     <b>add a standard handler for socket connection problems which improves diagnostics</b><br>
     <blockquote>connection refused, connection timed out, no route to host, etc, are classic IOExceptions that can be raised in a lot of parts of the code. The standard JDK exceptions are useless for debugging as they <br># don&apos;t include the destination (host, port) that can be used in diagnosing service dead/blocked problems<br># don&apos;t include any source hostname that can be used to handle routing issues<br># assume the reader understands the TCP stack.<br>It&apos;s obvious from the -user lists that a lot of people hit thes...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7465">HADOOP-7465</a>.
     Trivial sub-task reported by xiexianshan and fixed by xiexianshan (fs, ipc)<br>
     <b>A several tiny improvements for the LOG format</b><br>
     <blockquote>There are several fields in the log that the space characters are missed.<br>For instance:<br>src/java/org/apache/hadoop/ipc/Client.java(248): LOG.debug(&quot;The ping interval is&quot; + this.pingInterval + &quot;ms.&quot;);<br>src/java/org/apache/hadoop/fs/LocalDirAllocator.java(235):  LOG.warn( localDirs[i] + &quot;is not writable¥n&quot;, de);<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7463">HADOOP-7463</a>.
     Minor improvement reported by mahadev and fixed by mahadev <br>
     <b>Adding a configuration parameter to SecurityInfo interface.</b><br>
     <blockquote>HADOOP-6929 allowed to make implementations/providers of SecurityInfo to be configurable via service class loaders. For adding Security to TunnelProtocols, configuration is needed to figure out which particular interface getKerberosInfo is called for. Just the class name is not enough since its always TunnerProtocol for all the interfaces. I propose adding a config to getKerberosInfo, so that its easy for TunnerProtocols to get the information they need.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7460">HADOOP-7460</a>.
     Major improvement reported by dhruba and fixed by usmanm (fs)<br>
     <b>Support for pluggable Trash policies</b><br>
     <blockquote>It would be beneficial to make the Trash policy pluggable. One primary use-case for this is to archive files (in some remote store) when they get removed by Trash emptier.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7457">HADOOP-7457</a>.
     Blocker improvement reported by jghoman and fixed by jghoman (documentation)<br>
     <b>Remove out-of-date Chinese language documentation</b><br>
     <blockquote>The Chinese language documentation haven&apos;t been updated (other than copyright years and svn moves) since their original contribution several years ago.  Worse than no docs is out-of-date, wrong docs.  We should delete them from the source tree.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7451">HADOOP-7451</a>.
     Major improvement reported by mattf and fixed by mattf <br>
     <b>merge for MR-279: Generalize StringUtils#join</b><br>
     <blockquote>Fix incomplete merge from yahoo-merge branch to trunk: <br>-r 1079167: Generalize StringUtils::join (Chris Douglas)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7449">HADOOP-7449</a>.
     Major improvement reported by mattf and fixed by mattf <br>
     <b>merge for MR-279: add Data(In,Out)putByteBuffer to work with ByteBuffer similar to Data(In,Out)putBuffer for byte[]</b><br>
     <blockquote>Fix incomplete merge from yahoo-merge branch to trunk: <br>-r 1079163: Added Data(In,Out)putByteBuffer to work with ByteBuffer similar to Data(In,Out)putBuffer for byte[]. (Chris Douglas)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7448">HADOOP-7448</a>.
     Major improvement reported by mattf and fixed by mattf <br>
     <b>merge for MR-279: HttpServer /stacks servlet should use plain text content type</b><br>
     <blockquote>Fix incomplete merge from yahoo-merge branch to trunk: <br>-r 1079157: Fix content type for /stacks servlet (Luke Lu)<br>-r 1079164: No need to escape plain text (Luke Lu)<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7446">HADOOP-7446</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (native)<br>
     <b>Implement CRC32C native code using SSE4.2 instructions</b><br>
     <blockquote>Once HADOOP-7445 is implemented, we can get further performance improvements by implementing CRC32C using the hardware support available in SSE4.2. This support should be dynamically enabled based on CPU feature flags, and of course should be ifdeffed properly so that it doesn&apos;t break the build on architectures/platforms where it&apos;s not available.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7445">HADOOP-7445</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (native, util)<br>
     <b>Implement bulk checksum verification using efficient native code</b><br>
     <blockquote>Once HADOOP-7444 is implemented (&quot;bulk&quot; API for checksums), good performance gains can be had by implementing bulk checksum operations using JNI. This JIRA is to add checksum support to the native libraries. Of course if native libs are not available, it will still fall back to the pure-Java implementations.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7444">HADOOP-7444</a>.
     Major improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Add Checksum API to verify and calculate checksums &quot;in bulk&quot;</b><br>
     <blockquote>Currently, the various checksum types only provide the capability to calculate the checksum of a range of a byte array. For HDFS-2080, it&apos;s advantageous to provide an API that, given a buffer with some number of &quot;checksum chunks&quot;, can either calculate or verify the checksums of all of the chunks. For example, given a 4KB buffer and a 512-byte chunk size, it would calculate or verify 8 CRC32s in one call.<br><br>This allows efficient JNI-based checksum implementations since the cost of crossing the ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7443">HADOOP-7443</a>.
     Major new feature reported by tlipcon and fixed by tlipcon (io, util)<br>
     <b>Add CRC32C as another DataChecksum implementation</b><br>
     <blockquote>CRC32C is another checksum very similar to our existing CRC32, but with a different polynomial. The chief advantage of this other polynomial is that SSE4.2 includes hardware support for its calculation. HDFS-2080 is the umbrella JIRA which proposes using this new polynomial to save substantial amounts of CPU.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7442">HADOOP-7442</a>.
     Major bug reported by atm and fixed by atm (conf, documentation)<br>
     <b>Docs in core-default.xml still reference deprecated config &quot;topology.script.file.name&quot;</b><br>
     <blockquote>HADOOP-6233 renamed the config &quot;{{topology.script.file.name}}&quot; to &quot;{{net.topology.script.file.name}}&quot; but missed a few spots in the docs of core-default.xml.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7440">HADOOP-7440</a>.
     Major bug reported by tlipcon and fixed by tlipcon <br>
     <b>HttpServer.getParameterValues throws NPE for missing parameters</b><br>
     <blockquote>If the requested parameter was not specified in the request, the raw request&apos;s getParameterValues function returns null. Thus, trying to access {{unquoteValue.length}} throws NPE.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7438">HADOOP-7438</a>.
     Major improvement reported by raviprak and fixed by raviprak <br>
     <b>Using the hadoop-deamon.sh script to start nodes leads to a depricated warning </b><br>
     <blockquote>hadoop-daemon.sh calls common/bin/hadoop for hdfs/bin/hdfs tasks and so common/bin/hadoop complains its deprecated for those uses.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7437">HADOOP-7437</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (io)<br>
     <b>IOUtils.copybytes will suppress the stream closure exceptions. </b><br>
     <blockquote>{code}<br><br>public static void copyBytes(InputStream in, OutputStream out, long count,<br>      boolean close) throws IOException {<br>    byte buf[] = new byte[4096];<br>    long bytesRemaining = count;<br>    int bytesRead;<br><br>    try {<br>      .............<br>      .............<br>    } finally {<br>      if (close) {<br>        closeStream(out);<br>        closeStream(in);<br>      }<br>    }<br>  }<br><br>{code}<br><br>Here if any exception in closing the stream, it will get suppressed here.<br><br>So, better to follow the stream closure pattern ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7434">HADOOP-7434</a>.
     Minor improvement reported by yanjinshuang and fixed by yanjinshuang <br>
     <b>Display error when using &quot;daemonlog -setlevel&quot; with illegal level</b><br>
     <blockquote>While using the command with inexistent &quot;level&quot; like &quot;nomsg&quot;, there is no error message displayed,and the level &quot;DEBUG&quot; is set by default.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7430">HADOOP-7430</a>.
     Minor improvement reported by raviprak and fixed by raviprak (fs)<br>
     <b>Improve error message when moving to trash fails due to quota issue</b><br>
     <blockquote>-rm command doesn&apos;t suggest -skipTrash on failure.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7428">HADOOP-7428</a>.
     Major bug reported by tlipcon and fixed by tlipcon (ipc)<br>
     <b>IPC connection is orphaned with null &apos;out&apos; member</b><br>
     <blockquote>We had a situation a JT ended up in a state where a certain user could not submit a job, due to an NPE on the following line in {{sendParam}}:<br>{code}<br>synchronized (Connection.this.out) {<br>{code}<br>Looking at the code, my guess is that an RTE was thrown in setupIOstreams, which only catches IOE. This could leave the connection in a half-setup state which is never cleaned up and also cannot perform IPCs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7419">HADOOP-7419</a>.
     Major bug reported by tlipcon and fixed by bzheng <br>
     <b>new hadoop-config.sh doesn&apos;t manage classpath for HADOOP_CONF_DIR correctly</b><br>
     <blockquote>Since the introduction of the RPM packages, hadoop-config.sh incorrectly puts $HADOOP_HDFS_HOME/conf on the classpath regardless of whether HADOOP_CONF_DIR is already defined in the environment.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7402">HADOOP-7402</a>.
     Trivial bug reported by atm and fixed by atm (test)<br>
     <b>TestConfiguration doesn&apos;t clean up after itself</b><br>
     <blockquote>{{testGetFile}} and {{testGetLocalPath}} both create directories a, b, and c in the working directory from where the tests are run. They should clean up after themselves.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7392">HADOOP-7392</a>.
     Major improvement reported by tanping and fixed by tanping <br>
     <b>Implement capability of querying individual property of a mbean using JMXProxyServlet </b><br>
     <blockquote>Hadoop-7144 provides the capability to query all the properties of a mbean using JMXProxyServlet.  In addition to this, we add the capability to query an individual property of a mbean.  Client will send http request,<br><br>http://hostname/jmx?get=meanName::property<br><br>to query from server.  <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7389">HADOOP-7389</a>.
     Major bug reported by atm and fixed by atm (test)<br>
     <b>Use of TestingGroups by tests causes subsequent tests to fail</b><br>
     <blockquote>As mentioned in HADOOP-6671, {{UserGroupInformation.createUserForTesting(...)}} manipulates static state which can cause test cases which are run after a call to this function to fail.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7385">HADOOP-7385</a>.
     Minor bug reported by bharathm and fixed by bharathm <br>
     <b>Remove StringUtils.stringifyException(ie) in logger functions</b><br>
     <blockquote>Apache logger api has an overloaded function which can take the message and exception. I am proposing to clean the logging code with this api.<br>ie.:<br>Change the code from LOG.warn(msg, StringUtils.stringifyException(exception)); to LOG.warn(msg, exception);<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7384">HADOOP-7384</a>.
     Major improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Allow test-patch to be more flexible about patch format</b><br>
     <blockquote>Right now the test-patch process only accepts patches that are generated as &quot;-p0&quot; relative to common/, hdfs/, or mapreduce/. This has always been annoying for git users where the default patch format is -p1. It&apos;s also now annoying for SVN users who may generate a patch relative to trunk/ instead of the subproject subdirectory. We should auto-detect the correct patch level.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7383">HADOOP-7383</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon (build)<br>
     <b>HDFS needs to export protobuf library dependency in pom</b><br>
     <blockquote>MR builds are failing since the HDFS protobuf patch went in, since they aren&apos;t picking up protobuf as a transitive dependency. I think we just need to add it to the HDFS pom template.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7380">HADOOP-7380</a>.
     Major sub-task reported by atm and fixed by atm (ipc)<br>
     <b>Add client failover functionality to o.a.h.io.(ipc|retry)</b><br>
     <blockquote>Implementing client failover will likely require changes to {{o.a.h.io.ipc}} and/or {{o.a.h.io.retry}}. This JIRA is to track those changes.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7379">HADOOP-7379</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (io, ipc)<br>
     <b>Add ability to include Protobufs in ObjectWritable</b><br>
     <blockquote>                                              Protocol buffer-generated types may now be used as arguments or return values for Hadoop RPC.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7377">HADOOP-7377</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>Fix command name handling affecting DFSAdmin</b><br>
     <blockquote>When an error occurs in the get/set quota commands in DFSAdmin, they are displaying the following:<br>setQuota: failed to get SetQuotaCommand.NAME<br><br>The {{Command}} class expects the {{NAME}} field to be accessible, but for DFSAdmin, it&apos;s not.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7375">HADOOP-7375</a>.
     Major improvement reported by sanjay.radia and fixed by sanjay.radia <br>
     <b>Add resolvePath method to FileContext</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7374">HADOOP-7374</a>.
     Major improvement reported by eli and fixed by eli (scripts)<br>
     <b>Don&apos;t add tools.jar to the classpath when running Hadoop</b><br>
     <blockquote>                                              The scripts that run Hadoop no longer automatically add tools.jar from the JDK to the classpath (if it is present). If your job depends on tools.jar in the JDK you will need to add this dependency in your job.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7361">HADOOP-7361</a>.
     Minor improvement reported by umamaheswararao and fixed by umamaheswararao (fs)<br>
     <b>Provide overwrite option (-overwrite/-f) in put and copyFromLocal command line options</b><br>
     <blockquote>FileSystem has the API <br><br><br><br>*public void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst)*<br>     <br>                         <br>This API provides overwrite option. But the mapping command line doesn&apos;t have this option. To maintain the consistency and better usage  the command line option also can support the overwrite option like to put the files forcefully. ( put [-f] &lt;srcpath&gt; &lt;dstPath&gt;) and also for copyFromLocal command line option.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7360">HADOOP-7360</a>.
     Major improvement reported by daryn and fixed by kihwal (fs)<br>
     <b>FsShell does not preserve relative paths with globs</b><br>
     <blockquote>FsShell currently preserves relative paths that do not contain globs.  Unfortunately the method {{fs.globStatus()}} is fully qualifying all returned paths.  This is causing inconsistent display of paths.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7357">HADOOP-7357</a>.
     Trivial bug reported by philip and fixed by philip (test)<br>
     <b>hadoop.io.compress.TestCodec#main() should exit with non-zero exit code if test failed</b><br>
     <blockquote>It&apos;s convenient to run something like<br>{noformat}<br>HADOOP_CLASSPATH=hadoop-test-0.20.2.jar bin/hadoop org.apache.hadoop.io.compress.TestCodec  -count 3 -codec fo<br>{noformat}<br>but the error code it returns isn&apos;t interesting.<br><br>1-line patch attached fixes that.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7356">HADOOP-7356</a>.
     Blocker bug reported by eyang and fixed by eyang <br>
     <b>RPM packages broke bin/hadoop script for hadoop 0.20.205</b><br>
     <blockquote>hadoop-config.sh has been moved to libexec for binary package, but developers prefers to have hadoop-config.sh in bin.  Hadoo shell scripts should be modified to support both scenarios.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7353">HADOOP-7353</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>Cleanup FsShell and prevent masking of RTE stacktraces</b><br>
     <blockquote>{{FsShell}}&apos;s top level exception handler catches and displays exceptions.  Unfortunately it displays only the first line of an exception, which means an unexpected {{RuntimeExceptions}} like {{NullPointerException}} only display &quot;{{cmd: NullPointerException}}&quot;.  This user has no context to understand and/or accurately report the issue.<br><br>Found due to bugs such as {{HADOOP-7327}}.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7342">HADOOP-7342</a>.
     Minor bug reported by bharathm and fixed by bharathm <br>
     <b>Add an utility API in FileUtil for JDK File.list</b><br>
     <blockquote>Java File.list API can return null when disk is bad or directory is not a directory. This utility API in FileUtil will throw an exception when this happens rather than returning null. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7341">HADOOP-7341</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>Fix option parsing in CommandFormat</b><br>
     <blockquote>CommandFormat currently allows options in any location within the args.  This is not the intended behavior for FsShell commands.  Prior to the redesign, the commands used to expect option processing to stop at the first non-option.<br><br>CommandFormat was an existing class prior the redesign, but it was only used by &quot;count&quot; to find the -q flag.  All commands were converted to using this class, thus inherited the unintended behavior.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7337">HADOOP-7337</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (util)<br>
     <b>Annotate PureJavaCrc32 as a public API</b><br>
     <blockquote>The API of PureJavaCrc32 is stable.  It is incorrect to annotate it as private unstable.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7336">HADOOP-7336</a>.
     Minor bug reported by jnp and fixed by jnp <br>
     <b>TestFileContextResolveAfs will fail with default test.build.data property.</b><br>
     <blockquote>In TestFileContextResolveAfs if test.build.data property is not set and default is used, the test case will try to create that in the root directory and that will fail. /tmp should be used as default as in many other test cases. Normally, test.build.data will be set and this issue should not occur.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7333">HADOOP-7333</a>.
     Minor improvement reported by ecaspole and fixed by ecaspole (util)<br>
     <b>Performance improvement in PureJavaCrc32</b><br>
     <blockquote>I would like to propose a small patch to <br><br>  org.apache.hadoop.util.PureJavaCrc32.update(byte[] b, int off, int len)<br><br>Currently the method stores the intermediate result back into the data member &quot;crc.&quot; I noticed this method gets<br>inlined into DataChecksum.update() and that method appears as one of the hotter methods in a simple hprof profile collected while running terasort and gridmix.<br><br>If the code is modified to save the temporary result into a local and just once store the final result bac...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7331">HADOOP-7331</a>.
     Trivial improvement reported by tanping and fixed by tanping (scripts)<br>
     <b>Make hadoop-daemon.sh to return 1 if daemon processes did not get started</b><br>
     <blockquote>                                              hadoop-daemon.sh now returns a non-zero exit code if it detects that the daemon was not still running after 3 seconds.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7329">HADOOP-7329</a>.
     Minor improvement reported by xiexianshan and fixed by xiexianshan (fs)<br>
     <b>incomplete help message  is displayed for df -h option</b><br>
     <blockquote>The help message for the command &quot;hdfs dfs -help df&quot; is displayed like this:<br>&quot;-df [&lt;path&gt; ...]:    Shows the capacity, free and used space of the filesystem.<br>        If the filesystem has multiple partitions, and no path to a<br>        particular partition is specified, then the status of the root<br>        partitions will be shown.&quot;<br>and the information about df -h option is missed,despite the fact that df -h option is implemented.<br><br>Therefore,the expected message should be displayed like this:<br>&quot;-...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7327">HADOOP-7327</a>.
     Minor bug reported by mattf and fixed by mattf (fs)<br>
     <b>FileSystem.listStatus() throws NullPointerException instead of IOException upon access permission failure</b><br>
     <blockquote>Many processes that call listStatus() expect to handle IOException, but instead are getting runtime error NullPointerException, if the directory being scanned is visible but no-access to the running user id.  For example, if directory foo is drwxr-xr-x, and subdirectory foo/bar is drwx------, then trying to do listStatus(Path(foo/bar)) will cause a NullPointerException.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7324">HADOOP-7324</a>.
     Blocker bug reported by vicaya and fixed by priyomustafi (metrics)<br>
     <b>Ganglia plugins for metrics v2</b><br>
     <blockquote>Although, all metrics in metrics v2 are exposed via the standard JMX mechanisms, most users are using Ganglia to collect metrics.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7322">HADOOP-7322</a>.
     Minor bug reported by bharathm and fixed by bharathm <br>
     <b>Adding a util method in FileUtil for JDK File.listFiles</b><br>
     <blockquote>                                              Use of this new utility method avoids null result from File.listFiles(), and consequent NPEs.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7320">HADOOP-7320</a>.
     Major improvement reported by daryn and fixed by daryn <br>
     <b>Refactor FsShell&apos;s copy &amp; move commands</b><br>
     <blockquote>Need to refactor the move and copy commands to conform to the FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7316">HADOOP-7316</a>.
     Major improvement reported by jmhsieh and fixed by eli (documentation)<br>
     <b>Add public javadocs to FSDataInputStream and FSDataOutputStream</b><br>
     <blockquote>This is a method made public for testing.  In comments in HADOOP-7301 after commit, adding javadoc comments was requested.  This is a follow up jira to address it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7314">HADOOP-7314</a>.
     Major improvement reported by naisbitt and fixed by naisbitt <br>
     <b>Add support for throwing UnknownHostException when a host doesn&apos;t resolve</b><br>
     <blockquote>As part of MAPREDUCE-2489, we need support for having the resolve methods (for DNS mapping) throw UnknownHostExceptions.  (Currently, they hide the exception).  Since the existing &apos;resolve&apos; method is ultimately used by several other locations/components, I propose we add a new &apos;resolveValidHosts&apos; method.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7306">HADOOP-7306</a>.
     Major improvement reported by vicaya and fixed by vicaya (metrics)<br>
     <b>Start metrics system even if config files are missing</b><br>
     <blockquote>Per experience and discussion with HDFS-1922, it seems preferable to treat missing metrics config file as empty/default config, which is more compatible with metrics v1 behavior (the MBeans are always registered.)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7305">HADOOP-7305</a>.
     Minor improvement reported by nielsbasjes and fixed by nielsbasjes (build)<br>
     <b>Eclipse project files are incomplete</b><br>
     <blockquote>                                              Added missing library during creation of the eclipse project files.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7301">HADOOP-7301</a>.
     Major improvement reported by jmhsieh and fixed by jmhsieh <br>
     <b>FSDataInputStream should expose a getWrappedStream method</b><br>
     <blockquote>Ideally FSDataInputStream should expose a getWrappedStream method similarly to how FSDataOutputStream exposes a getWrappedStream method.  Exposing this is useful for verifying correctness in tests cases.  This FSDataInputStream type is the class that the o.a.h.fs.FileSystem.open call returns.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7298">HADOOP-7298</a>.
     Major test reported by tlipcon and fixed by tlipcon (test)<br>
     <b>Add test utility for writing multi-threaded tests</b><br>
     <blockquote>A lot of our tests spawn off multiple threads in order to check various synchronization issues, etc. It&apos;s often tedious to write these kinds of tests because you have to manually propagate exceptions back to the main thread, etc.<br><br>In HBase we have developed a testing utility which makes writing these kinds of tests much easier. I&apos;d like to copy that utility into Hadoop so we can use it here as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7292">HADOOP-7292</a>.
     Minor bug reported by vicaya and fixed by vicaya (metrics)<br>
     <b>Metrics 2 TestSinkQueue is racy</b><br>
     <blockquote>The TestSinkQueue is racy (Thread.yield is not enough to guarantee other intended thread getting run), though it&apos;s the first time (from HADOOP-7289) I saw it manifested here.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7289">HADOOP-7289</a>.
     Major improvement reported by szetszwo and fixed by eyang (build)<br>
     <b>ivy: test conf should not extend common conf</b><br>
     <blockquote>Otherwise, the same jars will appear in both {{build/ivy/lib/Hadoop-Common/common/}} and {{build/ivy/lib/Hadoop-Common/test/}}.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7287">HADOOP-7287</a>.
     Blocker bug reported by tlipcon and fixed by atm (conf)<br>
     <b>Configuration deprecation mechanism doesn&apos;t work properly for GenericOptionsParser/Tools</b><br>
     <blockquote>For example, you can&apos;t use -D options on the &quot;hadoop fs&quot; command line in order to specify the deprecated names of configuration options. The issue is that the ordering is:<br>- JVM starts<br>- GenericOptionsParser creates a Configuration object and calls set() for each of the options specified on command line<br>- DistributedFileSystem or other class eventually instantiates HdfsConfiguration which adds the deprecations<br>- Some class calls conf.get(&quot;new key&quot;) and sees the default instead of the version ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7286">HADOOP-7286</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s du/dus/df</b><br>
     <blockquote>                    The &amp;quot;Found X items&amp;quot; header on the output of the &amp;quot;du&amp;quot; command has been removed to more closely match unix. The displayed paths now correspond to the command line arguments instead of always being a fully qualified URI. For example, the output will have relative paths if the command line arguments are relative paths.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7285">HADOOP-7285</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s test</b><br>
     <blockquote>Need to refactor to conform to FsCommand subclass.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7284">HADOOP-7284</a>.
     Major bug reported by sanjay.radia and fixed by sanjay.radia <br>
     <b>Trash and shell&apos;s rm does not work for viewfs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7282">HADOOP-7282</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe (ipc)<br>
     <b>getRemoteIp could return null in cases where the call is ongoing but the ip went away.</b><br>
     <blockquote>getRemoteIp gets the ip from socket instead of the stored ip in Connection object. Thus calls to this function could return null when a client disconnected, but the rpc call is still ongoing...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7276">HADOOP-7276</a>.
     Major bug reported by scurrilous and fixed by scurrilous (native)<br>
     <b>Hadoop native builds fail on ARM due to -m32</b><br>
     <blockquote>The native build fails on machine targets where gcc does not support -m32. This is any target other than x86, SPARC, RS/6000, or PowerPC, such as ARM.<br><br>$ ant -Dcompile.native=true<br>...<br>     [exec] make  all-am<br>     [exec] make[1]: Entering directory<br>`/home/trobinson/dev/hadoop-common/build/native/Linux-arm-32&apos;<br>     [exec] /bin/bash ./libtool  --tag=CC   --mode=compile gcc<br>-DHAVE_CONFIG_H -I. -I/home/trobinson/dev/hadoop-common/src/native<br>-I/usr/lib/jvm/java-6-openjdk/include<br>-I/usr/lib/jvm/jav...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7275">HADOOP-7275</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s stat</b><br>
     <blockquote>Refactor to conform to the FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7272">HADOOP-7272</a>.
     Major improvement reported by sureshms and fixed by sureshms (ipc, security)<br>
     <b>Remove unnecessary security related info logs</b><br>
     <blockquote>Two info logs are printed when connection to RPC server is established, is not necessary. On a production cluster, these log lines made up of close to 50% of lines in the namenode log. I propose changing them into debug logs.<br><br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7271">HADOOP-7271</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Standardize error messages</b><br>
     <blockquote>The FsShell commands have no standard format for the same error message.  For instance, here is a snippet of the variations of just one of many error messages:<br><br>cmd: $path: No such file or directory<br>cmd: cannot stat `$path&apos;: No such file or directory<br>cmd: Can not find listing for $path<br>cmd: Cannot access $path: No such file or directory.<br>cmd: No such file or directory `$path&apos;<br>cmd: File does not exist: $path<br>cmd: File $path does not exist<br>... etc ...<br><br>These need to be common.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7268">HADOOP-7268</a>.
     Major bug reported by devaraj and fixed by jnp (fs, security)<br>
     <b>FileContext.getLocalFSFileContext() behavior needs to be fixed w.r.t tokens</b><br>
     <blockquote>FileContext.getLocalFSFileContext() instantiates a FileContext object upon the first call to it, and for all subsequent calls returns back that instance (a static localFsSingleton object). With security turned on, this causes some hard-to-debug situations when that fileContext is used for doing HDFS operations. This is because the UserGroupInformation is stored when a FileContext is instantiated. If the process in question wishes to use different UserGroupInformation objects for different fil...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7267">HADOOP-7267</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s rm/rmr/expunge</b><br>
     <blockquote>Refactor to conform to the FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7265">HADOOP-7265</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Keep track of relative paths</b><br>
     <blockquote>As part of the effort to standardize the display of paths, the PathData tracks the exact string used to create a path.  When obtaining a directory&apos;s contents, the relative nature of the original path should be preserved.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7264">HADOOP-7264</a>.
     Major improvement reported by vicaya and fixed by vicaya (io)<br>
     <b>Bump avro version to at least 1.4.1</b><br>
     <blockquote>Needed by mapreduce 2.0 avro support. Maybe we could jump to Avro 1.5. There is incompatible API changes from 1.3x to 1.4x (Utf8 to CharSequence in user facing APIs) not sure about 1.5x though.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7261">HADOOP-7261</a>.
     Major bug reported by sureshms and fixed by sureshms (test)<br>
     <b>Disable IPV6 for junit tests</b><br>
     <blockquote>IPV6 addresses not handles currently in the common library methods. IPV6 can return address as &quot;0:0:0:0:0:0:port&quot;. Some utility methods such as NetUtils#createSocketAddress(), NetUtils#normalizeHostName(), NetUtils#getHostNameOfIp() to name a few, do not handle IPV6 address and expect address to be of format host:port.<br><br>Until IPV6 is formally supported, I propose disabling IPV6 for junit tests to avoid problems seen in HDFS-1891.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7259">HADOOP-7259</a>.
     Major bug reported by owen.omalley and fixed by owen.omalley (build)<br>
     <b>contrib modules should include build.properties from parent.</b><br>
     <blockquote>Current build.properties in the hadoop root directory is not included by the contrib modules.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7258">HADOOP-7258</a>.
     Major bug reported by owen.omalley and fixed by owen.omalley <br>
     <b>Gzip codec should not return null decompressors</b><br>
     <blockquote>In HADOOP-6315, the gzip codec was changed to return a null codec with the intent to disallow pooling of the decompressors. Rather than break the interface, we can use an annotation to achieve the goal.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7257">HADOOP-7257</a>.
     Major new feature reported by sanjay.radia and fixed by sanjay.radia <br>
     <b>A client side mount table to give per-application/per-job file system view</b><br>
     <blockquote>                                              viewfs - client-side mount table.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7251">HADOOP-7251</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s getmerge</b><br>
     <blockquote>Need to refactor getmerge to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7250">HADOOP-7250</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s setrep</b><br>
     <blockquote>Need to refactor setrep to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7249">HADOOP-7249</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s chmod/chown/chgrp</b><br>
     <blockquote>Need to refactor permissions commands to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7241">HADOOP-7241</a>.
     Minor improvement reported by weiyj and fixed by weiyj (fs, test)<br>
     <b>fix typo of command &apos;hadoop fs -help tail&apos;</b><br>
     <blockquote>Fix the typo of command &apos;hadoop fs -help tail&apos;.<br><br>$ hadoop fs -help tail<br>-tail [-f] &lt;file&gt;:  Show the last 1KB of the file. <br>		The -f option shows apended data as the file grows. <br><br>The &quot;apended data&quot; should be &quot;appended data&quot;.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7238">HADOOP-7238</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s cat &amp; text</b><br>
     <blockquote>Need to refactor cat &amp; text to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7237">HADOOP-7237</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s touchz</b><br>
     <blockquote>Need to refactor touchz to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7236">HADOOP-7236</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s mkdir</b><br>
     <blockquote>Need to refactor tail to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7235">HADOOP-7235</a>.
     Major improvement reported by daryn and fixed by daryn <br>
     <b>Refactor FsShell&apos;s tail</b><br>
     <blockquote>Need to refactor tail to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7233">HADOOP-7233</a>.
     Major improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Refactor FsShell&apos;s ls</b><br>
     <blockquote>Need to refactor ls to conform to new FsCommand class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7231">HADOOP-7231</a>.
     Major bug reported by daryn and fixed by daryn (util)<br>
     <b>Fix synopsis for -count</b><br>
     <blockquote>The synopsis for the count command is wrong.<br>1) missing a space in &quot;-count[-q]&quot;<br>2) missing ellipsis for multiple path args</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7230">HADOOP-7230</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Move -fs usage tests from hdfs into common</b><br>
     <blockquote>The -fs usage tests are in hdfs which causes an unnecessary synchronization of a common &amp; hdfs bug when changing the text.  The usages have no ties to hdfs, so they should be moved into common.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7227">HADOOP-7227</a>.
     Major improvement reported by jnp and fixed by jnp (ipc)<br>
     <b>Remove protocol version check at proxy creation in Hadoop RPC.</b><br>
     <blockquote>                    1. Protocol version check is removed from proxy creation, instead version check is performed at server in every rpc call.<br/><br><br>2. This change is backward incompatible because format of the rpc messages is changed to include client version, client method hash and rpc version.<br/><br><br>3. rpc version is introduced which should change when the format of rpc messages is changed.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7223">HADOOP-7223</a>.
     Major bug reported by sureshms and fixed by sureshms (fs)<br>
     <b>FileContext createFlag combinations during create are not clearly defined</b><br>
     <blockquote>During file creation with FileContext, the expected behavior is not clearly defined for combination of createFlag EnumSet.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7216">HADOOP-7216</a>.
     Major bug reported by atm and fixed by daryn (test)<br>
     <b>HADOOP-7202 broke TestDFSShell in HDFS</b><br>
     <blockquote>The commit of HADOOP-7202 now requires that classes that extend {{FsCommand}} implement the {{void run(PathData)}} method. The {{Count}} class was changed to extend {{FsCommand}}, but renamed the {{run}} method and did not provide a replacement.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7215">HADOOP-7215</a>.
     Blocker bug reported by sureshms and fixed by sureshms (security)<br>
     <b>RPC clients must connect over a network interface corresponding to the host name in the client&apos;s kerberos principal key</b><br>
     <blockquote>HDFS-7104 introduced a change where RPC server matches client&apos;s hostname with the hostname specified in the client&apos;s Kerberos principal name. RPC client binds the socket to a random local address, which might not match the hostname specified in the principal name. This results authorization failure of the client at the server.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7214">HADOOP-7214</a>.
     Major new feature reported by atm and fixed by atm <br>
     <b>Hadoop /usr/bin/groups equivalent</b><br>
     <blockquote>Since user -&gt; groups resolution is done on the NN and JT machines, there should be a way for users to determine what groups they&apos;re a member of from the NN&apos;s and JT&apos;s perspective.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7210">HADOOP-7210</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (fs)<br>
     <b>Chown command is not working from FSShell.</b><br>
     <blockquote>chown command is not invoking the setOwner on FileSystem.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7208">HADOOP-7208</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao <br>
     <b>equals() and hashCode() implementation need to change in StandardSocketFactory</b><br>
     <blockquote>  In Hadoop IPC Client, we are using ClientCache which will maintain the HashMap to keep the Client references.<br>private Map&lt;SocketFactory, Client&gt; clients =<br>      new HashMap&lt;SocketFactory, Client&gt;();<br><br> Now let us say, we want use two standard factories with Hadoop. MyStandardSocketFactory (which extends StandardSocketFactory), and StandardSocketFactory. In this case, because of equals and hashcode implementation, MyStandardSocketFactory client can be overridden by StandardSocketFactoryClient<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7205">HADOOP-7205</a>.
     Trivial improvement reported by daryn and fixed by daryn <br>
     <b>automatically determine JAVA_HOME on OS X</b><br>
     <blockquote>OS X provides a java_home command that will return the user&apos;s selected jvm.  The hadoop-env.sh should use this command if JAVA_HOME is not set.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7202">HADOOP-7202</a>.
     Major improvement reported by daryn and fixed by daryn <br>
     <b>Improve Command base class</b><br>
     <blockquote>Need to extend the Command base class to allow all command to easily subclass from a code set of code that correctly handles globs and exit codes.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7194">HADOOP-7194</a>.
     Major bug reported by devaraj.k and fixed by devaraj.k (io)<br>
     <b>Potential Resource leak in IOUtils.java</b><br>
     <blockquote>{code:title=IOUtils.java|borderStyle=solid}<br><br>try {<br>      copyBytes(in, out, buffSize);<br>    } finally {<br>      if(close) {<br>        out.close();<br>        in.close();<br>      }<br>    }<br>{code} <br>In the above code if any exception throws from the out.close() statement, in.close() statement will not execute and the input stream will not be closed.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7193">HADOOP-7193</a>.
     Minor improvement reported by umamaheswararao and fixed by umamaheswararao (fs)<br>
     <b>Help message is wrong for touchz command.</b><br>
     <blockquote>                                              Updated the help for the touchz command.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7187">HADOOP-7187</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (metrics)<br>
     <b>Socket Leak in org.apache.hadoop.metrics.ganglia.GangliaContext</b><br>
     <blockquote>Init method is creating DatagramSocket. But this is not closed any where. <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7180">HADOOP-7180</a>.
     Minor improvement reported by daryn and fixed by daryn (fs)<br>
     <b>Improve CommandFormat</b><br>
     <blockquote>CommandFormat currently takes an array and offset for parsing and returns a list of arguments.  It&apos;d be much more convenient to have it process a list too.  It would also be nice to differentiate between too few and too many args instead of the generic &quot;Illegal number of arguments&quot;.  Finally, CommandFormat is completely devoid of tests.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7178">HADOOP-7178</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (fs)<br>
     <b>FileSystem should have an option to control the .crc file creations at Local.</b><br>
     <blockquote>When we copy the files from DFS to local, it is creating the .crc files in local filesystem for the verification of checksum. When user dont want to do any check sum verifications, this files will not be useful. <br><br>Command line already has an option ignoreCrc, to control this.<br>So, we should have the similar option on FileSystem also..... like fs.ignoreCrc().<br> This should set the setVerifyChecksum to false and also should select the non CheckSumFileSystem as local fs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7177">HADOOP-7177</a>.
     Trivial improvement reported by aw and fixed by aw (native)<br>
     <b>CodecPool should report which compressor it is using</b><br>
     <blockquote>Certain native compression libraries are overly verbose causing confusion while reading the task logs.  Let&apos;s actually say which compressor we got when we report it in the task logs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7175">HADOOP-7175</a>.
     Major bug reported by daryn and fixed by daryn (fs)<br>
     <b>Add isEnabled() to Trash</b><br>
     <blockquote>The moveToTrash method returns false in a number of cases.  It&apos;s not possible to discern if false means an error occurred. In particular, it&apos;s not possible to know if the trash is disabled vs. an error occurred.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7174">HADOOP-7174</a>.
     Minor bug reported by umamaheswararao and fixed by umamaheswararao (fs)<br>
     <b>null is displayed in the console,if the src path is invalid while doing copyToLocal operation from commandLine</b><br>
     <blockquote>When we perform copyToLocal operations from commandLine and if src Path is invalid <br><br>srcFS.globStatus(srcpath) will return null. So, when we find the length of resulted value, it will *throw NullPointerException*.<br><br> Since we are handling generic exception , it will display null as the message.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7172">HADOOP-7172</a>.
     Critical bug reported by tlipcon and fixed by tlipcon (io, security)<br>
     <b>SecureIO should not check owner on non-secure clusters that have no native support</b><br>
     <blockquote>The SecureIOUtils.openForRead function currently uses a racy stat/open combo if security is disabled and the native libraries are not available. This ends up shelling out to &quot;ls -ld&quot; which is very very slow. We&apos;ve seen this cause significant performance regressions on clusters that match this profile.<br><br>Since the racy permissions check doesn&apos;t buy us any security anyway, we should just fall back to a normal &quot;open&quot; without any stat() at all, if we can&apos;t use the native support to do it efficiently.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7171">HADOOP-7171</a>.
     Major bug reported by owen.omalley and fixed by jnp (security)<br>
     <b>Support UGI in FileContext API</b><br>
     <blockquote>The FileContext API needs to support UGI.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7167">HADOOP-7167</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Allow using a file to exclude certain tests from build</b><br>
     <blockquote>It would be nice to be able to exclude certain tests when running builds. For example, when a test is &quot;known flaky&quot;, you may want to exclude it from the main Hudson job, but not actually disable it in the codebase (so that it still runs as part of another Hudson job, for example).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7162">HADOOP-7162</a>.
     Minor bug reported by humanoid and fixed by humanoid (fs)<br>
     <b>FsShell: call srcFs.listStatus(src) twice</b><br>
     <blockquote>in file ./src/java/org/apache/hadoop/fs/FsShell.java line 555<br>call method twice:<br>1. for init variable<br>2. for getting data<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7159">HADOOP-7159</a>.
     Trivial improvement reported by schen and fixed by schen (ipc)<br>
     <b>RPC server should log the client hostname when read exception happened</b><br>
     <blockquote>This makes find mismatched clients easier</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7153">HADOOP-7153</a>.
     Minor improvement reported by nicktelford and fixed by nicktelford (io)<br>
     <b>MapWritable violates contract of Map interface for equals() and hashCode()</b><br>
     <blockquote>                                              MapWritable now implements equals() and hashCode() based on the map contents rather than object identity in order to correctly implement the Map interface.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7151">HADOOP-7151</a>.
     Minor bug reported by dvryaboy and fixed by dvryaboy <br>
     <b>Document need for stable hashCode() in WritableComparable</b><br>
     <blockquote>When a Writable is used as a key, HashPartitioner implicitly assumes that hashCode() will return the same value across different instances of the JVM. This is not a guaranteed behavior in Java, and Object&apos;s default hashCode() does not in fact do this, which can lead to subtle bugs. This requirement should be explicitly called out.<br><br>In addition the sample MyWritable in the javadoc for WritableComparable does not implement hashCode() and thus has a bug. That should be fixed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7144">HADOOP-7144</a>.
     Major new feature reported by vicaya and fixed by revans2 <br>
     <b>Expose JMX with something like JMXProxyServlet </b><br>
     <blockquote>Much of the Hadoop metrics and status info is available via JMX, especially since 0.20.100, and 0.22+ (HDFS-1318, HADOOP-6728 etc.) For operations staff not familiar JMX setup, especially JMX with SSL and firewall tunnelling, the usage can be daunting. Using a JMXProxyServlet (a la Tomcat) to translate JMX attributes into JSON output would make a lot of non-Java admins happy.<br><br>We could probably use Tomcat&apos;s JMXProxyServlet code directly, if it&apos;s already output some standard format (JSON or XM...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7136">HADOOP-7136</a>.
     Major task reported by nidaley and fixed by nidaley <br>
     <b>Remove failmon contrib</b><br>
     <blockquote>                                              Failmon removed from contrib codebase.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7133">HADOOP-7133</a>.
     Major improvement reported by mattf and fixed by mattf (util)<br>
     <b>CLONE to COMMON - HDFS-1445 Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file</b><br>
     <blockquote>                    This is the COMMON portion of a fix requiring coordinated change of COMMON and HDFS.  Please see &lt;a href=&quot;/jira/browse/HDFS-1445&quot; title=&quot;Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file&quot;&gt;&lt;strike&gt;HDFS-1445&lt;/strike&gt;&lt;/a&gt; for HDFS portion and release note.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7131">HADOOP-7131</a>.
     Minor improvement reported by umamaheswararao and fixed by umamaheswararao (io)<br>
     <b>set() and toString Methods of the org.apache.hadoop.io.Text class does not include the root exception, in the wrapping RuntimeException.</b><br>
     <blockquote> In below code snippets, we can include e, instead of e.toString(), so that caller can get complete trace.<br><br>1) <br>   /** Set to contain the contents of a string.<br>   */<br>  public void set(String string) {<br>    try {<br>      ByteBuffer bb = encode(string, true);<br>      bytes = bb.array();<br>      length = bb.limit();<br>    }catch(CharacterCodingException e) {<br>      throw new RuntimeException(&quot;Should not have happened &quot;,e.toString());<br>    }<br>  } <br>2)<br>   public String toString() {<br>    try {<br>      return decod...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7120">HADOOP-7120</a>.
     Major bug reported by szetszwo and fixed by szetszwo (test)<br>
     <b>200 new Findbugs warnings</b><br>
     <blockquote>ant test-patch on an empty patch over hdfs trunk.<br>{noformat}<br>     [exec] -1 overall.  <br>     [exec] <br>     [exec]     +1 @author.  The patch does not contain any @author tags.<br>     [exec] <br>     [exec]     -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.<br>     [exec]                         Please justify why no new tests are needed for this patch.<br>     [exec]                         Also please list what manual steps were performed to verify this patch.<br>     [ex...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7119">HADOOP-7119</a>.
     Major new feature reported by tucu00 and fixed by tucu00 (security)<br>
     <b>add Kerberos HTTP SPNEGO authentication support to Hadoop JT/NN/DN/TT web-consoles</b><br>
     <blockquote>                                              Adding support for Kerberos HTTP SPNEGO authentication to the Hadoop web-consoles<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7117">HADOOP-7117</a>.
     Major improvement reported by patrickangeles and fixed by qwertymaniac (conf)<br>
     <b>Move secondary namenode checkpoint configs from core-default.xml to hdfs-default.xml</b><br>
     <blockquote>                                              Removed references to the older fs.checkpoint.* properties that resided in core-site.xml<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7114">HADOOP-7114</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (fs)<br>
     <b>FsShell should dump all exceptions at DEBUG level</b><br>
     <blockquote>Most of the FsShell commands catch exceptions and then just print out an error like &quot;foo: &quot; + e.getLocalizedMessage(). This is fine when the exception is &quot;user-facing&quot; (eg permissions errors) but in the case of a user hitting a bug you get a useless error message with no stack trace. For example, something &quot;chmod: null&quot; in the case of a NullPointerException bug.<br><br>It would help debug these cases for users and developers if we also logged the exception with full trace at DEBUG level.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7112">HADOOP-7112</a>.
     Major improvement reported by tomwhite and fixed by tomwhite (conf, filecache)<br>
     <b>Issue a warning when GenericOptionsParser libjars are not on local filesystem</b><br>
     <blockquote>In GenericOptionsParser#getLibJars() any jars that are not local filesystem paths are silently ignored. We should issue a warning for users.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7111">HADOOP-7111</a>.
     Critical bug reported by tlipcon and fixed by atm (io)<br>
     <b>Several TFile tests failing when native libraries are present</b><br>
     <blockquote>When running tests with native libraries present, TestTFileByteArrays and TestTFileJClassComparatorByteArrays fail on trunk. They don&apos;t seem to fail in 0.20 with native libraries.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7098">HADOOP-7098</a>.
     Major bug reported by brainlounge and fixed by brainlounge (conf)<br>
     <b>tasktracker property not set in conf/hadoop-env.sh</b><br>
     <blockquote>For all cluster components, except TaskTracker the OPTS environment variable is set like this in hadoop-env.sh:<br>export HADOOP_&lt;COMPONENT&gt;_OPTS=&quot;-Dcom.sun.management.jmxremote $HADOOP_&lt;COMPONENT&gt;_OPTS&quot;<br><br>The provided patch fixes this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7096">HADOOP-7096</a>.
     Major improvement reported by ahmed.radwan and fixed by ahmed.radwan <br>
     <b>Allow setting of end-of-record delimiter for TextInputFormat</b><br>
     <blockquote>The patch for https://issues.apache.org/jira/browse/MAPREDUCE-2254 required minor changes to the LineReader class to allow extensions (see attached 2.patch). Description copied below:<br><br>It will be useful to allow setting the end-of-record delimiter for TextInputFormat. The current implementation hardcodes &apos;\n&apos;, &apos;\r&apos; or &apos;\r\n&apos; as the only possible record delimiters. This is a problem if users have embedded newlines in their data fields (which is pretty common). This is also a problem for other ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7090">HADOOP-7090</a>.
     Major bug reported by gokulm and fixed by umamaheswararao (fs/s3, io)<br>
     <b>Possible resource leaks in hadoop core code</b><br>
     <blockquote>It is always a good practice to close the IO streams in a finally block.. <br><br>For example, look at the following piece of code in the Writer class of BloomMapFile <br><br>{code:title=BloomMapFile .java|borderStyle=solid}<br>    public synchronized void close() throws IOException {<br>      super.close();<br>      DataOutputStream out = fs.create(new Path(dir, BLOOM_FILE_NAME), true);<br>      bloomFilter.write(out);<br>      out.flush();<br>      out.close();<br>    }<br>{code} <br><br>If an exception occurs during fs.create or o...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7089">HADOOP-7089</a>.
     Minor bug reported by eli and fixed by eli (scripts)<br>
     <b>Fix link resolution logic in hadoop-config.sh</b><br>
     <blockquote>                                              Updates hadoop-config.sh to always resolve symlinks when determining HADOOP_HOME. Bash built-ins or POSIX:2001 compliant cmds are now required.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7078">HADOOP-7078</a>.
     Trivial improvement reported by tlipcon and fixed by qwertymaniac <br>
     <b>Add better javadocs for RawComparator interface</b><br>
     <blockquote>The RawComparator interface is very important to understand for users implementing their own serialization classes. Right now the javadoc is woefully sparse. We should improve that.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7071">HADOOP-7071</a>.
     Minor bug reported by nidaley and fixed by nidaley (build)<br>
     <b>test-patch.sh has bad ps arg</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7061">HADOOP-7061</a>.
     Minor improvement reported by yaojingguo and fixed by yaojingguo (io)<br>
     <b>unprecise javadoc for CompressionCodec</b><br>
     <blockquote>In CompressionCodec.java, there is the following code:<br><br>  /**<br>   * Create a stream decompressor that will read from the given input stream.<br>   * <br>   * @param in the stream to read compressed bytes from<br>   * @return a stream to read uncompressed bytes from<br>   * @throws IOException<br>   */<br>  CompressionInputStream createInputStream(InputStream in) throws IOException;<br><br>&quot;stream decompressor&quot; should be &quot;{@link CompressionInputStream}&quot;.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7060">HADOOP-7060</a>.
     Major improvement reported by hairong and fixed by pkling (fs)<br>
     <b>A more elegant FileSystem#listCorruptFileBlocks API</b><br>
     <blockquote>I&apos;d like to change the newly added listCorruptFileBlocks signature to be:<br>{code}<br>/**<br>* Get all files with corrupt blocks under the given path<br>*/<br>RemoteIterator&lt;Path&gt; listCorruptFileBlocks(Path src) throws IOException;<br>{code}<br>This new API does not expose &quot;cookie&quot; to user although underlying implementation may still need to invoke multiple RPCs to get the whole list.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7059">HADOOP-7059</a>.
     Major improvement reported by nwatkins and fixed by nwatkins (native)<br>
     <b>Remove &quot;unused&quot; warning in native code</b><br>
     <blockquote>                                              Adds __attribute__ ((unused))<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7058">HADOOP-7058</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Expose number of bytes in FSOutputSummer buffer to implementatins</b><br>
     <blockquote>For HDFS-1497 it would be useful for an FSOutputSummer implementation to know how many bytes are in the FSOutputSummer buffer. This trivial patch adds a protected call to return this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7057">HADOOP-7057</a>.
     Minor bug reported by cos and fixed by cos (util)<br>
     <b>IOUtils.readFully and IOUtils.skipFully have typo in exception creation&apos;s message</b><br>
     <blockquote>{noformat}<br>        throw new IOException( &quot;Premeture EOF from inputStream&quot;);<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7055">HADOOP-7055</a>.
     Major bug reported by yaojingguo and fixed by yaojingguo (metrics)<br>
     <b>Update of commons logging libraries causes EventCounter to count logging events incorrectly</b><br>
     <blockquote>Hadoop 0.20.2 uses commons logging 1.0.4. EventCounter works correctly with this version of commons logging. Hadoop 0.21.0 uses commons logging 1.1.1 which causes EventCounter to count logging events incorrectly. I have verified it with Hadoop 0.21.0. After start-up of hadoop, I checked jvmmetrics.log after several minutes. In every metrics record, &quot;logError=0, logFatal=0, logInfo=3, logWarn=0&quot; was shown. The following text is an example.<br><br>jvm.metrics: hostName=jingguolin, processName=DataNod...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7053">HADOOP-7053</a>.
     Minor bug reported by yaojingguo and fixed by yaojingguo (conf)<br>
     <b>wrong FSNamesystem Audit logging setting in conf/log4j.properties</b><br>
     <blockquote>&quot;log4j.logger.org.apache.hadoop.fs.FSNamesystem.audit=WARN&quot; should be &quot;log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=WARN&quot;.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7052">HADOOP-7052</a>.
     Major bug reported by yaojingguo and fixed by yaojingguo (conf)<br>
     <b>misspelling of threshold in conf/log4j.properties</b><br>
     <blockquote>In &quot;log4j.threshhold=ALL&quot;, threshhold is a misspelling of threshold. So &quot;log4j.threshhold=ALL&quot; has no effect on the control of log4j logging.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7049">HADOOP-7049</a>.
     Trivial improvement reported by pkling and fixed by pkling (conf)<br>
     <b>TestReconfiguration should be junit v4</b><br>
     <blockquote>TestReconfiguration should be a junit v4 unit test. I&apos;ll also add some messages to the assertions.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7048">HADOOP-7048</a>.
     Minor improvement reported by yaojingguo and fixed by yaojingguo (io)<br>
     <b>Wrong description of Block-Compressed SequenceFile Format in SequenceFile&apos;s javadoc</b><br>
     <blockquote>Here is the following description for Block-Compressed SequenceFile Format in SequenceFile&apos;s javadoc:<br><br> * &lt;li&gt;<br> * Record &lt;i&gt;Block&lt;/i&gt;<br> *   &lt;ul&gt;<br> *     &lt;li&gt;Compressed key-lengths block-size&lt;/li&gt;<br> *     &lt;li&gt;Compressed key-lengths block&lt;/li&gt;<br> *     &lt;li&gt;Compressed keys block-size&lt;/li&gt;<br> *     &lt;li&gt;Compressed keys block&lt;/li&gt;<br> *     &lt;li&gt;Compressed value-lengths block-size&lt;/li&gt;<br> *     &lt;li&gt;Compressed value-lengths block&lt;/li&gt;<br> *     &lt;li&gt;Compressed values block-size&lt;/li&gt;<br> *     &lt;li&gt;Compressed values bloc...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7046">HADOOP-7046</a>.
     Blocker bug reported by nidaley and fixed by pocheung (security)<br>
     <b>1 Findbugs warning on trunk and branch-0.22</b><br>
     <blockquote>There is 1 findbugs warnings on trunk. See attached html file. This must be fixed or filtered out to get back to 0 warnings. The OK_FINDBUGS_WARNINGS property in src/test/test-patch.properties should also be set to 0 in the patch that fixes this issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7045">HADOOP-7045</a>.
     Minor bug reported by eli and fixed by eli (fs)<br>
     <b>TestDU fails on systems with local file systems with extended attributes</b><br>
     <blockquote>The test reports that the file takes an extra 4k on disk:<br><br>{noformat}<br>Testcase: testDU took 5.74 sec<br>        FAILED<br>expected:&lt;32768&gt; but was:&lt;36864&gt;<br>junit.framework.AssertionFailedError: expected:&lt;32768&gt; but was:&lt;36864&gt;<br>        at org.apache.hadoop.fs.TestDU.testDU(TestDU.java:79)<br>{noformat}<br><br>This is because du reports 32k for the file and 4k because the file system it lives on uses extended attributes.<br><br>{noformat}<br>common-branch-0.20 $ dd if=/dev/zero of=data bs=4096 count=8<br>8+0 records in<br>8+...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7042">HADOOP-7042</a>.
     Minor improvement reported by nidaley and fixed by nidaley (test)<br>
     <b>Update test-patch.sh to include failed test names and move test-patch.properties</b><br>
     <blockquote>As Jakob suggested, it would be helpful if the Jira messages left by Hudson included the list of failed tests.<br><br>Also, test-patch.properties must be moved out of the src/test/bin dir because it is project specific and the entire bin dir is svn included into other projects (hdfs and mapreduce)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7023">HADOOP-7023</a>.
     Major improvement reported by pkling and fixed by pkling <br>
     <b>Add listCorruptFileBlocks to FileSystem</b><br>
     <blockquote>                                              Add a new API listCorruptFileBlocks to FIleContext that returns a list of files that have corrupt blocks. <br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7015">HADOOP-7015</a>.
     Minor bug reported by sanjay.radia and fixed by sanjay.radia <br>
     <b>RawLocalFileSystem#listStatus does not deal with a  directory whose entries are changing ( e.g. in a multi-thread or multi-process environment)</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7014">HADOOP-7014</a>.
     Major improvement reported by cos and fixed by cos (test)<br>
     <b>Generalize CLITest structure and interfaces to facilitate upstream adoption (e.g. for web testing)</b><br>
     <blockquote>There&apos;s at least one use case where TestCLI infrastructure is helpful for testing projects outside of core Hadoop (e.g. Owl web testing). In order to make this acceptance easier for upstream project TestCLI needs to be refactored.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-7001">HADOOP-7001</a>.
     Major task reported by pkling and fixed by pkling (conf)<br>
     <b>Allow configuration changes without restarting configured nodes</b><br>
     <blockquote>Currently, changing the configuration on a node (e.g., the name node) requires that we restart the node. We propose a change that would allow us to make configuration changes without restarting. Nodes that support configuration changes at run time should implement the following interface:<br><br>interface ChangeableConfigured extends Configured {<br>   void changeConfiguration(Configuration newConf) throws ConfigurationChangeException;<br>}<br><br>The contract of changeConfiguration is as follows:<br>The node wil...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6995">HADOOP-6995</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (security)<br>
     <b>Allow wildcards to be used in ProxyUsers configurations</b><br>
     <blockquote>                                              When configuring proxy users and hosts, the special wildcard value &amp;quot;*&amp;quot; may be specified to match any host or any user.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6994">HADOOP-6994</a>.
     Major improvement reported by jnp and fixed by jnp <br>
     <b>Api to get delegation token in AbstractFileSystem</b><br>
     <blockquote>APIs to get delegation tokens is required in AbstractFileSystem. AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed. <br>A path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6949">HADOOP-6949</a>.
     Major improvement reported by navis and fixed by mattf (io)<br>
     <b>Reduces RPC packet size for primitive arrays, especially long[], which is used at block reporting</b><br>
     <blockquote>                    Increments the RPC protocol version in org.apache.hadoop.ipc.Server from 4 to 5.<br/><br><br>Introduces ArrayPrimitiveWritable for a much more efficient wire format to transmit arrays of primitives over RPC. ObjectWritable uses the new writable for array of primitives for RPC and continues to use existing format for on-disk data.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6939">HADOOP-6939</a>.
     Minor bug reported by tlipcon and fixed by tlipcon <br>
     <b>Inconsistent lock ordering in AbstractDelegationTokenSecretManager</b><br>
     <blockquote>AbstractDelegationTokenSecretManager.startThreads() is synchronized, which calls updateCurrentKey(), which calls logUpdateMasterKey. logUpdateMasterKey&apos;s implementation for HDFS&apos;s manager calls namesystem.logUpdateMasterKey() which is synchronized. Thus the lock order is ADTSM -&gt; FSN. In FSN.saveNamespace, though, it calls DTSM.saveSecretManagerState(), so the lock order is FSN -&gt; ADTSM.<br><br>I don&apos;t think this deadlock occurs in practice since saveNamespace won&apos;t occur until after the ADTSM has ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6929">HADOOP-6929</a>.
     Major improvement reported by sharadag and fixed by sharadag (ipc, security)<br>
     <b>RPC should have a way to pass Security information other than protocol annotations</b><br>
     <blockquote>Currently Hadoop RPC allows protocol annotations as the only way to pass security information. This becomes a problem if protocols are generated and not hand written. For example protocols generated via Avro and passed over Avro tunnel (AvroRpcEngine.java) can&apos;t pass the security information.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6921">HADOOP-6921</a>.
     Major sub-task reported by vicaya and fixed by vicaya <br>
     <b>metrics2: metrics plugins</b><br>
     <blockquote>                    Metrics names are standardized to CapitalizedCamelCase. See release note of &lt;a href=&quot;/jira/browse/HADOOP-6918&quot; title=&quot;Make metrics naming consistent&quot;&gt;HADOOP-6918&lt;/a&gt; and &lt;a href=&quot;/jira/browse/HADOOP-6920&quot; title=&quot;Metrics2: metrics instrumentation&quot;&gt;&lt;strike&gt;HADOOP-6920&lt;/strike&gt;&lt;/a&gt;.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6920">HADOOP-6920</a>.
     Major sub-task reported by vicaya and fixed by vicaya <br>
     <b>Metrics2: metrics instrumentation</b><br>
     <blockquote>                    Metrics names are standardized to use CapitalizedCamelCase. Some examples of this is:<br/><br><br># Metrics names using &amp;quot;_&amp;quot; is changed to new naming scheme. Eg: bytes_written changes to BytesWritten.<br/><br><br># All metrics names start with capitals. Example: threadsBlocked changes to ThreadsBlocked.<br/><br><br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6919">HADOOP-6919</a>.
     Major sub-task reported by vicaya and fixed by vicaya (metrics)<br>
     <b>Metrics2: metrics framework</b><br>
     <blockquote>                                              New metrics2 framework for Hadoop.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6912">HADOOP-6912</a>.
     Major bug reported by kzhang and fixed by kzhang (security)<br>
     <b>Guard against NPE when calling UGI.isLoginKeytabBased()</b><br>
     <blockquote>NPE can happen when isLoginKeytabBased() is called before a login is performed. See MAPREDUCE-1992 for an example.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6904">HADOOP-6904</a>.
     Major new feature reported by hairong and fixed by hairong (ipc)<br>
     <b>A baby step towards inter-version RPC communications</b><br>
     <blockquote>Currently RPC communications in Hadoop is very strict. If a client has a different version from that of the server, a VersionMismatched exception is thrown and the client can not connect to the server. This force us to update both client and server all at once if a RPC protocol is changed. But sometime different versions do not mean the client &amp; server are not compatible. It would be nice if we could relax this restriction and allows us to support inter-version communications.<br><br>My idea is tha...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6889">HADOOP-6889</a>.
     Major new feature reported by hairong and fixed by johnvijoe (ipc)<br>
     <b>Make RPC to have an option to timeout</b><br>
     <blockquote>Currently Hadoop RPC does not timeout when the RPC server is alive. What it currently does is that a RPC client sends a ping to the server whenever a socket timeout happens. If the server is still alive, it continues to wait instead of throwing a SocketTimeoutException. This is to avoid a client to retry when a server is busy and thus making the server even busier. This works great if the RPC server is NameNode.<br><br>But Hadoop RPC is also used for some of client to DataNode communications, for e...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6887">HADOOP-6887</a>.
     Major improvement reported by bharathm and fixed by vicaya (metrics)<br>
     <b>Need a separate metrics per garbage collector</b><br>
     <blockquote>In addition to current GC metrics which are the sum of all the collectors, Need separate metrics for monitoring young generation and old generation collections per collector w.r.t collection count and collection time. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6864">HADOOP-6864</a>.
     Major improvement reported by zasran and fixed by boryas (security)<br>
     <b>Provide a JNI-based implementation of ShellBasedUnixGroupsNetgroupMapping (implementation of GroupMappingServiceProvider)</b><br>
     <blockquote>The netgroups implementation of GroupMappingServiceProvider (see ShellBasedUnixGroupsNetgroupMapping.java) does a fork of a unix command to get the netgroups of a user. Since the group resolution happens in the servers, this might be costly. This jira aims at providing a JNI-based implementation for GroupMappingServiceProvider.<br><br>Note that this is similar to what https://issues.apache.org/jira/browse/HADOOP-6818 does for implementation of GroupMappingServiceProvider that  supports only unix gr...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6764">HADOOP-6764</a>.
     Major improvement reported by dms and fixed by dms (ipc)<br>
     <b>Add number of reader threads and queue length as configuration parameters in RPC.getServer</b><br>
     <blockquote>In HDFS-599 we are introducing multiple RPC servers running inside of the same process on different ports. Since one might want to configure these servers differently we need a good abstraction to pass configuration values to servers as parameters, not through Configuration.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6754">HADOOP-6754</a>.
     Major bug reported by kimballa and fixed by kimballa (io)<br>
     <b>DefaultCodec.createOutputStream() leaks memory</b><br>
     <blockquote>DefaultCodec.createOutputStream() creates a new Compressor instance in each OutputStream. Even if the OutputStream is closed, this leaks memory.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6683">HADOOP-6683</a>.
     Minor sub-task reported by xiaokang and fixed by xiaokang (io)<br>
     <b>the first optimization: ZlibCompressor does not fully utilize the buffer</b><br>
     <blockquote>                                              Improve the buffer utilization of ZlibCompressor to avoid invoking a JNI per write request.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6671">HADOOP-6671</a>.
     Major sub-task reported by gkesavan and fixed by tucu00 (build)<br>
     <b>To use maven for hadoop common builds</b><br>
     <blockquote>We are now able to publish hadoop artifacts to the maven repo successfully [ Hadoop-6382]<br>Drawbacks with the current approach:<br>* Use ivy for dependency management with ivy.xml<br>* Use maven-ant-task for artifact publishing to the maven repository<br>* pom files are not generated dynamically <br><br>To address this I propose we use maven to build hadoop-common, which would help us to manage dependencies, publish artifacts and have one single xml file(POM) for dependency management and artifact publishing...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6622">HADOOP-6622</a>.
     Major bug reported by jnp and fixed by eli (security)<br>
     <b>Token should not print the password in toString.</b><br>
     <blockquote>The toString method in Token should not print out the password.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6578">HADOOP-6578</a>.
     Minor improvement reported by tlipcon and fixed by pirroh (conf)<br>
     <b>Configuration should trim whitespace around a lot of value types</b><br>
     <blockquote>I&apos;ve seen multiple users make an error where they&apos;ve listed some whitespace around a class name (eg for configuring a scheduler). This results in a ClassNotFoundException which is very hard to debug, as you don&apos;t notice the whitespace in the exception! We should simply trim the whitespace in Configuration.getClass and Configuration.getClasses to avoid this class of user error.<br><br>Similarly, we should trim in getInt, getLong, etc - anywhere that whitespace doesn&apos;t have semantic meaning we should...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6508">HADOOP-6508</a>.
     Major bug reported by amareshwari and fixed by vicaya (metrics)<br>
     <b>Incorrect values for metrics with CompositeContext</b><br>
     <blockquote>In our clusters, when we use CompositeContext with two contexts, second context gets wrong values.<br>This problem is consistent on 500 (and above) node cluster.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6436">HADOOP-6436</a>.
     Major improvement reported by eli and fixed by rvs <br>
     <b>Remove auto-generated native build files </b><br>
     <blockquote>                                              The native build run when from trunk now requires autotools, libtool and openssl dev libraries.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6432">HADOOP-6432</a>.
     Major new feature reported by jnp and fixed by jnp <br>
     <b>Statistics support in FileContext</b><br>
     <blockquote>FileContext should have API to get statistics from underlying file systems.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6385">HADOOP-6385</a>.
     Minor new feature reported by sphillip and fixed by daryn (fs)<br>
     <b>dfs does not support -rmdir (was HDFS-639)</b><br>
     <blockquote>                                              The &amp;quot;rm&amp;quot; family of FsShell commands now supports -rmdir and -f options.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6376">HADOOP-6376</a>.
     Minor improvement reported by kaykay.unique and fixed by kaykay.unique (conf)<br>
     <b>slaves file to have a header specifying the format of conf/slaves file </b><br>
     <blockquote>When we open the file conf/slaves - it is not immediately obvious what the format of the file is ( a comma-separated list or one per each line). The docs confirm it is 1 per line. <br><br>Specifying the information by means of a comment in the template so that it is easy to modify the same, and self-explanatory. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6255">HADOOP-6255</a>.
     Major new feature reported by owen.omalley and fixed by eyang <br>
     <b>Create an rpm integration project</b><br>
     <blockquote>                                              Added RPM/DEB packages to build system.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-6158">HADOOP-6158</a>.
     Minor task reported by owen.omalley and fixed by eli (util)<br>
     <b>Move CyclicIteration to HDFS</b><br>
     <blockquote>I think we should move CyclicIteration from Common utils to HDFS.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-5647">HADOOP-5647</a>.
     Major bug reported by ravidotg and fixed by ravidotg (test)<br>
     <b>TestJobHistory fails if /tmp/_logs is not writable to. Testcase should not depend on /tmp</b><br>
     <blockquote>                                              Removed dependency of testcase on /tmp and made it to use test.build.data directory instead.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-2081">HADOOP-2081</a>.
     Major bug reported by owen.omalley and fixed by qwertymaniac (conf)<br>
     <b>Configuration getInt, getLong, and getFloat replace invalid numbers with the default value</b><br>
     <blockquote>                                              Invalid configuration values now result in a number format exception rather than the default value being used.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HADOOP-1886">HADOOP-1886</a>.
     Trivial improvement reported by shv and fixed by frankconrad (fs)<br>
     <b>Undocumented parameters in FilesSystem</b><br>
     <blockquote>Multiple create methods in public FileSystem class lack documentation for the following 2 parameters.<br>- long blockSize,<br>- Progressable progress<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2512">HDFS-2512</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (data-node, hdfs client)<br>
     <b>Add textual error message to data transfer protocol responses</b><br>
     <blockquote>Currently, the error response code from the DN has very little extra info. I had a situation the other day where the balancer was failing to move blocks, but just reported back &quot;error moving block&quot;, which didn&apos;t help much. We can easily add a &quot;message&quot; field to OpBlockResponseProto to communicate back the underlying issue (in this case, it was thread quota exceeded)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2501">HDFS-2501</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>add version prefix and root methods to webhdfs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2500">HDFS-2500</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Avoid file system operations in BPOfferService thread while processing deletes</b><br>
     <blockquote>While running a workload with concurrent writes and deletes, I saw a lot of NotReplicatedYetExceptions being thrown due to late arrivals of blockReceived reports from the DN. Looking at the DN logs, I found that the blockReceived message was being delayed as much as 15 seconds because the OfferService thread was blocked on file system operations processing deletes. We previously moved the deletions to another thread, but it still accesses the file system to determine the block length in the m...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2494">HDFS-2494</a>.
     Major sub-task reported by umamaheswararao and fixed by umamaheswararao (data-node)<br>
     <b>[webhdfs] When Getting the file using OP=OPEN with DN http address, ESTABLISHED sockets are growing.</b><br>
     <blockquote>As part of the reliable test,<br>Scenario:<br>Initially check the socket count. ---there are aroud 42 sockets are there.<br>open the file with DataNode http address using op=OPEN request parameter about 500 times in loop.<br>Wait for some time and check the socket count. --- There are thousands of ESTABLISHED sockets are growing. ~2052<br><br>Here is the netstat result:<br><br>C:\Users\uma&gt;netstat | grep 127.0.0.1 | grep ESTABLISHED |wc -l<br>2042<br>C:\Users\uma&gt;netstat | grep 127.0.0.1 | grep ESTABLISHED |wc -l<br>2042<br>C:\...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2493">HDFS-2493</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Remove reference to FSNamesystem in blockmanagement classes</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2485">HDFS-2485</a>.
     Trivial improvement reported by stevel@apache.org and fixed by stevel@apache.org (data-node)<br>
     <b>Improve code layout and constants in UnderReplicatedBlocks</b><br>
     <blockquote>Before starting HDFS-2472 I want to clean up the code in UnderReplicatedBlocks slightly<br># use constants for all the string levels<br># change the {{getUnderReplicatedBlockCount()}} method so that it works even if the corrupted block list is not the last queue<br># improve the javadocs<br># add some more curly braces and spaces to follow the style guidelines better<br><br>This is a trivial change as behaviour will not change at all. If committed it will go into trunk and 0.23 so that patches between the two ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2471">HDFS-2471</a>.
     Major new feature reported by sureshms and fixed by sureshms (documentation)<br>
     <b>Add Federation feature, configuration and tools documentation</b><br>
     <blockquote>This jira intends to add Federation documentation.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2467">HDFS-2467</a>.
     Major bug reported by owen.omalley and fixed by owen.omalley <br>
     <b>HftpFileSystem uses incorrect compare for finding delegation tokens</b><br>
     <blockquote>When looking for hdfs delegation tokens, Hftp converts the service to a string and compares it to a text.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2465">HDFS-2465</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (data-node, performance)<br>
     <b>Add HDFS support for fadvise readahead and drop-behind</b><br>
     <blockquote>                    HDFS now has the ability to use posix_fadvise and sync_data_range syscalls to manage the OS buffer cache. This support is currently considered experimental, and may be enabled by configuring the following keys:<br/><br><br>dfs.datanode.drop.cache.behind.writes - set to true to drop data out of the buffer cache after writing<br/><br><br>dfs.datanode.drop.cache.behind.reads - set to true to drop data out of the buffer cache when performing sequential reads<br/><br><br>dfs.datanode.sync.behind.writes - set to true to trigger dirty page writeback immediately after writing data<br/><br><br>dfs.datanode.readahead.bytes - set to a non-zero value to trigger readahead for sequential reads<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2453">HDFS-2453</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>tail using a webhdfs uri throws an error</b><br>
     <blockquote>/usr//bin/hadoop --config /etc/hadoop dfs -tail webhdfs://NN:50070/file <br>tail: HTTP_PARTIAL expected, received 200<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2452">HDFS-2452</a>.
     Major bug reported by shv and fixed by umamaheswararao (data-node)<br>
     <b>OutOfMemoryError in DataXceiverServer takes down the DataNode</b><br>
     <blockquote>OutOfMemoryError brings down DataNode, when DataXceiverServer tries to spawn a new data transfer thread.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2445">HDFS-2445</a>.
     Major bug reported by jeagles and fixed by jeagles (test)<br>
     <b>Incorrect exit code for hadoop-hdfs-test tests when exception thrown</b><br>
     <blockquote>Please see MAPREDUCE-3179 for a full description.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2441">HDFS-2441</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs returns two content-type headers</b><br>
     <blockquote>$ curl -i &quot;http://localhost:50070/webhdfs/path?op=GETFILESTATUS&quot;<br>HTTP/1.1 200 OK<br>Content-Type: text/html; charset=utf-8<br>Expires: Thu, 01-Jan-1970 00:00:00 GMT<br>........<br>Content-Type: application/json<br>Transfer-Encoding: chunked<br>Server: Jetty(6.1.26)<br><br><br>It should only return one content type header = application/json</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2439">HDFS-2439</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs open an invalid path leads to a 500 which states a npe, we should return a 404 with appropriate error message</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2436">HDFS-2436</a>.
     Major bug reported by arpitgupta and fixed by umamaheswararao <br>
     <b>FSNamesystem.setTimes(..) expects the path is a file.</b><br>
     <blockquote>FSNamesystem.setTimes(..) does not work if the path is a directory.<br><br>Arpit found this bug when testing webhdfs:<br>{quote}<br>settimes api is working when called on a file, but when called on a dir it returns a 404. I should be able to set time on both a file and a directory.<br>{quote}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2432">HDFS-2432</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs setreplication api should return a 403 when called on a directory</b><br>
     <blockquote>Currently the set replication api on a directory leads to a 200.<br><br>Request URI http://NN:50070/webhdfs/tmp/webhdfs_data/dir_replication_tests?op=SETREPLICATION&amp;replication=5<br>Request Method: PUT<br>Status Line: HTTP/1.1 200 OK<br>Response Content: {&quot;boolean&quot;:false}<br><br>Since we can determine that this call did not succeed (boolean=false) we should rather just return a 403</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2428">HDFS-2428</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs api parameter validation should be better</b><br>
     <blockquote>PUT Request: http://localhost:50070/webhdfs/some_path?op=MKDIRS&amp;permission=955<br><br>Exception returned<br><br><br>HTTP/1.1 500 Internal Server Error<br>{&quot;RemoteException&quot;:{&quot;className&quot;:&quot;com.sun.jersey.api.ParamException$QueryParamException&quot;,&quot;message&quot;:&quot;java.lang.NumberFormatException: For input string: \&quot;955\&quot;&quot;}} <br><br><br>We should return a 400 with appropriate error message</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2427">HDFS-2427</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs mkdirs api call creates path with 777 permission, we should default it to 755</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2424">HDFS-2424</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs liststatus json does not convert to a valid xml document</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2422">HDFS-2422</a>.
     Major bug reported by jwfbean and fixed by atm (name-node)<br>
     <b>The NN should tolerate the same number of low-resource volumes as failed volumes</b><br>
     <blockquote>We encountered a situation where the namenode dropped into safe mode after a temporary outage of an NFS mount.<br><br>At 12:10 the NFS server goes offline<br><br>Oct  8 12:10:05 &lt;namenode&gt; kernel: nfs: server &lt;nfs host&gt; not responding, timed out<br><br>This caused the namenode to conclude resource issues:<br><br>2011-10-08 12:10:34,848 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume &apos;&lt;nfs host&gt;&apos; is 0, which is below the configured reserved amount 104857600<br><br>Temporary lo...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2414">HDFS-2414</a>.
     Critical bug reported by revans2 and fixed by tlipcon (name-node, test)<br>
     <b>TestDFSRollback fails intermittently</b><br>
     <blockquote>When running TestDFSRollback repeatedly in a loop I observed a failure rate of about 3%.  Two separate stack traces are in the output and it appears to have something to do with not writing out a complete snapshot of the data for rollback.<br><br>{noformat}<br>-------------------------------------------------------------------------------<br>Test set: org.apache.hadoop.hdfs.TestDFSRollback<br>-------------------------------------------------------------------------------<br>Tests run: 1, Failures: 1, Errors: 0...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2412">HDFS-2412</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon <br>
     <b>Add backwards-compatibility layer for FSConstants</b><br>
     <blockquote>HDFS-1620 renamed FSConstants which we believed to be a private class. But currently the public APIs for safe-mode and datanode reports depend on constants in FSConstants. This is breaking HBase builds against 0.23. This JIRA is to provide a backward-compatibility route.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2411">HDFS-2411</a>.
     Major bug reported by arpitgupta and fixed by jnp <br>
     <b>with webhdfs enabled in secure mode the auth to local mappings are not being respected.</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2409">HDFS-2409</a>.
     Major bug reported by jnp and fixed by jnp <br>
     <b>_HOST in dfs.web.authentication.kerberos.principal.</b><br>
     <blockquote>This is HDFS part of HADOOP-7721. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2404">HDFS-2404</a>.
     Major sub-task reported by arpitgupta and fixed by sureshms <br>
     <b>webhdfs liststatus json response is not correct</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2403">HDFS-2403</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>The renewer in NamenodeWebHdfsMethods.generateDelegationToken(..) is not used</b><br>
     <blockquote>Below are some suggestions from Suresh.<br># renewer not used in #generateDelegationToken<br># put() does not use InputStream in and should not throw URISyntaxException<br># post() does not use InputStream in and should not throw URISyntaxException<br># get() should not throw URISyntaxException<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2401">HDFS-2401</a>.
     Major improvement reported by jeagles and fixed by jeagles (build)<br>
     <b>Running a set of methods in a Single Test Class</b><br>
     <blockquote>Instead of running every test method in a class, limit to specific testing methods as describe in the link below.<br><br>http://maven.apache.org/plugins/maven-surefire-plugin/examples/single-test.html<br><br>Upgrade to the latest version of maven-surefire-plugin that has this feature.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2395">HDFS-2395</a>.
     Critical sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs api&apos;s should return a root element in the json response</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2385">HDFS-2385</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>Support delegation token renewal in webhdfs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2371">HDFS-2371</a>.
     Major improvement reported by sureshms and fixed by sureshms (data-node)<br>
     <b>Refactor BlockSender.java for better readability</b><br>
     <blockquote>BlockSender.java is hard to read and understand. I propose refactoring it for better readability</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2368">HDFS-2368</a>.
     Major bug reported by arpitgupta and fixed by szetszwo <br>
     <b>defaults created for web keytab and principal, these properties should not have defaults</b><br>
     <blockquote>the following defaults are set in hdfs-defaults.xml<br><br>&lt;property&gt;<br>  &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt;<br>  &lt;value&gt;HTTP/${dfs.web.hostname}@${kerberos.realm}&lt;/value&gt;<br>  &lt;description&gt;<br>    The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.<br><br>    The HTTP Kerberos principal MUST start with &apos;HTTP/&apos; per Kerberos<br>    HTTP SPENGO specification.<br>  &lt;/description&gt;<br>&lt;/property&gt;<br><br>&lt;property&gt;<br>  &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt;<br>  &lt;value&gt;${user.home}/dfs.web....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2366">HDFS-2366</a>.
     Major sub-task reported by arpitgupta and fixed by szetszwo <br>
     <b>webhdfs throws a npe when ugi is null from getDelegationToken</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2363">HDFS-2363</a>.
     Minor sub-task reported by umamaheswararao and fixed by umamaheswararao (name-node)<br>
     <b>Move datanodes size printing to BlockManager from FSNameSystem&apos;s metasave API</b><br>
     <blockquote>{code}<br>      final List&lt;DatanodeDescriptor&gt; live = new ArrayList&lt;DatanodeDescriptor&gt;();<br>      final List&lt;DatanodeDescriptor&gt; dead = new ArrayList&lt;DatanodeDescriptor&gt;();<br>      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);<br>      out.println(&quot;Live Datanodes: &quot;+live.size());<br>      out.println(&quot;Dead Datanodes: &quot;+dead.size());<br>      blockManager.metaSave(out);<br>{code}<br><br>Logically all the dataNode related logic can be moved to BlockManager.<br><br>So, here metaSave API is getting the ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2361">HDFS-2361</a>.
     Critical bug reported by rajsaha and fixed by jnp (name-node)<br>
     <b>hftp is broken</b><br>
     <blockquote>Distcp with hftp is failing.<br><br>{noformat}<br>$hadoop   distcp hftp://&lt;NNhostname&gt;:50070/user/hadoopqa/1316814737/newtemp 1316814737/as<br>11/09/23 21:52:33 INFO tools.DistCp: srcPaths=[hftp://&lt;NNhostname&gt;:50070/user/hadoopqa/1316814737/newtemp]<br>11/09/23 21:52:33 INFO tools.DistCp: destPath=1316814737/as<br>Retrieving token from: https://&lt;NN IP&gt;:50470/getDelegationToken<br>Retrieving token from: https://&lt;NN IP&gt;:50470/getDelegationToken?renewer=mapred<br>11/09/23 21:52:34 INFO security.TokenCache: Got dt for h...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2356">HDFS-2356</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>webhdfs: support case insensitive query parameter names</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2355">HDFS-2355</a>.
     Major improvement reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Federation: enable using the same configuration file across all the nodes in the cluster.</b><br>
     <blockquote>                    This change allows when running multiple namenodes on different hosts, sharing the same configuration file across all the nodes in the cluster (Datanodes, NamNode, BackupNode, SecondaryNameNode), without the need to define dfs.federation.nameservice.id parameter.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2348">HDFS-2348</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>Support getContentSummary and getFileChecksum in webhdfs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2347">HDFS-2347</a>.
     Trivial bug reported by umamaheswararao and fixed by umamaheswararao (name-node)<br>
     <b>checkpointTxnCount&apos;s comment still saying about editlog size</b><br>
     <blockquote>As per the latest changes checkpoint will trigger based on transaction counts instead of editlog size.But checkpointTxnCount comment is still saying about editlog size.<br><br>{code}<br>private long checkpointTxnCount;    // size (in MB) of current Edit Log<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2344">HDFS-2344</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (test)<br>
     <b>Fix the TestOfflineEditsViewer test failure in 0.23 branch</b><br>
     <blockquote>TestOfflineEditsViewer test fails in 0.23 branch</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2340">HDFS-2340</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>Support getFileBlockLocations and getDelegationToken in webhdfs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2338">HDFS-2338</a>.
     Major sub-task reported by jnp and fixed by jnp <br>
     <b>Configuration option to enable/disable webhdfs.</b><br>
     <blockquote>                                              Added a conf property dfs.webhdfs.enabled for enabling/disabling webhdfs.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2333">HDFS-2333</a>.
     Major bug reported by ikelly and fixed by szetszwo <br>
     <b>HDFS-2284 introduced 2 findbugs warnings on trunk</b><br>
     <blockquote>When HDFS-2284 was submitted it made DFSOutputStream public which triggered two SC_START_IN_CTOR findbug warnings.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2332">HDFS-2332</a>.
     Major test reported by tlipcon and fixed by tlipcon (test)<br>
     <b>Add test for HADOOP-7629: using an immutable FsPermission as an IPC parameter</b><br>
     <blockquote>HADOOP-7629 fixes a bug where an immutable FsPermission would throw an error if used as the argument to fs.setPermission(). This JIRA is to add a test case for the common bugfix.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2331">HDFS-2331</a>.
     Major bug reported by abhijit.shingate and fixed by abhijit.shingate (hdfs client)<br>
     <b>Hdfs compilation fails</b><br>
     <blockquote>I am trying to perform complete build from trunk folder but the compilation fails.<br><br>*Commandline:*<br>mvn clean install  <br><br>*Error Message:*<br><br>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.<br>3.2:compile (default-compile) on project hadoop-hdfs: Compilation failure<br>[ERROR] \Hadoop\SVN\trunk\hadoop-hdfs-project\hadoop-hdfs\src\main\java\org<br>\apache\hadoop\hdfs\web\WebHdfsFileSystem.java:[209,21] type parameters of &lt;T&gt;T<br>cannot be determined; no unique maximal instance...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2323">HDFS-2323</a>.
     Major bug reported by tomwhite and fixed by tomwhite <br>
     <b>start-dfs.sh script fails for tarball install</b><br>
     <blockquote>I build Common and HDFS tarballs from trunk then tried to start a cluster with start-dfs.sh, but I got the following error:<br><br>{noformat}<br>Starting namenodes on [localhost ]<br>sbin/start-dfs.sh: line 55: /Users/tom/tmp/hadoop/libexec/../bin/hadoop-daemons.sh: No such file or directory<br>sbin/start-dfs.sh: line 68: /Users/tom/tmp/hadoop/libexec/../bin/hadoop-daemons.sh: No such file or directory<br>Starting secondary namenodes [0.0.0.0 ]<br>sbin/start-dfs.sh: line 88: /Users/tom/tmp/hadoop/libexec/../bin/h...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2322">HDFS-2322</a>.
     Major bug reported by tucu00 and fixed by tucu00 (build)<br>
     <b>the build fails in Windows because commons-daemon TAR cannot be fetched</b><br>
     <blockquote>For windows there is no commons-daemon TAR but a ZIP, plus the name follows a different convention. <br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2318">HDFS-2318</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>Provide authentication to webhdfs using SPNEGO</b><br>
     <blockquote>                                              Added two new conf properties dfs.web.authentication.kerberos.principal and dfs.web.authentication.kerberos.keytab for the SPNEGO servlet filter.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2317">HDFS-2317</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo <br>
     <b>Read access to HDFS using HTTP REST</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2314">HDFS-2314</a>.
     Major bug reported by vinodkv and fixed by tlipcon (test)<br>
     <b>MRV1 test compilation broken after HDFS-2197</b><br>
     <blockquote>Runing the following:<br>  At the trunk level: {{mvn clean install package -Dtar -Pdist -Dmaven.test.skip.exec=true}}<br>  In hadoop-mapreduce-project: {{ant jar-test -Dresolvers=internal}}<br><br>yields the errors:<br>{code}<br>    [javac] /home/vinodkv/Workspace/eclipse-workspace/apache-git/hadoop-common/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/security/authorize/TestServiceLevelAuthorization.java:62: cannot find symbol<br>    [javac] symbol  : method getRpcServer(org.apache.hadoop.hdfs.server...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2294">HDFS-2294</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Download of commons-daemon TAR should not be under target</b><br>
     <blockquote>Committed HDFS-2289 downloads commons-daemon TAR in the hadoop-hdfs/target/, earlier patches for HDFS-2289 were using hadoop-hdfs/download/ as the location for the download.<br><br>The motivation not to use the &apos;target/&apos; directory is that on every clean build the TAR will be downloaded from Apache archives. Using a &apos;download&apos; directory this happens once per workspace.<br><br>The patch was also adding the &apos;download/&apos; directory to the .gitignore file (it should also be svn ignored).<br><br>Besides downloading it...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2290">HDFS-2290</a>.
     Major bug reported by shv and fixed by benoyantony (name-node)<br>
     <b>Block with corrupt replica is not getting replicated</b><br>
     <blockquote>A block has one replica marked as corrupt and two good ones. countNodes() correctly detects that there are only 2 live replicas, and fsck reports the block as under-replicated. But ReplicationMonitor never schedules replication of good replicas.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2289">HDFS-2289</a>.
     Blocker bug reported by acmurthy and fixed by tucu00 <br>
     <b>jsvc isn&apos;t part of the artifact</b><br>
     <blockquote>Apparently we had something like this in build.xml:<br><br>&lt;property name=&quot;jsvc.location&quot; value=&quot;http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-i386.tar.gz&quot; /&gt;<br><br>Also, when I manually add in jsvc binary I get this error:<br>{noformat}<br>25/08/2011 23:47:18 29805 jsvc.exec error: Cannot find daemon loader org/apache/commons/daemon/support/DaemonLoader<br>25/08/2011 23:47:18 29778 jsvc.exec error: Service exit with a return value of 1<br>{noformat}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2286">HDFS-2286</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>DataXceiverServer logs AsynchronousCloseException at shutdown</b><br>
     <blockquote>During DN shutdown, the acceptor thread gets an AsynchronousCloseException, and logs it at WARN level. This exception is excepted, since another thread is closing the listener socket, so we should just swallow it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2284">HDFS-2284</a>.
     Major sub-task reported by sanjay.radia and fixed by szetszwo <br>
     <b>Write Http access to HDFS</b><br>
     <blockquote>HFTP allows on read access to HDFS via HTTP. Add write HTTP access to HDFS.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2273">HDFS-2273</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Refactor BlockManager.recentInvalidateSets to a new class</b><br>
     <blockquote>recentInvalidateSets and the associated methods can be moved out from BlockManager.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2267">HDFS-2267</a>.
     Trivial bug reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>DataXceiver thread name incorrect while waiting on op during keepalive</b><br>
     <blockquote>Since HDFS-941, the DataXceiver can spend time waiting for a second op to come from the client. Currently, its thread name indicates whatever the previous operation was, rather than something like &quot;Waiting in keepalive for a new request&quot; or something.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2266">HDFS-2266</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Add a Namesystem interface to avoid directly referring to FSNamesystem</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2265">HDFS-2265</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Remove unnecessary BlockTokenSecretManager fields/methods from BlockManager</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2260">HDFS-2260</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (hdfs client)<br>
     <b>Refactor BlockReader into an interface and implementation</b><br>
     <blockquote>For the new block reader in HDFS-2129, or the local block reader in HDFS-347, we need to be able to support different implementations. This JIRA is to simply refactor the current BlockReader into an interface and an implementation.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2258">HDFS-2258</a>.
     Major bug reported by shv and fixed by shv (name-node, test)<br>
     <b>TestLeaseRecovery2 fails as lease hard limit is not reset to default</b><br>
     <blockquote>TestLeaseRecovery2.testSoftLeaseRecovery() fails as lease hard limit remains set to 1 sec from the previous test case. If initial file creation in testSoftLeaseRecovery() takes longer than 1 sec, NN correctly reassigns the lease to itself and starts recovery. The test fails as the client cannot hflush() and close the file.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2245">HDFS-2245</a>.
     Major bug reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>BlockManager.chooseTarget(..) throws NPE</b><br>
     <blockquote>{noformat}<br>2011-08-10 20:20:51,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call: addBlock(/user/had<br>oopqa/passwd.1108102020.&lt;NN hostname&gt;.txt, DFSClient_NONMAPREDUCE_1875954430_1, null, null), rpc<br> version=1, client version=68, methodsFingerPrint=-1239577025 from &lt;gateway&gt;:38874, error:<br>java.io.IOException: java.lang.NullPointerException<br>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)<br>        at org.apache.had...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2241">HDFS-2241</a>.
     Major improvement reported by sureshms and fixed by sureshms <br>
     <b>Remove implementing FSConstants interface just to access the constants defined in the interface</b><br>
     <blockquote>Currently many classes implement FSConstants.java interface just for the convenience of accessing the constants defined in it. This could be done by using static imports or in some cases using FSConstants.&lt;CONSTANT_NAME&gt;, with no need for implementing the interface.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2240">HDFS-2240</a>.
     Critical bug reported by tlipcon and fixed by szetszwo (hdfs client)<br>
     <b>Possible deadlock between LeaseRenewer and its factory</b><br>
     <blockquote>Lock cycle detected by jcarder</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2239">HDFS-2239</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Reduce access levels of the fields and methods in FSNamesystem</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2238">HDFS-2238</a>.
     Minor improvement reported by szetszwo and fixed by umamaheswararao (name-node)<br>
     <b>NamenodeFsck.toString() uses StringBuilder with + operator </b><br>
     <blockquote>We should always use StringBuilder.append(..) but not + (string concatenation).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2237">HDFS-2237</a>.
     Minor sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Change UnderReplicatedBlocks from public to package private</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2235">HDFS-2235</a>.
     Major bug reported by eli and fixed by eli (name-node)<br>
     <b>Encode servlet paths</b><br>
     <blockquote>Hftp does not support paths which contain semicolons. The commented out test in HDFS-2234 illustrates this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2233">HDFS-2233</a>.
     Major test reported by eli and fixed by eli (name-node)<br>
     <b>Add WebUI tests with URI reserved chars in the path and filename</b><br>
     <blockquote>The web UI tests should cover paths where the path and filenames contain URI reserved characters. Ie Web UI coverage for HDFS-2235.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2232">HDFS-2232</a>.
     Blocker bug reported by shv and fixed by zero45 (test)<br>
     <b>TestHDFSCLI fails on 0.22 branch</b><br>
     <blockquote>Several HDFS CLI tests fail on 0.22 branch. I can see 2 reasons:<br># Not generic enough regular expression for host names and paths. Similar to MAPREDUCE-2304.<br># Some command outputs have new-line in the end.<br># And some seem to produce [much] more output than expected.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2230">HDFS-2230</a>.
     Major improvement reported by gkesavan and fixed by gkesavan (build)<br>
     <b>hdfs it not resolving the latest common test jars published post common mavenization</b><br>
     <blockquote>hdfs it not pulling the right common test jar.<br>hadoop-common test jar dependency in ivy.xml has to configure as type=tests and not as a separate module.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2229">HDFS-2229</a>.
     Blocker bug reported by vinodkv and fixed by szetszwo (name-node)<br>
     <b>Deadlock in NameNode</b><br>
     <blockquote>Either I am doing something incredibly stupid, or something about my environment is completely weird, or may be it really is a valid bug. I am running a NameNode deadlock consistently with 0.23 HDFS. I could never start NN successfully.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2228">HDFS-2228</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2227">HDFS-2227</a>.
     Major improvement reported by ikelly and fixed by ikelly <br>
     <b>HDFS-2018 Part 2 :  getRemoteEditLogManifest should pull it&apos;s information from FileJournalManager</b><br>
     <blockquote>This is the second part of HDFS-2018. This patch moves the code that selects the available RemoteEditLogManifest out of the transactional inspector and into FileJournalManager. <br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2226">HDFS-2226</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Clean up counting of operations in FSEditLogLoader</b><br>
     <blockquote>This is simple cleanup in FSEditLogLoader - rather than having a variable per operation type, we can just use an EnumMap to count how many instances of each opcode we&apos;ve hit.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2225">HDFS-2225</a>.
     Major improvement reported by ikelly and fixed by ikelly <br>
     <b>HDFS-2018 Part 1 : Refactor file management so its not in classes which should be generic</b><br>
     <blockquote>This is the first part of HDFS-2018 changes, to refactor some of the file management classes so they&apos;re in file specific places rather than in the generic classes. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2212">HDFS-2212</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Refactor double-buffering code out of EditLogOutputStreams</b><br>
     <blockquote>This is a small cleanup that makes EditLogFileOutputStream and EditLogBackupOutputStream more consistent with each other on how they buffer edits. It simply refactors the double-buffering behavior into a new class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2210">HDFS-2210</a>.
     Major task reported by eli and fixed by eli (contrib/hdfsproxy)<br>
     <b>Remove hdfsproxy</b><br>
     <blockquote>                                              The hdfsproxy contrib component is no longer supported.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2209">HDFS-2209</a>.
     Minor improvement reported by stevel@apache.org and fixed by stevel@apache.org (test)<br>
     <b>Make MiniDFS easier to embed in other apps</b><br>
     <blockquote>I&apos;ve been deploying MiniDFSCluster for some testing, and while using it/looking through the code I made some notes of where there are issues and improvement opportunities. This is mostly minor as its a test tool, but a risk of synchronization problems is there and does need addressing; the rest are all feature creep. <br><br>Field {{nameNode}} should be marked as volatile as the shutdown operation can be in a different thread than startup. Best of all, <br>add synchronized methods to set and get the f...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2205">HDFS-2205</a>.
     Major improvement reported by raviprak and fixed by raviprak (hdfs client)<br>
     <b>Log message for failed connection to datanode is not followed by a success message.</b><br>
     <blockquote>To avoid confusing users on whether their HDFS operation was succesful or not, a success message should be printed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2202">HDFS-2202</a>.
     Major new feature reported by eepayne and fixed by eepayne (balancer, data-node)<br>
     <b>Changes to balancer bandwidth should not require datanode restart.</b><br>
     <blockquote>                    New dfsadmin command added: [-setBalancerBandwidth &amp;lt;bandwidth&amp;gt;] where bandwidth is max network bandwidth in bytes per second that the balancer is allowed to use on each datanode during balacing.<br/><br><br>This is an incompatible change in 0.23.  The versions of ClientProtocol and DatanodeProtocol are changed.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2200">HDFS-2200</a>.
     Minor sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Set FSNamesystem.LOG to package private</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2199">HDFS-2199</a>.
     Major sub-task reported by szetszwo and fixed by umamaheswararao (name-node)<br>
     <b>Move blockTokenSecretManager from FSNamesystem to BlockManager</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2198">HDFS-2198</a>.
     Minor improvement reported by sureshms and fixed by sureshms (data-node, hdfs client, name-node)<br>
     <b>Remove hardcoded configuration keys</b><br>
     <blockquote>Remove hardcoded config keys in hdfs code. Will do it in a separate jira for test code.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2197">HDFS-2197</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Refactor RPC call implementations out of NameNode class</b><br>
     <blockquote>For HA, the NameNode will gain a bit of a state machine, to be able to transition between standby and active states. This would be cleaner in the code if the {{NameNode}} class were just a container for various services, as discussed in HDFS-1974. It&apos;s also nice for testing, where it would become easier to construct just the RPC handlers around a mock NameSystem, with no HTTP server, for example.<br><br>This JIRA is to move all of the protocol implementations out of {{NameNode}} into a separate {{N...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2196">HDFS-2196</a>.
     Major task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Make ant build system work with hadoop-common JAR generated by Maven</b><br>
     <blockquote>Some tweaks must be done in HDFS ivy configuration to work with HADOOP-6671.<br><br>This wil be a temporary fix until HFDS is mavenized.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2191">HDFS-2191</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move datanodeMap from FSNamesystem to DatanodeManager</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2187">HDFS-2187</a>.
     Major improvement reported by ikelly and fixed by ikelly <br>
     <b>HDFS-1580: Make EditLogInputStream act like an iterator over FSEditLogOps</b><br>
     <blockquote>This JIRA is for the input side changes moved out of HDFS-2149. EditLogInputStream has been changed to no longer be an InputStream implementation, but to return a stream of FSEditLogOp objects using readOp(). The upshot is that all that can ever be read from an EditLogInputStream is a op. No random hackery can be used to put other things in the stream. Version is now a property of the EditLogInputStream and retrieved using getVersion().</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2186">HDFS-2186</a>.
     Major bug reported by eli and fixed by eli (data-node)<br>
     <b>DN volume failures on startup are not counted</b><br>
     <blockquote>Volume failures detected on startup are not currently counted/reported as such. Eg if you have configured 4 volumes, 2 tolerated failures, and you start a DN with two failed volumes it will come up and report (to the NN) no failed volumes. The DN will still be able to tolerate 2 additional volume failures (ie it&apos;s OK with no valid volumes remaining). The intent of the volume failure toleration config value is that if more than this # of volumes of the total set of configured volumes have fail...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2180">HDFS-2180</a>.
     Major improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Refactor NameNode HTTP server into new class</b><br>
     <blockquote>As discussed in HDFS-1974, it would be nice to refactor some parts of NameNode.java out into their own classes. This JIRA is to move out the HTTP server.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2167">HDFS-2167</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2161">HDFS-2161</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (balancer, data-node, hdfs client, name-node, security)<br>
     <b>Move utilities to DFSUtil</b><br>
     <blockquote>Utilities include<br>- {{createNamenode(..)}}, {{createClientDatanodeProtocolProxy(..)}};<br>- {{stringifyToken(..)}}; and<br>- {{Random}} object.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2159">HDFS-2159</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (hdfs client)<br>
     <b>Deprecate DistributedFileSystem.getClient()</b><br>
     <blockquote>The DFSClient in DistributedFileSystem should not be accessed directly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2157">HDFS-2157</a>.
     Major improvement reported by atm and fixed by atm (documentation, name-node)<br>
     <b>Improve header comment in o.a.h.hdfs.server.namenode.NameNode</b><br>
     <blockquote>A developer new to HDFS pointed out to me that the header comment at the top of {{NameNode.java}} is a little out of date/inaccurate.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2156">HDFS-2156</a>.
     Major bug reported by owen.omalley and fixed by eyang <br>
     <b>rpm should only require the same major version as common</b><br>
     <blockquote>The rpm for hdfs should only require the same major version (eg. 0.23) of common.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2154">HDFS-2154</a>.
     Minor test reported by szetszwo and fixed by szetszwo (test)<br>
     <b>TestDFSShell should use test dir</b><br>
     <blockquote>The new test by HDFS-2131 creates files/directories under currently directory.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2153">HDFS-2153</a>.
     Minor bug reported by szetszwo and fixed by szetszwo (test)<br>
     <b>DFSClientAdapter should be put under test</b><br>
     <blockquote>{{DFSClientAdapter}} is a test utility but it is put in src/java.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2149">HDFS-2149</a>.
     Major sub-task reported by ikelly and fixed by ikelly (name-node)<br>
     <b>Move EditLogOp serialization formats into FsEditLogOp implementations</b><br>
     <blockquote>On trunk serialisation of editlog ops is in FSEditLog#log* and deserialisation is in FSEditLogOp.*Op . This improvement is to move the serialisation code into one place, i.e under FSEditLogOp.*Op.<br><br>This is part of HDFS-1580.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2147">HDFS-2147</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move cluster network topology to block management</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2144">HDFS-2144</a>.
     Major improvement reported by raviprak and fixed by raviprak (name-node)<br>
     <b>If SNN shuts down during initialization it does not log the cause</b><br>
     <blockquote>SNN should log messages when it shuts down because of authentication issues.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2143">HDFS-2143</a>.
     Major improvement reported by raviprak and fixed by raviprak <br>
     <b>Federation: we should link to the live nodes and dead nodes to cluster web console</b><br>
     <blockquote>The dfsclusterhealth page shows the number of live and dead nodes. It would be nice to link those numbers to the page containing the list of those nodes</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2141">HDFS-2141</a>.
     Major sub-task reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Remove NameNode roles Active and Standby (they become states)</b><br>
     <blockquote>In HDFS, following roles are supported in NameNodeRole: ACTIVE, BACKUP, CHECKPOINT and STANDBY.<br><br>Active and Standby are the state of the NameNode. While Backup and CheckPoint are the name/role of the daemons that are started. This mixes up the run time state of NameNode with the daemon role. I propose changing the NameNodeRole to: NAMENODE, BACKUP, CHECKPOINT. HDFS-1974 will introduce the states active and standby to the daemon that is running in the role NAMENODE.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2140">HDFS-2140</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move Host2NodesMap to block management</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2134">HDFS-2134</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move DecommissionManager to block management</b><br>
     <blockquote>Datanode management including {{DecommissionManager}} should belong to block management.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2132">HDFS-2132</a>.
     Major bug reported by atm and fixed by atm <br>
     <b>Potential resource leak in EditLogFileOutputStream.close</b><br>
     <blockquote>{{EditLogFileOutputStream.close(...)}} sequentially closes a series of underlying resources. If any of the calls to {{close()}} throw before the last one, the later resources will never be closed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2131">HDFS-2131</a>.
     Major test reported by umamaheswararao and fixed by umamaheswararao (test)<br>
     <b>Tests for HADOOP-7361</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2118">HDFS-2118</a>.
     Minor improvement reported by eli and fixed by eli (data-node)<br>
     <b>Couple dfs data dir improvements</b><br>
     <blockquote>Some small dfs data dir improvements: <br>* DataNode#getDataDirsFromURIs should indicate which directory failed. <br>* FSDataset#FSDataset should use getTrimmedStrings when reading dfs.data.dir config.<br>* Fixes a spelling mistake in DataXceiver and DataXceiverServer</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2116">HDFS-2116</a>.
     Minor improvement reported by eli and fixed by zero45 (test)<br>
     <b>Cleanup TestStreamFile and TestByteRangeInputStream </b><br>
     <blockquote>TestStreamFile and TestByteRangeInputStream should use mockito. This would allow the private URLOpener class to be removed from ByteRangeInputStream. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2114">HDFS-2114</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>re-commission of a decommissioned node does not delete excess replica</b><br>
     <blockquote>If a decommissioned node is removed from the decommissioned list, namenode does not delete the excess replicas it created while the node was decommissioned.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2112">HDFS-2112</a>.
     Major sub-task reported by szetszwo and fixed by umamaheswararao (name-node)<br>
     <b>Move ReplicationMonitor to block management</b><br>
     <blockquote>Replication should be handled by block manager instead of name system. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2111">HDFS-2111</a>.
     Major test reported by qwertymaniac and fixed by qwertymaniac (data-node, test)<br>
     <b>Add tests for ensuring that the DN will start with a few bad data directories (Part 1 of testing DiskChecker)</b><br>
     <blockquote>Add tests to ensure that given multiple data dirs, if a single is bad, the DN should still start up.<br><br>This is to check DiskChecker&apos;s functionality used in instantiating DataNodes</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2110">HDFS-2110</a>.
     Minor improvement reported by eli and fixed by eli (name-node)<br>
     <b>Some StreamFile and ByteRangeInputStream cleanup</b><br>
     <blockquote>StreamFile#sendPartialData can be cleaned up, has some System.out.printlns, no javadoc, and the byte copying method should be pulled out to IOUtils. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2109">HDFS-2109</a>.
     Major bug reported by bharathm and fixed by bharathm (hdfs client)<br>
     <b>Store uMask as member variable to DFSClient.Conf</b><br>
     <blockquote>As a part of removing reference to conf in DFSClient, I am proposing replacing FsPermission.getUMask(conf) everywhere in DFSClient class with<br>dfsClientConf.uMask by storing uMask as a member variable to DFSClient.Conf. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2108">HDFS-2108</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move datanode heartbeat handling to BlockManager</b><br>
     <blockquote>Logically, datanodes should heartbeat to block manager instead of name system.  Therefore, we should move datanode heartbeat handling code to {{BlockManager}}.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2107">HDFS-2107</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Move block management code to a package</b><br>
     <blockquote>                                              Moved block management codes to a new package org.apache.hadoop.hdfs.server.blockmanagement.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2100">HDFS-2100</a>.
     Minor test reported by atm and fixed by atm (test)<br>
     <b>Improve TestStorageRestore</b><br>
     <blockquote>Though running multiple 2NNs isn&apos;t supported, accidentally doing so should not result in HDFS metadata corruptions. We should add a test case to exercise this possibility when name.dir.storage.restore is enabled, which is a particularly delicate code path.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2096">HDFS-2096</a>.
     Major task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Mavenization of hadoop-hdfs</b><br>
     <blockquote>Same as HADOOP-6671 for hdfs</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2092">HDFS-2092</a>.
     Major bug reported by bharathm and fixed by bharathm (hdfs client)<br>
     <b>Create a light inner conf class in DFSClient</b><br>
     <blockquote>At present, DFSClient stores reference to configuration object. Since, these configuration objects are pretty big at times can blot the processes which has multiple DFSClient objects like in TaskTracker. This is an attempt to remove the reference of conf object in DFSClient. <br><br>This patch creates a light inner conf class and copies the required keys from the Configuration object.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2087">HDFS-2087</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node, hdfs client)<br>
     <b>Add methods to DataTransferProtocol interface</b><br>
     <blockquote>                                              Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2086">HDFS-2086</a>.
     Major bug reported by tanping and fixed by tanping (name-node)<br>
     <b>If the include hosts list contains host name, after restarting namenode, datanodes registrant is denied </b><br>
     <blockquote>As the title describes the problem:  if the include host list contains host name, after restarting namenodes, the datanodes registrant is denied by namenodes.  This is because after namenode is restarted, the still alive data node will try to register itself with the namenode and it identifies itself with its *IP address*.  However, namenode only allows all the hosts in its hosts list to registrant and all of them are hostnames. So namenode would deny the datanode registration.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2083">HDFS-2083</a>.
     Major new feature reported by tanping and fixed by tanping <br>
     <b>Adopt JMXJsonServlet into HDFS in order to query statistics</b><br>
     <blockquote>HADOOP-7144 added JMXJsonServlet into Common.  It gives the capability to query statistics and metrics exposed via JMX to be queried through HTTP.  We adopt this into HDFS.  This provides the alternative solution to HDFS-1874.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2082">HDFS-2082</a>.
     Major bug reported by atm and fixed by atm <br>
     <b>SecondaryNameNode web interface doesn&apos;t show the right info</b><br>
     <blockquote>HADOOP-3741 introduced some useful info to the 2NN web UI. This broke when security was added.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2073">HDFS-2073</a>.
     Minor improvement reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Namenode is missing @Override annotations</b><br>
     <blockquote>NameNode implements several protocols. The methods that implement the interface do not have @Override. Also @inheritdoc is used, which is not needed with @Override.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2069">HDFS-2069</a>.
     Trivial sub-task reported by raviphulari and fixed by qwertymaniac (documentation)<br>
     <b>Incorrect default trash interval value in the docs</b><br>
     <blockquote>Current HDFS architecture information about Trash is incorrectly documented as  - <br>{color:red} <br>The current default policy is to delete files from /trash that are more than 6 hours old. In the future, this policy will be configurable through a well defined interface.<br>{color}<br><br>It should be something like - <br><br>Current default trash interval is set to 0 (Deletes file without storing in trash ) . This value is configurable parameter stored as fs.trash.interval stored in core-site.xml . <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2067">HDFS-2067</a>.
     Major bug reported by tlipcon and fixed by szetszwo (data-node, hdfs client)<br>
     <b>Bump DATA_TRANSFER_VERSION in trunk for protobufs</b><br>
     <blockquote>Forgot to bump DATA_TRANSFER_VERSION in HDFS-2058. We need to do this since the protobufs are incompatible with the old writables.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2066">HDFS-2066</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node, hdfs client, name-node)<br>
     <b>Create a package and individual class files for DataTransferProtocol</b><br>
     <blockquote>{{DataTransferProtocol}} contains quite a few classes.  It is better to create a package and put the classes into individual files.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2065">HDFS-2065</a>.
     Major bug reported by bharathm and fixed by umamaheswararao <br>
     <b>Fix NPE in DFSClient.getFileChecksum</b><br>
     <blockquote>The following code can throw NPE if callGetBlockLocations returns null.<br><br>If server returns null <br><br>{code}<br>    List&lt;LocatedBlock&gt; locatedblocks<br>        = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE).getLocatedBlocks();<br>{code}<br><br>The right fix for this is server should throw right exception.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2061">HDFS-2061</a>.
     Minor bug reported by mattf and fixed by mattf (name-node)<br>
     <b>two minor bugs in BlockManager block report processing</b><br>
     <blockquote>In a recent review of HDFS-1295 patches (speedup for block report processing), found two very minor bugs in BlockManager, as documented in following comments.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2058">HDFS-2058</a>.
     Major new feature reported by tlipcon and fixed by tlipcon <br>
     <b>DataTransfer Protocol using protobufs</b><br>
     <blockquote>We&apos;ve been talking about this for a long time... would be nice to use something like protobufs or Thrift for some of our wire protocols.<br><br>I knocked together a prototype of DataTransferProtocol on top of proto bufs that seems to work.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2056">HDFS-2056</a>.
     Minor improvement reported by tanping and fixed by tanping (documentation, tools)<br>
     <b>Update fetchdt usage</b><br>
     <blockquote>Update the usage of fetchdt.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2055">HDFS-2055</a>.
     Major new feature reported by traviscrawford and fixed by traviscrawford (libhdfs)<br>
     <b>Add hflush support to libhdfs</b><br>
     <blockquote>                                              Add hdfsHFlush to libhdfs.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2054">HDFS-2054</a>.
     Minor improvement reported by kihwal and fixed by kihwal (data-node)<br>
     <b>BlockSender.sendChunk() prints ERROR for connection closures encountered  during transferToFully()</b><br>
     <blockquote>The addition of ERROR was part of HDFS-1527. In environments where clients tear down FSInputStream/connection before reaching the end of stream, this error message often pops up. Since these are not really errors and especially not the fault of data node, the message should be toned down at least. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2053">HDFS-2053</a>.
     Minor bug reported by miguno and fixed by miguno (name-node)<br>
     <b>Bug in INodeDirectory#computeContentSummary warning</b><br>
     <blockquote>*How to reproduce*<br><br>{code}<br># create test directories<br>$ hadoop fs -mkdir /hdfs-1377/A<br>$ hadoop fs -mkdir /hdfs-1377/B<br>$ hadoop fs -mkdir /hdfs-1377/C<br><br># ...add some test data (few kB or MB) to all three dirs...<br><br># set space quota for subdir C only<br>$ hadoop dfsadmin -setSpaceQuota 1g /hdfs-1377/C<br><br># the following two commands _on the parent dir_ trigger the warning<br>$ hadoop fs -dus /hdfs-1377<br>$ hadoop fs -count -q /hdfs-1377<br>{code}<br><br>Warning message in the namenode logs:<br><br>{code}<br>2011-06-09 09:42...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2046">HDFS-2046</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (build, test)<br>
     <b>Force entropy to come from non-true random for tests</b><br>
     <blockquote>Same as HADOOP-7335 but for HDFS</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2041">HDFS-2041</a>.
     Major bug reported by tlipcon and fixed by tlipcon <br>
     <b>Some mtimes and atimes are lost when edit logs are replayed</b><br>
     <blockquote>The refactoring in HDFS-2003 allowed findbugs to expose two potential bugs:<br>- the atime field logged with OP_MKDIR is unused<br>- the timestamp field logged with OP_CONCAT_DELETE is unused<br><br>The concat issue is definitely real. The atime for MKDIR might always be identical to mtime in that case, in which case it could be ignored.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2040">HDFS-2040</a>.
     Minor improvement reported by eli and fixed by eli <br>
     <b>Only build libhdfs if a flag is passed</b><br>
     <blockquote>In HDFS-2022 we made ant binary build libhdfs unconditionally, this is a pain for users who now need to get the native toolchain working to create a tarball to test a change, and inconsistent with common and MR (see MAPREDUCE-2559) which only build native code if a flag is passed. Let&apos;s revert to the previous behavior of requiring -Dlibhdfs be passed at build time. We could also create a new ant target that doesn&apos;t build the native code, however restoring the old behavior seems simplest.  <br><br><br>  </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2034">HDFS-2034</a>.
     Minor bug reported by johnvijoe and fixed by johnvijoe (hdfs client)<br>
     <b>length in getBlockRange becomes -ve when reading only from currently being written blk</b><br>
     <blockquote>This came up during HDFS-1907. Posting an example that Todd posted in HDFS-1907 that brought out this issue.<br>{quote}<br>Here&apos;s an example sequence to describe what I mean:<br>1. open file, write one and a half blocks<br>2. call hflush<br>3. another reader asks for the first byte of the second block<br>{quote}<br><br>In this case since offset is greater than the completed block length, the math in getBlockRange() of DFSInputStreamer.java will set &quot;length&quot; to negative.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2030">HDFS-2030</a>.
     Minor bug reported by bharathm and fixed by bharathm <br>
     <b>Fix the usability of namenode upgrade command</b><br>
     <blockquote>Fixing the Namenode upgrade option along the same line as Namenode format option. <br><br>If clusterid is not given then clusterid will be automatically generated for the upgrade but if clusterid is given then it will be honored.<br><br> </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2029">HDFS-2029</a>.
     Trivial improvement reported by szetszwo and fixed by johnvijoe (test)<br>
     <b>Improve TestWriteRead</b><br>
     <blockquote>Let&apos;s fix code style and remove redundant codes.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2024">HDFS-2024</a>.
     Trivial improvement reported by cwchung and fixed by cwchung (test)<br>
     <b>Eclipse format HDFS Junit test hdfs/TestWriteRead.java </b><br>
     <blockquote>Eclipse format the file src/test/../hdfs/TestWriteRead.java. This is in preparation of HDFS-1968. <br>So the patch should have only formatting changes such as white space.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2022">HDFS-2022</a>.
     Major bug reported by eli and fixed by eyang (build)<br>
     <b>ant binary should build libhdfs</b><br>
     <blockquote>Post HDFS-1963 ant binary fails w/ the following. The bin-package is trying to copy from the c++ lib dir which doesn&apos;t exist yet. The binary target should check for the existence of this dir or would also be reasonable to depend on the compile-c++-libhdfs (since this is the binary target).<br><br>{noformat}<br>/home/eli/src/hdfs4/build.xml:1115: /home/eli/src/hdfs4/build/c++/Linux-amd64-64/lib not found.<br>{noformat}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2021">HDFS-2021</a>.
     Major bug reported by cwchung and fixed by johnvijoe (data-node)<br>
     <b>TestWriteRead failed with inconsistent visible length of a file </b><br>
     <blockquote>The junit test failed when iterates a number of times with larger chunk size on Linux. Once a while, the visible number of bytes seen by a reader is slightly less than what was supposed to be. <br><br>When run with the following parameter, it failed more often on Linux ( as reported by John George) than my Mac:<br>  private static final int WR_NTIMES = 300;<br>  private static final int WR_CHUNK_SIZE = 10000;<br><br>Adding more debugging output to the source, this is a sample of the output:<br>Caused by: java.io....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2020">HDFS-2020</a>.
     Major bug reported by sureshms and fixed by sureshms (data-node, test)<br>
     <b>TestDFSUpgradeFromImage fails</b><br>
     <blockquote>Datanode has a singleton datanodeObject. When running MiniDFSCluster with multiple datanodes, the singleton can point to only one of the datanodes. TestDFSUpgradeFromImage fails related to initialization of this singleton.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2019">HDFS-2019</a>.
     Minor bug reported by bharathm and fixed by bharathm (data-node)<br>
     <b>Fix all the places where Java method File.list is used with FileUtil.list API</b><br>
     <blockquote>This new method FileUtil.list will throw an exception when disk is bad rather than returning null. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2014">HDFS-2014</a>.
     Critical bug reported by tlipcon and fixed by eyang (scripts)<br>
     <b>bin/hdfs no longer works from a source checkout</b><br>
     <blockquote>bin/hdfs now appears to depend on ../libexec, which doesn&apos;t exist inside of a source checkout:<br><br>todd@todd-w510:~/git/hadoop-hdfs$ ./bin/hdfs namenode<br>./bin/hdfs: line 22: /home/todd/git/hadoop-hdfs/bin/../libexec/hdfs-config.sh: No such file or directory<br>./bin/hdfs: line 138: cygpath: command not found<br>./bin/hdfs: line 161: exec: : not found<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2011">HDFS-2011</a>.
     Major bug reported by raviprak and fixed by raviprak (name-node)<br>
     <b>Removal and restoration of storage directories on checkpointing failure doesn&apos;t work properly</b><br>
     <blockquote>Removal and restoration of storage directories on checkpointing failure doesn&apos;t work properly. Sometimes it throws a NullPointerException and sometimes it doesn&apos;t take off a failed storage directory</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-2003">HDFS-2003</a>.
     Major improvement reported by ikelly and fixed by ikelly <br>
     <b>Separate FSEditLog reading logic from editLog memory state building logic</b><br>
     <blockquote>Currently FSEditLogLoader has code for reading from an InputStream interleaved with code which updates the FSNameSystem and FSDirectory. This makes it difficult to read an edit log without having a whole load of other object initialised, which is problematic if you want to do things like count how many transactions are in a file etc. <br><br>This patch separates the reading of the stream and the building of the memory state. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1999">HDFS-1999</a>.
     Major bug reported by atm and fixed by atm (test)<br>
     <b>Tests use deprecated configs</b><br>
     <blockquote>A few of the HDFS tests (not intended to test deprecation) use config keys which are deprecated.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1998">HDFS-1998</a>.
     Minor bug reported by tanping and fixed by tanping (scripts)<br>
     <b>make refresh-namodenodes.sh refreshing all namenodes</b><br>
     <blockquote>refresh-namenodes.sh is used to refresh name nodes in the cluster to check for updates of include/exclude list.  It is used when decommissioning or adding a data node.  Currently it only refreshes the name node who serves the defaultFs, if there is defaultFs defined.  Fix it by refreshing all the name nodes in the cluster.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1996">HDFS-1996</a>.
     Major improvement reported by szetszwo and fixed by eyang (build)<br>
     <b>ivy: hdfs test jar should be independent to common test jar</b><br>
     <blockquote>hdfs tests and common tests may require different libraries, e.g. common tests need ftpserver for testing {{FTPFileSystem}} but hdfs does not.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1995">HDFS-1995</a>.
     Minor improvement reported by tanping and fixed by tanping <br>
     <b>Minor modification to both dfsclusterhealth and dfshealth pages for Web UI</b><br>
     <blockquote>Four small modifications/fixes:<br>on dfshealthpage:<br>1) fix remaining% to be remaining / total ( it was mistaken as used / total)<br>on dfsclusterhealth page:<br>1) makes the table header 8em wide<br>2) fix the typo(inconsistency) Total Files and Blocks =&gt; Total Files and Directories<br>3) make the DFS Used = sum of block pool used space of every name space.  And change the label names accordingly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1990">HDFS-1990</a>.
     Minor bug reported by ram_krish and fixed by umamaheswararao (data-node)<br>
     <b>Resource leaks in HDFS</b><br>
     <blockquote>Possible resource leakage in HDFS.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1986">HDFS-1986</a>.
     Minor bug reported by tanping and fixed by tanping (tools)<br>
     <b>Add an option for user to return http or https ports regardless of security is on/off in DFSUtil.getInfoServer()</b><br>
     <blockquote>Currently DFSUtil.getInfoServer gets http port with security off and httpS port with security on.  However, we want to return http port regardless of security on/off for Cluster UI to use.  Add in a third Boolean parameter for user to decide whether to check security or not.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1983">HDFS-1983</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Fix path display for copy &amp; rm</b><br>
     <blockquote>This will also fix a few misc broken tests.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1968">HDFS-1968</a>.
     Minor test reported by cwchung and fixed by cwchung (test)<br>
     <b>Enhance TestWriteRead to support File Append and Position Read </b><br>
     <blockquote>Desirable to enhance TestWriteRead to support command line options to do: <br>(1) File Append  <br>(2) Position Read (currently supporting sequential read).   </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1966">HDFS-1966</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node, hdfs client)<br>
     <b>Encapsulate individual DataTransferProtocol op header</b><br>
     <blockquote>                                              Added header classes for individual DataTransferProtocol op headers.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1964">HDFS-1964</a>.
     Major bug reported by atm and fixed by atm <br>
     <b>Incorrect HTML unescaping in DatanodeJspHelper.java</b><br>
     <blockquote>HDFS-1575 introduced some HTML unescaping of parameters so that viewing a file would work for paths containing HTML-escaped characters, but in two of the places did the unescaping either too early or too late.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1963">HDFS-1963</a>.
     Major new feature reported by eyang and fixed by eyang (build)<br>
     <b>HDFS rpm integration project</b><br>
     <blockquote>                                              Create HDFS RPM package<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1959">HDFS-1959</a>.
     Minor improvement reported by eli and fixed by eli <br>
     <b>Better error message for missing namenode directory</b><br>
     <blockquote>Starting the namenode with a missing NN directory currently results in two stack traces, &quot;Expecting a line not the end of stream&quot; from DF and an NPE. Let&apos;s make this more user-friendly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1958">HDFS-1958</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Format confirmation prompt should be more lenient of its input</b><br>
     <blockquote>As reported on the mailing list, the namenode format prompt only accepts &apos;Y&apos;. We should also accept &apos;y&apos; and &apos;yes&apos; (non-case-sensitive).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1955">HDFS-1955</a>.
     Major bug reported by mattf and fixed by mattf (name-node)<br>
     <b>FSImage.doUpgrade() was made too fault-tolerant by HDFS-1826</b><br>
     <blockquote>Prior to HDFS-1826, doUpgrade() would fail if any of the storage directories failed to successfully write the new fsimage or edits files.<br>Now it appears to &quot;succeed&quot; even if some or all of the individual directories fail.<br><br>There is some discussion about whether doUpgrade() should have some fault tolerance, but for now make it fail on any single storage directory failure, as before.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1953">HDFS-1953</a>.
     Minor bug reported by tanping and fixed by tanping <br>
     <b>Change name node mxbean name in cluster web console</b><br>
     <blockquote>name node mxbean name is changed after the new metrics framework is checked.  Need to change this in ClusterJspHelper.java in order for cluster web console to work again.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1952">HDFS-1952</a>.
     Major bug reported by mattf and fixed by azuriel <br>
     <b>FSEditLog.open() appears to succeed even if all EDITS directories fail</b><br>
     <blockquote>FSEditLog.open() appears to &quot;succeed&quot; even if all of the individual directories failed to allow creation of an EditLogOutputStream.  The problem and solution are essentially similar to that of HDFS-1505.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1945">HDFS-1945</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node, hdfs client)<br>
     <b>Removed deprecated fields in DataTransferProtocol</b><br>
     <blockquote>                                              Removed the deprecated fields in DataTransferProtocol.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1943">HDFS-1943</a>.
     Blocker bug reported by weiyj and fixed by weiyj (scripts)<br>
     <b>fail to start datanode while start-dfs.sh is executed by root user</b><br>
     <blockquote>When start-dfs.sh is run by root user, we got the following error message:<br># start-dfs.sh<br>Starting namenodes on [localhost ]<br>localhost: namenode running as process 2556. Stop it first.<br>localhost: starting datanode, logging to /usr/hadoop/hadoop-common-0.23.0-SNAPSHOT/bin/../logs/hadoop-root-datanode-cspf01.out<br>localhost: Unrecognized option: -jvm<br>localhost: Could not create the Java virtual machine.<br><br>The -jvm options should be passed to jsvc when we starting a secure<br>datanode, but it still pa...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1939">HDFS-1939</a>.
     Major improvement reported by szetszwo and fixed by eyang (build)<br>
     <b>ivy: test conf should not extend common conf</b><br>
     <blockquote>                                              * Removed duplicated jars in test class path.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1938">HDFS-1938</a>.
     Minor bug reported by szetszwo and fixed by eyang (build)<br>
     <b> Reference ivy-hdfs.classpath not found.</b><br>
     <blockquote>{noformat}<br>$ant test-system<br>...<br>BUILD FAILED<br>/export/crawlspace/tsz/hdfs/h1/src/test/aop/build/aop.xml:129: The following error occurred while executing this line:<br>/export/crawlspace/tsz/hdfs/h1/src/test/aop/build/aop.xml:183: The following error occurred while executing this line:<br>/export/crawlspace/tsz/hdfs/h1/src/test/aop/build/aop.xml:193: The following error occurred while executing this line:<br>/export/crawlspace/tsz/hdfs/h1/build.xml:449: Reference ivy-hdfs.classpath not found.<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1936">HDFS-1936</a>.
     Blocker bug reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Updating the layout version from HDFS-1822 causes upgrade problems.</b><br>
     <blockquote>In HDFS-1822 and HDFS-1842, the layout versions for 203, 204, 22 and trunk were changed. Some of the namenode logic that depends on layout version is broken because of this. Read the comment for more description.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1934">HDFS-1934</a>.
     Major bug reported by bharathm and fixed by bharathm <br>
     <b>Fix NullPointerException when File.listFiles() API returns null</b><br>
     <blockquote>While testing Disk Fail Inplace, We encountered the NPE from this part of the code. <br><br>File[] files = dir.listFiles();<br>for (File f : files) {<br>...<br>}<br><br>This is kinda of an API issue. When a disk is bad (or name is not a directory), this API (listFiles, list) return null rather than throwing an exception. This &apos;for loop&apos; throws a NPE exception. And same applies to dir.list() API.<br><br>Fix all the places where null condition was not checked.<br> </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1933">HDFS-1933</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Update tests for FsShell&apos;s &quot;test&quot;</b><br>
     <blockquote>Fix tests broken by refactoring.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1931">HDFS-1931</a>.
     Major test reported by daryn and fixed by daryn <br>
     <b>Update tests for du/dus/df</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1928">HDFS-1928</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Fix path display for touchz</b><br>
     <blockquote>Fix expected text in TestHDFSCLI</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1927">HDFS-1927</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe (name-node)<br>
     <b>audit logs could ignore certain xsactions and also could contain &quot;ip=null&quot;</b><br>
     <blockquote>Namenode audit logs could be ignoring certain transactions that are successfully completed. This is because it check if the RemoteIP is null to decide if a transaction is remote or not. In certain cases, RemoteIP could return null but the xsaction could still be &quot;remote&quot;. An example is a case where a client gets killed while in the middle of the transaction. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1923">HDFS-1923</a>.
     Major sub-task reported by mattf and fixed by szetszwo (test)<br>
     <b>Intermittent recurring failure in TestFiDataTransferProtocol2.pipeline_Fi_29</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1922">HDFS-1922</a>.
     Major sub-task reported by mattf and fixed by vicaya (test)<br>
     <b>Recurring failure in TestJMXGet.testNameNode since build 477 on May 11</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1921">HDFS-1921</a>.
     Blocker bug reported by atm and fixed by mattf <br>
     <b>Save namespace can cause NN to be unable to come up on restart</b><br>
     <blockquote>I discovered this in the course of trying to implement a fix for HDFS-1505.<br><br>Per the comment for {{FSImage.saveNamespace(...)}}, the algorithm for save namespace proceeds in the following order:<br><br># rename current to lastcheckpoint.tmp for all of them,<br># save image and recreate edits for all of them,<br># rename lastcheckpoint.tmp to previous.checkpoint.<br><br>The problem is that step 3 occurs regardless of whether or not an error occurs for all storage directories in step 2. Upon restart, the NN will...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1920">HDFS-1920</a>.
     Major bug reported by scurrilous and fixed by scurrilous (libhdfs)<br>
     <b>libhdfs does not build for ARM processors</b><br>
     <blockquote>$ ant compile -Dcompile.native=true -Dcompile.c++=1 -Dlibhdfs=1 -Dfusedfs=1<br>...<br>create-libhdfs-configure:<br>...<br>     [exec] configure: error: Unsupported CPU architecture &quot;armv7l&quot;<br><br>Once the CPU arch check is fixed in src/c++/libhdfs/m4/apsupport.m4, then next issue is -m32:<br><br>$ ant compile -Dcompile.native=true -Dcompile.c++=1 -Dlibhdfs=1 -Dfusedfs=1<br>...<br><br>compile-c++-libhdfs:<br>     [exec] /bin/bash ./libtool --tag=CC   --mode=compile gcc -DPACKAGE_NAME=\&quot;libhdfs\&quot; -DPACKAGE_TARNAME=\&quot;libhdfs\&quot; -D...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1917">HDFS-1917</a>.
     Major bug reported by eyang and fixed by eyang (build)<br>
     <b>Clean up duplication of dependent jar files</b><br>
     <blockquote>                                              Remove packaging of duplicated third party jar files<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1914">HDFS-1914</a>.
     Major bug reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Federation: namenode storage directory must be configurable specific to a namenode</b><br>
     <blockquote>Federation allows common configuration where namenode specific configuration are in the same configuration suffixed by nameservice ID. When namenodes use an external storage directory (NFS), in order to make namenodes use different directories on the external server, the storage directory configuration must also allow specific configuration, using nameservice ID.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1912">HDFS-1912</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Update tests for FsShell standardized error messages</b><br>
     <blockquote>Need to update the FsShell based tests for commonized error messages.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1911">HDFS-1911</a>.
     Major test reported by sanjay.radia and fixed by sanjay.radia <br>
     <b>HDFS tests for viewfs</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1908">HDFS-1908</a>.
     Minor bug reported by szetszwo and fixed by szetszwo (test)<br>
     <b>DataTransferTestUtil$CountdownDoosAction.run(..) throws NullPointerException</b><br>
     <blockquote>In [build #426|https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/426//testReport/org.apache.hadoop.hdfs.server.datanode/TestFiDataTransferProtocol2/pipeline_Fi_17/],<br>{noformat}<br>2011-04-28 07:20:10,559 ERROR datanode.DataNode (DataXceiver.java:run(133)) - DatanodeRegistration(127.0.0.1:50589,<br> storageID=DS-499221794-127.0.1.1-50589-1303975177998, infoPort=58607, ipcPort=52786):DataXceiver<br>java.lang.NullPointerException<br>	at org.apache.hadoop.fi.DataTransferTestUtil$CountdownDoosAction.r...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1907">HDFS-1907</a>.
     Major bug reported by cwchung and fixed by johnvijoe (hdfs client)<br>
     <b>BlockMissingException upon concurrent read and write: reader was doing file position read while writer is doing write without hflush</b><br>
     <blockquote>BlockMissingException is thrown under this test scenario:<br>Two different processes doing concurrent file r/w: one read and the other write on the same file<br>  - writer keep doing file write<br>  - reader doing position file read from beginning of the file to the visible end of file, repeatedly<br><br>The reader is basically doing:<br>  byteRead = in.read(currentPosition, buffer, 0, byteToReadThisRound);<br>where CurrentPostion=0, buffer is a byte array buffer, byteToReadThisRound = 1024*10000;<br><br>Usually it doe...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1906">HDFS-1906</a>.
     Minor improvement reported by sureshms and fixed by sureshms (hdfs client)<br>
     <b>Remove logging exception stack trace when one of the datanode targets to read from is not reachable</b><br>
     <blockquote>When client fails to connect to one of the datanodes from the list of block locations returned, exception stack trace is printed in the client log. This is an expected failure scenario that is handled at the client, by going to the next location. Printing entire stack trace is unnecessary and just printing the exception message should be sufficient.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1905">HDFS-1905</a>.
     Minor bug reported by bharathm and fixed by bharathm (name-node)<br>
     <b>Improve the usability of namenode -format </b><br>
     <blockquote>While setting up 0.23 version based cluster, i ran into this issue. When i issue a format namenode command, which got changed in 23, it should let the user know to how to use this command in case where complete options were not specified.<br><br>./hdfs namenode -format<br><br>I get the following error msg, still its not clear what and how user should use this command.<br><br>11/05/09 15:36:25 ERROR namenode.NameNode: java.lang.IllegalArgumentException: Format must be provided with clusterid<br>	at org.apache.hado...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1903">HDFS-1903</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Fix path display for rm/rmr</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1902">HDFS-1902</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Fix path display for setrep</b><br>
     <blockquote>See HDFS-1901.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1899">HDFS-1899</a>.
     Major improvement reported by tlipcon and fixed by yuzhihong@gmail.com <br>
     <b>GenericTestUtils.formatNamenode is misplaced</b><br>
     <blockquote>This function belongs in DFSTestUtil, the standard place for putting cluster-related utils.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1898">HDFS-1898</a>.
     Critical bug reported by tlipcon and fixed by tlipcon <br>
     <b>Tests failing on trunk due to use of NameNode.format</b><br>
     <blockquote>After federation merge, NameNode.format no longer works on trunk. Unclear why these tests aren&apos;t failing on Hudson, but some seem to fail for me in my checkout (including TestEditLogFileOutputStream for example)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1890">HDFS-1890</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (hdfs client)<br>
     <b>A few improvements on the LeaseRenewer.pendingCreates map</b><br>
     <blockquote>- The class is better to be just a {{Map}} instead of a {{SortedMap}}.<br>- The value type is better to be {{DFSOutputStream}} instead of {{OutputStream}}.<br>- The variable name is better to be filesBeingWritten instead of pendingCreates since we have append.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1889">HDFS-1889</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>incorrect path in start/stop dfs script</b><br>
     <blockquote>HADOOP_HOME in start-dfs.sh and stop-dfs.sh should be changed to HADOOP_HDFS_HOME because hdfs script is in the hdfs<br>directory and not common directory</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1888">HDFS-1888</a>.
     Major bug reported by sureshms and fixed by sureshms <br>
     <b>MiniDFSCluster#corruptBlockOnDatanodes() access must be public for MapReduce contrib raid</b><br>
     <blockquote>HDFS-1052 during code merge the method was made package private. It needs to be public for access in MapReduce contrib raid.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1884">HDFS-1884</a>.
     Major sub-task reported by mattf and fixed by atm (test)<br>
     <b>Improve TestDFSStorageStateRecovery</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1883">HDFS-1883</a>.
     Major sub-task reported by mattf and fixed by  (test)<br>
     <b>Recurring failures in TestBackupNode since HDFS-1052</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1881">HDFS-1881</a>.
     Major bug reported by tanping and fixed by tanping (data-node)<br>
     <b>Federation: after taking snapshot the current directory of datanode is empty</b><br>
     <blockquote>After taking a snapshot in Federation (by starting up namenode with option -upgrade), it appears that the current directory of data node does not contain the block files.  We have also verified that upgrading from 20 to Federation does not have this problem.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1877">HDFS-1877</a>.
     Minor test reported by cwchung and fixed by cwchung (test)<br>
     <b>Create a functional test for file read/write</b><br>
     <blockquote>It would be a great to have a tool, running on a real grid, to perform function test (and stress tests to certain extent) for the file operations. The tool would be written in Java and makes HDFS API calls to read, write, append, hflush hadoop files. The tool would be usable standalone, or as a building block for other regression or stress test suites (written in shell, perl, python, etc).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1876">HDFS-1876</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon <br>
     <b>One MiniDFSCluster ignores numDataNodes parameter</b><br>
     <blockquote>After the federation merge, one of the MiniDFSCluster constructors ignores its numDataNodes argument, thus causing TestFileInputFormat to fail (MAPREDUCE-2466)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1875">HDFS-1875</a>.
     Major bug reported by eepayne and fixed by eepayne (test)<br>
     <b>MiniDFSCluster hard-codes dfs.datanode.address to localhost</b><br>
     <blockquote>When creating RPC addresses that represent the communication sockets for each simulated DataNode, the MiniDFSCluster class hard-codes the address of the dfs.datanode.address port to be &quot;127.0.0.1:0&quot;<br><br>The DataNodeCluster test tool uses the MiniDFSCluster class to create a selected number of simulated datanodes on a single host. In the DataNodeCluster setup, the NameNode is not simulated but is started as a separate daemon.<br><br>The problem is that if the write requrests into the simulated datanode...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1873">HDFS-1873</a>.
     Major new feature reported by tanping and fixed by tanping <br>
     <b>Federation Cluster Management Web Console</b><br>
     <blockquote>The Federation cluster management console provides <br># Cluster summary information that shows overall cluster utilization.  A list of the name nodes that reports the used space, files and directories, blocks, live and dead datanodes <br>of each name space.<br># decommissioning status of all the data nodes who are decommissioning in process or decommissioned.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1871">HDFS-1871</a>.
     Major bug reported by sureshms and fixed by sureshms (test)<br>
     <b>Tests using MiniDFSCluster fail to compile due to HDFS-1052 changes</b><br>
     <blockquote>MiniDFSCluster public method signature changes from HDFS-1052 breaks build of mapreduce tests.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1870">HDFS-1870</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (hdfs client)<br>
     <b>Refactor DFSClient.LeaseChecker</b><br>
     <blockquote>Two simply changes:<br>- move the inner class {{LeaseChecker}} from {{DFSClient}} to a separated class;<br>- rename {{LeaseChecker}} to {{LeaseRenewer}} since it is actually renewing lease instead of checking lease.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1869">HDFS-1869</a>.
     Major bug reported by daryn and fixed by daryn (name-node)<br>
     <b>mkdirs should use the supplied permission for all of the created directories</b><br>
     <blockquote>                    A multi-level mkdir is now POSIX compliant.  Instead of creating intermediate directories with the permissions of the parent directory, intermediate directories are created with permission bits of rwxrwxrwx (0777) as modified by the current umask, plus write and search permission for the owner.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1865">HDFS-1865</a>.
     Major improvement reported by szetszwo and fixed by szetszwo (hdfs client)<br>
     <b>Share LeaseChecker thread among DFSClients</b><br>
     <blockquote>Each {{DFSClient}} runs a {{LeaseChecker}} thread within a JVM.  The number threads could be reduced by sharing the threads.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1862">HDFS-1862</a>.
     Major test reported by atm and fixed by atm (test)<br>
     <b>Improve test reliability of HDFS-1594</b><br>
     <blockquote>One of the tests I wrote in HDFS-1594 seems to be flaky in Hudson runs, despite passing reliably on my box. This JIRA is to track improving the reliability of this test.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1861">HDFS-1861</a>.
     Major improvement reported by eli and fixed by eli (data-node)<br>
     <b>Rename dfs.datanode.max.xcievers and bump its default value</b><br>
     <blockquote>Reasonably sized jobs and HBase easily exhaust the current default for dfs.datanode.max.xcievers. 4096 works better in practice.<br><br>Let&apos;s also deprecate it in favor of a more intuitive name, eg dfs.datanode.max.receiver.threads.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1856">HDFS-1856</a>.
     Major sub-task reported by mattf and fixed by mattf (test)<br>
     <b>TestDatanodeBlockScanner waits forever, errs without giving information</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1855">HDFS-1855</a>.
     Major test reported by mattf and fixed by mattf (test)<br>
     <b>TestDatanodeBlockScanner.testBlockCorruptionRecoveryPolicy() part 2 fails in two different ways</b><br>
     <blockquote>The second part of test case TestDatanodeBlockScanner.testBlockCorruptionRecoveryPolicy(), &quot;corrupt replica recovery for two corrupt replicas&quot;, always fails, half the time with a checksum error upon block replication, and half the time by timing out upon failure to delete the second corrupt replica.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1854">HDFS-1854</a>.
     Major sub-task reported by mattf and fixed by mattf (test)<br>
     <b>make failure message more useful in DFSTestUtil.waitReplication()</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1846">HDFS-1846</a>.
     Major improvement reported by atm and fixed by atm (name-node)<br>
     <b>Don&apos;t fill preallocated portion of edits log with 0x00</b><br>
     <blockquote>HADOOP-2330 added a feature to preallocate space in the local file system for the NN transaction log. That change seeks past the current end of the file and writes out some data, which on most systems results in the intervening data in the file being filled with zeros. Most underlying file systems have special handling for sparse files, and don&apos;t actually allocate blocks on disk for blocks of a file which consist completely of 0x00.<br><br>I&apos;ve seen cases in the wild where the volume an edits dir i...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1845">HDFS-1845</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>symlink comes up as directory after namenode restart</b><br>
     <blockquote>When a symlink is first created, it get added to EditLogs. When namenode is restarted, it reads from this editlog and represents a symlink correctly and saves this information to its image. If the namenode is restarted again, it reads its from this FSImage, but thinks that a symlink is a directory. This is because it uses &quot;Block[] blocks&quot; to determine if an INode is a directory, a file, or symlink. Since both a directory and a symlink has blocks as null, it thinks that a symlink is a directory.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1844">HDFS-1844</a>.
     Major test reported by daryn and fixed by daryn (test)<br>
     <b>Move -fs usage tests from hdfs into common</b><br>
     <blockquote>The -fs usage tests are in hdfs which causes an unnecessary synchronization of a common &amp; hdfs bug when changing the text.  The usages have no ties to hdfs, so they should be moved into common.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1843">HDFS-1843</a>.
     Minor improvement reported by bharathm and fixed by bharathm <br>
     <b>Discover file not found early for file append </b><br>
     <blockquote>                                              I have committed this. Thanks to Bharath!<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1840">HDFS-1840</a>.
     Major improvement reported by szetszwo and fixed by szetszwo (hdfs client)<br>
     <b>Terminate LeaseChecker when all writing files are closed.</b><br>
     <blockquote>In {{DFSClient}}, when there are files opened for write, a {{LeaseChecker}} thread is started for updating the leases periodically.  However, it never terminates when when all writing files are closed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1835">HDFS-1835</a>.
     Major bug reported by johnyoh and fixed by johnyoh (data-node)<br>
     <b>DataNode.setNewStorageID pulls entropy from /dev/random</b><br>
     <blockquote>DataNode.setNewStorageID uses SecureRandom.getInstance(&quot;SHA1PRNG&quot;) which always pulls fresh entropy.<br><br>It wouldn&apos;t be so bad if this were only the 120 bits needed by sha1, but the default impl of SecureRandom actually uses a BufferedInputStream around /dev/random and pulls 1024 bits of entropy for this one call.<br><br>If you are on a system without much entropy coming in, this call can block and block others.<br><br>Can we just change this to use &quot;new SecureRandom().nextInt(Integer.MAX_VALUE)&quot; instead?</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1833">HDFS-1833</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (data-node)<br>
     <b>Refactor BlockReceiver</b><br>
     <blockquote>There are repeated codes for creating log/error messages in BlockReceiver.  Also, some comment in the codes are incorrect, e.g.<br>{code}<br>private int numTargets;     // number of downstream datanodes including myself<br>{code}<br>but the count indeed excludes the current datanode.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1831">HDFS-1831</a>.
     Major improvement reported by sureshms and fixed by sureshms (name-node)<br>
     <b>HDFS equivalent of HADOOP-7223 changes to handle FileContext createFlag combinations</b><br>
     <blockquote>During file creation with FileContext, the expected behavior is not clearly defined for combination of createFlag EnumSet. This is HDFS equivalent of HADOOP-7223</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1829">HDFS-1829</a>.
     Major bug reported by mattf and fixed by mattf (name-node)<br>
     <b>TestNodeCount waits forever, errs without giving information</b><br>
     <blockquote>In three locations in the code, TestNodeCount waits forever on a condition. Failures result in Hudson/Jenkins &quot;Timeout occurred&quot; error message with no information about where or why. Need to replace with TimeoutExceptions that throw a stack trace and useful info about the failure mode.<br><br>Also investigate possible cause of failure.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1828">HDFS-1828</a>.
     Major sub-task reported by mattf and fixed by mattf (name-node)<br>
     <b>TestBlocksWithNotEnoughRacks intermittently fails assert</b><br>
     <blockquote>In server.namenode.TestBlocksWithNotEnoughRacks.testSufficientlyReplicatedBlocksWithNotEnoughRacks <br>assert fails at curReplicas == REPLICATION_FACTOR, but it seems that it should go higher initially, and if the test doesn&apos;t wait for it to go back down, it will fail false positive.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1827">HDFS-1827</a>.
     Major bug reported by mattf and fixed by mattf (name-node)<br>
     <b>TestBlockReplacement waits forever, errs without giving information</b><br>
     <blockquote>In method checkBlocks(), TestBlockReplacement waits forever on a condition. Failures result in Hudson/Jenkins &quot;Timeout occurred&quot; error message with no information about where or why. Need to replace with TimeoutException that throws a stack trace and useful info about the failure mode.<br><br>Also investigate possible cause of failure.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1826">HDFS-1826</a>.
     Major sub-task reported by hairong and fixed by mattf (name-node)<br>
     <b>NameNode should save image to name directories in parallel during upgrade</b><br>
     <blockquote>                                              I&amp;#39;ve committed this. Thanks, Matt!<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1823">HDFS-1823</a>.
     Blocker bug reported by tomwhite and fixed by tomwhite (scripts)<br>
     <b>start-dfs.sh script fails if HADOOP_HOME is not set</b><br>
     <blockquote>HDFS portion of HADOOP-6953</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1822">HDFS-1822</a>.
     Blocker bug reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Editlog opcodes overlap between 20 security and later releases</b><br>
     <blockquote>Same opcode are used for different operations between 0.20.security, 0.22 and 0.23. This results in failure to load editlogs on later release, especially during upgrades.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1821">HDFS-1821</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe <br>
     <b>FileContext.createSymlink with kerberos enabled sets wrong owner</b><br>
     <blockquote>TEST SETUP<br>Using attached sample hdfs java program that illustrates the issue.<br>Using cluster with Kerberos enabled on cluster<br><br># Compile instructions<br>$ javac Symlink.java -cp `hadoop classpath`<br>$ jar -cfe Symlink.jar Symlink Symlink.class<br><br># create test file for symlink to use<br>1. hadoop fs -touchz /user/username/filetest<br><br># Create symlink using file context<br>2. hadoop jar Symlink.jar ln /user/username/filetest /user/username/testsymlink<br><br># Verify owner of test file<br>3. hadoop jar Symlink.jar ls...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1818">HDFS-1818</a>.
     Major bug reported by atm and fixed by atm (test)<br>
     <b>TestHDFSCLI is failing on trunk</b><br>
     <blockquote>The commit of HADOOP-7202 changed the output of a few FsShell commands. Since HDFS tests rely on the precise format of this output, TestHDFSCLI is now failing.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1817">HDFS-1817</a>.
     Trivial improvement reported by szetszwo and fixed by szetszwo (test)<br>
     <b>Split TestFiDataTransferProtocol.java into two files</b><br>
     <blockquote>{{TestFiDataTransferProtocol}} has tests from pipeline_Fi_01 to _16 and pipeline_Fi_39 to _51.  It is natural to split them into two files.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1814">HDFS-1814</a>.
     Major new feature reported by atm and fixed by atm (hdfs client, name-node)<br>
     <b>HDFS portion of HADOOP-7214 - Hadoop /usr/bin/groups equivalent</b><br>
     <blockquote>                                              Introduces a new command, &amp;quot;hdfs groups&amp;quot;, which displays what groups are associated with a user as seen by the NameNode.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1812">HDFS-1812</a>.
     Minor bug reported by umamaheswararao and fixed by umamaheswararao (test)<br>
     <b>Address the cleanup issues in TestHDFSCLI.java</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1808">HDFS-1808</a>.
     Major bug reported by mattf and fixed by mattf (data-node, name-node)<br>
     <b>TestBalancer waits forever, errs without giving information</b><br>
     <blockquote>In three locations in the code, TestBalancer waits forever on a condition.  Failures result in Hudson/Jenkins &quot;Timeout occurred&quot; error message with no information about where or why.  Need to replace with TimeoutExceptions that throw a stack trace and useful info about the failure mode.<br><br>In waitForHeartBeat(), it is waiting on an exact match for a value that may be coarsely quantized -- i.e., significant deviation from the exact &quot;expected&quot; result may occur.  Replace with an allowed range of r...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1806">HDFS-1806</a>.
     Major bug reported by mattf and fixed by mattf (data-node, name-node)<br>
     <b>TestBlockReport.blockReport_08() and _09() are timing-dependent and likely to fail on fast servers</b><br>
     <blockquote>Method waitForTempReplica() polls every 100ms during block replication, attempting to &quot;catch&quot; a datanode in the state of having a TEMPORARY replica.  But examination of a current Hudson test failure log shows that the replica goes from &quot;start&quot; to &quot;TEMPORARY&quot; to &quot;FINALIZED&quot; in only 50ms, so of course the poll usually misses it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1797">HDFS-1797</a>.
     Major bug reported by tlipcon and fixed by tlipcon <br>
     <b>New findbugs warning introduced by HDFS-1120</b><br>
     <blockquote>HDFS-1120 introduced a new findbugs warning:<br><br>Unread field: org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.curVolume<br><br>This JIRA is to fix the simple error.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1789">HDFS-1789</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (data-node, hdfs client)<br>
     <b>Refactor frequently used codes from DFSOutputStream, BlockReceiver and DataXceiver</b><br>
     <blockquote>Below are frequently used codes <br>- Check block tokens in {{DataXceiver}}<br>- Log/error messages in {{BlockReceiver}}<br><br><br>In addition, I will refactor {{DFSOutputStream}} for testing)<br>- create socket for pipeline</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1786">HDFS-1786</a>.
     Minor bug reported by szetszwo and fixed by umamaheswararao (test)<br>
     <b>Some cli test cases expect a &quot;null&quot; message</b><br>
     <blockquote>There are a few tests cases specified in {{TestHDFSCLI}} and {{TestDFSShell}} expecting &quot;null&quot; messages.<br>e.g. in {{testHDFSConf.xml}},<br>{code}<br>          &lt;expected-output&gt;get: null&lt;/expected-output&gt;<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1785">HDFS-1785</a>.
     Major improvement reported by szetszwo and fixed by szetszwo (data-node)<br>
     <b>Cleanup BlockReceiver and DataXceiver</b><br>
     <blockquote>{{clientName.length()}} is used multiple times for determining whether the source is a client or a datanode.<br>{code}<br>if (clientName.length() == 0) {<br>//it is a datanode<br>}<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1782">HDFS-1782</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe (name-node)<br>
     <b>FSNamesystem.startFileInternal(..) throws NullPointerException</b><br>
     <blockquote>I&apos;m observing when there is one balancer running trying to run another one results in<br>&quot;Java.lang.NullPointerException&quot; error. I was hoping to see message &quot;Another balancer is running. <br>Exiting....  Exiting ...&quot;. This is a reproducible issue.<br><br>Details<br>========<br><br>1) Cluster -&gt;elrond<br><br>[hdfs@]$ hadoop version<br><br>2) Run first balancer<br>[hdfs]$ hdfs balancer<br>1<br>through XX.XX.XX.XX:1004 is succeeded.<br>[hdfs@]$ hdfs balancer<br>11/03/09 16:34:32 INFO balancer.Balancer: namenodes = <br>java.io.IOException: java.l...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1781">HDFS-1781</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe (scripts)<br>
     <b>jsvc executable delivered into wrong package...</b><br>
     <blockquote>The jsvc executable is delivered in the 0.22 hdfs package, but the script that uses it (bin/hdfs) refers to<br>$HADOOP_HOME/bin/jsvc to find it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1776">HDFS-1776</a>.
     Major bug reported by dms and fixed by bharathm <br>
     <b>Bug in Concat code</b><br>
     <blockquote>There is a bug in the concat code. Specifically: in INodeFile.appendBlocks() we need to first reassign the blocks list and then go through it and update the INode pointer. Otherwise we are not updating the inode pointer on all of the new blocks in the file.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1774">HDFS-1774</a>.
     Minor improvement reported by umamaheswararao and fixed by umamaheswararao (data-node)<br>
     <b>Small optimization to FSDataset</b><br>
     <blockquote> Inner class FSDir constructor is doing duplicate iterations over the listed files in the passed directory. We can optimize this to single loop and also we can avoid isDirectory check which will perform some native invocations. <br><br>  Consider a case: one directory has only one child directory and 10000 files. <br><br>1) First loop will get the number of children directories.<br><br>2) if (numChildren &gt; 0) , This condition will satisfy and again it will iterate 10001 times and also will check isDirectory.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1773">HDFS-1773</a>.
     Minor improvement reported by tanping and fixed by tanping (name-node)<br>
     <b>Remove a datanode from cluster if include list is not empty and this datanode is removed from both include and exclude lists</b><br>
     <blockquote>Our service engineering team who operates the clusters on a daily basis founds it is confusing that after a data node is decommissioned, there is no way to make the cluster forget about this data node and it always remains in the dead node list.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1770">HDFS-1770</a>.
     Minor test reported by eli and fixed by eli <br>
     <b>TestFiRename fails due to invalid block size</b><br>
     <blockquote>HDFS-1763 exposed a bug in TestFiRename or HDFS (see HADOOP-70800) which fails due to the following:<br><br>{quote}<br>Internal error: default blockSize is not a multiple of default bytesPerChecksum <br>java.io.IOException: Internal error: default blockSize is not a multiple of default bytesPerChecksum <br>{quote}<br><br>Previously this test passed because it used dfs.block.size (instead of dfs.blocksize), though the behavior should be equivalent since on deprecates the other.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1767">HDFS-1767</a>.
     Major sub-task reported by mattf and fixed by mattf (data-node)<br>
     <b>Namenode should ignore non-initial block reports from datanodes when in safemode during startup</b><br>
     <blockquote>Consider a large cluster that takes 40 minutes to start up.  The datanodes compete to register and send their Initial Block Reports (IBRs) as fast as they can after startup (subject to a small sub-two-minute random delay, which isn&apos;t relevant to this discussion).  <br><br>As each datanode succeeds in sending its IBR, it schedules the starting time for its regular cycle of reports, every hour (or other configured value of dfs.blockreport.intervalMsec). In order to spread the reports evenly across th...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1763">HDFS-1763</a>.
     Minor improvement reported by eli and fixed by eli <br>
     <b>Replace hard-coded option strings with variables from DFSConfigKeys</b><br>
     <blockquote>There are some places in the code where we use hard-coded strings instead of the equivalent DFSConfigKeys define, and a couple places where the default is defined multiple places (once in DFSConfigKeys and once elsewhere, though both have the same value). This is error-prone, and also a pain in that it prevents eclipse from easily showing you all the places where a particular config option is used. Let&apos;s replace all the uses of the hard-coded option strings with uses of the corresponding vari...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1761">HDFS-1761</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node)<br>
     <b>Add a new DataTransferProtocol operation, Op.TRANSFER_BLOCK, instead of using RPC</b><br>
     <blockquote>                                              Add a new DataTransferProtocol operation, Op.TRANSFER_BLOCK, for transferring RBW/Finalized with acknowledgement and without using RPC.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1760">HDFS-1760</a>.
     Major bug reported by daryn and fixed by daryn (name-node)<br>
     <b>problems with getFullPathName</b><br>
     <blockquote>FSDirectory&apos;s getFullPathName method is flawed.  Given a list of inodes, it starts at index 1 instead of 0 (based on the assumption that inode[0] is always the root inode) and then builds the string with &quot;/&quot;+inode[i].  This means the empty string is returned for the root, or when requesting the full path of the parent dir for top level items.<br><br>In addition, it&apos;s not guaranteed that the list of inodes starts with the root inode.  The inode lookup routine will only fill the inode array with the ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1757">HDFS-1757</a>.
     Major improvement reported by eli and fixed by eli (contrib/fuse-dfs)<br>
     <b>Don&apos;t compile fuse-dfs by default</b><br>
     <blockquote>The infra machines don&apos;t have fuse headers, therefore we shouldn&apos;t compile fuse-dfs by default.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1751">HDFS-1751</a>.
     Major new feature reported by daryn and fixed by daryn (data-node)<br>
     <b>Intrinsic limits for HDFS files, directories</b><br>
     <blockquote>Enforce a configurable limit on:<br>  the length of a path component<br>  the number of names in a directory<br><br>The intention is to prevent a too-long name or a too-full directory. This is not about RPC buffers, the length of command lines, etc. There may be good reasons for those kinds of limits, but that is not the intended scope of this feature. Consequently, a reasonable implementation might be to extend the existing quota checker so that it faults the creation of a name that violates the limits....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1750">HDFS-1750</a>.
     Major bug reported by szetszwo and fixed by szetszwo <br>
     <b>fs -ls hftp://file not working</b><br>
     <blockquote>{noformat}<br>hadoop dfs -touchz /tmp/file1 # create file. OK<br>hadoop dfs -ls /tmp/file1  # OK<br>hadoop dfs -ls hftp://namenode:50070/tmp/file1 # FAILED: not seeing the file<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1748">HDFS-1748</a>.
     Major bug reported by szetszwo and fixed by szetszwo (balancer)<br>
     <b>Balancer utilization classification is incomplete</b><br>
     <blockquote>{code}<br>//Balancer.java<br>  /* Return true if the given datanode is overUtilized */<br>  private boolean isOverUtilized(BalancerDatanode datanode) {<br>    return datanode.utilization &gt; (avgUtilization+threshold);<br>  }<br>  <br>  /* Return true if the given datanode is above average utilized<br>   * but not overUtilized */<br>  private boolean isAboveAvgUtilized(BalancerDatanode datanode) {<br>    return (datanode.utilization &lt;= (avgUtilization+threshold))<br>        &amp;&amp; (datanode.utilization &gt; avgUtilization);<br>  }<br>  <br>  ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1741">HDFS-1741</a>.
     Major improvement reported by cos and fixed by cos (build)<br>
     <b>Provide a minimal pom file to allow integration of HDFS into Sonar analysis</b><br>
     <blockquote>In order to user Sonar facility a project has to be either build by Maven or has a special pom &apos;wrapper&apos;. Let&apos;s provide a minimal one to allow just that. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1739">HDFS-1739</a>.
     Minor improvement reported by umamaheswararao and fixed by umamaheswararao (data-node)<br>
     <b>When DataNode throws DiskOutOfSpaceException, it will be helpfull to the user if we log the available volume size and configured block size.</b><br>
     <blockquote>DataNode will throw DiskOutOfSpaceException for new blcok write if available volume size is less than configured blcok size.<br> So, it will be helpfull to the user if we log this details.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1734">HDFS-1734</a>.
     Major bug reported by umamaheswararao and fixed by umamaheswararao (name-node)<br>
     <b>&apos;Chunk size to view&apos; option is not working in Name Node UI.</b><br>
     <blockquote>  1. Write a file to DFS<br>  2. Browse the file using Name Node UI.<br>  3. give the chunk size to view as 100 and click the refresh.<br><br>  It will say Invalid input ( getnstamp absent )<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1731">HDFS-1731</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Allow using a file to exclude certain tests from build</b><br>
     <blockquote>It would be nice to be able to exclude certain tests when running builds. For example, when a test is &quot;known flaky&quot;, you may want to exclude it from the main Hudson job, but not actually disable it in the codebase (so that it still runs as part of another Hudson job, for example).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1728">HDFS-1728</a>.
     Minor bug reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>SecondaryNameNode.checkpointSize is in byte but not MB.</b><br>
     <blockquote>The unit of SecondaryNameNode.checkpointSize is byte but not MB as stated in the following comment.<br>{code}<br>//SecondaryNameNode.java<br>   private long checkpointSize;    // size (in MB) of current Edit Log<br>{code}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1727">HDFS-1727</a>.
     Minor bug reported by umamaheswararao and fixed by sravankorumilli <br>
     <b>fsck command can display command usage if user passes any illegal argument</b><br>
     <blockquote>In fsck command if user passes the arguments like<br>./hadoop fsck -test -files -blocks -racks<br>In this case it will take / and will display whole DFS information regarding to files,blocks,racks.<br><br>But here, we are hiding the user mistake. Instead of this, we can display the command usage if user passes any invalid argument like above.<br><br>If user passes illegal optional arguments like<br>./hadoop fsck /test -listcorruptfileblocks instead of<br>./hadoop fsck /test -list-corruptfileblocks also we can displa...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1723">HDFS-1723</a>.
     Minor improvement reported by aw and fixed by jimplush <br>
     <b>quota errors messages should use the same scale</b><br>
     <blockquote>                                              Updated the Quota exceptions to now use human readable output.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1703">HDFS-1703</a>.
     Minor sub-task reported by tanping and fixed by tanping (scripts)<br>
     <b>HDFS federation: Improve start/stop scripts and add script to decommission datanodes</b><br>
     <blockquote>This Jira covers two issues:<br><br># Startup scripts should start namenodes, secondary namenodes and datanodes on hosts retunred by getConfig (new feature).  This patch is spread out to both common(HADOOP-7179) and hdfs (this Jira).  <br># Decommission script to decommission datanodes</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1692">HDFS-1692</a>.
     Major bug reported by bharathm and fixed by bharathm (data-node)<br>
     <b>In secure mode, Datanode process doesn&apos;t exit when disks fail.</b><br>
     <blockquote>In secure mode, when disks fail more than volumes tolerated, datanode process doesn&apos;t exit properly and it just hangs even though shutdown method is called. <br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1691">HDFS-1691</a>.
     Minor bug reported by humanoid and fixed by humanoid (tools)<br>
     <b>double static declaration in Configuration.addDefaultResource(&quot;hdfs-default.xml&quot;);</b><br>
     <blockquote>in /src/java/org/apache/hadoop/hdfs/tools/DFSck.java<br>double declaration <br><br>static{<br>    Configuration.addDefaultResource(&quot;hdfs-default.xml&quot;);<br>    Configuration.addDefaultResource(&quot;hdfs-site.xml&quot;);<br>}<br><br>1. in head class<br>2. before main<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1675">HDFS-1675</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (data-node)<br>
     <b>Transfer RBW between datanodes</b><br>
     <blockquote>                                              Added a new stage TRANSFER_RBW to DataTransferProtocol<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1665">HDFS-1665</a>.
     Minor bug reported by szetszwo and fixed by szetszwo (balancer)<br>
     <b>Balancer sleeps inadequately</b><br>
     <blockquote>The value of {{dfs.heartbeat.interval}} is in seconds.  Balancer seems misused it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1656">HDFS-1656</a>.
     Major bug reported by jnp and fixed by jnp <br>
     <b>getDelegationToken in HftpFileSystem should renew TGT if needed.</b><br>
     <blockquote>Fetching of delegation tokens in HftpFileSystem will fail if TGT has expired. The TGT should be renewed first if needed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1636">HDFS-1636</a>.
     Minor improvement reported by tlipcon and fixed by qwertymaniac (name-node)<br>
     <b>If dfs.name.dir points to an empty dir, namenode format shouldn&apos;t require confirmation</b><br>
     <blockquote>                                              If dfs.name.dir points to an empty dir, namenode -format no longer requires confirmation.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1630">HDFS-1630</a>.
     Major improvement reported by hairong and fixed by hairong (name-node)<br>
     <b>Checksum fsedits</b><br>
     <blockquote>HDFS-903 calculates a MD5 checksum to a saved image, so that we could verify the integrity of the image at the loading time.<br><br>The other half of the story is how to verify fsedits. Similarly we could use the checksum approach. But since a fsedit file is growing constantly, a checksum per file does not work. I am thinking to add a checksum per transaction. Is it doable or too expensive?</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1629">HDFS-1629</a>.
     Major sub-task reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>Add a method to BlockPlacementPolicy for not removing the chosen nodes</b><br>
     <blockquote>{{BlockPlacementPolicy}} supports chosen nodes in some of the {{chooseTarget(..)}} methods.  The chosen nodes will be removed from the output array.  For adding new datanodes to an existing pipeline, it is useful to keep the chosen nodes in the output array.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1628">HDFS-1628</a>.
     Minor improvement reported by rramya and fixed by johnvijoe (name-node)<br>
     <b>AccessControlException should display the full path</b><br>
     <blockquote>org.apache.hadoop.security.AccessControlException should display the full path for which the access is denied. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1627">HDFS-1627</a>.
     Major bug reported by hairong and fixed by hairong (name-node)<br>
     <b>Fix NullPointerException in Secondary NameNode</b><br>
     <blockquote>Secondary NameNode should not reset namespace if no new image is downloaded from the primary NameNode.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1626">HDFS-1626</a>.
     Minor improvement reported by acmurthy and fixed by szetszwo (name-node)<br>
     <b>Make BLOCK_INVALIDATE_LIMIT configurable</b><br>
     <blockquote>                                              Added a new configuration property dfs.block.invalidate.limit for FSNamesystem.blockInvalidateLimit.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1625">HDFS-1625</a>.
     Minor bug reported by tlipcon and fixed by szetszwo (test)<br>
     <b>TestDataNodeMXBean fails if disk space usage changes during test run</b><br>
     <blockquote>I&apos;ve seen this on our internal hudson - we get failures like:<br><br>null expected:&lt;...:{&quot;freeSpace&quot;:857683[43552],&quot;usedSpace&quot;:28672,&quot;...&gt; but was:&lt;...:{&quot;freeSpace&quot;:857683[59936],&quot;usedSpace&quot;:28672,&quot;...&gt;<br><br>because some other build on the same build slave used up some disk space during the middle of the test.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1620">HDFS-1620</a>.
     Minor improvement reported by szetszwo and fixed by qwertymaniac <br>
     <b>Rename HdfsConstants -&gt; HdfsServerConstants, FSConstants -&gt; HdfsConstants</b><br>
     <blockquote>                                              Rename HdfsConstants interface to HdfsServerConstants, FSConstants interface to HdfsConstants<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1612">HDFS-1612</a>.
     Minor bug reported by joecrobak and fixed by joecrobak (documentation)<br>
     <b>HDFS Design Documentation is outdated</b><br>
     <blockquote>I was trying to discover details about the Secondary NameNode, and came across the description below in the HDFS design doc.<br><br>{quote}<br>The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1611">HDFS-1611</a>.
     Minor bug reported by umamaheswararao and fixed by umamaheswararao (hdfs client, name-node)<br>
     <b>Some logical issues need to address.</b><br>
     <blockquote>Title: Some code level logical issues.<br><br>Description:<br><br>1. DFSClient:  <br>  Consider the below case, if we enable only info, then below log will never be logged.<br> if (ClientDatanodeProtocol.LOG.isDebugEnabled()) {<br>      ClientDatanodeProtocol.LOG.info(&quot;ClientDatanodeProtocol addr=&quot; + addr);<br>    }<br><br>2.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean()<br>  <br>  catch (NotCompliantMBeanException e) {<br>      e.printStackTrace();<br>    }<br><br>  We can avoid using stackTace(). Better to add log me...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1606">HDFS-1606</a>.
     Major new feature reported by szetszwo and fixed by szetszwo (data-node, hdfs client, name-node)<br>
     <b>Provide a stronger data guarantee in the write pipeline</b><br>
     <blockquote>                    Added two configuration properties, dfs.client.block.write.replace-datanode-on-failure.enable and dfs.client.block.write.replace-datanode-on-failure.policy.  Added a new feature to replace datanode on failure in DataTransferProtocol.  Added getAdditionalDatanode(..) in ClientProtocol.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1602">HDFS-1602</a>.
     Major bug reported by cos and fixed by boryas (name-node)<br>
     <b>NameNode storage failed replica restoration is broken</b><br>
     <blockquote>NameNode storage restore functionality doesn&apos;t work (as HDFS-903 demonstrated). This needs to be either disabled, or removed, or fixed. This feature also fails HDFS-1496</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1601">HDFS-1601</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Pipeline ACKs are sent as lots of tiny TCP packets</b><br>
     <blockquote>I noticed in an hbase benchmark that the packet counts in my network monitoring seemed high, so took a short pcap trace and found that each pipeline ACK was being sent as five packets, the first four of which only contain one byte. We should buffer these bytes and send the PipelineAck as one TCP packet.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1600">HDFS-1600</a>.
     Major bug reported by szetszwo and fixed by tlipcon (build, test)<br>
     <b>editsStored.xml cause release audit warning</b><br>
     <blockquote>The file {{src/test/hdfs/org/apache/hadoop/hdfs/tools/offlineEditsViewer/editsStored.xml}} for any new patch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1598">HDFS-1598</a>.
     Major bug reported by szetszwo and fixed by szetszwo (name-node)<br>
     <b>ListPathsServlet excludes .*.crc files</b><br>
     <blockquote>The {{.*.crc}} files are excluded by default.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1596">HDFS-1596</a>.
     Major improvement reported by patrickangeles and fixed by qwertymaniac (documentation, name-node)<br>
     <b>Move secondary namenode checkpoint configs from core-default.xml to hdfs-default.xml</b><br>
     <blockquote>                                              Removed references to the older fs.checkpoint.* properties that resided in core-site.xml<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1594">HDFS-1594</a>.
     Major bug reported by devaraj.k and fixed by atm (name-node)<br>
     <b>When the disk becomes full Namenode is getting shutdown and not able to recover</b><br>
     <blockquote>                    Implemented a daemon thread to monitor the disk usage for periodically and if the disk usage reaches the threshold value, put the name node into Safe mode so that no modification to file system will occur. Once the disk usage reaches below the threshold, name node will be put out of the safe mode. Here threshold value and interval to check the disk usage are configurable. <br/><br><br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1592">HDFS-1592</a>.
     Major bug reported by bharathm and fixed by bharathm <br>
     <b>Datanode startup doesn&apos;t honor volumes.tolerated </b><br>
     <blockquote>Datanode startup doesn&apos;t honor volumes.tolerated for hadoop 20 version.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1588">HDFS-1588</a>.
     Major improvement reported by zasran and fixed by zasran <br>
     <b>Add dfs.hosts.exclude to DFSConfigKeys and use constant in stead of hardcoded string</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1585">HDFS-1585</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon (test)<br>
     <b>HDFS-1547 broke MR build</b><br>
     <blockquote>Added a parameter to startDatanodes without maintaining old API</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1583">HDFS-1583</a>.
     Major improvement reported by liangly and fixed by liangly (name-node)<br>
     <b>Improve backup-node sync performance by wrapping RPC parameters</b><br>
     <blockquote>The journal edit records are sent by the active name-node to the backup-node with RPC:<br>{code:}<br>  public void journal(NamenodeRegistration registration,<br>                      int jAction,<br>                      int length,<br>                      byte[] records) throws IOException;<br>{code}<br>During the name-node throughput benchmark, the size of byte array _records_ is around *8000*.  Then the serialization and deserialization is time-consuming. I wrote a simple application to test RPC with byte arr...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1582">HDFS-1582</a>.
     Major improvement reported by rvs and fixed by rvs (libhdfs)<br>
     <b>Remove auto-generated native build files</b><br>
     <blockquote>                                              The native build run when from trunk now requires autotools, libtool and openssl dev libraries.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1573">HDFS-1573</a>.
     Trivial improvement reported by tlipcon and fixed by tlipcon (hdfs client)<br>
     <b>LeaseChecker thread name trace not that useful</b><br>
     <blockquote>The LeaseChecker thread in DFSClient will put a stack trace in its thread name, theoretically to help debug cases where these threads get leaked. However it just shows the stack trace of whoever is asking for the thread&apos;s name, not the stack trace of when the thread was allocated. I&apos;d like to fix this so that you can see where the thread got started, which was presumably its original intent.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1568">HDFS-1568</a>.
     Minor improvement reported by tlipcon and fixed by fwiffo (data-node)<br>
     <b>Improve DataXceiver error logging</b><br>
     <blockquote>In supporting customers we often see things like SocketTimeoutExceptions or EOFExceptions coming from DataXceiver, but the logging isn&apos;t very good. For example, if we get an IOE while setting up a connection to the downstream mirror in writeBlock, the IP of the downstream mirror isn&apos;t logged on the DN side.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1560">HDFS-1560</a>.
     Minor bug reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>dfs.data.dir permissions should default to 700</b><br>
     <blockquote>                                              The permissions on datanode data directories (configured by dfs.datanode.data.dir.perm) now default to 0700. Upon startup, the datanode will automatically change the permissions to match the configured value.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1557">HDFS-1557</a>.
     Major sub-task reported by ikelly and fixed by ikelly (name-node)<br>
     <b>Separate Storage from FSImage</b><br>
     <blockquote>FSImage currently derives from Storage and FSEditLog has to call methods directly on FSImage to access the filesystem. This JIRA is to separate the Storage class out into NNStorage so that FSEditLog is less dependent on FSImage. From this point, the other parts of the circular dependency should be easy to fix.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1551">HDFS-1551</a>.
     Major bug reported by gkesavan and fixed by gkesavan (build)<br>
     <b>fix the pom template&apos;s version</b><br>
     <blockquote>pom templates in the ivy folder should be updated to the latest version hadoo-common dependencies.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1547">HDFS-1547</a>.
     Major improvement reported by sureshms and fixed by sureshms (name-node)<br>
     <b>Improve decommission mechanism</b><br>
     <blockquote>                    Summary of changes to the decommissioning process:<br/><br><br># After nodes are decommissioned, they are not shutdown. The decommissioned nodes are not used for writes. For reads, the decommissioned nodes are given as the last location to read from.<br/><br><br># Number of live and dead decommissioned nodes are displayed in the namenode webUI.<br/><br><br># Decommissioned nodes free capacity is not count towards the the cluster free capacity.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1541">HDFS-1541</a>.
     Major sub-task reported by hairong and fixed by hairong (name-node)<br>
     <b>Not marking datanodes dead When namenode in safemode</b><br>
     <blockquote>In a big cluster, when namenode starts up,  it takes a long time for namenode to process block reports from all datanodes. Because heartbeats processing get delayed, some datanodes are erroneously marked as dead, then later on they have to register again, thus wasting time.<br><br>It would speed up starting time if the checking of dead nodes is disabled when namenode in safemode.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1540">HDFS-1540</a>.
     Major bug reported by dhruba and fixed by dhruba (data-node)<br>
     <b>Make Datanode handle errors to namenode.register call more elegantly</b><br>
     <blockquote>When a datanode receives a &quot;Connection reset by peer&quot; from the namenode.register(), it exits. This causes many datanodes to die.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1539">HDFS-1539</a>.
     Major improvement reported by dhruba and fixed by dhruba (data-node, hdfs client, name-node)<br>
     <b>prevent data loss when a cluster suffers a power loss</b><br>
     <blockquote>we have seen an instance where a external outage caused many datanodes to reboot at around the same time.  This resulted in many corrupted blocks. These were recently written blocks; the current implementation of HDFS Datanodes do not sync the data of a block file when the block is closed.<br><br>1. Have a cluster-wide config setting that causes the datanode to sync a block file when a block is finalized.<br>2. Introduce a new parameter to the FileSystem.create() to trigger the new behaviour, i.e. cau...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1536">HDFS-1536</a>.
     Major improvement reported by hairong and fixed by hairong <br>
     <b>Improve HDFS WebUI</b><br>
     <blockquote>                                              On web UI, missing block number now becomes accurate and under-replicated blocks do not include missing blocks.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1534">HDFS-1534</a>.
     Minor improvement reported by eli and fixed by eli (name-node)<br>
     <b>Fix some incorrect logs in FSDirectory</b><br>
     <blockquote>FSDirectory#removeBlock has the wrong debug log, it copied it from the add block log.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1533">HDFS-1533</a>.
     Major bug reported by pkling and fixed by pkling (hdfs client)<br>
     <b>A more elegant FileSystem#listCorruptFileBlocks API (HDFS portion)</b><br>
     <blockquote>This is the HDFS portion of HADOOP-7060.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1526">HDFS-1526</a>.
     Major bug reported by hairong and fixed by hairong (hdfs client)<br>
     <b>Dfs client name for a map/reduce task should have some randomness</b><br>
     <blockquote>                                              Make a client name has this format: DFSClient_applicationid_randomint_threadid, where applicationid = mapred.task.id or else = &amp;quot;NONMAPREDUCE&amp;quot;.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1524">HDFS-1524</a>.
     Blocker bug reported by hairong and fixed by hairong (name-node)<br>
     <b>Image loader should make sure to read every byte in image file</b><br>
     <blockquote>When I work on HDFS-1070, I come across a very strange bug. Occasionally when loading a compressed image, NameNode throws an exception complaining that the image file is corrupt. However, the result returned by the md5sum utility matches the checksum value stored in the VERSION file. <br><br>It turns out the image loader leaves 4 bytes unread after loading all the real data of an image. Those unread bytes may be some compression-related meta-info. The image loader should make sure to read to the en...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1523">HDFS-1523</a>.
     Major bug reported by cos and fixed by cos (test)<br>
     <b>TestLargeBlock is failing on trunk</b><br>
     <blockquote>TestLargeBlock is failing for more than a week not on 0.22 and trunk with<br>{noformat}<br>java.io.IOException: Premeture EOF from inputStream<br>	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:118)<br>	at org.apache.hadoop.hdfs.BlockReader.readChunk(BlockReader.java:275)<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1518">HDFS-1518</a>.
     Minor improvement reported by yaojingguo and fixed by yaojingguo (name-node)<br>
     <b>Wrong description in FSNamesystem&apos;s javadoc</b><br>
     <blockquote>&quot;4)  machine --&gt; blocklist (inverted #2)&quot; should be &quot;4)  machine --&gt; blocklist (inverted #3)&quot;</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1516">HDFS-1516</a>.
     Major bug reported by cos and fixed by cos (build)<br>
     <b>mvn-install is broken after 0.22 branch creation</b><br>
     <blockquote>Version HAS to be bumped for system testing framework artifacts (as mentioned in the build.xml file)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1513">HDFS-1513</a>.
     Minor improvement reported by eli and fixed by eli <br>
     <b>Fix a number of warnings</b><br>
     <blockquote>There are a bunch of warnings besides DeprecatedUTF8, HDFS-1512 and two warnings caused by a Java bug (http://bugs.sun.com/view_bug.do?bug_id=646014) that we can fix.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1511">HDFS-1511</a>.
     Blocker bug reported by nidaley and fixed by jghoman <br>
     <b>98 Release Audit warnings on trunk and branch-0.22</b><br>
     <blockquote>There are 98 release audit warnings on trunk. See attached txt file. These must be fixed or filtered out to get back to a reasonably small number of warnings. The OK_RELEASEAUDIT_WARNINGS property in src/test/test-patch.properties should also be set appropriately in the patch that fixes this issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1510">HDFS-1510</a>.
     Minor improvement reported by nidaley and fixed by nidaley <br>
     <b>Add test-patch.properties required by test-patch.sh</b><br>
     <blockquote>Related to HADOOP-7042.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1509">HDFS-1509</a>.
     Major improvement reported by dhruba and fixed by dhruba (name-node)<br>
     <b>Resync discarded directories in fs.name.dir during saveNamespace command</b><br>
     <blockquote>In the current implementation, if the Namenode encounters an error while writing to a fs.name.dir directory it stops writing new edits to that directory. My proposal is to make  the namenode write the fsimage to all configured directories in fs.name.dir, and from then on, continue writing fsedits to all configured directories.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1506">HDFS-1506</a>.
     Major improvement reported by hairong and fixed by hairong (name-node)<br>
     <b>Refactor fsimage loading code</b><br>
     <blockquote>I plan to do some code refactoring to make HDFS-1070 simpler. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1505">HDFS-1505</a>.
     Blocker bug reported by tlipcon and fixed by atm <br>
     <b>saveNamespace appears to succeed even if all directories fail to save</b><br>
     <blockquote>After HDFS-1071, saveNamespace now appears to &quot;succeed&quot; even if all of the individual directories failed to save.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1503">HDFS-1503</a>.
     Minor bug reported by eli and fixed by tlipcon (test)<br>
     <b>TestSaveNamespace fails</b><br>
     <blockquote>Will attach the full log. Here&apos;s the relevant snippet:<br><br>{noformat}<br>Exception in thread &quot;FSImageSaver for /home/eli/src/hdfs4/build/test/data/dfs/na<br>me1 of type IMAGE_AND_EDITS&quot; java.lang.RuntimeException: Injected fault: saveFSI<br>mage second time<br>....<br>        at java.lang.Thread.run(Thread.java:619)<br>Exception in thread &quot;FSImageSaver for /home/eli/src/hdfs4/build/test/data/dfs/na<br>me2 of type IMAGE_AND_EDITS&quot; java.lang.StackOverflowError<br>        at java.util.Arrays.copyOf(Arrays.java:2882)<br>{nofo...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1502">HDFS-1502</a>.
     Minor bug reported by eli and fixed by hairong <br>
     <b>TestBlockRecovery triggers NPE in assert</b><br>
     <blockquote>{noformat}<br>Testcase: testRBW_RWRReplicas took 10.333 sec<br>        Caused an ERROR<br>null<br>java.lang.NullPointerException<br>        at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:1881)<br>        at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testSyncReplicas(TestBlockRecovery.java:144)<br>        at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testRBW_RWRReplicas(TestBlockRecovery.java:305)<br>{noformat}<br><br>{noformat}<br>        Block reply = r.datanode.update...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1486">HDFS-1486</a>.
     Major improvement reported by cos and fixed by cos (test)<br>
     <b>Generalize CLITest structure and interfaces to facilitate upstream adoption (e.g. for web testing)</b><br>
     <blockquote>HDFS part of HADOOP-7014. HDFS side of TestCLI doesn&apos;t require any special changes but needs to be aligned with Common</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1481">HDFS-1481</a>.
     Major improvement reported by hairong and fixed by hairong (name-node)<br>
     <b>NameNode should validate fsimage before rolling</b><br>
     <blockquote>We had an incident that the fsimage at secondary NameNode was truncated but got uploaded to the primary NameNode. The primary NameNode simply rolled the image without checking its integrity, therefore causing the fsimage to corrupt. The primary NameNode should check the new image&apos;s integrity before rolling fsimage.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1480">HDFS-1480</a>.
     Major bug reported by mary and fixed by tlipcon (name-node)<br>
     <b>All replicas of a block can end up on the same rack when some datanodes are decommissioning.</b><br>
     <blockquote>It appears that all replicas of a block can end up in the same rack. The likelihood of such replicas seems to be directly related to decommissioning of nodes. <br><br>Post rolling OS upgrade (decommission 3-10% of nodes, re-install etc, add them back) of a running cluster, all replicas of about 0.16% of blocks ended up in the same rack.<br><br>Hadoop Namenode UI etc doesn&apos;t seem to know about such incorrectly replicated blocks. &quot;hadoop fsck ..&quot; does report that the blocks must be replicated on additional...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1476">HDFS-1476</a>.
     Major improvement reported by pkling and fixed by pkling (name-node)<br>
     <b>listCorruptFileBlocks should be functional while the name node is still in safe mode</b><br>
     <blockquote>This would allow us to detect whether missing blocks can be fixed using Raid and if that is the case exit safe mode earlier.<br><br>One way to make listCorruptFileBlocks available before the name node has exited from safe mode would be to perform a scan of the blocks map on each call to listCorruptFileBlocks to determine if there are any blocks with no replicas. This scan could be parallelized by dividing the space of block IDs into multiple intervals than can be scanned independently.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1473">HDFS-1473</a>.
     Major sub-task reported by tlipcon and fixed by tlipcon (name-node)<br>
     <b>Refactor storage management into separate classes than fsimage file reading/writing</b><br>
     <blockquote>Currently the FSImage class is responsible both for storage management (eg moving around files, tracking file names, the VERSION file, etc) as well as for the actual serialization and deserialization of the &quot;fsimage&quot; file within the storage directory.<br><br>I&apos;d like to refactor the loading and saving code into new classes. This will make testing easier and also make the major changes in HDFS-1073 easier to understand.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1467">HDFS-1467</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon (data-node)<br>
     <b>Append pipeline never succeeds with more than one replica</b><br>
     <blockquote>TestPipelines appears to be failing on trunk:<br><br>Should be RBW replica after sequence of calls append()/write()/hflush() expected:&lt;RBW&gt; but was:&lt;FINALIZED&gt;<br>junit.framework.AssertionFailedError: Should be RBW replica after sequence of calls append()/write()/hflush() expected:&lt;RBW&gt; but was:&lt;FINALIZED&gt;<br>        at org.apache.hadoop.hdfs.TestPipelines.pipeline_01(TestPipelines.java:109)<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1463">HDFS-1463</a>.
     Major bug reported by dhruba and fixed by dhruba (name-node)<br>
     <b>accessTime updates should not occur in safeMode</b><br>
     <blockquote>FSNamesystem.getBlockLocations sometimes need to update the accessTime of files. If the namenode is in safemode, this call should fail.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1458">HDFS-1458</a>.
     Major improvement reported by hairong and fixed by hairong (name-node)<br>
     <b>Improve checkpoint performance by avoiding unnecessary image downloads</b><br>
     <blockquote>If secondary namenode could verify that the image it has on its disk is the same as the one in the primary NameNode, it could skip downloading the image from the primary NN, thus completely eliminating the image download overhead.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1448">HDFS-1448</a>.
     Major new feature reported by zasran and fixed by zasran (tools)<br>
     <b>Create multi-format parser for edits logs file, support binary and XML formats initially</b><br>
     <blockquote>                    Offline edits viewer feature adds oev tool to hdfs script. Oev makes it possible to convert edits logs to/from native binary and XML formats. It uses the same framework as Offline image viewer.<br/><br><br><br/><br><br>Example usage:<br/><br><br><br/><br><br>$HADOOP_HOME/bin/hdfs oev -i edits -o output.xml<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1445">HDFS-1445</a>.
     Major sub-task reported by mattf and fixed by mattf (data-node)<br>
     <b>Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file</b><br>
     <blockquote>                    Batch hardlinking during &amp;quot;upgrade&amp;quot; snapshots, cutting time from aprx 8 minutes per volume to aprx 8 seconds.  Validated in both Linux and Windows.  Depends on prior integration with patch for &lt;a href=&quot;/jira/browse/HADOOP-7133&quot; title=&quot;CLONE to COMMON - HDFS-1445 Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file&quot;&gt;&lt;strike&gt;HADOOP-7133&lt;/strike&gt;&lt;/a&gt;.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1442">HDFS-1442</a>.
     Major improvement reported by jnp and fixed by jnp <br>
     <b>Api to get delegation token in Hdfs</b><br>
     <blockquote>FileContext uses Hdfs instead of DistributedFileSystem. We need to add delegation token APIs in Hdfs class as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1398">HDFS-1398</a>.
     Major sub-task reported by tanping and fixed by  <br>
     <b>HDFS federation: Upgrade and rolling back of Federation</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1381">HDFS-1381</a>.
     Major bug reported by jghoman and fixed by jimplush (test)<br>
     <b>HDFS javadocs hard-code references to dfs.namenode.name.dir and dfs.datanode.data.dir parameters</b><br>
     <blockquote>                                              Updated the JavaDocs to appropriately represent the new Configuration Keys that are used in the code. The docs did not match the code.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1378">HDFS-1378</a>.
     Major improvement reported by tlipcon and fixed by atm (name-node)<br>
     <b>Edit log replay should track and report file offsets in case of errors</b><br>
     <blockquote>Occasionally there are bugs or operational mistakes that result in corrupt edit logs which I end up having to repair by hand. In these cases it would be very handy to have the error message also print out the file offsets of the last several edit log opcodes so it&apos;s easier to find the right place to edit in the OP_INVALID marker. We could also use this facility to provide a rough estimate of how far along edit log replay the NN is during startup (handy when a 2NN has died and replay takes a w...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1377">HDFS-1377</a>.
     Blocker bug reported by eli and fixed by eli (name-node)<br>
     <b>Quota bug for partial blocks allows quotas to be violated </b><br>
     <blockquote>There&apos;s a bug in the quota code that causes them not to be respected when a file is not an exact multiple of the block size. Here&apos;s an example:<br><br>{code}<br>$ hadoop fs -mkdir /test<br>$ hadoop dfsadmin -setSpaceQuota 384M /test<br>$ ls dir/ | wc -l   # dir contains 101 files<br>101<br>$ du -ms dir        # each is 3mb<br>304	dir<br>$ hadoop fs -put dir /test<br>$ hadoop fs -count -q /test<br>        none             inf       402653184      -550502400            2          101          317718528 hdfs://haus01.sf.clouder...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1371">HDFS-1371</a>.
     Major bug reported by knoguchi and fixed by tanping (hdfs client, name-node)<br>
     <b>One bad node can incorrectly flag many files as corrupt</b><br>
     <blockquote>On our cluster, 12 files were reported as corrupt by fsck even though the replicas on the datanodes were healthy.<br>Turns out that all the replicas (12 files x 3 replicas per file) were reported corrupt from one node.<br><br>Surprisingly, these files were still readable/accessible from dfsclient (-get/-cat) without any problems.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1360">HDFS-1360</a>.
     Minor bug reported by tlipcon and fixed by tlipcon (test)<br>
     <b>TestBlockRecovery should bind ephemeral ports</b><br>
     <blockquote>TestBlockRecovery starts up a DN, but doesn&apos;t configure the various ports to be ephemeral, so the test fails if run on a machine where another DN is already running.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1335">HDFS-1335</a>.
     Major improvement reported by hairong and fixed by hairong (hdfs client, name-node)<br>
     <b>HDFS side of HADOOP-6904: first step towards inter-version communications between dfs client and NameNode</b><br>
     <blockquote>The idea is that for getProtocolVersion, NameNode checks if the client and server versions are compatible if the server version is greater than the client version. If no, throws a VersionIncompatible exception; otherwise, returns the server version.<br><br>On the dfs client side, when creating a NameNode proxy, catches the VersionMismatch exception and then checks if the client version and the server version are compatible if the client version is greater than the server version. If not compatible,...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1332">HDFS-1332</a>.
     Minor improvement reported by tlipcon and fixed by yuzhihong@gmail.com (name-node)<br>
     <b>When unable to place replicas, BlockPlacementPolicy should log reasons nodes were excluded</b><br>
     <blockquote>Whenever the block placement policy determines that a node is not a &quot;good target&quot; it could add the reason for exclusion to a list, and then when we log &quot;Not able to place enough replicas&quot; we could say why each node was refused. This would help new users who are having issues on pseudo-distributed (eg because their data dir is on /tmp and /tmp is full). Right now it&apos;s very difficult to figure out the issue.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1330">HDFS-1330</a>.
     Major new feature reported by hairong and fixed by johnvijoe (data-node)<br>
     <b>Make RPCs to DataNodes timeout</b><br>
     <blockquote>This jira aims to make client/datanode or datanode/datanode RPC to have a timeout of DataNode#socketTimeout.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1321">HDFS-1321</a>.
     Minor bug reported by garymurry and fixed by jimplush (name-node)<br>
     <b>If service port and main port are the same, there is no clear log message explaining the issue.</b><br>
     <blockquote>                                              Added a check to match the sure RPC and HTTP Port&amp;#39;s on the NameNode were not set to the same value, otherwise an IOException is throw with the appropriate message.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1295">HDFS-1295</a>.
     Major sub-task reported by dhruba and fixed by mattf (name-node)<br>
     <b>Improve namenode restart times by short-circuiting the first block reports from datanodes</b><br>
     <blockquote>The namenode restart is dominated by the performance of processing block reports. On a 2000 node cluster with 90 million blocks,  block report processing takes 30 to 40 minutes. The namenode &quot;diffs&quot; the contents of the incoming block report with the contents of the blocks map, and then applies these diffs to the blocksMap, but in reality there is no need to compute the &quot;diff&quot; because this is the first block report from the datanode.<br><br>This code change improves block report processing time by 3...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1257">HDFS-1257</a>.
     Major bug reported by rvadali and fixed by eepayne (name-node)<br>
     <b>Race condition on FSNamesystem#recentInvalidateSets introduced by HADOOP-5124</b><br>
     <blockquote>HADOOP-5124 provided some improvements to FSNamesystem#recentInvalidateSets. But it introduced unprotected access to the data structure recentInvalidateSets. Specifically, FSNamesystem.computeInvalidateWork accesses recentInvalidateSets without read-lock protection. If there is concurrent activity (like reducing replication on a file) that adds to recentInvalidateSets, the name-node crashes with a ConcurrentModificationException.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1217">HDFS-1217</a>.
     Major improvement reported by szetszwo and fixed by lakshman (name-node)<br>
     <b>Some methods in the NameNdoe should not be public</b><br>
     <blockquote>There are quite a few NameNode methods which are not required to be public.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1206">HDFS-1206</a>.
     Major bug reported by szetszwo and fixed by cos (test)<br>
     <b>TestFiHFlush fails intermittently</b><br>
     <blockquote>When I was testing HDFS-1114, the patch passed all tests except TestFiHFlush.  Then, I tried to print out some debug messages, however, TestFiHFlush succeeded after added the messages.<br><br>TestFiHFlush probably depends on the speed of BlocksMap.  If BlocksMap is slow enough, then it will pass.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1189">HDFS-1189</a>.
     Major bug reported by xiaokang and fixed by johnvijoe (name-node)<br>
     <b>Quota counts missed between clear quota and set quota</b><br>
     <blockquote>HDFS Quota counts will be missed between a clear quota operation and a set quota.<br><br>When setting quota for a dir, the INodeDirectory will be replaced by INodeDirectoryWithQuota and dir.isQuotaSet() becomes true. When INodeDirectoryWithQuota  is newly created, quota counting will be performed. However, when clearing quota, the quota conf is set to -1 and dir.isQuotaSet() becomes false while INodeDirectoryWithQuota will NOT be replaced back to INodeDirectory.<br><br>FSDirectory.updateCount just update...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1149">HDFS-1149</a>.
     Major bug reported by tlipcon and fixed by atm (name-node)<br>
     <b>Lease reassignment is not persisted to edit log</b><br>
     <blockquote>During lease recovery, the lease gets reassigned to a special NN holder. This is not currently persisted to the edit log, which means that after an NN restart, the original leaseholder could end up allocating more blocks or completing a file that has already started recovery.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1120">HDFS-1120</a>.
     Major improvement reported by hammer and fixed by qwertymaniac (data-node)<br>
     <b>Make DataNode&apos;s block-to-device placement policy pluggable</b><br>
     <blockquote>                                              Make the DataNode&amp;#39;s block-volume choosing policy pluggable.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1117">HDFS-1117</a>.
     Major improvement reported by vicaya and fixed by vicaya <br>
     <b>HDFS portion of HADOOP-6728 (ovehaul metrics framework)</b><br>
     <blockquote>                    Metrics names are standardized to use CapitalizedCamelCase. Some examples:<br/><br><br># Metrics names using &amp;quot;_&amp;quot; is changed to new naming scheme. Eg: bytes_written changes to BytesWritten.<br/><br><br># All metrics names start with capitals. Example: threadsBlocked changes to ThreadsBlocked.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1073">HDFS-1073</a>.
     Major improvement reported by sanjay.radia and fixed by tlipcon <br>
     <b>Simpler model for Namenode&apos;s fs Image and edit Logs </b><br>
     <blockquote>                    The NameNode&amp;#39;s storage layout for its name directories has been reorganized to be more robust. Each edit now has a unique transaction ID, and each file is associated with a transaction ID (for checkpoints) or a range of transaction IDs (for edit logs).<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1070">HDFS-1070</a>.
     Major sub-task reported by hairong and fixed by hairong (name-node)<br>
     <b>Speedup NameNode image loading and saving by storing local file names</b><br>
     <blockquote>                    This changes the fsimage format to be <br/><br><br>root directory-1 directory-2 ... directoy-n.<br/><br><br>Each directory stores all its children in the following format:<br/><br><br>Directory_full_path_name num_of_children child-1 ... child-n.<br/><br><br>Each inode stores only the last component of its path name into fsimage.<br/><br><br>This change requires an upgrade at deployment.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1052">HDFS-1052</a>.
     Major new feature reported by sureshms and fixed by sureshms (name-node)<br>
     <b>HDFS scalability with multiple namenodes</b><br>
     <blockquote>HDFS currently uses a single namenode that limits scalability of the cluster. This jira proposes an architecture to scale the nameservice horizontally using multiple namenodes.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-1001">HDFS-1001</a>.
     Minor bug reported by bcwalrus and fixed by bcwalrus (data-node)<br>
     <b>DataXceiver and BlockReader disagree on when to send/recv CHECKSUM_OK</b><br>
     <blockquote>Running the TestPread with additional debug statements reveals that the BlockReader sends CHECKSUM_OK when the DataXceiver doesn&apos;t expect it. Currently it doesn&apos;t matter since DataXceiver closes the connection after each op, and CHECKSUM_OK is the last thing on the wire. But if we want to cache connections, they need to agree on the exchange of CHECKSUM_OK.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-863">HDFS-863</a>.
     Major bug reported by tlipcon and fixed by kengoodhope (test)<br>
     <b>Potential deadlock in TestOverReplicatedBlocks</b><br>
     <blockquote>TestOverReplicatedBlocks.testProcesOverReplicateBlock synchronizes on namesystem.heartbeats without synchronizing on namesystem first. Other places in the code synchronize namesystem, then heartbeats. It&apos;s probably unlikely to occur in this test case, but it&apos;s a simple fix.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-780">HDFS-780</a>.
     Major test reported by eli and fixed by eli (contrib/fuse-dfs)<br>
     <b>Revive TestFuseDFS</b><br>
     <blockquote>Looks like TestFuseDFS has bit rot. Let&apos;s revive it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-560">HDFS-560</a>.
     Minor improvement reported by stevel@apache.org and fixed by stevel@apache.org (build)<br>
     <b>Proposed enhancements/tuning to hadoop-hdfs/build.xml</b><br>
     <blockquote>sibling list of HADOOP-6206, enhancements to the hdfs build for easier single-system build/test</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-420">HDFS-420</a>.
     Major improvement reported by dbrodsky and fixed by bockelman (contrib/fuse-dfs)<br>
     <b>Fuse-dfs should cache fs handles</b><br>
     <blockquote>Fuse-dfs should cache fs handles on a per-user basis. This significantly increases performance, and has the side effect of fixing the current code which leaks fs handles.<br><br>The original bug description follows:<br><br>I run the following test:<br><br>1.  Run hadoop DFS in single node mode<br>2.  start up fuse_dfs<br>3.  copy my source tree, about 250 megs, into the DFS<br>     cp -av * /mnt/hdfs/<br><br>in /var/log/messages I keep seeing:<br><br>Dec 22 09:02:08 bodum fuse_dfs: ERROR: hdfs trying to utime /bar/backend-trunk2/s...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/HDFS-73">HDFS-73</a>.
     Blocker bug reported by rangadi and fixed by umamaheswararao (hdfs client)<br>
     <b>DFSOutputStream does not close all the sockets</b><br>
     <blockquote>When DFSOutputStream writes to multiple blocks, it closes only the socket opened for the last block. When it is done with writing to one block it should close the socket.<br><br>I noticed this when I was fixing HADOOP-3067. After fixing HADOOP-3067, there were still a lot of sockets open (but not enough to fail the tests). These sockets were used to write to blocks.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3322">MAPREDUCE-3322</a>.
     Major improvement reported by acmurthy and fixed by acmurthy (documentation, mrv2)<br>
     <b>Create a better index.html for maven docs</b><br>
     <blockquote>Create a better index.html for maven docs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3321">MAPREDUCE-3321</a>.
     Minor bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Disable some failing legacy tests for MRv2 builds to go through</b><br>
     <blockquote>By-product of MR-3214. Disable tests for the short term until fixes are available for all tests.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3313">MAPREDUCE-3313</a>.
     Blocker bug reported by ravidotg and fixed by hitesh (mrv2, test)<br>
     <b>TestResourceTrackerService failing in trunk some times</b><br>
     <blockquote>TestResourceTrackerService is failing in trunk sometimes with the following error:<br><br>testDecommissionWithIncludeHosts(org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService)  Time elapsed: 0.876 sec  &lt;&lt;&lt; ERROR!<br>java.lang.NullPointerException<br>  at org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics.getNumDecommisionedNMs(ClusterMetrics.java:78)<br>  at org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService.testDecommissionWithIncludeHosts(TestResourceTr...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3306">MAPREDUCE-3306</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (mrv2, nodemanager)<br>
     <b>Cannot run apps after MAPREDUCE-2989</b><br>
     <blockquote>Seeing this in NM logs when trying to run jobs.<br>{code}<br>2011-10-28 21:40:21,263 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Processing application_1319818154209_0001 of type APPLICATION_INITED<br>2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..<br>java.util.NoSuchElementException<br>        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)<br>        at java.util.HashMap$ValueIterator....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3304">MAPREDUCE-3304</a>.
     Major bug reported by raviprak and fixed by raviprak (mrv2, test)<br>
     <b>TestRMContainerAllocator#testBlackListedNodes fails intermittently</b><br>
     <blockquote>Thanks to Hitesh for verifying!<br><br>bq. The heartbeat event should be drained before the schedule call.<br>bq. -- Hitesh<br><br>I can see this test fail intermittently on my Mac OSX 10.5 and Fedora 14 machines. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3296">MAPREDUCE-3296</a>.
     Major bug reported by vinodkv and fixed by vinodkv (build)<br>
     <b>Pending(9) findBugs warnings</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3295">MAPREDUCE-3295</a>.
     Critical bug reported by mahadev and fixed by  <br>
     <b>TestAMAuthorization failing on branch 0.23.</b><br>
     <blockquote>The test seems to fail both on Mac and linux. Trace in the next comment.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3292">MAPREDUCE-3292</a>.
     Critical bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>In secure mode job submission fails with Provider org.apache.hadoop.mapreduce.security.token.JobTokenIndentifier$Renewer not found.</b><br>
     <blockquote>This happens when you submit a job to a secure cluster. Also, its only the first time the error shows up. On the next submission of the job, the job passes.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3290">MAPREDUCE-3290</a>.
     Major bug reported by rramya and fixed by acmurthy (mrv2)<br>
     <b>list-active-trackers throws NPE</b><br>
     <blockquote>bin/mapred -list-active-trackers throws NPE in mrV2. Trace in the next comment.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3288">MAPREDUCE-3288</a>.
     Blocker bug reported by rramya and fixed by mahadev (mrv2)<br>
     <b>Mapreduce 23 builds failing</b><br>
     <blockquote>Hadoop mapreduce 0.23 builds are failing.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3285">MAPREDUCE-3285</a>.
     Blocker bug reported by acmurthy and fixed by sseth (mrv2)<br>
     <b>Tests on branch-0.23 failing </b><br>
     <blockquote>Most are failing with some kerberos login exception:<br><br>Running org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks<br>Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.548 sec &lt;&lt;&lt; FAILURE!<br>--<br>Running org.apache.hadoop.yarn.server.resourcemanager.TestAppManager<br>Tests run: 8, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.125 sec &lt;&lt;&lt; FAILURE!<br>Running org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger<br>Tests run: 3, Failures: 0, Errors: 1, S...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3284">MAPREDUCE-3284</a>.
     Major bug reported by rramya and fixed by acmurthy (mrv2)<br>
     <b>bin/mapred queue fails with JobQueueClient ClassNotFoundException</b><br>
     <blockquote>bin/mapred queue fails with the following exception:<br><br>{code}<br><br>-bash$ bin/mapred queue<br>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/JobQueueClient<br>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.JobQueueClient<br>        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)<br>        at java.security.AccessController.doPrivileged(Native Method)<br>        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br>        at java.lang....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3282">MAPREDUCE-3282</a>.
     Critical bug reported by rramya and fixed by acmurthy (mrv2)<br>
     <b>bin/mapred job -list throws exception</b><br>
     <blockquote>bin/mapred job -list throws exception when mapreduce.framework.name is set to &quot;yarn&quot;<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3281">MAPREDUCE-3281</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (test)<br>
     <b>TestLinuxContainerExecutorWithMocks failing on trunk.</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3279">MAPREDUCE-3279</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>TestJobHistoryParsing broken</b><br>
     <blockquote>Broken after 3264, the test was verifying against the default user.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3275">MAPREDUCE-3275</a>.
     Critical improvement reported by revans2 and fixed by revans2 (documentation, mrv2)<br>
     <b>Add docs for WebAppProxy</b><br>
     <blockquote>In my haste to get the WebAppProxy code in the documentation for it was neglected.  This is to fix that.  Docs need to be added to ClusterSetup.html about how to configure and use the WebAppProxy.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3274">MAPREDUCE-3274</a>.
     Blocker bug reported by revans2 and fixed by revans2 (applicationmaster, mrv2)<br>
     <b>Race condition in MR App Master Preemtion can cause a dead lock</b><br>
     <blockquote>There appears to be a race condition in the MR App Master in relation to preempting reducers to let a mapper run.  In the particular case that I have been debugging a reducer was selected for preemption that did not have a container assigned to it yet. When the container became available that reduce started running and the previous TA_KILL event appears to have been ignored.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3269">MAPREDUCE-3269</a>.
     Blocker bug reported by rramya and fixed by mahadev (mrv2)<br>
     <b>Jobsummary logs not being moved to a separate file</b><br>
     <blockquote>The jobsummary logs are not being moved to a separate file. Below is the configuration in log4j.properties:<br><br>{noformat}<br>mapred.jobsummary.logger=INFO,console<br>log4j.logger.org.apache.hadoop.mapreduce.jobhistory.JobSummary=${mapred.jobsummary.logger}<br>log4j.additivity.org.apache.hadoop.mapreduce.jobhistory.JobSummary=false<br>log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender<br>log4j.appender.JSA.File=${hadoop.log.dir}/mapred-jobsummary.log<br>log4j.appender.JSA.layout=org.apache.log4j.Pattern...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3264">MAPREDUCE-3264</a>.
     Blocker bug reported by tlipcon and fixed by acmurthy (mrv2)<br>
     <b>mapreduce.job.user.name needs to be set automatically</b><br>
     <blockquote>Currently in MR2 I have to manually specify mapreduce.job.user.name for each job. It&apos;s not picking it up from the security infrastructure, at least when running with DefaultContainerExecutor. This is obviously incorrect.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3263">MAPREDUCE-3263</a>.
     Blocker bug reported by rramya and fixed by hitesh (build, mrv2)<br>
     <b>compile-mapred-test target fails</b><br>
     <blockquote>Compile mapred test target is broken due to which the builds are not archiving the test jars.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3262">MAPREDUCE-3262</a>.
     Critical bug reported by hitesh and fixed by hitesh (mrv2, nodemanager)<br>
     <b>A few events are not handled by the NodeManager in failure scenarios</b><br>
     <blockquote>Need to handle kill container event in localization failed state. <br>Need to handle resource localized in localization failed state. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3261">MAPREDUCE-3261</a>.
     Major bug reported by criccomini and fixed by  (applicationmaster)<br>
     <b>AM unable to release containers</b><br>
     <blockquote>I&apos;m probably doing something wrong here, but I can&apos;t figure it out.<br><br>My ApplicationMaster is sending an AllocateRequest with ContainerIds to release. My ResourceManager logs say:<br><br>2011-10-25 10:02:52,236 WARN  resourcemanager.RMAuditLogger (RMAuditLogger.java:logFailure(207)) - USER=criccomi	IP=127.0.0.1	OPERATION=AM Released Container	TARGET=FifoScheduler	RESULT=FAILURE	DESCRIPTION=Trying to release container not owned by app or with invalid id	PERMISSIONS=Unauthorized access or invalid cont...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3259">MAPREDUCE-3259</a>.
     Blocker bug reported by kihwal and fixed by kihwal (mrv2, nodemanager)<br>
     <b>ContainerLocalizer should get the proper java.library.path from LinuxContainerExecutor</b><br>
     <blockquote>As seen in MAPREDUCE-2915, java.library.path is not being passed when the LCE spawns a JVM for ContainerLocalizer. <br><br>However, unlike branch-0.20-security, the task runtime in 0.23 is unaffected by this. This is because tasks&apos; run-time environment is specified in the launch script by client. Setting LD_LIBRARY_PATH is the primary way of specifying the locations of required native library in this case. The config property, mapreduce.admin.user.env is always set in the job environment and the de...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3258">MAPREDUCE-3258</a>.
     Blocker bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>Job counters missing from AM and history UI</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3257">MAPREDUCE-3257</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2, resourcemanager, security)<br>
     <b>Authorization checks needed for AM-&gt;RM protocol</b><br>
     <blockquote>This is like MAPREDUCE-3256, but for AM-&gt;RM protocol.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3256">MAPREDUCE-3256</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2, nodemanager, security)<br>
     <b>Authorization checks needed for AM-&gt;NM protocol</b><br>
     <blockquote>We already authenticate requests to NM from any AM. We also need to authorize the requests, otherwise a rogue AM, *but with proper tokens and thus authenticated to talk to NM*, could either launch or kill a container with different ContainerID. We have two options:<br> - Remove the explicit passing of the ContainerId as part of the API and instead get it from the RPC layer. In this case, we will need a ContainerToken for each container.<br> - Do explicit authorization checks without relying on gett...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3254">MAPREDUCE-3254</a>.
     Blocker bug reported by rramya and fixed by acmurthy (contrib/streaming, mrv2)<br>
     <b>Streaming jobs failing with PipeMapRunner ClassNotFoundException</b><br>
     <blockquote>ClassNotFoundException: org.apache.hadoop.streaming.PipeMapRunner encountered while running streaming jobs. Stack trace in the next comment.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3253">MAPREDUCE-3253</a>.
     Blocker bug reported by daijy and fixed by acmurthy (mrv2)<br>
     <b>ContextFactory throw NoSuchFieldException</b><br>
     <blockquote>I see exceptions from ContextFactory when I am running Pig unit test:<br>Caused by: java.lang.IllegalArgumentException: Can&apos;t find field<br>        at org.apache.hadoop.mapreduce.ContextFactory.&lt;clinit&gt;(ContextFactory.java:139)<br>Caused by: java.lang.NoSuchFieldException: reporter<br>        at java.lang.Class.getDeclaredField(Class.java:1882)<br>        at org.apache.hadoop.mapreduce.ContextFactory.&lt;clinit&gt;(ContextFactory.java:126)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3252">MAPREDUCE-3252</a>.
     Critical bug reported by tlipcon and fixed by tlipcon (mrv2, task)<br>
     <b>MR2: Map tasks rewrite data once even if output fits in sort buffer</b><br>
     <blockquote>I found that, even if the output of a map task fits entirely in its sort buffer, it was rewriting the output entirely rather than just renaming the first spill into place. This is due to RawLocalFileSystem.rename() falling back to a copy if renameTo() fails. The first rename attempt was failing because no one has called mkdir for the output directory yet.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3250">MAPREDUCE-3250</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>When AM restarts, client keeps reconnecting to the new AM and prints a lots of logs.</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3249">MAPREDUCE-3249</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>Recovery of MR AMs with reduces fails the subsequent generation of the job</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3242">MAPREDUCE-3242</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Trunk compilation broken with bad interaction from MAPREDUCE-3070 and MAPREDUCE-3239.</b><br>
     <blockquote>Looks like patch command threw away some of the changes when I committed MAPREDUCE-3239 after MAPREDUCE-3070.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3240">MAPREDUCE-3240</a>.
     Blocker bug reported by vinodkv and fixed by hitesh (mrv2, nodemanager)<br>
     <b>NM should send a SIGKILL for completed containers also</b><br>
     <blockquote>This is to address the containers which exit properly after spawning sub-processes themselves. We don&apos;t want to leave these sub-process-tree or else they can pillage the NM&apos;s resources.<br><br>Today, we already have code to send SIGKILL to the whole process-trees (because of single sessionId resulting from  setsid) when the container is alive. We need to obtain the PID of the containers when they start and use that PID to send signal for completed containers&apos; case also.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3239">MAPREDUCE-3239</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>Use new createSocketAddr API in MRv2 to give better error messages on misconfig</b><br>
     <blockquote>HADOOP-7749 added a NetUtils call which will include the configuration name as part of the exception message. This is handy if you accidentally specify some invalid string, or forget to specify a required parameter. This JIRA is to make MR2 use the new API.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3233">MAPREDUCE-3233</a>.
     Blocker sub-task reported by karams and fixed by mahadev (mrv2)<br>
     <b>AM fails to restart when first AM is killed</b><br>
     <blockquote>Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml. Started yarn cluster.<br>Sumbitted Sleep Job of 100K maps tasks as following -:<br>$HADOOP_COMMON_HOME/bin/hadoop jar $HADOOP_MAPRED_HOME/hadoop-test.jar sleep -m 100000 -r 0 -mt 1000 -rt 1000<br><br>when around 53K tasks go, login node running AppMaster, and killed AppMaster with kill -9<br><br>Resource Manager tried restart AM uptio max-retris but failed with following -:<br>{code}<br>11/10/19 15:29:09 INFO mapreduce.Job: Job job_1319036155027_0002 failed...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3228">MAPREDUCE-3228</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>MR AM hangs when one node goes bad</b><br>
     <blockquote>Found this on one of the gridmix runs, again. One of the nodes went real bad, the job had three containers running on the node. Eventually, AM marked the tasks as timedout and initiated cleanup of the failed containers via {{stopContainer()}}. The later got stuck at the faulty node, the tasks are stuck in FAIL_CONTAINER_CLEANUP stage and the job lies in there waiting for ever.<br><br>Thanks to [~Karams] for helping with this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3226">MAPREDUCE-3226</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (mrv2, task)<br>
     <b>Few reduce tasks hanging in a gridmix-run</b><br>
     <blockquote>In a gridmix run with ~1000 jobs, one job is getting stuck because of 2-3 hanging reducers. All of the them are stuck after downloading all map outputs and have the following thread dump.<br><br>{code}<br>&quot;EventFetcher for fetching Map Completion Events&quot; daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]<br>   java.lang.Thread.State: TIMED_WAITING (sleeping)<br>        at java.lang.Thread.sleep(Native Method)<br>        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFe...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3212">MAPREDUCE-3212</a>.
     Minor bug reported by kam_iitkgp and fixed by kamesh (mrv2)<br>
     <b>Message displays while executing yarn command should be proper</b><br>
     <blockquote>execute yarn command without any arguments. It displays<br>{noformat}Usage: hadoop [--config confdir] COMMAND {noformat}.<br>Rather the message should be<br>{noformat}Usage: yarn [--config confdir] COMMAND{noformat}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3209">MAPREDUCE-3209</a>.
     Major bug reported by vinodkv and fixed by vinodkv (build, mrv2)<br>
     <b>Jenkins reports 160 FindBugs warnings</b><br>
     <blockquote>See<br>https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1055//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-common.html<br>https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1055//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html<br>https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1055//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3208">MAPREDUCE-3208</a>.
     Minor bug reported by liangzhwa and fixed by liangzhwa (mrv2)<br>
     <b>NPE while flushing TaskLogAppender</b><br>
     <blockquote>NPE will be throwed out while calling flush() of TaskLogAppender,if the QuietWriter isn&apos;t initialized in advance.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3205">MAPREDUCE-3205</a>.
     Blocker improvement reported by tlipcon and fixed by tlipcon (mrv2, nodemanager)<br>
     <b>MR2 memory limits should be pmem, not vmem</b><br>
     <blockquote>                    Resource limits are now expressed and enforced in terms of physical memory, rather than virtual memory. The virtual memory limit is set as a configurable multiple of the physical limit. The NodeManager&amp;#39;s memory usage is now configured in units of MB rather than GB.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3204">MAPREDUCE-3204</a>.
     Major bug reported by sureshms and fixed by tucu00 (build)<br>
     <b>mvn site:site fails on MapReduce</b><br>
     <blockquote>This problem does not happen on 0.23. See details in the next comment.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3203">MAPREDUCE-3203</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Fix some javac warnings in MRAppMaster.</b><br>
     <blockquote>MAPREDUCE-2762 accidentally introduced a couple of javac warning. This jira is to fix some of them in MRAppMaster. We have plenty more to fix but I dont intend to fix them all here. This is just so that the hudson bot does not -1 other patches with javac warnings.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3199">MAPREDUCE-3199</a>.
     Major bug reported by vinodkv and fixed by vinodkv (mrv2, test)<br>
     <b>TestJobMonitorAndPrint is broken on trunk</b><br>
     <blockquote>I bisected this down to MAPREDUCE-3003 changes. The parent project for client-core changed to hadoop-project which doesn&apos;t have the log4j configuration unlike the previous parent hadoop-mapreduce-client.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3198">MAPREDUCE-3198</a>.
     Trivial bug reported by hitesh and fixed by acmurthy (mrv2)<br>
     <b>Change mode for hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/resources/mock-container-executor to 755 </b><br>
     <blockquote>The file is checked in with 644 permissions. TestLinuxContainerExecutorWithMocks changes the file mode to add executable permission if needed resulting in a modified file for &apos;git/svn status&apos; when tests are run. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3197">MAPREDUCE-3197</a>.
     Major bug reported by anupamseth and fixed by mahadev (mrv2)<br>
     <b>TestMRClientService failing on building clean checkout of branch 0.23</b><br>
     <blockquote>A clean checkout of the branch 0.23 source tree does not pass TestMRClientService#test(), which fails with the error message &quot;Num diagnostics is not correct expected &lt;2&gt; but was:&lt;1&gt; upon running &quot;mvn clean install assembly:assembly&quot; inside MR directory.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3196">MAPREDUCE-3196</a>.
     Major bug reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>TestLinuxContainerExecutorWithMocks fails on Mac OSX</b><br>
     <blockquote>TestLinuxContainerExecutorWithMocks uses /bin/true which isn&apos;t present. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3192">MAPREDUCE-3192</a>.
     Major bug reported by jnp and fixed by jnp <br>
     <b>Fix Javadoc warning in JobClient.java and Cluster.java</b><br>
     <blockquote>Javadoc warnings in JobClient.java and Cluster.java need to be fixed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3190">MAPREDUCE-3190</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>bin/yarn should barf early if HADOOP_COMMON_HOME or HADOOP_HDFS_HOME are not set</b><br>
     <blockquote>Currently, if these env vars are not set when you run bin/yarn, it will crash with various ClassNotFoundExceptions, having added {{/share/hadoop/hdfs}} to the classpath. Rather, we should check for these env vars in the wrapper script and display a reasonable error message.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3189">MAPREDUCE-3189</a>.
     Major improvement reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>Add link decoration back to MR2&apos;s CSS</b><br>
     <blockquote>I found the MRv2 web UI very difficult to use because it&apos;s not clear which items are links and which aren&apos;t. I&apos;d like to change the CSS so that links are underlined, making it easier to see them (since they&apos;re also not in any different color)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3188">MAPREDUCE-3188</a>.
     Major bug reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>Lots of errors in logs when daemon startup fails</b><br>
     <blockquote>Since the MR2 daemons are made up of lots of component services, if one of those components fails to start, it will cause the others to shut down as well, even if they haven&apos;t fully finished starting up. Currently, this causes the error output to have a bunch of NullPointerExceptions, IllegalStateExceptions, etc, which mask the actual root cause error at the top.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3187">MAPREDUCE-3187</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>Add names for various unnamed threads in MR2</b><br>
     <blockquote>Simple patch to add thread names for all the places we use Executors, etc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3186">MAPREDUCE-3186</a>.
     Blocker bug reported by ramgopalnaali and fixed by eepayne (mrv2)<br>
     <b>User jobs are getting hanged if the Resource manager process goes down and comes up while job is getting executed.</b><br>
     <blockquote>                    New Yarn configuration property:<br/><br><br><br/><br><br>Name: yarn.app.mapreduce.am.scheduler.connection.retries<br/><br><br>Description: Number of times AM should retry to contact RM if connection is lost.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3185">MAPREDUCE-3185</a>.
     Critical bug reported by mahadev and fixed by jeagles (mrv2)<br>
     <b>RM Web UI does not sort the columns in some cases.</b><br>
     <blockquote>While running lots of jobs on a MRv2 cluster the RM web UI shows this error on loading the RM web UI:<br><br>&quot;DataTables warning (table id = &apos;apps&apos;): Added data (size 8) does not match known number of columns (9)&quot;<br><br>After ignoring the error, the column sorting on Web UI stops working.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3183">MAPREDUCE-3183</a>.
     Trivial bug reported by hitesh and fixed by hitesh (build)<br>
     <b>hadoop-assemblies/src/main/resources/assemblies/hadoop-mapreduce-dist.xml missing license header</b><br>
     <blockquote>Re-assigning as this is part of the mavenization related changes and requires a delayed merge to the 23 branch. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3181">MAPREDUCE-3181</a>.
     Blocker bug reported by anupamseth and fixed by acmurthy (mrv2)<br>
     <b>Terasort fails with Kerberos exception on secure cluster</b><br>
     <blockquote>We are seeing the following Kerberos exception upon trying to run terasort on secure single and multi-node clusters using the latest build from branch 0.23.<br><br>java.io.IOException: Can&apos;t get JobTracker Kerberos principal for use as renewer<br>        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:106)<br>        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:90)<br>        at org.apache.hadoop.mapre...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3179">MAPREDUCE-3179</a>.
     Major bug reported by jeagles and fixed by jeagles (mrv2, test)<br>
     <b>Incorrect exit code for hadoop-mapreduce-test tests when exception thrown</b><br>
     <blockquote>Exit code for test jar is 0 despite exception thrown<br><br>hadoop jar hadoop-mapreduce-test-0.23.0-SNAPSHOT.jar loadgen -Dmapreduce.job.acl-view -m 18 -r 0 -outKey org.apache.hadoop.io.Text -outValue org.apache.hadoop.io.Text -indir nonexistentdir<br><br>Loadgen output snippet<br>org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://machine.name.example.com:9000/user/exampleuser/nonexistentdir<br>        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:23...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3176">MAPREDUCE-3176</a>.
     Blocker bug reported by raviprak and fixed by hitesh (mrv2, test)<br>
     <b>ant mapreduce tests are timing out</b><br>
     <blockquote>Secondary YARN builds started taking inordinately long and lots of tests started failing. Usually the secondary build would take ~ 2 hours. But recently even after 7 hours it wasn&apos;t done. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3175">MAPREDUCE-3175</a>.
     Blocker sub-task reported by tgraves and fixed by jeagles (mrv2)<br>
     <b>Yarn httpservers not created with access Control lists</b><br>
     <blockquote>RM, NM, job history, and application master httpservers are not created with access Control lists. I believe this means that anyone can access any of the standard servlets that check to see if the user has administrator access - like /jmx, /stacks, etc and ops has no way to restrict access to these things.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3171">MAPREDUCE-3171</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>normalize nodemanager native code compilation with common/hdfs native</b><br>
     <blockquote>Use same build pattern as used by common/hdfs native:<br><br>* rename src/c to src/native<br>* run autoreconf, configure and make under target not to pollute the src tree<br>* use maven-make-plugin in an identical way as in common/hdfs native</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3170">MAPREDUCE-3170</a>.
     Critical bug reported by mahadev and fixed by hitesh (build, mrv1, mrv2)<br>
     <b>Trunk nightly commit builds are failing.</b><br>
     <blockquote>Looks like the trunk commit builds are failing after MAPREDUCE-3148 and MAPREDUCE-3126  were committed. I suspect its MAPREDUCE-3148.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3167">MAPREDUCE-3167</a>.
     Minor bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>container-executor is not being packaged with the assembly target.</b><br>
     <blockquote>Looks like MAPREDUCE-2988 broke this. This is a temporary fix until we get a full fledged maven dist tar working. Trivial fix.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3166">MAPREDUCE-3166</a>.
     Major bug reported by ravidotg and fixed by ravidotg (tools/rumen)<br>
     <b>Make Rumen use job history api instead of relying on current history file name format</b><br>
     <blockquote>                                              Makes Rumen use job history api instead of relying on current history file name format.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3165">MAPREDUCE-3165</a>.
     Blocker bug reported by acmurthy and fixed by tlipcon (applicationmaster, mrv2)<br>
     <b>Ensure logging option is set on child command line</b><br>
     <blockquote>Currently the logging config is set in env in MapReduceChildJVM - we need to set it on command line.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3163">MAPREDUCE-3163</a>.
     Blocker bug reported by tlipcon and fixed by mahadev (job submission, mrv2)<br>
     <b>JobClient spews errors when killing MR2 job</b><br>
     <blockquote>When I used the &quot;hadoop job&quot; command line to kill a running MR2 job, I got a bunch of error spew on the console, despite the kill actually taking effect.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3162">MAPREDUCE-3162</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (mrv2, nodemanager)<br>
     <b>Separate application-init and container-init event types in NM&apos;s ApplicationImpl FSM</b><br>
     <blockquote>Currently, the ApplicationImpl receives an INIT_APPLICATION event on every container initialization. Only on the first one does it really mean to init the application, whereas all subsequent events are for specific containers. This JIRA is to separate the events into INIT_APPLICATION, sent once and only once per application, and INIT_CONTAINER, which is sent for every container. The first container sends INIT_APPLICATION followed by INIT_CONTAINER.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3161">MAPREDUCE-3161</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>Improve javadoc and fix some typos in MR2 code</b><br>
     <blockquote>Just some simple cleanup, documentation, typos in variable names, etc. The only code change is to refactor ResourceLocalizationService so each event type is handled in its own method instead of a giant switch statement (just using eclipse&apos;s Extract Method - no semantic change)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3159">MAPREDUCE-3159</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon (mrv2)<br>
     <b>DefaultContainerExecutor removes appcache dir on every localization</b><br>
     <blockquote>The DefaultContainerExecutor currently has code that removes the application dir from appcache/ in the local directories on every task localization. This causes any concurrent executing tasks from the same job to fail.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3158">MAPREDUCE-3158</a>.
     Major bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Fix trunk build failures</b><br>
     <blockquote>https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk-Commit/1060/<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3157">MAPREDUCE-3157</a>.
     Major bug reported by ravidotg and fixed by ravidotg (tools/rumen)<br>
     <b>Rumen TraceBuilder is skipping analyzing 0.20 history files</b><br>
     <blockquote>                                              Fixes TraceBuilder to handle 0.20 history file names also.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3154">MAPREDUCE-3154</a>.
     Major improvement reported by abhijit.shingate and fixed by abhijit.shingate (client, mrv2)<br>
     <b>Validate the Jobs Output Specification as the first statement in JobSubmitter.submitJobInternal(Job, Cluster) method</b><br>
     <blockquote>Presently the output specification is validated after getting new JobId from ClientRMService, Copying the job jar, Configuration file, archives etc.<br><br>Instead of that move following Job Output specification validation call to the begining of JobSubmitter.submitJobInternal(Job, Cluster) method.<br><br>{code}<br>checkSpecs(job);<br>{code}<br><br>This will avoid unnecessary work in case of invalid output specs.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3153">MAPREDUCE-3153</a>.
     Major bug reported by vinodkv and fixed by mahadev (mrv2, test)<br>
     <b>TestFileOutputCommitter.testFailAbort() is failing on trunk on Jenkins</b><br>
     <blockquote>This mostly is caused by MAPREDUCE-2702.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3148">MAPREDUCE-3148</a>.
     Blocker sub-task reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>Port MAPREDUCE-2702 to old mapred api</b><br>
     <blockquote>Port MAPREDUCE-2702 to old mapred api</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3146">MAPREDUCE-3146</a>.
     Critical sub-task reported by vinodkv and fixed by sseth (mrv2, nodemanager)<br>
     <b>Add a MR specific command line to dump logs for a given TaskAttemptID</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3144">MAPREDUCE-3144</a>.
     Critical sub-task reported by vinodkv and fixed by sseth (mrv2)<br>
     <b>Augment JobHistory to include information needed for serving aggregated logs.</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3141">MAPREDUCE-3141</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2, security)<br>
     <b>Yarn+MR secure mode is broken, uncovered after MAPREDUCE-3056</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3140">MAPREDUCE-3140</a>.
     Major bug reported by kam_iitkgp and fixed by subrotosanyal (mrv2)<br>
     <b>Invalid JobHistory URL for failed applications</b><br>
     <blockquote>After completion of the applications execution (application has failed though), to verify the job history, I clicked on the JobHistory hyper-link displayed as part of the application details.In this case, it is displaying [http://n/A].</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3138">MAPREDUCE-3138</a>.
     Blocker bug reported by acmurthy and fixed by owen.omalley (client, mrv2)<br>
     <b>Allow for applications to deal with MAPREDUCE-954</b><br>
     <blockquote>MAPREDUCE-954 changed the context-objs api to interfaces. This breaks Pig. We need a bridge for them to move to 0.23.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3137">MAPREDUCE-3137</a>.
     Trivial sub-task reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Fix broken merge of MR-2719 to 0.23 branch for the distributed shell test case </b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3136">MAPREDUCE-3136</a>.
     Blocker sub-task reported by acmurthy and fixed by acmurthy (documentation, mrv2)<br>
     <b>Add docs for setting up real-world MRv2 clusters</b><br>
     <blockquote>Add docs for setting up real-world MRv2 clusters - MR portion of http://hadoop.apache.org/common/docs/stable/cluster_setup.html</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3134">MAPREDUCE-3134</a>.
     Blocker sub-task reported by acmurthy and fixed by acmurthy (documentation, mrv2, scheduler)<br>
     <b>Add documentation for CapacityScheduler</b><br>
     <blockquote>Add documentation for CapacityScheduler in MRv2 similar to http://hadoop.apache.org/common/docs/stable/capacity_scheduler.html.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3133">MAPREDUCE-3133</a>.
     Major improvement reported by jeagles and fixed by jeagles (build)<br>
     <b>Running a set of methods in a Single Test Class</b><br>
     <blockquote>Instead of running every test method in a class, limit to specific testing methods as describe in the link below.<br><br>http://maven.apache.org/plugins/maven-surefire-plugin/examples/single-test.html<br><br>Upgrade to the latest version of maven-surefire-plugin that has this feature.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3127">MAPREDUCE-3127</a>.
     Blocker sub-task reported by amolkekre and fixed by acmurthy (mrv2, resourcemanager)<br>
     <b>Unable to restrict users based on resourcemanager.admin.acls value set</b><br>
     <blockquote>Setting the following property in yarn-site.xml with user ids to restrict ability to run<br>&apos;rmadmin -refreshQueues is not honoured<br><br>&lt;property&gt;<br>&lt;name&gt;yarn.server.resourcemanager.admin.acls&lt;/name&gt;<br>&lt;value&gt;hadoop1&lt;/value&gt;<br>&lt;description&gt;&lt;/description&gt;<br>&lt;final&gt;&lt;/final&gt;<br>&lt;/property&gt;<br><br>Should it be the same for rmadmin -refreshNodes?</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3126">MAPREDUCE-3126</a>.
     Blocker bug reported by tgraves and fixed by acmurthy (mrv2)<br>
     <b>mr job stuck because reducers using all slots and mapper isn&apos;t scheduled</b><br>
     <blockquote>The command in MAPREDUCE-3124 run and this job got hung with 1 Map task waiting for resources and 7 Reducers running (2 waiting).  The mapper got scheduler, then AM scheduled the reducers, the map task failed and tried to start a new attempt but reducers were using all the slots.   <br><br>I will try to add some more info from the logs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3125">MAPREDUCE-3125</a>.
     Critical bug reported by tgraves and fixed by hitesh (mrv2)<br>
     <b>app master web UI shows reduce task progress 100% even though reducers not complete and state running/scheduled</b><br>
     <blockquote>ran same command as MAPREDUCE-3124. The app master web ui was displaying the reduce task progress as 100% even though the states were still running/scheduled.  Each of those reduce tasks had attempts that failed or killed and another one unassigned. Attaching screenshots.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3124">MAPREDUCE-3124</a>.
     Blocker bug reported by tgraves and fixed by johnvijoe (mrv2)<br>
     <b>mapper failed with failed to load native libs</b><br>
     <blockquote>hadoop jar hadoop-mapreduce-examples-*.jar sort -Dmapreduce.job.acl-view<br>-job=* -Dmapreduce.map.output.compress=true <br>-Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec <br>-Dmapreduce.output.fileoutputformat.compress=true  -Dmapreduce.output.fileoutputformat.compression.type=NONE -Dmap<br>reduce.output.fileoutputformat.compression.codec=org.apache.hadoop.io.compress.GzipCodec  -outKey<br>org.apache.hadoop.io.Text -outValue org.apache.hadoop.io.Text  Compression/textinput Co...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3123">MAPREDUCE-3123</a>.
     Blocker bug reported by tgraves and fixed by hitesh (mrv2)<br>
     <b>Symbolic links with special chars causing container/task.sh to fail</b><br>
     <blockquote>the following job throws an exception when you have the special characters in it.<br><br>hadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&amp;*()-_+= -input Streaming/streaming-980/input.txt  -mapper &apos;xargs cat&apos; -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*<br><br>Exception:<br>2011-09-27 20:58:48...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3114">MAPREDUCE-3114</a>.
     Major bug reported by subrotosanyal and fixed by subrotosanyal (mrv2)<br>
     <b>Invalid ApplicationMaster URL in Applications Page</b><br>
     <blockquote>When the Application is in Accepted state and user tries to click the ApplicationMaster URL in Applications Page, it ends up in Invalid HTTP URL. <br>The screenshot attached with this Issue makes it more clear.<br><br>The HTTP url formed is: http://n/A</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3113">MAPREDUCE-3113</a>.
     Minor improvement reported by xiexianshan and fixed by xiexianshan (mrv2)<br>
     <b>the scripts yarn-daemon.sh and yarn are not working properly</b><br>
     <blockquote>When we execute them on any path but $YARN_HOME with bash -x option,it is giving the error as follows:<br>(Of course we should set the path variable of that scritps into the .bashrc or profile in advance)<br>{code}<br>/usr/share/hadoop/hadoop-mapreduce-0.24.0-SNAPSHOT/bin/yarn: line 55: /usr/share/hadoop/yarn-config.sh:  No such file or directory<br>{code} </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3112">MAPREDUCE-3112</a>.
     Major bug reported by eyang and fixed by eyang (contrib/streaming)<br>
     <b>Calling hadoop cli inside mapreduce job leads to errors</b><br>
     <blockquote>                    Removed inheritance of certain server environment variables (HADOOP_OPTS and HADOOP_ROOT_LOGGER) in task attempt process.<br/><br><br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3110">MAPREDUCE-3110</a>.
     Major bug reported by devaraj.k and fixed by vinodkv (mrv2, test)<br>
     <b>TestRPC.testUnknownCall() is failing</b><br>
     <blockquote>{code:xml}<br>Failed tests: <br>  testUnknownCall(org.apache.hadoop.yarn.TestRPC): null expected:&lt;...icationId called on []org.apache.hadoop.ya...&gt; but was:&lt;...icationId called on [interface ]org.apache.hadoop.ya...&gt;<br><br>Tests run: 65, Failures: 1, Errors: 0, Skipped: 0<br><br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3104">MAPREDUCE-3104</a>.
     Blocker sub-task reported by vinodkv and fixed by vinodkv (mrv2, resourcemanager, security)<br>
     <b>Implement Application ACLs, Queue ACLs and their interaction</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3103">MAPREDUCE-3103</a>.
     Blocker sub-task reported by vinodkv and fixed by mahadev (mrv2, security)<br>
     <b>Implement Job ACLs for MRAppMaster</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3099">MAPREDUCE-3099</a>.
     Major sub-task reported by mahadev and fixed by mahadev <br>
     <b>Add docs for setting up a single node MRv2 cluster.</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3098">MAPREDUCE-3098</a>.
     Blocker sub-task reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Report Application status as well as ApplicationMaster status in GetApplicationReportResponse </b><br>
     <blockquote>Currently, an application report received by the client from the RM/ASM for a given application returns the status of the application master. It does not return the status of the application i.e. whether that particular job succeeded or failed. <br><br>The AM status would be one of FINISHED (SUCCEEDED should be renamed to FINISHED as AM state does not indicate overall success/failure), FAILED or KILLED. <br>The final state sent by the AM to the RM in the FinishApplicationMasterRequest should be expose...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3095">MAPREDUCE-3095</a>.
     Major bug reported by johnvijoe and fixed by johnvijoe (mrv2)<br>
     <b>fairscheduler ivy including wrong version for hdfs</b><br>
     <blockquote>fairscheduler ivy.xml includes the common version for hdfs dependency. This could break builds that have different common and hdfs version numbers. The reason we dont see it on the jenkins build is because we use the same version number for common and hdfs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3092">MAPREDUCE-3092</a>.
     Minor bug reported by devaraj.k and fixed by devaraj.k (mrv2)<br>
     <b>Remove JOB_ID_COMPARATOR usage in JobHistory.java</b><br>
     <blockquote>As part of the defect MAPREDUCE-2965, JobId.compareTo() has been implemented. Usage of JOB_ID_COMPARATOR in JobHistory.java can be removed because comparison is handling by JobId itself. <br> <br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3090">MAPREDUCE-3090</a>.
     Major improvement reported by acmurthy and fixed by acmurthy (applicationmaster, mrv2)<br>
     <b>Change MR AM to use ApplicationAttemptId rather than &lt;applicationId, startCount&gt; everywhere</b><br>
     <blockquote>Change MR AM to use ApplicationAttemptId rather than &lt;applicationId, startCount&gt; everywhere, particularly after MAPREDUCE-3055</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3087">MAPREDUCE-3087</a>.
     Critical bug reported by raviprak and fixed by raviprak (mrv2)<br>
     <b>CLASSPATH not the same after MAPREDUCE-2880</b><br>
     <blockquote>After MAPREDUCE-2880, my classpath was missing key jar files. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3081">MAPREDUCE-3081</a>.
     Major bug reported by vitthal_gogate and fixed by  (contrib/vaidya)<br>
     <b>Change the name format for hadoop core and vaidya jar to be hadoop-{core/vaidya}-{version}.jar in vaidya.sh</b><br>
     <blockquote>                                              contrib/vaidya/bin/vaidya.sh script fixed to use appropriate jars and classpath <br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3078">MAPREDUCE-3078</a>.
     Blocker bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2, resourcemanager)<br>
     <b>Application&apos;s progress isn&apos;t updated from AM to RM.</b><br>
     <blockquote>It helps to be able to monitor the application-progress from the RM UI itself.<br><br>Bits of it is already there, even the AM-RM API (in AllocateRequest). We just need to make sure the progress is produced and consumed properly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3073">MAPREDUCE-3073</a>.
     Blocker bug reported by mahadev and fixed by mahadev <br>
     <b>Build failure for MRv1 caused due to changes to MRConstants.</b><br>
     <blockquote>When runnning ant -Dresolvers=internal binary, the build seems to be failing with:<br><br>  [javac] public class JobTracker implements MRConstants,<br>InterTrackerProtocol,<br>   [javac]                                    ^<br>   [javac] <br>/home/y/var/builds/thread2/workspace/Cloud-Yarn-0.23-Secondary/hadoop-mapred<br>uce-project/src/java/org/apache/hadoop/mapred/TaskTracker.java:131:<br>interface expected here<br>   [javac]     implements MRConstants, TaskUmbilicalProtocol, Runnable,<br>TTConfig {<br>   [javac]           ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3071">MAPREDUCE-3071</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>app master configuration web UI link under the Job menu opens up application menu</b><br>
     <blockquote>If you go to the app master web UI for a particular job. The job menu on the left side displays links for overview, counters, configuration, etc..<br><br>If you click on the configuration one, it closes the job menu and opens the application menu on that left side. It shouldn&apos;t do this. It should leave the job menu open.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3070">MAPREDUCE-3070</a>.
     Blocker bug reported by raviteja and fixed by devaraj.k (mrv2, nodemanager)<br>
     <b>NM not able to register with RM after NM restart</b><br>
     <blockquote>After stopping NM gracefully then starting NM, NM registration fails with RM with Duplicate registration from the node! error.<br><br><br>{noformat} <br>2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager<br>org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager<br>	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)<br>	at org.apache.hadoop.yarn.server.nodemanager.NodeMa...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3068">MAPREDUCE-3068</a>.
     Blocker bug reported by vinodkv and fixed by criccomini (mrv2)<br>
     <b>Should set MALLOC_ARENA_MAX for all YARN daemons and AMs/Containers</b><br>
     <blockquote>This is same as HADOOP-7154 but for yarn. RM, NM, AM and containers should all have this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3067">MAPREDUCE-3067</a>.
     Blocker bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Container exit status not set properly to launched process&apos;s exit code on successful completion of process</b><br>
     <blockquote>When testing the distributed shell sample app master, the container exit status was being returned incorrectly. <br><br>11/09/21 11:32:58 INFO DistributedShell.ApplicationMaster: Got container status for containerID= container_1316629955324_0001_01_000002, state=COMPLETE, exitStatus=-1000, diagnostics=</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3066">MAPREDUCE-3066</a>.
     Major bug reported by criccomini and fixed by criccomini (mrv2, nodemanager)<br>
     <b>YARN NM fails to start</b><br>
     <blockquote>Please check conf.get() calls. Every time I svn up, I get one of these.<br><br><br>2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.<br>2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager<br>org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager<br>	at org.apache.hadoop.yarn.service.Co...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3064">MAPREDUCE-3064</a>.
     Blocker bug reported by tgraves and fixed by venug <br>
     <b>27 unit test failures with  Invalid &quot;mapreduce.jobtracker.address&quot; configuration value for JobTracker: &quot;local&quot;</b><br>
     <blockquote>unit test failure here: https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk-Commit/946/<br><br>	Test Result (27 failures / +27)<br><br>    org.apache.hadoop.mapred.TestCollect.testCollect<br>    org.apache.hadoop.mapred.TestComparators.testDefaultMRComparator<br>    org.apache.hadoop.mapred.TestComparators.testUserMRComparator<br>    org.apache.hadoop.mapred.TestComparators.testUserValueGroupingComparator<br>    org.apache.hadoop.mapred.TestComparators.testAllUserComparators<br>    org.apache.hado...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3062">MAPREDUCE-3062</a>.
     Major bug reported by criccomini and fixed by criccomini (mrv2, nodemanager, resourcemanager)<br>
     <b>YARN NM/RM fail to start</b><br>
     <blockquote>2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager<br>java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address<br>	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)<br>	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)<br>	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)<br>	at org.apache.hadoop.yarn.service.CompositeService....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3059">MAPREDUCE-3059</a>.
     Blocker bug reported by karams and fixed by devaraj.k (mrv2)<br>
     <b>QueueMetrics do not have metrics for aggregate containers-allocated and aggregate containers-released</b><br>
     <blockquote>QueueMetrics for ResourceManager do not have any metrics for aggregate containers-allocated and containers-released.<br><br>We need the aggregates of containers-allocated and containers-released to figure out the rate at which RM is dishing out containers. NodeManager do have containers-launched and container-released metrics, but this is not across all nodes; so to get the cluster level aggregate, we need to preprocess NM metrics from all nodes - which is troublesome.<br><br>Currently, we do have Alloca...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3058">MAPREDUCE-3058</a>.
     Critical bug reported by karams and fixed by vinodkv (contrib/gridmix, mrv2)<br>
     <b>Sometimes task keeps on running while its Syslog says that it is shutdown</b><br>
     <blockquote>While running GridMixV3, one of the jobs got stuck for 15 hrs. After clicking on the Job-page, found one of its reduces to be stuck. Looking at syslog of the stuck reducer, found this:<br>Task-logs&apos; head:<br><br>{code}<br>2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).<br>2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started<br>{code}<br><br>Task-logs&apos; tail:<br>{code}<br>2011-09-19 18:06:4...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3057">MAPREDUCE-3057</a>.
     Blocker bug reported by karams and fixed by eepayne (jobhistoryserver, mrv2)<br>
     <b>Job History Server goes of OutOfMemory with 1200 Jobs and Heap Size set to 10 GB</b><br>
     <blockquote>History server was started with -Xmx10000m<br>Ran GridMix V3 with 1200 Jobs trace in STRESS mode on 350 nodes with each node 4 NMS.<br>All jobs finished as reported by RM Web UI and HADOOP_MAPRED_HOME/bin/mapred job -list all<br>But found that GridMix job client was stuck while trying connect to HistoryServer<br>Then tried to do HADOOP_MAPRED_HOME/bin/mapred job -status jobid<br>JobClient also got stuck while looking for token to connect to History server<br>Then looked at History Server logs and found History...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3056">MAPREDUCE-3056</a>.
     Blocker bug reported by devaraj.k and fixed by devaraj.k (applicationmaster, mrv2)<br>
     <b>Jobs are failing when those are submitted by other users</b><br>
     <blockquote>MR cluster is started by the user &apos;root&apos;. If any other users other than &apos;root&apos; submit a job, it is failing always.<br><br>Find the conatiner logs in the comments section.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3055">MAPREDUCE-3055</a>.
     Minor bug reported by hitesh and fixed by vinodkv (mrv2)<br>
     <b>Simplify parameter passing to Application Master from Client. SImplify approach to pass info such  appId, ClusterTimestamp and failcount required by App Master.</b><br>
     <blockquote>The Application master needs the application attempt id to register with the Applications Manager. To create an appAttemptId object, the appId object(needs cluster timestamp and app id) and failCount are needed.<br><br>Currently, all clients need to pass in the appId, cluster timestamp and fail count to the app master for the required objects to be constructed. <br><br>We could look at simplifying this by providing either placeholders that would have values replaced by the app master launcher or setting ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3054">MAPREDUCE-3054</a>.
     Blocker bug reported by sseth and fixed by mahadev (mrv2)<br>
     <b>Unable to kill submitted jobs</b><br>
     <blockquote>Found by Philip Su<br><br>The &quot;mapred job -kill&quot; command<br>appears to succeed, but listing the jobs again shows that the job supposedly killed is still there. <br><br>{code}<br>mapred job -list<br>Total jobs:2<br>JobId   State   StartTime       UserName        Queue   Priority        SchedulingInfo<br>job_1316203984216_0002  PREP    1316204924937   hadoopqa        default NORMAL<br>job_1316203984216_0001  PREP    1316204031206   hadoopqa        default NORMAL<br><br>mapred job -kill job_1316203984216_0002<br>Killed job job_131620...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3053">MAPREDUCE-3053</a>.
     Major bug reported by criccomini and fixed by vinodkv (mrv2, resourcemanager)<br>
     <b>YARN Protobuf RPC Failures in RM</b><br>
     <blockquote>When I try to register my ApplicationMaster with YARN&apos;s RM, it fails.<br><br>In my ApplicationMaster&apos;s logs:<br><br>Exception in thread &quot;main&quot; java.lang.reflect.UndeclaredThrowableException<br>	at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)<br>	at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)<br>	at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)<br>	at kafka.yarn.ApplicationM...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3050">MAPREDUCE-3050</a>.
     Blocker bug reported by revans2 and fixed by revans2 (mrv2, resourcemanager)<br>
     <b>YarnScheduler needs to expose Resource Usage Information</b><br>
     <blockquote>Before the recent refactor The nodes had information in them about how much resources they were using.  This information is not hidden inside SchedulerNode.  Similarly resource usage information about an application, or in aggregate is only available through the Scheduler and there is not interface to pull it out.<br><br>We need to expose APIs to get Resource and Container information from the scheduler, in aggregate across the entire cluster, per application, per node, and ideally also per queue i...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3048">MAPREDUCE-3048</a>.
     Major bug reported by vinodkv and fixed by vinodkv (build)<br>
     <b>Fix test-patch to run tests via &quot;mvn clean install test&quot;</b><br>
     <blockquote>Some tests like the ones failing at MAPREDUCE-3040 depend on the generated jars. TestMRJobs for e.g. won&apos;t run if we simply run &quot;mvn clean test&quot;.<br><br>I propose that we change test-patch to run tests using &quot;mvn clean install test&quot;.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3044">MAPREDUCE-3044</a>.
     Blocker bug reported by rramya and fixed by mahadev (mrv2)<br>
     <b>Pipes jobs stuck without making progress</b><br>
     <blockquote>A simple example pipes job gets stuck without making any progress. The AM is launched but the maps do not make any progress.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3042">MAPREDUCE-3042</a>.
     Major bug reported by criccomini and fixed by criccomini (mrv2, resourcemanager)<br>
     <b>YARN RM fails to start</b><br>
     <blockquote>                                              Simple typo fix to allow ResourceManager to start instead of fail<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3041">MAPREDUCE-3041</a>.
     Blocker bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>Enhance YARN Client-RM protocol to provide access to information such as cluster&apos;s Min/Max Resource capabilities similar to that of AM-RM protocol</b><br>
     <blockquote>To request a container to launch an application master, the client needs to know the min/max resource capabilities so as to be able to make a proper resource request when submitting a new application.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3040">MAPREDUCE-3040</a>.
     Major bug reported by tgraves and fixed by acmurthy (mrv2)<br>
     <b>TestMRJobs, TestMRJobsWithHistoryService, TestMROldApiJobs fail</b><br>
     <blockquote>Running org.apache.hadoop.mapreduce.v2.TestMRJobs<br>Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 6.229 sec &lt;&lt;&lt; FAILURE!<br>Running org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService<br>Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.887 sec &lt;&lt;&lt; FAILURE!<br>Running org.apache.hadoop.mapreduce.v2.TestMROldApiJobs<br>Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 6.067 sec &lt;&lt;&lt; FAILURE!<br><br>All of them have the exception:<br><br><br>java.lang.NullPointerExcept...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3038">MAPREDUCE-3038</a>.
     Blocker bug reported by tgraves and fixed by naisbitt (mrv2)<br>
     <b>job history server not starting because conf() missing HsController</b><br>
     <blockquote>Exception starting history server.<br><br><br>Sep 19, 2011 6:51:53 PM com.google.inject.MessageProcessor visit<br>INFO: An exception was caught and reported. Message: org.apache.hadoop.yarn.webapp.WebAppException: conf() not found in class org.apache.hadoop.mapreduce.v2.hs.webapp.HsController                                                                                 org.apache.hadoop.yarn.webapp.WebAppException: conf() not found in class org.apache.hadoop.mapreduce.v2.hs.webapp.HsController<br>    at o...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3036">MAPREDUCE-3036</a>.
     Blocker bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>Some of the Resource Manager memory metrics go negative.</b><br>
     <blockquote>ReservedGB seems to always be decremented when a container is released, even though the container never reserved any memory.<br>AvailableGB also seems to be able to go negative in a few situations.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3035">MAPREDUCE-3035</a>.
     Critical bug reported by karams and fixed by chaku88 (mrv2)<br>
     <b>MR V2 jobhistory does not contain rack information</b><br>
     <blockquote>When topology.node.switch.mapping.impl is set to enable rack-locality resolution via the topology script, from the RM web-UI, we can see the rack information for each node. Running a job also reveals the information about rack-local map tasks launched at end of job completion on the client side.<br><br>But the hostname field for attempts in the JobHistory does not contain this rack information.<br><br>In case of hadoop-0.20 securiy or MRV1, hostname field of job history does contain rackid/hostname where...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3033">MAPREDUCE-3033</a>.
     Blocker bug reported by karams and fixed by hitesh (job submission, mrv2)<br>
     <b>JobClient requires mapreduce.jobtracker.address config even when mapreduce.framework.name is set to yarn</b><br>
     <blockquote>If mapreduce.jobtracker.address is not set in mapred-site.xml and mapreduce.framework.name is set yarn, job submission fails :<br><br>Tried to submit sleep job with maps 1 task. Job submission failed with following exception -:<br>{code}<br>11/09/19 13:19:20 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC<br>11/09/19 13:19:20 INFO mapred.ResourceMgrDelegate: Connecting to ResourceManager at &lt;RMHost&gt;:8040<br>11/09/19 13:19:20 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProt...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3032">MAPREDUCE-3032</a>.
     Blocker bug reported by vinodkv and fixed by devaraj.k (applicationmaster, mrv2)<br>
     <b>JobHistory doesn&apos;t have error information from failed tasks</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3031">MAPREDUCE-3031</a>.
     Blocker bug reported by karams and fixed by sseth (mrv2)<br>
     <b>Job Client goes into infinite loop when we kill AM</b><br>
     <blockquote>Started a cluster. Submitted a sleep job with around 10000 maps and 1000 reduces.<br>Killed AM with kill -9 by which time already 7000 thousands maps got completed.<br><br>On the RM webUI, Application is stuck in Application.RUNNING state. And JobClient goes into an infinite loop as RM keeps telling the client that the application is running.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3030">MAPREDUCE-3030</a>.
     Blocker bug reported by devaraj.k and fixed by devaraj.k (mrv2, resourcemanager)<br>
     <b>RM is not processing heartbeat and continuously giving the message &apos;Node not found rebooting&apos;</b><br>
     <blockquote>{code:title=Node Manager Logs|borderStyle=solid}<br>2011-09-19 13:39:29,816 INFO  webapp.WebApps (WebApps.java:start(162)) - Registered webapp guice modules<br>2011-09-19 13:39:29,817 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is started.<br>2011-09-19 13:39:29,818 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:Dispatcher is started.<br>2011-09-19 13:39:29,819 INFO  nodemanager.NodeStatusUpd...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3028">MAPREDUCE-3028</a>.
     Blocker bug reported by kamrul and fixed by raviprak (mrv2)<br>
     <b>Support job end notification in .next /0.23</b><br>
     <blockquote>Oozie primarily depends on  the job end notification to determine when the job finishes. In the current version,  job end notification is implemented in job tracker. Since job tracker will be removed in the upcoming hadoop release (.next), we wander where this support will move. I think this best effort notification could be implemented in the new Application Manager as one of the last step of job completion.<br><br>Whatever implementation will it be, Oozie badly needs this feature to be continued ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3023">MAPREDUCE-3023</a>.
     Major bug reported by raviprak and fixed by raviprak (mrv2)<br>
     <b>Queue state is not being translated properly (is always assumed to be running)</b><br>
     <blockquote>During translation of QueueInfo, <br><br>bq. TypeConverter.java:435 : queueInfo.toString(), QueueState.RUNNING,<br>ought to be <br>bq. queueInfo.toString(), QueueState.getState(queueInfo.getQueueState().toString().toLowerCase()),</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3021">MAPREDUCE-3021</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>all yarn webapps use same base name of &quot;yarn/&quot;</b><br>
     <blockquote>All of the yarn webapps (resource manager, node manager, app master, job history) use the same base url of /yarn/.  This doesn&apos;t lend itself very well to filters be able to differentiate them to say allow some to be not authenticated and other to be authenticated.  Perhaps we should rename them based on component.<br><br>There are also things in the code that hardcode paths to &quot;/yarn&quot; that should be fixed up.<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3020">MAPREDUCE-3020</a>.
     Major bug reported by chaku88 and fixed by chaku88 (jobhistoryserver)<br>
     <b>Node link in reduce task attempt page is not working [Job History Page]</b><br>
     <blockquote>RM UI -&gt; Applications -&gt; Application(Job History) -&gt; Reduce Tasks -&gt; Task ID -&gt; Node link is not working<br>hostname for ReduceAttemptFinishedEvent is coming wrong when loading from history file.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3018">MAPREDUCE-3018</a>.
     Blocker bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Streaming jobs with -file option fail to run.</b><br>
     <blockquote>Streaming jobs fail to run with the -file option.<br>hadoop jar streaming.jar -input input.txt -output Out -mapper &quot;mapper.sh&quot; -reducer NONE -file path_to_mapper.sh<br><br>fails to run.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3017">MAPREDUCE-3017</a>.
     Blocker bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>The Web UI shows FINISHED for killed/successful/failed jobs.</b><br>
     <blockquote>The RM web ui shows FINISHED status for all the jobs even if they failed/killed or were successful. This should be fixed. Only the jobs where the AM crashes are marked as Failed.  </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3014">MAPREDUCE-3014</a>.
     Major improvement reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Rename and invert logic of &apos;-cbuild&apos; profile to &apos;native&apos; and off by default</b><br>
     <blockquote>This would align MR modules with common &amp; hdfs modules.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3013">MAPREDUCE-3013</a>.
     Major sub-task reported by vinodkv and fixed by vinodkv (mrv2, security)<br>
     <b>Remove YarnConfiguration.YARN_SECURITY_INFO</b><br>
     <blockquote>We don&apos;t need this anymore since RPC client uses SecurityUtil to pick it up via going through the providers for SecurityInfo interface. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3007">MAPREDUCE-3007</a>.
     Major sub-task reported by vinodkv and fixed by vinodkv (jobhistoryserver, mrv2)<br>
     <b>JobClient cannot talk to JobHistory server in secure mode</b><br>
     <blockquote>In secure mode, Jobclient cannot connect to HistoryServer. Thanks to [~karams] for finding this out.<br><br>{code}<br>11/09/14 09:57:51 INFO mapred.ClientServiceDelegate: Application state is completed. Redirecting to job history server<br>11/09/14 09:57:51 INFO security.ApplicationTokenSelector: Looking for a token with service &lt;history-server&gt;:10020<br>11/09/14 09:57:51 INFO security.ApplicationTokenSelector: Token kind is YARN_APPLICATION_TOKEN and the token&apos;s service name is &lt;Am-ip&gt;:46257<br>11/09/14 09:57...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3006">MAPREDUCE-3006</a>.
     Major bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>MapReduce AM exits prematurely before completely writing and closing the JobHistory file</b><br>
     <blockquote>[~Karams] was executing a sleep job with 100,000 tasks on a 350 node cluster to test MR AM&apos;s scalability and ran into this. The job ran successfully but the history was not available.<br><br>I debugged around and figured that the job is finishing prematurely before the JobHistory is written. In most of the cases, we don&apos;t see this bug as we have a 5 seconds sleep in AM towards the end.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3005">MAPREDUCE-3005</a>.
     Major bug reported by vinodkv and fixed by acmurthy (mrv2)<br>
     <b>MR app hangs because of a NPE in ResourceManager</b><br>
     <blockquote>The app hangs and it turns out to be a NPE in ResourceManager. This happened two of five times on [~karams]&apos;s sort runs on a big cluster.<br>{code}<br>2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler<br>java.lang.NullPointerException<br>        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)<br>        at org.apache.hadoop.yarn.serve...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3004">MAPREDUCE-3004</a>.
     Minor bug reported by hitesh and fixed by hitesh (mrv2)<br>
     <b>sort example fails in shuffle/reduce stage as it assumes a local job by default </b><br>
     <blockquote>Log trace when running sort on a single node setup:<br><br>11/09/13 17:01:06 INFO mapreduce.Job:  map 100% reduce 0%<br>11/09/13 17:01:10 INFO mapreduce.Job: Task Id : attempt_1315949787252_0009_r_000000_0, Status : FAILED<br>java.lang.UnsupportedOperationException: Incompatible with LocalRunner<br>	at org.apache.hadoop.mapred.YarnOutputFiles.getInputFile(YarnOutputFiles.java:200)<br>	at org.apache.hadoop.mapred.ReduceTask.getMapFiles(ReduceTask.java:183)<br>	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3003">MAPREDUCE-3003</a>.
     Major bug reported by tomwhite and fixed by tucu00 (build)<br>
     <b>Publish MR JARs to Maven snapshot repository</b><br>
     <blockquote>Currently this is failing since no distribution management section is defined in the POM.<br><br>https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-trunk-Commit/883/consoleFull</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-3001">MAPREDUCE-3001</a>.
     Blocker improvement reported by revans2 and fixed by revans2 (jobhistoryserver, mrv2)<br>
     <b>Map Reduce JobHistory and AppMaster UI should have ability to display task specific counters.</b><br>
     <blockquote>Map Reduce JobHistory and AppMaster UI should have ability to display task specific counters.  I think the best way to do this is to include in the Nav Block a task specific section with task links when a task is selected.  Counters is already set up to deal with a task passed in.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2999">MAPREDUCE-2999</a>.
     Critical bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>hadoop.http.filter.initializers not working properly on yarn UI</b><br>
     <blockquote>Currently httpserver only has *.html&quot;, &quot;*.jsp as user facing urls when you add a filter. For the new web framework in yarn, the pages no longer have the *.html or *.jsp and thus they are not properly being filtered.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2998">MAPREDUCE-2998</a>.
     Critical bug reported by naisbitt and fixed by vinodkv (mrv2)<br>
     <b>Failing to contact Am/History for jobs: java.io.EOFException in DataInputStream</b><br>
     <blockquote>I am getting an exception frequently when running my jobs on a single-node cluster.  It happens with basically any job I run: sometimes the job will work, but most of the time I get this exception (in this case, I was running a simple wordcount from the examples jar - where I got the exception 4 times in a row, and then the job worked the fifth time I submitted it). <br>Sometimes restarting the namenode, resourcemanager, and historyserver helps - but not always.  Several other developers have se...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2997">MAPREDUCE-2997</a>.
     Major bug reported by vinodkv and fixed by vinodkv (applicationmaster, mrv2)<br>
     <b>MR task fails before launch itself with an NPE in ContainerLauncher</b><br>
     <blockquote>Exception found on the AM web UI while the application is running:<br>{code}<br>Container launch failed for container_1315908079531_0002_01_000387 : java.lang.NullPointerException<br>  at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:162)<br>  at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:204)<br>  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2996">MAPREDUCE-2996</a>.
     Blocker bug reported by vinodkv and fixed by jeagles (jobhistoryserver, mrv2)<br>
     <b>Log uberized information into JobHistory and use the same via CompletedJob</b><br>
     <blockquote>We always print the uberized info on the UI to be false irrespective of whether it is uberized or not.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2995">MAPREDUCE-2995</a>.
     Major bug reported by vinodkv and fixed by vinodkv (mrv2)<br>
     <b>MR AM crashes when a container-launch hangs on a faulty NM</b><br>
     <blockquote>AM tries to launch containers on a faulty node which blocks several/all of the {{StartContainer}} requests. Eventually, RM expires the container-allocations, informs the AM about container-expiry. But AM crashes with an INTERNAL_ERROR as the event is unexpected.<br>{code}<br>11/09/12 14:11:38 ERROR impl.TaskAttemptImpl: Can&apos;t handle this event at current state<br>org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_COMPLETED at ASSIGNED<br>        at org.apache.hadoop....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2994">MAPREDUCE-2994</a>.
     Major bug reported by devaraj.k and fixed by devaraj.k (mrv2, resourcemanager)<br>
     <b>Parse Error is coming for App ID when we click application link on the RM UI</b><br>
     <blockquote>{code:xml}<br>Caused by: org.apache.hadoop.yarn.YarnException: Error parsing app ID: application_1315895242400_1<br>	at org.apache.hadoop.yarn.util.Apps.throwParseException(Apps.java:60)<br>	at org.apache.hadoop.yarn.util.Apps.toAppID(Apps.java:43)<br>	at org.apache.hadoop.yarn.util.Apps.toAppID(Apps.java:38)<br>	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:74)<br>	... 30 more<br>{code}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2991">MAPREDUCE-2991</a>.
     Major bug reported by priyomustafi and fixed by priyomustafi (scheduler)<br>
     <b>queueinfo.jsp fails to show queue status if any Capacity scheduler queue name has dash/hiphen in it.</b><br>
     <blockquote>If any queue name has a dash/hiphen in it, the queueinfo.jsp doesn&apos;t show any queue information.  This is happening because the queue name is used to create javascript variables and javascript doesn&apos;t allow dash in variable names.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2990">MAPREDUCE-2990</a>.
     Blocker improvement reported by mahadev and fixed by subrotosanyal (mrv2)<br>
     <b>Health Report on Resource Manager UI is null if the NM&apos;s are all healthy.</b><br>
     <blockquote>The web UI on the RM for the link Nodes shows that Health-report as null when the NM is healthy. <br><br>This is a simple fix where in we can check for null in NodesPage.java and put something meaningful instead of null.<br>NodesPage.java:<br>{code}<br><br>render(..)<br><br>td((health.getHealthReport() == null) ?&quot;REPORT HEALTHY&quot;: health.getHealthReport());<br><br>{code}<br>Or something like that.<br><br> </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2989">MAPREDUCE-2989</a>.
     Critical sub-task reported by sseth and fixed by sseth (mrv2)<br>
     <b>JobHistory should link to task logs</b><br>
     <blockquote>The log link on the task attempt page is currently broken - since it relies on a ContainerId. We should either pass the containerId via a history event - or some kind of field with information about the log location.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2988">MAPREDUCE-2988</a>.
     Critical sub-task reported by eepayne and fixed by revans2 (mrv2, security, test)<br>
     <b>Reenable TestLinuxContainerExecutor reflecting the current NM code. </b><br>
     <blockquote>TestLinuxContainerExecutor is currently disabled completely.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2987">MAPREDUCE-2987</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>RM UI display logged in user as null</b><br>
     <blockquote>All the pages of the UI, currently show &quot;Logged in as: null&quot; instead of the correct username</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2986">MAPREDUCE-2986</a>.
     Critical task reported by anupamseth and fixed by anupamseth (mrv2, test)<br>
     <b>Multiple node managers support for the MiniYARNCluster</b><br>
     <blockquote>The current MiniYARNCluster can only support 1 node manager, which is not enough for the full test purposes.<br><br>Would like to have a simulator that can support multiple node managers as the real scenario. This might be beneficial for hadoop users, testers and developers.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2985">MAPREDUCE-2985</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>findbugs error in ResourceLocalizationService.handle(LocalizationEvent)</b><br>
     <blockquote>hudson mapreduce is reporting a findbugs error:<br>https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/707//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-nodemanager.html<br><br>WMI 	Method org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.handle(LocalizationEvent) makes inefficient use of keySet iterator instead of entrySet iterator<br>	<br><br>Bug type WMI_WRONG_MAP_ITERATOR (click for details)<br>In class org.a...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2984">MAPREDUCE-2984</a>.
     Minor bug reported by devaraj.k and fixed by devaraj.k (mrv2, nodemanager)<br>
     <b>Throwing NullPointerException when we open the container page</b><br>
     <blockquote>{code:xml}<br>Caused by: java.lang.NullPointerException<br>	at org.apache.hadoop.yarn.api.records.ContainerId.compareTo(ContainerId.java:97)<br>	at org.apache.hadoop.yarn.api.records.ContainerId.compareTo(ContainerId.java:23)<br>	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:819)<br>	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1640)<br>	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock.render(ContainerPage.java:70)...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2979">MAPREDUCE-2979</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>Remove ClientProtocolProvider configuration under mapreduce-client-core</b><br>
     <blockquote>ClientProtocolProvider configuration exists under the job-client and core modules. It&apos;s really only required in job-client. The version in core points to JobTrackerClientProtocolProvider which causes<br><br>java.util.ServiceConfigurationError: org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider: Provider org.apache.hadoop.mapred.JobTrackerClientProtocolProvider not found<br>        at java.util.ServiceLoader.fail(ServiceLoader.java:214)<br>        at java.util.ServiceLoader.access$400(ServiceLoad...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2977">MAPREDUCE-2977</a>.
     Blocker sub-task reported by owen.omalley and fixed by acmurthy (mrv2, resourcemanager, security)<br>
     <b>ResourceManager needs to renew and cancel tokens associated with a job</b><br>
     <blockquote>The JobTracker currently manages tokens for the applications and the resource manager needs the same functionality.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2975">MAPREDUCE-2975</a>.
     Blocker bug reported by mahadev and fixed by mahadev <br>
     <b>ResourceManager Delegate is not getting initialized with yarn-site.xml as default configuration.</b><br>
     <blockquote>MAPREDUCE-2937 accidentally changes ResourceMgrDelegate so that it does not pick up yarn-site.xml as a default resource. Will upload patch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2971">MAPREDUCE-2971</a>.
     Blocker bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>ant build mapreduce fails  protected access  jc.displayJobList(jobs);</b><br>
     <blockquote>Running the ant target in the hadoop-mapreduce-project directory fails with:<br><br>[jsp-compile] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.<br>    [javac] /home/tgraves/branch23/branch-0.23/hadoop-mapreduce-project/build.xml:398: warning: &apos;includeantruntime&apos; was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds<br>    [javac] Compiling 50 source files to /home/tgraves/branch23/branch-0.23/hadoop-mapreduce-project/build/classes<br>   ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2970">MAPREDUCE-2970</a>.
     Major bug reported by venug and fixed by venug (job submission, mrv2)<br>
     <b>Null Pointer Exception while submitting a Job, If mapreduce.framework.name property is not set.</b><br>
     <blockquote>If mapreduce.framework.name property is not set in mapred-site.xml, Null pointer Exception is thrown.<br><br>java.lang.NullPointerException<br>	at org.apache.hadoop.mapreduce.Cluster$1.run(Cluster.java:133)<br>	at org.apache.hadoop.mapreduce.Cluster$1.run(Cluster.java:1)<br>	at java.security.AccessController.doPrivileged(Native Method)<br>	at javax.security.auth.Subject.doAs(Subject.java:396)<br>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)<br>	at org.apache.hadoop.mapreduc...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2966">MAPREDUCE-2966</a>.
     Major improvement reported by abhijit.shingate and fixed by abhijit.shingate (applicationmaster, jobhistoryserver, nodemanager, resourcemanager)<br>
     <b>Add ShutDown hooks for MRV2 processes</b><br>
     <blockquote>NodeManager registers a shudown hook in case of JVM exit.<br>Similar way, all other processes RM, HistoryServer, MRAppMaster should also handle the shutdown gracefully in case of JVM exit.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2965">MAPREDUCE-2965</a>.
     Blocker bug reported by vinodkv and fixed by sseth (mrv2)<br>
     <b>Streamline hashCode(), equals(), compareTo() and toString() for all IDs</b><br>
     <blockquote>MAPREDUCE-2954 moved these methods to the record interfaces from the PB impls for ContainerId, ApplicationId and ApplicationAttemptId. This is good as they don&apos;t need to be tied to the implementation.<br><br>We should do the same for all IDs. In fact some of these are missing for IDs like MR AM JobId, TaskId etc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2963">MAPREDUCE-2963</a>.
     Critical bug reported by mahadev and fixed by sseth <br>
     <b>TestMRJobs hangs waiting to connect to history server.</b><br>
     <blockquote>TestMRJobs is hanging waiting to connect to history server. I will post the logs next.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2961">MAPREDUCE-2961</a>.
     Blocker improvement reported by mahadev and fixed by vinodkv (mrv2)<br>
     <b>Increase the default threadpool size for container launching in the application master.</b><br>
     <blockquote>Currently the default threadpool size is 10 for launching containers in ContainerLauncherImpl. We should increase that to 100 for a reasonable default, so that container launching is not backed up by a small thread pool size.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2958">MAPREDUCE-2958</a>.
     Critical bug reported by tgraves and fixed by acmurthy (mrv2)<br>
     <b>mapred-default.xml not merged from mr279</b><br>
     <blockquote>I have been running wordcount out of the 23 examples jar.  It says it succeeds but doesn&apos;t actually output a file.<br><br>hadoop jar examples/hadoop-mapreduce-0.23.0-SNAPSHOT/hadoop-mapreduce-examples-0.23.0-SNAPSHOT.jar wordcount input output2<br><br>input file is really basic:<br>fdksajl<br>dlkfsajlfljda;j<br>kldfsjallj<br>test<br>one<br>two<br>test</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2954">MAPREDUCE-2954</a>.
     Critical bug reported by vinodkv and fixed by sseth (mrv2)<br>
     <b>Deadlock in NM with threads racing for ApplicationAttemptId</b><br>
     <blockquote>Found this:<br>{code}<br>Java stack information for the threads listed above:<br>===================================================<br>&quot;Thread-45&quot;:<br>        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.getApplicationId(ApplicationAttemptIdPBImpl.java:101)<br>        - waiting to lock &lt;0xb6a43ba0&gt; (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)<br>        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdP...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2953">MAPREDUCE-2953</a>.
     Major bug reported by vinodkv and fixed by tgraves (mrv2, resourcemanager)<br>
     <b>JobClient fails due to a race in RM, removes staged files and in turn crashes MR AM</b><br>
     <blockquote>[~Karams] ran into this multiple times. MR JobClient crashes immediately.<br><br>{code}<br>11/09/08 10:52:35 INFO mapreduce.JobSubmitter: number of splits:2094<br>11/09/08 10:52:36 INFO mapred.YARNRunner: AppMaster capability = memory: 2048,<br>11/09/08 10:52:36 INFO mapred.YARNRunner: Command to launch container for ApplicationMaster is : $JAVA_HOME/bin/java -Dhadoop.root.logger=INFO,console -Xmx1536m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1315478927026 1 &lt;FAILCOUNT&gt; 1&gt;&lt;LOG_DIR&gt;/stdout 2&gt;&lt;LOG_DIR&gt;/...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2952">MAPREDUCE-2952</a>.
     Blocker bug reported by vinodkv and fixed by acmurthy (mrv2, resourcemanager)<br>
     <b>Application failure diagnostics are not consumed in a couple of cases</b><br>
     <blockquote>When Container crashes, the reason for failures isn&apos;t propagated because of a bug in _RMAppAttemptImpl.AMContainerCrashedTransition_ which simply discards the diagnostics of the container. Also RMAppAttemptImpl.diagnostics is never consumed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2949">MAPREDUCE-2949</a>.
     Major bug reported by raviteja and fixed by raviteja (mrv2, nodemanager)<br>
     <b>NodeManager in a inconsistent state if a service startup fails.</b><br>
     <blockquote>When a service startup fails at the Nodemanager, the Nodemanager JVM doesnot exit as the following threads are still running.<br><br>Daemon Thread [Timer for &apos;NodeManager&apos; metrics system] (Running)	<br>Thread [pool-1-thread-1] (Running)	<br>Thread [Thread-11] (Running)	<br>Thread [DestroyJavaVM] (Running).<br><br>As a result, the NodeManager keeps running even though no services are started.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2948">MAPREDUCE-2948</a>.
     Major bug reported by milindb and fixed by mahadev (contrib/streaming)<br>
     <b>Hadoop streaming test failure, post MR-2767</b><br>
     <blockquote>After removing LinuxTaskController in MAPREDUCE-2767, one of the tests in contrib/streaming: TestStreamingAsDifferentUser.java is failing since it imports import org.apache.hadoop.mapred.ClusterWithLinuxTaskController. Patch forthcoming.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2947">MAPREDUCE-2947</a>.
     Major bug reported by vinodkv and fixed by vinodkv (mrv2)<br>
     <b>Sort fails on YARN+MR with lots of task failures</b><br>
     <blockquote>[~karams](the great man the world hardly knows about) found lots of failing tasks while running sort on a 350 node cluster. The failed tasks eventually failed the job and this happening consistently on the big cluster.<br>{quote}<br>Container launch failed for container_1315410418107_0002_01_002511 : RemoteTrace: java.lang.IllegalArgumentException at java.nio.Buffer.position(Buffer.java:218) at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:129) at java.nio.ByteBuffer.get(ByteBuffer.java:675) at c...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2938">MAPREDUCE-2938</a>.
     Trivial bug reported by acmurthy and fixed by acmurthy (mrv2, scheduler)<br>
     <b>Missing log stmt for app submission fail CS</b><br>
     <blockquote>Missing log stmt for app submission fail CS</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2937">MAPREDUCE-2937</a>.
     Critical bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Errors in Application failures are not shown in the client trace.</b><br>
     <blockquote>The client side does not show enough information on why the job failed. Here is step to reproduce it:<br><br>1) set the scheduler to be capacity scheduler with queues a, b<br>2) submit a job to a queue that is not a,b<br><br>The job just fails without saying why it failed. We should have enough trace log at the client side to let the user know why it failed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2936">MAPREDUCE-2936</a>.
     Major bug reported by vinodkv and fixed by vinodkv <br>
     <b>Contrib Raid compilation broken after HDFS-1620</b><br>
     <blockquote>After working around MAPREDUCE-2935 by removing TestServiceLevelAuthorization and runing the following:<br>At the trunk level: mvn clean install package -Dtar -Pdist -Dmaven.test.skip.exec=true<br>In hadoop-mapreduce-project: ant compile-contrib -Dresolvers=internal<br><br>yields 14 errors.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2933">MAPREDUCE-2933</a>.
     Blocker sub-task reported by acmurthy and fixed by acmurthy (applicationmaster, mrv2, nodemanager, resourcemanager)<br>
     <b>Change allocate call to return ContainerStatus for completed containers rather than Container </b><br>
     <blockquote>Change allocate call to return ContainerStatus for completed containers rather than Container, we should do this all the way from the NodeManager too.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2930">MAPREDUCE-2930</a>.
     Major improvement reported by sharadag and fixed by decster (mrv2)<br>
     <b>Generate state graph from the State Machine Definition</b><br>
     <blockquote>                                              Generate state graph from State Machine Definition<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2925">MAPREDUCE-2925</a>.
     Major bug reported by devaraj.k and fixed by devaraj.k (mrv2)<br>
     <b>job -status &lt;JOB_ID&gt; is giving continuously info message for completed jobs on the console</b><br>
     <blockquote>This below message is coming continuously on the console.<br><br>{code:xml}<br>11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Failed to contact AM for job job_1314955256658_0009  Will retry..<br>11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Application state is completed. Redirecting to job history server null<br>11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Failed to contact AM for job job_1314955256658_0009  Will retry..<br>11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Application ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2917">MAPREDUCE-2917</a>.
     Major bug reported by acmurthy and fixed by acmurthy (mrv2, resourcemanager)<br>
     <b>Corner case in container reservations</b><br>
     <blockquote>Saw a corner case in container reservations where the node on which the AM is running was reserved, and hence never fulfilled leaving the application hanging.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2916">MAPREDUCE-2916</a>.
     Major bug reported by mahadev and fixed by mahadev <br>
     <b>Ivy build for MRv1 fails with bad organization for common daemon.</b><br>
     <blockquote>This jira is to ignore ivy resolve errors because of bad poms in common daemons.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2913">MAPREDUCE-2913</a>.
     Critical bug reported by revans2 and fixed by jeagles (mrv2, test)<br>
     <b>TestMRJobs.testFailingMapper does not assert the correct thing.</b><br>
     <blockquote>{code}<br>    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, <br>        events[0].getStatus().FAILED);<br>    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, <br>        events[1].getStatus().FAILED);<br>{code}<br><br>when optimized would be<br><br>{code}<br>    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, <br>        TaskCompletionEvent.Status.FAILED);<br>    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, <br>        TaskCompletionEvent.Status.FAILED);<br>{code}<br><br>obviously these assertions will neve...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2909">MAPREDUCE-2909</a>.
     Major sub-task reported by acmurthy and fixed by acmurthy (documentation, mrv2)<br>
     <b>Docs for remaining records in yarn-api</b><br>
     <blockquote>MAPREDUCE-2891 , MAPREDUCE-2897 &amp; MAPREDUCE-2898 added javadocs for core protocols (i.e. AMRMProtocol, ClientRMProtocol &amp; ContainerManager). Most &apos;records&apos; also have javadocs - this jira is to track the remaining ones.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2908">MAPREDUCE-2908</a>.
     Critical bug reported by mahadev and fixed by vinodkv (mrv2)<br>
     <b>Fix findbugs warnings in Map Reduce.</b><br>
     <blockquote>In the current trunk/0.23 codebase there are 5 findbugs warnings which cause the precommit CI builds to -1 the patches.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2907">MAPREDUCE-2907</a>.
     Major bug reported by raviprak and fixed by raviprak (mrv2, resourcemanager)<br>
     <b>ResourceManager logs filled with [INFO] debug messages from org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue</b><br>
     <blockquote>I see a lot of info messages (probably used for debugging during development)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2904">MAPREDUCE-2904</a>.
     Major bug reported by sharadag and fixed by sharadag <br>
     <b>HDFS jars added incorrectly to yarn classpath</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2899">MAPREDUCE-2899</a>.
     Major sub-task reported by acmurthy and fixed by acmurthy (mrv2, resourcemanager)<br>
     <b>Replace major parts of ApplicationSubmissionContext with a ContainerLaunchContext</b><br>
     <blockquote>We can replace major parts of ApplicationSubmissionContext with a ContainerLaunchContext.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2898">MAPREDUCE-2898</a>.
     Major sub-task reported by acmurthy and fixed by acmurthy (documentation, mrv2)<br>
     <b>Docs for core protocols in yarn-api - ContainerManager</b><br>
     <blockquote>Track docs for ContainerManager and related apis/records.  </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2897">MAPREDUCE-2897</a>.
     Major sub-task reported by acmurthy and fixed by acmurthy (documentation, mrv2)<br>
     <b>Docs for core protocols in yarn-api - ClientRMProtocol</b><br>
     <blockquote>Track docs for ClientRMProtocol and related apis/records.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2896">MAPREDUCE-2896</a>.
     Major sub-task reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>Remove all apis other than getters and setters in all org/apache/hadoop/yarn/api/records/*</b><br>
     <blockquote>Remove all apis other than getters and setters in all org/apache/hadoop/yarn/api/records/*.<br><br>We initially added some list manipulation methods etc. which are ungainly and need to go.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2894">MAPREDUCE-2894</a>.
     Blocker improvement reported by acmurthy and fixed by  (mrv2)<br>
     <b>Improvements to YARN apis</b><br>
     <blockquote>Ticket to track improvements to YARN apis.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2893">MAPREDUCE-2893</a>.
     Trivial improvement reported by viirya and fixed by viirya (client)<br>
     <b>Removing duplicate service provider in hadoop-mapreduce-client-jobclient</b><br>
     <blockquote>There is duplicate provider class name in the configuration file of ClientProtocolProvider under hadoop-mapreduce-client-jobclient. Although it will be ignored.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2891">MAPREDUCE-2891</a>.
     Major sub-task reported by acmurthy and fixed by acmurthy (documentation, mrv2)<br>
     <b>Docs for core protocols in yarn-api - AMRMProtocol</b><br>
     <blockquote>We need to add docs for AMRMProtocol</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2890">MAPREDUCE-2890</a>.
     Blocker improvement reported by acmurthy and fixed by  (documentation, mrv2)<br>
     <b>Documentation for MRv2</b><br>
     <blockquote>Let&apos;s use this jira to track docs for all of MRv2.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2889">MAPREDUCE-2889</a>.
     Critical sub-task reported by acmurthy and fixed by hitesh (documentation, mrv2)<br>
     <b>Add docs for writing new application frameworks</b><br>
     <blockquote>We need to add docs for writing new application frameworks, including examples, javadocs and sample apps.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2887">MAPREDUCE-2887</a>.
     Major improvement reported by sanjay.radia and fixed by sanjay.radia <br>
     <b>MR changes to match HADOOP-7524 (multiple RPC protocols)</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2886">MAPREDUCE-2886</a>.
     Critical bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Fix Javadoc warnings in MapReduce.</b><br>
     <blockquote>On the current trunk and 0.23, there are 73 javadoc warnings which is causing the buildbot to -1 every patch in MR. We need to fix this to stabilize the CI precommit builds.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2885">MAPREDUCE-2885</a>.
     Blocker bug reported by acmurthy and fixed by acmurthy <br>
     <b>mapred-config.sh doesn&apos;t look for $HADOOP_COMMON_HOME/libexec/hadoop-config.sh</b><br>
     <blockquote>mapred-config.sh doesn&apos;t look for $HADOOP_COMMON_HOME/libexec/hadoop-config.sh and thus fails to find it and errors out.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2882">MAPREDUCE-2882</a>.
     Minor bug reported by tlipcon and fixed by tlipcon (test)<br>
     <b>TestLineRecordReader depends on ant jars</b><br>
     <blockquote>This test is currently importing an ant utility class to read a file - this dependency doesn&apos;t work in mavenized land.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2881">MAPREDUCE-2881</a>.
     Major bug reported by gkesavan and fixed by gkesavan (build)<br>
     <b>mapreduce ant compilation fails &quot;java.lang.IllegalStateException: impossible to get artifacts&quot;</b><br>
     <blockquote>[ivy:resolve] 	found com.cenqua.clover#clover;3.0.2 in fs<br>[ivy:resolve] <br>[ivy:resolve] :: problems summary ::<br>[ivy:resolve] :::: ERRORS<br>[ivy:resolve] 	impossible to get artifacts when data has not been loaded. IvyNode = log4j#log4j;1.2.16<br>[ivy:resolve] <br>[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS<br><br>BUILD FAILED<br>/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/build.xml:451: The following error occurred while executing t...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2880">MAPREDUCE-2880</a>.
     Blocker improvement reported by vicaya and fixed by acmurthy (mrv2)<br>
     <b>Fix classpath construction for MRv2</b><br>
     <blockquote>MRConstants.java refers a hard-coded version of MR AM jar. The build config works around with a symlink. The deployment currently needs symlink workaround as well. We need to fix this so that we can actually launch arbitrary versions of AMs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2877">MAPREDUCE-2877</a>.
     Major bug reported by mahadev and fixed by mahadev <br>
     <b>Add missing Apache license header in some files in MR and also add the rat plugin to the poms.</b><br>
     <blockquote>Some of the files in MR have a missing Apache header files. We also need to add the apache-rat plugin to be able to run rat automatically via the top level pom. <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2876">MAPREDUCE-2876</a>.
     Critical bug reported by revans2 and fixed by anupamseth (mrv2)<br>
     <b>ContainerAllocationExpirer appears to use the incorrect configs</b><br>
     <blockquote>ContainerAllocationExpirer sets the expiration interval to be RMConfig.CONTAINER_LIVELINESS_MONITORING_INTERVAL but uses AMLIVELINESS_MONITORING_INTERVAL as the interval.  This is very different from what AMLivelinessMonitor does.<br><br>There should be two configs RMConfig.CONTAINER_LIVELINESS_MONITORING_INTERVAL for the monitoring interval and RMConfig.CONTAINER_EXPIRY_INTERVAL for the expiry.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2874">MAPREDUCE-2874</a>.
     Major bug reported by tgraves and fixed by eepayne (mrv2)<br>
     <b>ApplicationId printed in 2 different formats and has 2 different toString routines that are used</b><br>
     <blockquote>Looks like the ApplicationId is now printed in 2 different formats.  ApplicationIdPBImpl.java has a toString routine that prints it in the format: return &quot;application_&quot; + this.getClusterTimestamp() + &quot;_&quot; + this.getId();<br><br>While the webapps use ./hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Apps.java toString that prints it like:     <br>return _join(&quot;app&quot;, id.getClusterTimestamp(), id.getId());  </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2868">MAPREDUCE-2868</a>.
     Major bug reported by tgraves and fixed by mahadev (build)<br>
     <b>ant build broken in hadoop-mapreduce dir</b><br>
     <blockquote>The ant build target doesn&apos;t work in the hadoop-mapreduce directory since the mavenization of hdfs changes were checked in.<br><br>Error it gives is:<br>[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::<br>[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::<br>[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::<br>[ivy:resolve]           :: org.apache.avro#avro-ipc;working@host: not found<br>[ivy:resolve]           :: org.apache.hadoop#hadoop-alfredo;work...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2867">MAPREDUCE-2867</a>.
     Major bug reported by mahadev and fixed by mahadev <br>
     <b>Remove Unused TestApplicaitonCleanup in resourcemanager/applicationsmanager.</b><br>
     <blockquote>TestApplicationCleanup in resourcemanager/applicationsmanager doesnt do anything. There is already a test in resourcemanager/TestApplicationCleanup which tests all the cleanup code for container and applications. We should remove the unused one in the trunk.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2864">MAPREDUCE-2864</a>.
     Major improvement reported by revans2 and fixed by revans2 (jobhistoryserver, mrv2, nodemanager, resourcemanager)<br>
     <b>Renaming of configuration property names in yarn</b><br>
     <blockquote>Now that YARN has been put in to trunk we should do something similar to MAPREDUCE-849.  We should go back and look at all of the configurations that have been added in and rename them as needed to be consistent and subdivided by component.<br><br># We should use all lowercase in the config names. e.g., we should use appsmanager instead of appsManager etc.<br># history server config names should be prefixed with mapreduce instead of yarn.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2860">MAPREDUCE-2860</a>.
     Major bug reported by mahadev and fixed by mahadev (mrv2)<br>
     <b>Fix log4j logging in the maven test cases.</b><br>
     <blockquote>At present the logging in the new test cases is broken because surefire isnt able to find the log4j properties file. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2859">MAPREDUCE-2859</a>.
     Major bug reported by gkesavan and fixed by gkesavan <br>
     <b>mapreduce trunk is broken with eclipse plugin contrib</b><br>
     <blockquote>ant compile with eclipse home fails mapreduce trunk builds.<br><br>$ANT_HOME/bin/ant -Dversion=${VERSION} -Declipse.home=$ECLIPSE_HOME compile<br><br>compile:<br>     [echo] contrib: eclipse-plugin <br>    [javac] Compiling 45 source files to /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/contrib/eclipse-plugin/classes<br>    [javac] /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopServer.java:39...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2858">MAPREDUCE-2858</a>.
     Blocker sub-task reported by vicaya and fixed by revans2 (applicationmaster, mrv2, security)<br>
     <b>MRv2 WebApp Security</b><br>
     <blockquote>                    A new server has been added to yarn.  It is a web proxy that sits in front of the AM web UI.  The server is controlled by the yarn.web-proxy.address config.  If that config is set, and it points to an address that is different then the RM web interface then a separate proxy server needs to be launched.<br/><br><br><br/><br><br>This can be done by running <br/><br><br><br/><br><br>yarn-daemon.sh start proxyserver<br/><br><br><br/><br><br>If a separate proxy server is needed other configs also may need to be set, if security is enabled.<br/><br><br>yarn.web-proxy.principal<br/><br><br>yarn.web-proxy.keytab<br/><br><br><br/><br><br>The proxy server is stateless and should be able to support a VIP or other load balancing sitting in front of multiple instances of this server.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2854">MAPREDUCE-2854</a>.
     Major bug reported by tgraves and fixed by tgraves <br>
     <b>update INSTALL with config necessary run mapred on yarn</b><br>
     <blockquote>The following config is needed to run mapreduce on yarn framework.  Document it in the INSTALL doc.<br><br>&lt;property&gt;<br>&lt;name&gt; mapreduce.framework.name&lt;/name&gt;<br>&lt;value&gt;yarn&lt;/value&gt;<br>&lt;/property&gt;<br><br><br>The INSTALL doc also still references the old 22 mapred examples jar.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2848">MAPREDUCE-2848</a>.
     Major improvement reported by vicaya and fixed by vicaya <br>
     <b>Upgrade avro to 1.5.2</b><br>
     <blockquote>Upgrade avro to the current version requires some code changes in mapreduce due to avro package split. The mapreduce part of the change will be part of the atomic commit of HADOOP-7264 after MAPREDUCE-279 is merged to trunk. The jira is for mapreduce change log.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2846">MAPREDUCE-2846</a>.
     Blocker bug reported by aw and fixed by owen.omalley (task, task-controller, tasktracker)<br>
     <b>a small % of all tasks fail with DefaultTaskController</b><br>
     <blockquote>                                              Fixed a race condition in writing the log index file that caused tasks to &amp;#39;fail&amp;#39;.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2844">MAPREDUCE-2844</a>.
     Trivial bug reported by rramya and fixed by raviteja (mrv2)<br>
     <b>[MR-279] Incorrect node ID info </b><br>
     <blockquote>The node ID info for the nodemanager entires on the RM UI incorrectly displays the value of $yarn.server.nodemanager.address instead of the ID.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2843">MAPREDUCE-2843</a>.
     Major bug reported by rramya and fixed by abhijit.shingate (mrv2)<br>
     <b>[MR-279] Node entries on the RM UI are not sortable</b><br>
     <blockquote>The nodemanager entries on the RM UI is not sortable unlike the other web pages. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2840">MAPREDUCE-2840</a>.
     Minor bug reported by tgraves and fixed by jeagles (mrv2)<br>
     <b>mr279 TestUberAM.testSleepJob test fails</b><br>
     <blockquote>Currently the TestUberAM.testSleepJob  is failing on the mr279 branch. <br><br>snippet of failure:<br>junit.framework.AssertionFailedError: null<br>	at junit.framework.Assert.fail(Assert.java:47)<br>	at junit.framework.Assert.assertTrue(Assert.java:20)<br>	at junit.framework.Assert.assertTrue(Assert.java:27)<br>	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testSleepJob(TestMRJobs.java:150)<br>	at org.apache.hadoop.mapreduce.v2.TestUberAM.testSleepJob(TestUberAM.java:58)<br>	at sun.reflect.NativeMethodAccessorImpl.invok...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2839">MAPREDUCE-2839</a>.
     Major bug reported by sseth and fixed by sseth <br>
     <b>MR Jobs fail on a secure cluster with viewfs</b><br>
     <blockquote>TokenCache needs to use the new FileSystem.getDelegationTokens api for it to work with viewfs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2821">MAPREDUCE-2821</a>.
     Blocker bug reported by rramya and fixed by mahadev (mrv2)<br>
     <b>[MR-279] Missing fields in job summary logs </b><br>
     <blockquote>The following fields are missing in the job summary logs in mrv2:<br>- numSlotsPerMap<br>- numSlotsPerReduce<br>- clusterCapacity (Earlier known as clusterMapCapacity and clusterReduceCapacity in 0.20.x)<br><br>The first two fields are important to know if the job was a High RAM job or not and the last field is important to know the total available resource in the cluster during job execution.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2808">MAPREDUCE-2808</a>.
     Minor bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>pull MAPREDUCE-2797 into mr279 branch</b><br>
     <blockquote>The ant tar command fails in the mapreduce directory on the mr279 branch.  The issue was a change in hdfs and was fixed on trunk with jira MAPREDUCE-2797.  Pull that change into mr279.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2807">MAPREDUCE-2807</a>.
     Major sub-task reported by sharadag and fixed by sharadag (applicationmaster, mrv2, resourcemanager)<br>
     <b>MR-279: AM restart does not work after RM refactor</b><br>
     <blockquote>When the AM crashes, RM is not able to launch a new App attempt.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2805">MAPREDUCE-2805</a>.
     Minor improvement reported by szetszwo and fixed by szetszwo (contrib/raid)<br>
     <b>Update RAID for HDFS-2241</b><br>
     <blockquote>{noformat}<br>src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/datanode/RaidBlockSender.java:44: interface expected here<br>    [javac] public class RaidBlockSender implements java.io.Closeable, FSConstants {<br>    [javac]                                                            ^<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2802">MAPREDUCE-2802</a>.
     Critical improvement reported by rramya and fixed by jeagles (mrv2)<br>
     <b>[MR-279] Jobhistory filenames should have jobID to help in better parsing </b><br>
     <blockquote>For jobID such as job_1312933838300_0007, jobhistory file names are named as job%5F1312933838300%5F0007_&lt;submit_time&gt;_ramya_&lt;jobname&gt;_&lt;finish_time&gt;_1_1_SUCCEEDED.jhist It would be easier for parsing if the jobIDs were a part of the filenames.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2800">MAPREDUCE-2800</a>.
     Major bug reported by rramya and fixed by sseth (mrv2)<br>
     <b>clockSplits, cpuUsages, vMemKbytes, physMemKbytes is set to -1 in jhist files</b><br>
     <blockquote>clockSplits, cpuUsages, vMemKbytes, physMemKbytes  is set to -1 for all the map tasks for the last 4 progress interval in the jobhistory files.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2797">MAPREDUCE-2797</a>.
     Major bug reported by szetszwo and fixed by szetszwo (contrib/raid, test)<br>
     <b>Some java files cannot be compiled</b><br>
     <blockquote>Due to the changes in HDFS-2239, the following files cannot be compiled (Thanks Amar for pointing them out.)<br>1. src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCache.java<br>2. src/test/mapred/org/apache/hadoop/mapreduce/security/TestBinaryTokenFile.java<br>3. src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCacheOldApi.java<br>4. src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRaid.java</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2796">MAPREDUCE-2796</a>.
     Major bug reported by rramya and fixed by devaraj.k (mrv2)<br>
     <b>[MR-279] Start time for all the apps is set to 0</b><br>
     <blockquote>The start time for all the apps in the output of &quot;job -list&quot; is set to 0</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2794">MAPREDUCE-2794</a>.
     Blocker bug reported by rramya and fixed by johnvijoe (mrv2)<br>
     <b>[MR-279] Incorrect metrics value for AvailableGB per queue per user</b><br>
     <blockquote>AvailableGB per queue is not the same as AvailableGB per queue per user when the user limit is set to 100%.<br>i.e. if the total available GB of the cluster is 60, and queue &quot;default&quot; has 92% capacity with 100% as the user limit, AvailableGB per queue default = 55 (i.e. 0.92*60) whereas AvailableGB per queue for user ramya is 56 (however it should be 55 = 0.92*60*1) <br><br>Also, unlike the AvailableGB/queue, AvailableGB/queue/user is not decremented when user ramya is running apps on the &quot;default&quot; qu...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2792">MAPREDUCE-2792</a>.
     Blocker sub-task reported by rramya and fixed by vinodkv (mrv2, security)<br>
     <b>[MR-279] Replace IP addresses with hostnames</b><br>
     <blockquote>Currently, all the logs, UI, CLI have IP addresses of the NM/RM, which are difficult to manage. It will be useful to have hostnames like in 0.20.x for easier debugging and maintenance purpose. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2791">MAPREDUCE-2791</a>.
     Blocker bug reported by rramya and fixed by devaraj.k (mrv2)<br>
     <b>[MR-279] Missing/incorrect info on job -status CLI </b><br>
     <blockquote>There are a couple of details missing/incorrect on the job -status command line output for completed jobs:<br><br>1. Incorrect job file<br>2. map() completion is always 0<br>3. reduce() completion is always set to 0<br>4. history URL is empty<br>5. Missing launched map tasks<br>6. Missing launched reduce tasks <br><br><br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2789">MAPREDUCE-2789</a>.
     Major bug reported by rramya and fixed by eepayne (mrv2)<br>
     <b>[MR:279] Update the scheduling info on CLI</b><br>
     <blockquote>                                              &amp;quot;mapred/job -list&amp;quot; now contains map/reduce, container, and resource information.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2788">MAPREDUCE-2788</a>.
     Major bug reported by ahmed.radwan and fixed by ahmed.radwan (mrv2)<br>
     <b>Normalize requests in FifoScheduler.allocate to prevent NPEs later</b><br>
     <blockquote>The assignContainer() method in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue can cause the scheduler to crash if the ResourseRequest capability memory == 0 (divide by zero).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2783">MAPREDUCE-2783</a>.
     Critical bug reported by tgraves and fixed by eepayne (mrv2)<br>
     <b>mr279 job history handling after killing application</b><br>
     <blockquote>The job history/application tracking url handling during kill is not consistent. Currently if you kill a job that was running the tracking url points to job history, but job history server doesn&apos;t have the job.  </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2782">MAPREDUCE-2782</a>.
     Major test reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>MR-279: Unit (mockito) tests for CS</b><br>
     <blockquote>Add (true) unit tests for CapacityScheduler</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2781">MAPREDUCE-2781</a>.
     Minor bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>mr279 RM application finishtime not set</b><br>
     <blockquote>The RM Application finishTime isn&apos;t being set.  Looks like it got lost in the RM refactor.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2779">MAPREDUCE-2779</a>.
     Major bug reported by mingma and fixed by mingma (job submission)<br>
     <b>JobSplitWriter.java can&apos;t handle large job.split file</b><br>
     <blockquote>We use cascading MultiInputFormat. MultiInputFormat sometimes generates big job.split used internally by hadoop, sometimes it can go beyond 2GB.<br><br>In JobSplitWriter.java, the function that generates such file uses 32bit signed integer to compute offset into job.split.<br><br><br>writeNewSplits<br>...<br>        int prevCount = out.size();<br>...<br>        int currCount = out.size();<br><br>writeOldSplits<br>...<br>      long offset = out.size();<br>...<br>      int currLen = out.size();<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2776">MAPREDUCE-2776</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Fix some of the yarn findbug warnings</b><br>
     <blockquote>Fix / ignore some of the findbug warnings in the yarn module.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2775">MAPREDUCE-2775</a>.
     Blocker bug reported by rramya and fixed by devaraj.k (mrv2)<br>
     <b>[MR-279] Decommissioned node does not shutdown</b><br>
     <blockquote>A Nodemanager which is decommissioned by an admin via refreshnodes does not automatically shutdown. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2774">MAPREDUCE-2774</a>.
     Minor bug reported by rramya and fixed by venug (mrv2)<br>
     <b>[MR-279] Add a startup msg while starting RM/NM</b><br>
     <blockquote>Add a startup msg while starting NM/RM indicating the version, build details etc. This will help in easier parsing of logs and debugging.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2773">MAPREDUCE-2773</a>.
     Minor bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>[MR-279] server.api.records.NodeHealthStatus renamed but not updated in client NodeHealthStatus.java</b><br>
     <blockquote>On the mr279 branch, you can&apos;t successfully run the ant target from the mapreduce directory since the checkin of the RM refactor.  <br><br>The issue is the NodeHealthStatus rename from org.apache.hadoop.yarn.server.api.records.NodeHealthStatus to org.apache.hadoop.yarn.api.records.NodeHealthStatus but the client mapreduce/src/java/org/apache/hadoop/mapred/NodeHealthStatus.java wasn&apos;t updated with the change</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2772">MAPREDUCE-2772</a>.
     Major bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: mrv2 no longer compiles against trunk after common mavenization.</b><br>
     <blockquote>mrv2 no longer compiles against trunk after common mavenization</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2767">MAPREDUCE-2767</a>.
     Blocker bug reported by milindb and fixed by milindb (security)<br>
     <b>Remove Linux task-controller from 0.22 branch</b><br>
     <blockquote>There&apos;s a potential security hole in the task-controller as it stands. Based on the discussion on general@, removing task-controller from the 0.22 branch will pave way for 0.22.0 release. (This was done for the 0.21.0 release as well: see MAPREDUCE-2014.) We can roll a 0.22.1 release with the task-controller when it is fixed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2766">MAPREDUCE-2766</a>.
     Blocker sub-task reported by rramya and fixed by hitesh (mrv2)<br>
     <b>[MR-279] Set correct permissions for files in dist cache</b><br>
     <blockquote>Currently, the files in both public and private dist cache are having 777 permission. Also, the group ownership of files on private cache have to be set to $TT_SPECIAL_GROUP<br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2764">MAPREDUCE-2764</a>.
     Major bug reported by daryn and fixed by owen.omalley <br>
     <b>Fix renewal of dfs delegation tokens</b><br>
     <blockquote>                                              Generalizes token renewal and canceling to a common interface and provides a plugin interface for adding renewers for new kinds of tokens. Hftp changed to store the tokens as HFTP and renew them over http.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2763">MAPREDUCE-2763</a>.
     Major bug reported by rramya and fixed by  (mrv2)<br>
     <b>IllegalArgumentException while using the dist cache</b><br>
     <blockquote>IllegalArgumentException is seen while using distributed cache to cache some files and custom jars in classpath.<br><br>A simple way to reproduce this error is by using a streaming job:<br>hadoop jar hadoop-streaming.jar -libjars file://&lt;path to custom jar&gt; -input &lt;path to input file&gt; -output out -mapper &quot;cat&quot; -reducer NONE -cacheFile  hdfs://&lt;path to some file&gt;#linkname<br><br>This is a regression introduced and the same command works fine on 0.20.x</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2762">MAPREDUCE-2762</a>.
     Blocker bug reported by rramya and fixed by mahadev (mrv2)<br>
     <b>[MR-279] - Cleanup staging dir after job completion</b><br>
     <blockquote>The files created under the staging dir have to be deleted after job completion. Currently, all job.* files remain forever in the ${yarn.apps.stagingDir}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2760">MAPREDUCE-2760</a>.
     Minor bug reported by tlipcon and fixed by tlipcon (documentation)<br>
     <b>mapreduce.jobtracker.split.metainfo.maxsize typoed in mapred-default.xml</b><br>
     <blockquote>The configuration mapreduce.jobtracker.split.metainfo.maxsize is incorrectly included in mapred-default.xml as mapreduce.*job*.split.metainfo.maxsize. It seems that {{jobtracker}} is correct, since this is a JT-wide property rather than a job property.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2756">MAPREDUCE-2756</a>.
     Minor bug reported by revans2 and fixed by revans2 (client, mrv2)<br>
     <b>JobControl can drop jobs if an error occurs</b><br>
     <blockquote>If you run a pig job with UDFs that has not been recompiled for MRV2.  There are situations where pig will fail with an error message stating that Hadoop failed and did not give a reason.  There is even the possibility of deadlock if an Error is thrown and the JobControl thread dies.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2754">MAPREDUCE-2754</a>.
     Blocker bug reported by rramya and fixed by raviteja (mrv2)<br>
     <b>MR-279: AM logs are incorrectly going to stderr and error messages going incorrectly to stdout</b><br>
     <blockquote>The log messages for AM container are going into stderr instead of syslog. Also, stderr and stdout roles are reversed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2751">MAPREDUCE-2751</a>.
     Blocker bug reported by vinodkv and fixed by sseth (mrv2)<br>
     <b>[MR-279] Lot of local files left on NM after the app finish.</b><br>
     <blockquote>This ticket is about app-only files which should be cleaned after app-finish.<br><br>I see these undeleted after app-finish:<br>/tmp/nm-local-dir/0/nmPrivate/application_1305091029545_0001/*<br>/tmp/nm-local-dir/0/nmPrivate/container_1305019205843_0001_000002/*<br>/tmp/nm-local-dir/0/usercache/nobody/appcache/application_1305091029545_0001/*<br><br>We should check for other left-over files too, if any.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2749">MAPREDUCE-2749</a>.
     Major bug reported by vinodkv and fixed by tgraves (mrv2)<br>
     <b>[MR-279] NM registers with RM even before it starts various servers</b><br>
     <blockquote>In case NM eventually fails to start the ContainerManager server because of say a port clash, RM will have to wait for expiry to detect the NM crash.<br><br>It is desirable to make NM register with RM only after it can start all of its components successfully.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2747">MAPREDUCE-2747</a>.
     Blocker sub-task reported by vinodkv and fixed by revans2 (mrv2, nodemanager, security)<br>
     <b>[MR-279] [Security] Cleanup LinuxContainerExecutor binary sources</b><br>
     <blockquote>There are a lot of references to the old task-controller nomenclature still, job/task refs instead of app/container.<br><br>Also the configuration file is named as taskcontroller.cfg and the configured variables are also from the mapred world (mrv1). These SHOULD  be fixed before we make a release. Marking this as blocker.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2746">MAPREDUCE-2746</a>.
     Blocker sub-task reported by vinodkv and fixed by acmurthy (mrv2, security)<br>
     <b>[MR-279] [Security] Yarn servers can&apos;t communicate with each other with hadoop.security.authorization set to true</b><br>
     <blockquote>Because of this problem, till now, we&apos;ve been testing YARN+MR with {{hadoop.security.authorization}} set to false. We need to register yarn communication protocols in the implementation of the authorization related PolicyProvider (MapReducePolicyProvider.java).<br><br>[~devaraj] also found this issue independently.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2741">MAPREDUCE-2741</a>.
     Major task reported by tucu00 and fixed by tucu00 (build)<br>
     <b>Make ant build system work with hadoop-common JAR generated by Maven</b><br>
     <blockquote>Some tweaks must be done in MAPRED &amp; its contribs ivy configuration to work with HADOOP-6671.<br><br>This wil be a temporary fix until MAPRED is mavenized.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2740">MAPREDUCE-2740</a>.
     Major bug reported by tlipcon and fixed by tlipcon <br>
     <b>MultipleOutputs in new API creates needless TaskAttemptContexts</b><br>
     <blockquote>MultipleOutputs.write creates a new TaskAttemptContext, which we&apos;ve seen to take a significant amount of CPU. The TaskAttemptContext constructor creates a JobConf, gets current UGI, etc. I don&apos;t see any reason it needs to do this, instead of just creating a single TaskAttemptContext when the InputFormat is created (or lazily but cached as a member)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2738">MAPREDUCE-2738</a>.
     Blocker bug reported by rramya and fixed by revans2 (mrv2)<br>
     <b>Missing cluster level stats on the RM UI</b><br>
     <blockquote>Cluster usage information such as the following are currently not available in the RM UI. <br><br>- Total number of apps submitted so far<br>- Total number of containers running/total memory usage <br>- Total capacity of the cluster (in terms of memory)<br>- Reserved memory<br>- Total number of NMs - sorting based on Node IDs is an option but when there are lost NMs or restarted NMs, the node ids does not correspond to the actual value<br>- Blacklisted NMs - sorting based on health-status and counting manually is...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2737">MAPREDUCE-2737</a>.
     Major bug reported by rramya and fixed by sseth (mrv2)<br>
     <b>Update the progress of jobs on client side</b><br>
     <blockquote>The progress of the jobs are not being correctly updated on the client side. The map progress halts at 66% and both map/reduce progress % does not display 100 when the job completes.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2736">MAPREDUCE-2736</a>.
     Major task reported by eli and fixed by eli (jobtracker, tasktracker)<br>
     <b>Remove unused contrib components dependent on MR1</b><br>
     <blockquote>                                              The pre-MR2 MapReduce implementation (JobTracker, TaskTracer, etc) and contrib components are no longer supported. This implementation is currently supported in the 0.20.20x releases.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2735">MAPREDUCE-2735</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>MR279: finished applications should be added to an application summary log</b><br>
     <blockquote>When an application finishes it should be added to an application summary log for historical purposes.  jira MAPREDUCE-2649 is going to start purging applications from RM when certain limits are hit which makes this more critical. We also need to save the information early enough after the app finishes so we don&apos;t lose the info if the RM does get restarted.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2732">MAPREDUCE-2732</a>.
     Major bug reported by szetszwo and fixed by szetszwo (test)<br>
     <b>Some tests using FSNamesystem.LOG cannot be compiled</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2727">MAPREDUCE-2727</a>.
     Major bug reported by naisbitt and fixed by naisbitt (mrv2)<br>
     <b>MR-279: SleepJob throws divide by zero exception when count = 0</b><br>
     <blockquote>When the count is 0 for mappers or reducers, a divide-by-zero exception is thrown.  There are existing checks to error out when count &lt; 0, which obviously doesn&apos;t handle the 0 case.  This is causing the MRReliabilityTest to fail.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2726">MAPREDUCE-2726</a>.
     Blocker improvement reported by naisbitt and fixed by naisbitt (mrv2)<br>
     <b>MR-279: Add the jobFile to the web UI</b><br>
     <blockquote>MAPREDUCE:2716 adds the jobfile information to the ApplicationReport.  With that information available, we should add the jobfile to the web UI as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2719">MAPREDUCE-2719</a>.
     Major new feature reported by sharadag and fixed by hitesh (mrv2)<br>
     <b>MR-279: Write a shell command application</b><br>
     <blockquote>                                              Adding a simple, DistributedShell application as an alternate framework to MapReduce and to act as an illustrative example for porting applications to YARN.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2716">MAPREDUCE-2716</a>.
     Major bug reported by naisbitt and fixed by naisbitt (mrv2)<br>
     <b>MR279: MRReliabilityTest job fails because of missing job-file.</b><br>
     <blockquote>The ApplicationReport should have the jobFile (e.g. hdfs://localhost:9000/tmp/hadoop-&lt;USER&gt;/mapred/staging/&lt;USER&gt;/.staging/job_201107121640_0001/job.xml)<br><br><br>Without it, jobs such as MRReliabilityTest fail with the following error (caused by the fact that jobFile is hardcoded to &quot;&quot; in TypeConverter.java):<br>e.g. java.lang.IllegalArgumentException: Can not create a Path from an empty string<br>        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)<br>        at org.apache.hadoop.fs.Path.&lt;init&gt;(...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2711">MAPREDUCE-2711</a>.
     Major bug reported by szetszwo and fixed by szetszwo (contrib/raid)<br>
     <b>TestBlockPlacementPolicyRaid cannot be compiled</b><br>
     <blockquote>{{TestBlockPlacementPolicyRaid}} access internal {{FSNamesystem}} directly.  It cannot be compiled after HDFS-2147.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2710">MAPREDUCE-2710</a>.
     Major bug reported by szetszwo and fixed by szetszwo (client)<br>
     <b>Update DFSClient.stringifyToken(..) in JobSubmitter.printTokens(..) for HDFS-2161</b><br>
     <blockquote>{{DFSClient.stringifyToken(..)}} was removed by HDFS-2161.  {{JobSubmitter.printTokens(..)}} won&apos;t be compiled.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2708">MAPREDUCE-2708</a>.
     Blocker sub-task reported by sharadag and fixed by sharadag (applicationmaster, mrv2)<br>
     <b>[MR-279] Design and implement MR Application Master recovery</b><br>
     <blockquote>Design recovery of MR AM from crashes/node failures. The running job should recover from the state it left off.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2707">MAPREDUCE-2707</a>.
     Major improvement reported by jnp and fixed by jnp <br>
     <b>ProtoOverHadoopRpcEngine without using TunnelProtocol over WritableRpc</b><br>
     <blockquote>ProtoOverHadoopRpcEngine is introduced in MR-279, which uses TunnelProtocol over WritableRpcEngine. This jira removes the tunnel protocol and lets ProtoOverHadoopRpcEngine directly interact with ipc.Client and ipc.Server.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2706">MAPREDUCE-2706</a>.
     Major bug reported by naisbitt and fixed by naisbitt (mrv2)<br>
     <b>MR-279: Submit jobs beyond the max jobs per queue limit no longer gets logged</b><br>
     <blockquote>Submitting jobs over the queue limits used to print log messages such as these:<br>hadoop-mapred-jobtracker-HOSTNAME.log. ... INFO<br>org.apache.hadoop.mapred.CapacityTaskScheduler: default has 10 active tasks for user MYUSER, cannot initialize<br>job_XXX with 10 tasks since it will exceed limit of 15 active tasks per user for this queue<br>and<br>hadoop-mapred-jobtracker-HOSTNAME.log ... INFO org.apache.hadoop.mapred.CapacityTaskScheduler: default already has 2 running jobs and 0 initializing jobs; cannot ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2705">MAPREDUCE-2705</a>.
     Major bug reported by tgraves and fixed by tgraves (tasktracker)<br>
     <b>tasks localized and launched serially by TaskLauncher - causing other tasks to be delayed</b><br>
     <blockquote>The current TaskLauncher serially launches new tasks one at a time. During the launch it does the localization and then starts the map/reduce task.  This can cause any other tasks to be blocked waiting for the current task to be localized and started. In some instances we have seen a task that has a large file to localize (1.2MB) block another task for about 40 minutes. This particular task being blocked was a cleanup task which caused the job to be delayed finishing for the 40 minutes.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2702">MAPREDUCE-2702</a>.
     Blocker sub-task reported by sharadag and fixed by sharadag (applicationmaster, mrv2)<br>
     <b>[MR-279] OutputCommitter changes for MR Application Master recovery</b><br>
     <blockquote>                                              Enhance OutputCommitter and FileOutputCommitter to allow for recover of tasks across job restart.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2701">MAPREDUCE-2701</a>.
     Major improvement reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: app/Job.java needs UGI for the user that launched it</b><br>
     <blockquote>./mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/Job.java is missing some data that is needed by the Job History GUI.  It needs the UGI for the user that launched it.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2697">MAPREDUCE-2697</a>.
     Major bug reported by acmurthy and fixed by acmurthy (mrv2)<br>
     <b>Enhance CS to cap concurrently running jobs</b><br>
     <blockquote>Enhance CS to cap concurrently running jobs ala 0.20.203</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2696">MAPREDUCE-2696</a>.
     Major sub-task reported by acmurthy and fixed by sseth (mrv2, nodemanager)<br>
     <b>Container logs aren&apos;t getting cleaned up when LogAggregation is disabled</b><br>
     <blockquote>Container logs aren&apos;t getting cleaned up when log-aggregation is disabled.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2693">MAPREDUCE-2693</a>.
     Critical bug reported by amolkekre and fixed by hitesh (mrv2)<br>
     <b>NPE in AM causes it to lose containers which are never returned back to RM</b><br>
     <blockquote>The following exception in AM of an application at the top of queue causes this. Once this happens, AM keeps obtaining<br>containers from RM and simply loses them. Eventually on a cluster with multiple jobs, no more scheduling happens<br>because of these lost containers.<br><br>It happens when there are blacklisted nodes at the app level in AM. A bug in AM<br>(RMContainerRequestor.containerFailedOnHost(hostName)) is causing this - nodes are simply getting removed from the<br>request-table. We should make sure ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2691">MAPREDUCE-2691</a>.
     Major improvement reported by amolkekre and fixed by sseth (mrv2)<br>
     <b>Finish up the cleanup of distributed cache file resources and related tests.</b><br>
     <blockquote>Implement cleanup of distributed cache file resources</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2690">MAPREDUCE-2690</a>.
     Major bug reported by rramya and fixed by eepayne (mrv2)<br>
     <b>Construct the web page for default scheduler</b><br>
     <blockquote>Currently, the web page for default scheduler reads as &quot;Under construction&quot;. This is a long known issue, but could not find a tracking ticket. Hence opening one.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2689">MAPREDUCE-2689</a>.
     Major bug reported by rramya and fixed by  (mrv2)<br>
     <b>InvalidStateTransisiton when AM is not assigned to a job</b><br>
     <blockquote>In cases where an AM is not being assigned to a job, RELEASED at COMPLETED invalid event is observed. This is easily reproducible in cases such as MAPREDUCE-2687.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2687">MAPREDUCE-2687</a>.
     Blocker bug reported by rramya and fixed by mahadev (mrv2)<br>
     <b>Non superusers unable to launch apps in both secure and non-secure cluster</b><br>
     <blockquote>Apps of non superuser fail to succeed in both secure and non-secure environment. Only the superuser(i.e. one who started/owns the mrv2 cluster) is able to launch apps successfully. However, when a normal user submits a job, the job fails.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2682">MAPREDUCE-2682</a>.
     Trivial improvement reported by acmurthy and fixed by vinodkv <br>
     <b>Add a -classpath option to bin/mapred</b><br>
     <blockquote>We should have a bin/mapred classpath switch, MR-279 uses this in the branch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2680">MAPREDUCE-2680</a>.
     Minor improvement reported by acmurthy and fixed by acmurthy <br>
     <b>Enhance job-client cli to show queue information for running jobs</b><br>
     <blockquote>It&apos;d be very useful to display queue-information for running jobs alongwith jobid, user, start-time etc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2679">MAPREDUCE-2679</a>.
     Trivial improvement reported by acmurthy and fixed by acmurthy <br>
     <b>MR-279: Merge MR-279 related minor patches into trunk</b><br>
     <blockquote>Jira to track very minor and misc. changes to trunk for MR-279</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2678">MAPREDUCE-2678</a>.
     Major bug reported by naisbitt and fixed by naisbitt (contrib/capacity-sched)<br>
     <b>MR-279: minimum-user-limit-percent no longer honored</b><br>
     <blockquote>MR-279: In the capacity-scheduler.xml configuration, the &apos;minimum-user-limit-percent&apos; property is no longer honored. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2677">MAPREDUCE-2677</a>.
     Major bug reported by rramya and fixed by revans2 (mrv2)<br>
     <b>MR-279: 404 error while accessing pages from history server</b><br>
     <blockquote>Accessing the following pages from the history server, causes 404 HTTP error<br>1. Cluster-&gt; About <br>2. Cluster -&gt; Applications<br>3. Cluster -&gt; Scheduler<br>4. Application -&gt; About</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2676">MAPREDUCE-2676</a>.
     Major improvement reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: JobHistory Job page needs reformatted</b><br>
     <blockquote>The Job page, The Maps page and the Reduces page for the job history server needs to be reformatted.<br><br>The Job Overview needs to add in the User, a link to the Job Conf, and the Job ACLs<br>It also needs Submitted at, launched at, and finished at, depending on how they relates to Started and Elapsed.<br><br>In the attempts table we need to remove the new and the running columns<br>In the tasks table we need to remove progress, pending, and running columns and add in a failed count column<br>We also need to i...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2675">MAPREDUCE-2675</a>.
     Major improvement reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: JobHistory Server main page needs to be reformatted</b><br>
     <blockquote>The main page of the Job History Server is based off of the Application Master code.  It needs to be reformatted to be more useful and better match what was there before.<br><br>- The Active Jobs title needs to be replaced with something more appropriate (i.e. Retired Jobs)<br>- The table of jobs should have the following columns in it<br>  - Submit time, Job Id, Job Name, User and just because I think it would be useful state, maps completed, maps failed, reduces completed, reduces failed<br>- The table ne...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2672">MAPREDUCE-2672</a>.
     Major improvement reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: JobHistory Server needs Analysis this job</b><br>
     <blockquote>The JobHistory Server needs to implement the Analysis this job functionality from the previous server.<br><br>This should include the following info<br>Hadoop Job ID <br>User : <br>JobName : <br>JobConf : <br>Submitted At : <br>Launched At :  (including duration)<br>Finished At :  (including duration)<br>Status :<br><br>Time taken by best performing Map task &lt;TASK_LINK&gt;:<br>Average time taken by Map tasks:<br>Worse performing map tasks: (including task links and duration)<br>The last Map task &lt;TASK_LINK&gt; finished at (relative to the Job...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2670">MAPREDUCE-2670</a>.
     Trivial bug reported by eli and fixed by eli <br>
     <b>Fixing spelling mistake in FairSchedulerServlet.java</b><br>
     <blockquote>&quot;Admininstration&quot; is misspelled.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2668">MAPREDUCE-2668</a>.
     Blocker bug reported by revans2 and fixed by tgraves (mrv2)<br>
     <b>MR-279: APPLICATION_STOP is never sent to AuxServices</b><br>
     <blockquote>APPLICATION_STOP is never sent to the AuxServices only APPLICATION_INIT.  This means that all map intermediate data will never be deleted.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2667">MAPREDUCE-2667</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>MR279: mapred job -kill leaves application in RUNNING state</b><br>
     <blockquote>the mapred job -kill command doesn&apos;t seem to fully clean up the application.<br><br>If you kill a job and run mapred job -list again it still shows up as running:<br><br>mapred job -kill job_1310072430717_0003<br>Killed job job_1310072430717_0003<br><br> mapred job -list<br>Total jobs:1<br>JobId   State   StartTime       UserName        Queue   Priority        SchedulingInfo<br>job_1310072430717_0003  RUNNING 0       tgraves default NORMAL  98.139.92.22:19888/yarn/job/job_1310072430717_3_3<br><br>Running kill again will error o...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2666">MAPREDUCE-2666</a>.
     Blocker sub-task reported by revans2 and fixed by jeagles (mrv2)<br>
     <b>MR-279: Need to retrieve shuffle port number on ApplicationMaster restart</b><br>
     <blockquote>MAPREDUCE-2652 allows ShuffleHandler to return the port it is operating on.  In the case of an ApplicationMaster crash where it needs to be restarted that information is lost.  We either need to re-query it from each of the NodeManagers or to persist it to the JobHistory logs and retrieve it again.  The job history logs is probably the simpler solution.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2664">MAPREDUCE-2664</a>.
     Major improvement reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Implement JobCounters for MRv2 + Fix for Map Data Locality</b><br>
     <blockquote>MRv2 is currently not setting any Job Counters.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2663">MAPREDUCE-2663</a>.
     Minor bug reported by ahmed.radwan and fixed by ahmed.radwan (mrv2)<br>
     <b>MR-279: Refactoring StateMachineFactory inner classes</b><br>
     <blockquote>The code for ApplicableSingleTransition and ApplicableMultipleTransition inner classes is almost identical. For maintainability, it is better to refactor them into a single inner class.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2661">MAPREDUCE-2661</a>.
     Minor bug reported by ahmed.radwan and fixed by ahmed.radwan (mrv2)<br>
     <b>MR-279: Accessing MapTaskImpl from TaskImpl</b><br>
     <blockquote>We are directly accessing MapTaskImpl in TaskImpl.InitialScheduleTransition.transition(..). It&apos;ll be better to reorganize the code so each subclass can provide its own behavior instead of explicitly checking for the subclass type. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2655">MAPREDUCE-2655</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>MR279: Audit logs for YARN </b><br>
     <blockquote>We need audit logs for YARN components:<br><br>ResourceManager:<br> - All the refresh* protocol access points - refreshQueues, refreshNodes, refreshProxyUsers,<br>refreshUserToGroupMappings.<br> - All app-submissions, app-kills to RM.<br> - Illegal and successful(?) AM registrations.<br> - Illegal container allocations/deallocations from AMs<br> - Successful container allocations/deallocations from AMs too?<br><br>NodeManager:<br> - Illegal container launches from AMs<br> - Successful container launches from AMs too?<br><br>Not sure ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2652">MAPREDUCE-2652</a>.
     Major bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: Cannot run multiple NMs on a single node </b><br>
     <blockquote>Currently in MR-279 the Auxiliary services, like ShuffleHandler, have no way to communicate information back to the applications.  Because of this the Map Reduce Application Master has hardcoded in a port of 8080 for shuffle.  This prevents the configuration &quot;mapreduce.shuffle.port&quot; form ever being set to anything but 8080.  The code should be updated to allow this information to be returned to the application master.  Also the data needs to be persisted to the task log so that on restart the...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2649">MAPREDUCE-2649</a>.
     Major bug reported by tgraves and fixed by tgraves (mrv2)<br>
     <b>MR279: Fate of finished Applications on RM</b><br>
     <blockquote>                    New config added:<br/><br><br>&amp;nbsp;&amp;nbsp;&amp;nbsp;// the maximum number of completed applications the RM keeps &amp;lt;name&amp;gt;yarn.server.resourcemanager.expire.applications.completed.max&amp;lt;/name&amp;gt;<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2646">MAPREDUCE-2646</a>.
     Critical bug reported by sharadag and fixed by sharadag (applicationmaster, mrv2)<br>
     <b>MR-279: AM with same sized maps and reduces hangs in presence of failing maps</b><br>
     <blockquote>Currently AM can assign a container given by RM to any map or reduce. However RM allocates for a particular priority. This leads to AM and RM data structures going out of sync.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2644">MAPREDUCE-2644</a>.
     Major bug reported by jwills and fixed by jwills (mrv2)<br>
     <b>NodeManager fails to create containers when NM_LOG_DIR is not explicitly set in the Configuration</b><br>
     <blockquote>If the yarn configuration does not explicitly specify a value for the yarn.server.nodemanager.log.dir property, container allocation will fail on the NodeManager w/an NPE when the LocalDirAllocator goes to create the temp directory. In most of the code, we handle this by defaulting to /tmp/logs, but we cannot do this in the LocalDirAllocator context, so we need to set the default value explicitly in the Configuration.<br><br>Marking this as major b/c it&apos;s annoying to bump into it when you&apos;re gettin...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2641">MAPREDUCE-2641</a>.
     Minor sub-task reported by jwills and fixed by jwills (mrv2)<br>
     <b>Fix the ExponentiallySmoothedTaskRuntimeEstimator and its unit test</b><br>
     <blockquote>Fixed the ExponentiallySmoothedTaskRuntimeEstimator so that it can run and pass the test defined for it in TestRuntimeEstimators.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2630">MAPREDUCE-2630</a>.
     Minor bug reported by jwills and fixed by jwills (mrv2)<br>
     <b>MR-279: refreshQueues leads to NPEs when used w/FifoScheduler</b><br>
     <blockquote>The RM&apos;s admin service exposes a method refreshQueues that is used to update the queue configuration when used with the CapacityScheduler, but if it is used with the FifoScheduler, it will set the containerTokenSecretManager/clusterTracker fields on the FifoScheduler to null, which eventually leads to NPE. Since the FifoScheduler only has one queue that cannot be refreshed, the correct behavior is for the refreshQueues call to be a no-op.<br><br>I will attach a patch that fixes this by splitting th...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2629">MAPREDUCE-2629</a>.
     Minor improvement reported by ecaspole and fixed by ecaspole (task)<br>
     <b>Class loading quirk prevents inner class method compilation</b><br>
     <blockquote>While profiling jobs like terasort and gridmix, I noticed that a<br>method &quot;org.apache.hadoop.mapreduce.task.ReduceContextImpl.access<br>$000&quot; is near the top. It turns out that this is because the<br>ReduceContextImpl class has a member backupStore which is accessed<br>from an inner class ReduceContextImpl$ValueIterator. Due to the way<br>synthetic accessor methods work, every access of backupStore results<br>in a call to access$000 to the outer class. For some portion of the<br>run, backupStore is null and the ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2628">MAPREDUCE-2628</a>.
     Minor bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>MR-279: Add compiled on date to NM and RM info/about page</b><br>
     <blockquote>Compiled on dates were present on the JobTracker UI. Bring compiled on dates to resource manager and node<br>manager UI. <br><br>NM and RM retrieves build version for hadoop and yarn version via the getBuildVersion util api. This function used to<br>contain the compiled on date, but since has been removed since that function is used to determine hadoop compatible<br>versions, but was too restrictive with build date being present. Instead, a getDate call should be used to retrieve the<br>compiled on date.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2625">MAPREDUCE-2625</a>.
     Minor bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>MR-279: Add Node Manager Version to NM info page</b><br>
     <blockquote>Hadoop and YARN versions are missing from the NM info page</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2624">MAPREDUCE-2624</a>.
     Major improvement reported by szetszwo and fixed by szetszwo (contrib/raid)<br>
     <b>Update RAID for HDFS-2107</b><br>
     <blockquote>HDFS-2107 is going to move BlockPlacementPolicy to another package.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2623">MAPREDUCE-2623</a>.
     Minor improvement reported by jimplush and fixed by qwertymaniac (test)<br>
     <b>Update ClusterMapReduceTestCase to use MiniDFSCluster.Builder</b><br>
     <blockquote>Looking at test class ClusterMapReduceTestCase it issues a warning that the dfsCluster = new MiniDFSCluster(conf, 2, reformatDFS, null); line of code is deprecated and MiniDFSCluster.Builder should be used instead. It notes that the current API will be phased out in version 24. I propose to update the test class to the most up to date code as it&apos;s referenced several places on the internet as an example of how to write a Hadoop Unit Test.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2622">MAPREDUCE-2622</a>.
     Minor task reported by qwertymaniac and fixed by qwertymaniac (test)<br>
     <b>Remove the last remaining reference to &quot;io.sort.mb&quot;</b><br>
     <blockquote>TestLocalRunner still carries &quot;io.sort.mb&quot;, which must be updated to &quot;mapreduce.task.io.sort.mb&quot; (MRJobConfig.IO_SORT_MB).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2620">MAPREDUCE-2620</a>.
     Major bug reported by szetszwo and fixed by szetszwo (contrib/raid)<br>
     <b>Update RAID for HDFS-2087</b><br>
     <blockquote>DataTransferProtocol was changed by HDFS-2087.  Need to update RAID.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2618">MAPREDUCE-2618</a>.
     Major bug reported by naisbitt and fixed by naisbitt (mrv2)<br>
     <b>MR-279: 0 map, 0 reduce job fails with Null Pointer Exception</b><br>
     <blockquote>A 0 map, 0 reduce job fails with an NPE. This case works fine on hadoop-0.20.x. The job should succeed and run setup/cleanup code - with no tasks.  Below is the stacktrace:<br><br>11/06/05 19:35:37 WARN mapred.ClientServiceDelegate:<br> StackTrace: java.lang.NullPointerException<br>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTaskAttemptCompletionEvents(JobImpl.java:498)<br>        at<br>org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler.getTaskAttemptComplet...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2615">MAPREDUCE-2615</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: KillJob should go through AM whenever possible</b><br>
     <blockquote>KillJob currently goes directly to the RM - which effectively causes the AM and tasks to be killed via a signal. History information is not recorded in this case.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2611">MAPREDUCE-2611</a>.
     Major improvement reported by sseth and fixed by  (mrv2)<br>
     <b>MR 279: Metrics, finishTimes, etc in JobHistory</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2606">MAPREDUCE-2606</a>.
     Major bug reported by tucu00 and fixed by tucu00 <br>
     <b>Remove IsolationRunner</b><br>
     <blockquote>                                              IsolationRunner is no longer maintained. See &lt;a href=&quot;/jira/browse/MAPREDUCE-2637&quot; title=&quot;Providing options to debug the mapreduce user code (Mapper, Reducer, Combiner, Sort implementations)&quot;&gt;MAPREDUCE-2637&lt;/a&gt; for its replacement. <br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2603">MAPREDUCE-2603</a>.
     Major bug reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>Gridmix system tests are failing due to high ram emulation enable by default for normal mr jobs in the trace which exceeds the solt capacity.</b><br>
     <blockquote>In Gridmix high ram emulation enable by default.Because of this feature, some of the gridmix system tests are hanging for some time and then failing after timeout. Actually the failure case was occurring whenever reserved slot capacity exceeds the cluster slot capacity.So for fixing the issue by disabling the high ram emulation in the tests which are using the normal mr jobs in the traces.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2602">MAPREDUCE-2602</a>.
     Major improvement reported by ahmed.radwan and fixed by ahmed.radwan <br>
     <b>Allow setting of end-of-record delimiter for TextInputFormat (for the old API)</b><br>
     <blockquote>Since there are users who are still using the old MR API, it will be useful to modify the org.apache.hadoop.mapred.LineRecordReader and org.apache.hadoop.mapred.TextInputFormat to be able to use custom (user-specified) end-of-record delimiters. This will make use of the LineReader improvement introduced in HADOOP-7096 that enables the LineReader to break lines at user-specified delimiters. <br><br>Note: MAPREDUCE-2254 already added this improvement to the new API (but not the old API).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2598">MAPREDUCE-2598</a>.
     Minor bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: miscellaneous UI, NPE fixes for JobHistory, UI</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2596">MAPREDUCE-2596</a>.
     Major improvement reported by acmurthy and fixed by amar_kamat (benchmarks, contrib/gridmix)<br>
     <b>Gridmix should notify job failures</b><br>
     <blockquote>                    Gridmix now prints a summary information after every run. It summarizes the runs w.r.t input trace details, input data statistics, cli arguments, data-gen runtime, simulation runtimes etc and also the cluster w.r.t map slots, reduce slots, jobtracker-address, hdfs-address etc.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2595">MAPREDUCE-2595</a>.
     Minor bug reported by tgraves and fixed by tgraves <br>
     <b>MR279: update yarn INSTALL doc</b><br>
     <blockquote>yarn install doc needs to be updated after unsplit: http://svn.apache.org/repos/asf/hadoop/common/branches/MR-279/mapreduce/INSTALL</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2588">MAPREDUCE-2588</a>.
     Major bug reported by szetszwo and fixed by szetszwo (contrib/raid)<br>
     <b>Raid is not compile after DataTransferProtocol refactoring</b><br>
     <blockquote>Raid is directly using {{DataTransferProtocol}}.  It cannot be compiled after HDFS-2066.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2587">MAPREDUCE-2587</a>.
     Minor bug reported by tgraves and fixed by tgraves <br>
     <b>MR279: Fix RM version in the cluster-&gt;about page </b><br>
     <blockquote>The Resource Manager version in the Cluster-&gt;About page always shows 1.0-SNAPSHOT. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2582">MAPREDUCE-2582</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Cleanup JobHistory event generation</b><br>
     <blockquote>Generate JobHistoryEvents for the correct transitions. Fix missing / incorrect values being set.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2581">MAPREDUCE-2581</a>.
     Trivial bug reported by david_syer and fixed by tim_s <br>
     <b>Spelling errors in log messages (MapTask)</b><br>
     <blockquote>Spelling errors in log messages (MapTask) - e.g. search for &quot;recieve&quot; (should be &quot;receive&quot;).  A decent IDE should detect these errors as well.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2580">MAPREDUCE-2580</a>.
     Minor improvement reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: RM UI should redirect finished jobs to History UI</b><br>
     <blockquote>The RM UI currently has a link to the AM UI. After an application finishes (AM not available), the RM UI should link to the history UI.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2576">MAPREDUCE-2576</a>.
     Trivial bug reported by sherri_chen and fixed by tim_s <br>
     <b>Typo in comment in SimulatorLaunchTaskAction.java</b><br>
     <blockquote>This JIRA is to track a fix to a super-trivial issue of a typo of &quot;or&quot; misspelled as &quot;xor &quot; in Line 24 of SimulatorLaunchTaskAction.java</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2575">MAPREDUCE-2575</a>.
     Major bug reported by tgraves and fixed by tgraves (test)<br>
     <b>TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test</b><br>
     <blockquote>TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2573">MAPREDUCE-2573</a>.
     Major bug reported by tlipcon and fixed by revans2 <br>
     <b>New findbugs warning after MAPREDUCE-2494</b><br>
     <blockquote>MAPREDUCE-2494 introduced the following findbugs warning in trunk:<br>TrackerDistributedCacheManager.java:739, SIC_INNER_SHOULD_BE_STATIC, Priority: Low<br>Should org.apache.hadoop.mapreduce.filecache.TrackerDistributedCacheManager$CacheDir be a _static_ inner class?<br><br>This class is an inner class, but does not use its embedded reference to the object which created it.  This reference makes the instances of the class larger, and may keep the reference to the creator object alive longer than necessar...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2569">MAPREDUCE-2569</a>.
     Minor bug reported by jeagles and fixed by jeagles (mrv2)<br>
     <b>MR-279: Restarting resource manager with root capacity not equal to 100 percent should result in error</b><br>
     <blockquote>root.capacity is set to 90% without failure</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2566">MAPREDUCE-2566</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: YarnConfiguration should reloadConfiguration if instantiated with a non YarnConfiguration object</b><br>
     <blockquote>YarnConfiguration(conf) uses the ctor Configuration(conf) which is effectively a clone. If the configuration object is created before YarnConfiguration has been loaded - yarn-site.xml will not be available to the configuration.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2563">MAPREDUCE-2563</a>.
     Major task reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>Gridmix high ram jobs emulation system tests.</b><br>
     <blockquote>                                              Adds system tests to test the High-Ram feature in Gridmix.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2559">MAPREDUCE-2559</a>.
     Major bug reported by eyang and fixed by eyang (build)<br>
     <b>ant binary fails due to missing c++ lib dir</b><br>
     <blockquote>Post MAPRED-2521 ant binary fails without &quot;-Dcompile.c++=true -Dcompile.native=true&quot;. The bin-package is trying to copy from the c++ lib dir which doesn&apos;t exist yet. The binary target should check for the existence of this dir or would also be reasonable to depend on the compile-c++ (since this is the binary target).<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2556">MAPREDUCE-2556</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: NodeStatus.getNodeHealthStatus().setBlah broken</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2554">MAPREDUCE-2554</a>.
     Major task reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>Gridmix distributed cache emulation system tests.</b><br>
     <blockquote>                                              Adds distributed cache related system tests to Gridmix.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2552">MAPREDUCE-2552</a>.
     Minor bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: NPE when requesting attemptids for completed jobs </b><br>
     <blockquote>While constructing a CompletedJob instance on the JobHistory server - successfuleAttempt is not populated. Causes an NPE when listing completed attempts for a job via the CLI.<br><br>CLI: hadoop job -list-attempt-ids &lt;job_id&gt; MAP completed</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2551">MAPREDUCE-2551</a>.
     Major improvement reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Implement JobSummaryLog</b><br>
     <blockquote>Implement JobSummary log for MR.Next<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2550">MAPREDUCE-2550</a>.
     Blocker bug reported by eyang and fixed by eyang (build)<br>
     <b>bin/mapred no longer works from a source checkout</b><br>
     <blockquote>Developer may want to run hadoop without extracting tarball.  It would be nice if existing method to run mapred scripts from source code is preserved for developers.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2544">MAPREDUCE-2544</a>.
     Major task reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>Gridmix compression emulation system tests.</b><br>
     <blockquote>                                              Adds system tests for testing the compression emulation feature of Gridmix.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2543">MAPREDUCE-2543</a>.
     Major new feature reported by amar_kamat and fixed by amar_kamat (contrib/gridmix)<br>
     <b>[Gridmix] Add support for HighRam jobs</b><br>
     <blockquote>                                              Adds High-Ram feature emulation in Gridmix.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2541">MAPREDUCE-2541</a>.
     Critical bug reported by decster and fixed by decster (tasktracker)<br>
     <b>Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception</b><br>
     <blockquote>The race condition goes like this:<br>Thread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())<br>Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());<br>When SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).<br>When this value(totalMemoryUse...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2537">MAPREDUCE-2537</a>.
     Minor bug reported by revans2 and fixed by revans2 (mrv2)<br>
     <b>MR-279: The RM writes its log to yarn-mapred-resourcemanager-&lt;RM_Host&gt;.out</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2536">MAPREDUCE-2536</a>.
     Minor test reported by daryn and fixed by daryn (test)<br>
     <b>TestMRCLI broke due to change in usage output</b><br>
     <blockquote>One of the tests broke because it checks the FsShell mv usage line that is emitted after an error.  The usage was updated to from &quot;-mv &lt;src&gt; &lt;dst&gt;&quot; to &quot;-mv &lt;src&gt; ... &lt;dst&gt;&quot;, so the &quot;...&quot; broke the test.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2534">MAPREDUCE-2534</a>.
     Major bug reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Fix CI breaking hard coded version in jobclient pom</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2533">MAPREDUCE-2533</a>.
     Major new feature reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Metrics for reserved resource in ResourceManager</b><br>
     <blockquote>Add metrics for reserved resources.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2532">MAPREDUCE-2532</a>.
     Major new feature reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Metrics for NodeManager</b><br>
     <blockquote>Metrics for node manager. Requires a recent (last night) update of hadoop common in the yahoo-merge branch. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2531">MAPREDUCE-2531</a>.
     Blocker bug reported by revans2 and fixed by revans2 (client)<br>
     <b>org.apache.hadoop.mapred.jobcontrol.getAssignedJobID throw class cast exception </b><br>
     <blockquote>When using a combination of the mapred and mapreduce APIs (PIG) it is possible to have the following exception<br><br>Caused by: java.lang.ClassCastException: org.apache.hadoop.mapreduce.JobID cannot be cast to<br>org.apache.hadoop.mapred.JobID<br>        at org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID(Job.java:71)<br>        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:239)<br>        at org.apache.pig.PigServer.launchPlan(PigSe...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2529">MAPREDUCE-2529</a>.
     Major bug reported by tgraves and fixed by tgraves (tasktracker)<br>
     <b>Recognize Jetty bug 1342 and handle it</b><br>
     <blockquote>                    Added 2 new config parameters:<br/><br><br>mapreduce.reduce.shuffle.catch.exception.stack.regex<br/><br><br>mapreduce.reduce.shuffle.catch.exception.message.regex<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2527">MAPREDUCE-2527</a>.
     Major new feature reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Metrics for MRAppMaster</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2522">MAPREDUCE-2522</a>.
     Major sub-task reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Security for JobHistory service</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2521">MAPREDUCE-2521</a>.
     Major new feature reported by eyang and fixed by eyang (build)<br>
     <b>Mapreduce RPM integration project</b><br>
     <blockquote>                                              Created rpm and debian packages for MapReduce. <br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2518">MAPREDUCE-2518</a>.
     Major bug reported by weiyj and fixed by weiyj (distcp)<br>
     <b>missing t flag in distcp help message &apos;-p[rbugp]&apos;</b><br>
     <blockquote>&apos;t: modification and access times&apos; flag is defined but<br>missing in distcp help message &apos;-p[rbugp]&apos;. should be<br>changed to -p[rbugpt].<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2517">MAPREDUCE-2517</a>.
     Major task reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>Porting Gridmix v3 system tests into trunk branch.</b><br>
     <blockquote>                                              Adds system tests to Gridmix. These system tests cover various features like job types (load and sleep), user resolvers (round-robin, submitter-user, echo) and  submission modes (stress, replay and serial).<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2514">MAPREDUCE-2514</a>.
     Trivial bug reported by jeagles and fixed by jeagles (tasktracker)<br>
     <b>ReinitTrackerAction class name misspelled RenitTrackerAction in task tracker log</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2509">MAPREDUCE-2509</a>.
     Major bug reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Fix NPE in UI for pending attempts</b><br>
     <blockquote>The task attempts page gets a 500 (and NPE in the AM logs) if the attempt is pending (not running yet).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2504">MAPREDUCE-2504</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: race in JobHistoryEventHandler stop </b><br>
     <blockquote>The condition to stop the eventHandling thread currently requires it to be &apos;stopped&apos; AND interrupted. If an interrupt arrives after a take, but before handleEvent is called - the interrupt status ends up being handled by hadoop.util.Shell.runCommand() - which ignores it (and in the process resets the flag).<br>The eventHandling thread subsequently hangs on eventQueue.take()<br>This currently randomly fails unit tests - and can hang MR AMs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2501">MAPREDUCE-2501</a>.
     Major improvement reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Attach sources in builds</b><br>
     <blockquote>Attach sources to builds for various reasons, one of which is better debuggability on clusters.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2500">MAPREDUCE-2500</a>.
     Major bug reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: PB factories are not thread safe</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2497">MAPREDUCE-2497</a>.
     Trivial bug reported by rrh and fixed by eli <br>
     <b>missing spaces in error messages</b><br>
     <blockquote>Error message(s) are missing spaces.  Here&apos;s an example output:<br>  11/05/15 09:44:10 WARN mapred.JobClient: Error reading task outputhttp://<br>Generated from this line of source.<br><br>./src/mapred/org/apache/hadoop/mapred/JobClient.java:      LOG.warn(&quot;Error reading task output&quot; + ioe.getMessage()); <br><br>The 1st arg to LOG.warn should end with a &apos; &apos;.<br><br>There may be other instances of this problem in the source base.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2495">MAPREDUCE-2495</a>.
     Minor improvement reported by revans2 and fixed by revans2 (distributed-cache)<br>
     <b>The distributed cache cleanup thread has no monitoring to check to see if it has died for some reason</b><br>
     <blockquote>The cleanup thread in the distributed cache handles IOExceptions and the like correctly, but just to be a bit more defensive it would be good to monitor the thread, and check that it is still alive regularly, so that the distributed cache does not fill up the entire disk on the node. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2494">MAPREDUCE-2494</a>.
     Major improvement reported by revans2 and fixed by revans2 (distributed-cache)<br>
     <b>Make the distributed cache delete entires using LRU priority</b><br>
     <blockquote>                    Added config option mapreduce.tasktracker.cache.local.keep.pct to the TaskTracker.  It is the target percentage of the local distributed cache that should be kept in between garbage collection runs.  In practice it will delete unused distributed cache entries in LRU order until the size of the cache is less than mapreduce.tasktracker.cache.local.keep.pct of the maximum cache size.  This is a floating point value between 0.0 and 1.0.  The default is 0.95.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2492">MAPREDUCE-2492</a>.
     Major improvement reported by amar_kamat and fixed by amar_kamat (task)<br>
     <b>[MAPREDUCE] The new MapReduce API should make available task&apos;s progress to the task</b><br>
     <blockquote>                                              Map and Reduce task can access the attempt&amp;#39;s overall progress via TaskAttemptContext.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2490">MAPREDUCE-2490</a>.
     Trivial improvement reported by jeagles and fixed by jeagles (jobtracker)<br>
     <b>Log blacklist debug count</b><br>
     <blockquote>Gain some insight into blacklist increments/decrements by enhancing the debug logging</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2489">MAPREDUCE-2489</a>.
     Major bug reported by naisbitt and fixed by naisbitt (jobtracker)<br>
     <b>Jobsplits with random hostnames can make the queue unusable</b><br>
     <blockquote>We saw an issue where a custom InputSplit was returning invalid hostnames for the splits that were then causing the JobTracker to attempt to excessively resolve host names.  This caused a major slowdown for the JobTracker.  We should prevent invalid InputSplit hostnames from affecting everyone else.<br><br>I propose we implement some verification for the hostnames to try to ensure that we only do DNS lookups on valid hostnames (and fail otherwise).  We could also fail the job after a certain number...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2483">MAPREDUCE-2483</a>.
     Major bug reported by eyang and fixed by eyang (build)<br>
     <b>Clean up duplication of dependent jar files</b><br>
     <blockquote>                                              Removed duplicated hadoop-common library dependencies.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2480">MAPREDUCE-2480</a>.
     Major bug reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: mr app should not depend on hard-coded version of shuffle</b><br>
     <blockquote>The following commit introduced a dependency of shuffle with hard-coded version for mr app:<br>{noformat}<br>commit 6f69742140516be7493c9a9177b81d0516cc9539<br>Author: Vinod Kumar Vavilapalli &lt;vinodkv@apache.org&gt;<br>Date:   Wed May 4 06:53:52 2011 +0000<br><br>    Adding user log handling for YARN. Making NM put the user-logs on DFS and providing log-dump tools. Contributed by Vinod Kumar Vavilapalli.<br>{noformat}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2478">MAPREDUCE-2478</a>.
     Major improvement reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Improve history server</b><br>
     <blockquote>                                              Looks great. I just committed this. Thanks Siddharth!<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2475">MAPREDUCE-2475</a>.
     Major bug reported by sureshms and fixed by sureshms (test)<br>
     <b>Disable IPV6 for junit tests</b><br>
     <blockquote>IPV6 addresses not handles currently in the common library methods. IPV6 can return address as &quot;0:0:0:0:0:0:port&quot;. Some utility methods such as NetUtils#createSocketAddress(), NetUtils#normalizeHostName(), NetUtils#getHostNameOfIp() to name a few, do not handle IPV6 address and expect address to be of format host:port.<br><br>Until IPV6 is formally supported, I propose disabling IPV6 for junit tests to avoid problems seen in HDFS-1891.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2474">MAPREDUCE-2474</a>.
     Minor improvement reported by qwertymaniac and fixed by qwertymaniac (documentation)<br>
     <b>Add docs to the new API Partitioner on how to access Job Configuration data</b><br>
     <blockquote>                                              Improve the Partitioner interface&amp;#39;s docs to help fetch Job Configuration objects.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2473">MAPREDUCE-2473</a>.
     Major new feature reported by atm and fixed by atm (jobtracker)<br>
     <b>MR portion of HADOOP-7214 - Hadoop /usr/bin/groups equivalent</b><br>
     <blockquote>                                              Introduces a new command, &amp;quot;mapred groups&amp;quot;, which displays what groups are associated with a user as seen by the JobTracker.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2470">MAPREDUCE-2470</a>.
     Major bug reported by drizzt321 and fixed by revans2 (client)<br>
     <b>Receiving NPE occasionally on RunningJob.getCounters() call</b><br>
     <blockquote>This is running in a Java daemon that is used as an interface (Thrift) to get information and data from MR Jobs. Using JobClient.getJob(JobID) I successfully get a RunningJob object (I&apos;m checking for NULL), and then rarely I get an NPE when I do RunningJob.getCounters(). This seems to occur after the daemon has been up and running for a while, and in the event of an Exception, I close the JobClient, set it to NULL, and a new one should then be created on the next request for data. Yet, I stil...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2469">MAPREDUCE-2469</a>.
     Major improvement reported by amar_kamat and fixed by amar_kamat (task)<br>
     <b>Task counters should also report the total heap usage of the task</b><br>
     <blockquote>                                              Task attempt&amp;#39;s total heap usage gets recorded and published via counters as COMMITTED_HEAP_BYTES.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2467">MAPREDUCE-2467</a>.
     Major bug reported by sureshms and fixed by sureshms (contrib/raid)<br>
     <b>HDFS-1052 changes break the raid contrib module in MapReduce</b><br>
     <blockquote>Raid contrib module requires changes to work with the federation changes made in HDFS-1052.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2466">MAPREDUCE-2466</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon <br>
     <b>TestFileInputFormat.testLocality failing after federation merge</b><br>
     <blockquote>This test is failing, I believe due to federation merge. It&apos;s only finding one location for the test file instead of the expected two.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2463">MAPREDUCE-2463</a>.
     Major bug reported by devaraj.k and fixed by devaraj.k (jobtracker)<br>
     <b>Job History files are not moving to done folder when job history location is hdfs location</b><br>
     <blockquote>If &quot;mapreduce.jobtracker.jobhistory.location&quot; is configured as HDFS location then either during initialization of Job Tracker (while moving old job history files) or after completion of the job, history files are not moving to done and giving following exception.<br><br>{code:xml} <br>2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.<br>java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2462">MAPREDUCE-2462</a>.
     Minor improvement reported by sseth and fixed by sseth (mrv2)<br>
     <b>MR 279: Write job conf along with JobHistory, other minor improvements</b><br>
     <blockquote>Write the job xml along with the job history file. Split some common functionality into a helper class, etc.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2460">MAPREDUCE-2460</a>.
     Blocker bug reported by tlipcon and fixed by tlipcon <br>
     <b>TestFairSchedulerSystem failing on Hudson</b><br>
     <blockquote>Seems to have been failing for a while. For example: https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk/655/testReport/junit/org.apache.hadoop.mapred/TestFairSchedulerSystem/testFairSchedulerSystem/</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2459">MAPREDUCE-2459</a>.
     Major improvement reported by macyang and fixed by macyang (harchive)<br>
     <b>Cache HAR filesystem metadata</b><br>
     <blockquote>Each HAR file system has two index files that contains information on how files are stored in the part files. During the block location calculation, these indexes are reread for every file in the archive. Caching the indexes and the status of the part files will greatly reduce the number of name node operations during the job setup time.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2458">MAPREDUCE-2458</a>.
     Major bug reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Rename sanitized pom.xml in build directory to work around IDE bug</b><br>
     <blockquote>The sanitized pom.xml in target directory apparently triggered a bug in NetBeans (http://netbeans.org/bugzilla/show_bug.cgi?id=198162) causing it to fail to recognize the generated sources. The work-around is to rename the generated pom.xml to saner-pom.xml</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2456">MAPREDUCE-2456</a>.
     Trivial improvement reported by naisbitt and fixed by naisbitt (jobtracker)<br>
     <b>Show the reducer taskid and map/reduce tasktrackers for &quot;Failed fetch notification #_ for task attempt...&quot; log messages</b><br>
     <blockquote>This jira is to provide more useful log information for debugging the &quot;Too many fetch-failures&quot; error.<br><br>Looking at the JobTracker node, we see messages like this:<br>&quot;2010-12-14 00:00:06,911 INFO org.apache.hadoop.mapred.JobInProgress: Failed fetch notification #8 for task<br>attempt_201011300729_189729_m_007458_0&quot;.<br><br>I would be useful to see which reducer is reporting the error here.<br><br>So, I propose we add the following to these log messages:<br>  1. reduce task ID<br>  2. TaskTracker nodenames for both t...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2455">MAPREDUCE-2455</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite (build, client)<br>
     <b>Remove deprecated JobTracker.State in favour of JobTrackerStatus</b><br>
     <blockquote>MAPREDUCE-2337 deprecated getJobTrackerState() on ClusterStatus, this issue is to remove the getter (in favour of getJobTrackerStatus(), which will remain) so there is no longer a direct dependency of the public API on JobTracker. This is for MAPREDUCE-1638.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2452">MAPREDUCE-2452</a>.
     Major bug reported by devaraj and fixed by devaraj (jobtracker)<br>
     <b>Delegation token cancellation shouldn&apos;t hold global JobTracker lock</b><br>
     <blockquote>Currently, when the JobTracker cancels a job&apos;s delegation token (at the end of the job), it holds the global lock. This is not desired.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2451">MAPREDUCE-2451</a>.
     Trivial bug reported by tgraves and fixed by tgraves (jobtracker)<br>
     <b>Log the reason string of healthcheck script</b><br>
     <blockquote>The information on why a specific TaskTracker got blacklisted is not stored anywhere. The jobtracker web ui will show the detailed reason string until the TT gets unblacklisted.  After that it is lost.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2449">MAPREDUCE-2449</a>.
     Minor improvement reported by jzemerick and fixed by jzemerick (contrib/eclipse-plugin)<br>
     <b>Allow for command line arguments when performing &quot;Run on Hadoop&quot; action.</b><br>
     <blockquote>It is currently not possible to specify command line arguments when creating a run configuration for &quot;Run on Hadoop.&quot; This patch adds a text box to the RunOnHadoopWizard dialog for providing command line arguments. The arguments are then stored as part of the run configuration. Additionally (as a result), this patch prevents the creation of duplicate run configuration creation by seeing if the original configuration has been changed first.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2440">MAPREDUCE-2440</a>.
     Major bug reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Name clashes in TypeConverter</b><br>
     <blockquote>public static TaskTrackerInfo[] fromYarn(List&lt;NodeManagerInfo&gt; nodes) has the same erasure as<br>public static JobStatus[] fromYarn(List&lt;Application&gt; applications)<br><br>Not detected by the current JDK 6 but still wrong according to the JLS 8.4.2.<br><br>See also: http://bugs.sun.com/view_bug.do?bug_id=6182950<br><br>The patch renames the former signature to fromYarnNodes and the later fromYarnApps.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2439">MAPREDUCE-2439</a>.
     Major bug reported by mahadev and fixed by sseth (mrv2)<br>
     <b>MR-279: Fix YarnRemoteException to give more details.</b><br>
     <blockquote>Fix YarnRemoteException to add more details.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2438">MAPREDUCE-2438</a>.
     Major new feature reported by mahadev and fixed by ramach (mrv2)<br>
     <b>MR-279: WebApp for Job History</b><br>
     <blockquote>Add webapp for job history server in MR-279 branch.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2434">MAPREDUCE-2434</a>.
     Major new feature reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: ResourceManager metrics</b><br>
     <blockquote>                                              I just committed this. Thanks Luke!<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2433">MAPREDUCE-2433</a>.
     Blocker bug reported by vicaya and fixed by mahadev (mrv2)<br>
     <b>MR-279: YARNApplicationConstants hard code app master jar version</b><br>
     <blockquote>YARNApplicationConstants hard code version string in HADOOP_MAPREDUCE_CLIENT_APP_JAR_NAME and consequently YARN_MAPREDUCE_APP_JAR_PATH<br><br>This is a blocker.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2432">MAPREDUCE-2432</a>.
     Major improvement reported by vicaya and fixed by vicaya (mrv2)<br>
     <b>MR-279: Install sanitized poms for downstream sanity</b><br>
     <blockquote>Due to [MNG-4223|http://jira.codehaus.org/browse/MNG-4223], the installed POMs of MR-279 is downstream hostile. E.g., it&apos;s impossible to use versions of hadoop-mapreduce-client-core.version in ivy other than 1.0-SNAPSHOT without changing the multiple POMs, rendering the version properties (hadoop-mapreduce.version and yarn.version) practically useless.<br><br>This patch will install POMs with version (only) properties expanded. This patch also use inheritance and dependencyManagement to make POMs D...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2430">MAPREDUCE-2430</a>.
     Major task reported by nidaley and fixed by nidaley <br>
     <b>Remove mrunit contrib</b><br>
     <blockquote>                                              MRUnit is now available as a separate Apache project.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2429">MAPREDUCE-2429</a>.
     Major bug reported by acmurthy and fixed by sseth (tasktracker)<br>
     <b>Check jvmid during task status report</b><br>
     <blockquote>Currently TT doens&apos;t check to ensure jvmid is relevant during communication with the Child via TaskUmbilicalProtocol.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2428">MAPREDUCE-2428</a>.
     Blocker bug reported by tomwhite and fixed by tomwhite <br>
     <b>start-mapred.sh script fails if HADOOP_HOME is not set</b><br>
     <blockquote>MapReduce portion of HADOOP-6953</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2426">MAPREDUCE-2426</a>.
     Trivial test reported by tlipcon and fixed by tlipcon (contrib/fair-share)<br>
     <b>Make TestFairSchedulerSystem fail with more verbose output</b><br>
     <blockquote>The TestFairSchedulerSystem test failed here: https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk/644/testReport/junit/org.apache.hadoop.mapred/TestFairSchedulerSystem/testFairSchedulerSystem/<br><br>with a failed assertion {{assertTrue(contents.contains(&quot;&lt;/svg&gt;&quot;));}}. We should make the assertion failure include the value of {{contents}}<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2424">MAPREDUCE-2424</a>.
     Major improvement reported by roelofs and fixed by roelofs (mrv2)<br>
     <b>MR-279: counters/UI/etc. for uber-AppMaster (in-cluster LocalJobRunner for MRv2)</b><br>
     <blockquote>Polish uber-AM (MAPREDUCE-2405).  Specifically:<br>* uber-specific counters (&quot;command-line UI&quot;)<br>* GUI indicators<br>** RM all-containers level<br>** multi-job app level [if exists]<br>** single-job level<br>* fix uber-decision (&quot;is this a small job?&quot;):<br>** memory criterion<br>** input-bytes criterion<br>* disable speculation<br>* isUber() method (somewhere) for unit tests to use<br>* delete (most of) old UberTask code (MAPREDUCE-1220; came in with initial MR-279 branch)<br>* implement non-RPC, local version of umbilical<br>* ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2422">MAPREDUCE-2422</a>.
     Major sub-task reported by tomwhite and fixed by tomwhite (client)<br>
     <b>Removed unused internal methods from DistributedCache</b><br>
     <blockquote>DistributedCache has a number of deprecated methods that are no longer used ever since TrackerDistributedCacheManager was introduced in MAPREDUCE-476. Removing these methods (which are not user-facing) will make it possible to complete MAPREDUCE-1638 by keeping DistributedCache in the API tree, and TrackerDistributedCacheManager, TaskDistributedCacheManager in the implementation tree.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2420">MAPREDUCE-2420</a>.
     Major bug reported by boryas and fixed by boryas <br>
     <b>JobTracker should be able to renew delegation token over HTTP</b><br>
     <blockquote>in case JobTracker has to talk to a NameNode running a different version (RPC version mismatch), Jobtracker should be able to fall back to HTTP renewal.<br><br>Example of the case - running distcp between different versions using hfpt.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2417">MAPREDUCE-2417</a>.
     Major bug reported by ravidotg and fixed by ravidotg (contrib/gridmix)<br>
     <b>In Gridmix, in RoundRobinUserResolver mode, the testing/proxy users are not associated with unique users in a trace</b><br>
     <blockquote>                                              Fixes Gridmix in RoundRobinUserResolver mode to map testing/proxy users to unique users in a trace.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2416">MAPREDUCE-2416</a>.
     Major bug reported by ravidotg and fixed by ravidotg (contrib/gridmix)<br>
     <b>In Gridmix, in RoundRobinUserResolver, the list of groups for a user obtained from users-list-file is incorrect</b><br>
     <blockquote>                                              Removes the restriction of specifying group names in users-list file for Gridmix in RoundRobinUserResolver mode.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2414">MAPREDUCE-2414</a>.
     Major improvement reported by acmurthy and fixed by sseth (mrv2)<br>
     <b>MR-279: Use generic interfaces for protocols</b><br>
     <blockquote>Use generic interfaces for protocols for MAPREDUCE-279.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2409">MAPREDUCE-2409</a>.
     Major bug reported by sseth and fixed by sseth (distributed-cache)<br>
     <b>Distributed Cache does not differentiate between file /archive for files with the same path</b><br>
     <blockquote>If a &apos;global&apos; file is specified as a &apos;file&apos; by one job - subsequent jobs cannot override this source file to be an &apos;archive&apos; (until the TT cleans up it&apos;s cache or a TT restart).<br>The other way around as well -&gt; &apos;archive&apos; to &apos;file&apos;<br><br>In case of an accidental submission using the wrong type - some of the tasks for the second job will end up seeing the source file as an archive, others as a file.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2408">MAPREDUCE-2408</a>.
     Major new feature reported by ravidotg and fixed by amar_kamat (contrib/gridmix)<br>
     <b>Make Gridmix emulate usage of data compression</b><br>
     <blockquote>                    Emulates the MapReduce compression feature in Gridmix. By default, compression emulation is turned on. Compression emulation can be disabled by setting &amp;#39;gridmix.compression-emulation.enable&amp;#39; to &amp;#39;false&amp;#39;.  Use &amp;#39;gridmix.compression-emulation.map-input.decompression-ratio&amp;#39;, &amp;#39;gridmix.compression-emulation.map-output.compression-ratio&amp;#39; and &amp;#39;gridmix.compression-emulation.reduce-output.compression-ratio&amp;#39; to configure the compression ratios at map input, map output and reduce output side respectively. Currently, compression ratios in the range [0.07, 0.68] are supported. Gridmix auto detects whether map-input, map output and reduce output should emulate compression based on original job&amp;#39;s compression related configuration parameters.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2407">MAPREDUCE-2407</a>.
     Major new feature reported by ravidotg and fixed by ravidotg (contrib/gridmix)<br>
     <b>Make Gridmix emulate usage of Distributed Cache files</b><br>
     <blockquote>                                              Makes Gridmix emulate HDFS based distributed cache files and local file system based distributed cache files.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2405">MAPREDUCE-2405</a>.
     Major improvement reported by mahadev and fixed by roelofs (mrv2)<br>
     <b>MR-279: Implement uber-AppMaster (in-cluster LocalJobRunner for MRv2)</b><br>
     <blockquote>                                              An efficient implementation of small jobs by running all tasks in the MR ApplicationMaster JVM, there-by affecting lower latency.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2403">MAPREDUCE-2403</a>.
     Major improvement reported by mahadev and fixed by ramach (mrv2)<br>
     <b>MR-279: Improve job history event handling in AM to log to HDFS</b><br>
     <blockquote>Improve the job history event handling in the application master to log to HDFS in the staging directory for the job and also move it to the required location for the job history server to use.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2395">MAPREDUCE-2395</a>.
     Critical bug reported by tlipcon and fixed by rvadali (contrib/raid)<br>
     <b>TestBlockFixer timing out on trunk</b><br>
     <blockquote>In recent Hudson builds, TestBlockFixer has been timing out. Not clear how long it has been broken since MAPREDUCE-2394 was hiding the RAID tests from Hudson&apos;s test result parsing.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2381">MAPREDUCE-2381</a>.
     Major improvement reported by philip and fixed by philip <br>
     <b>JobTracker instrumentation not consistent about error handling</b><br>
     <blockquote>In the current code, if the class specified by the JobTracker instrumentation config property is not there, the JobTracker fails to start with a ClassNotFound.  If it&apos;s there, but it can&apos;t load for whatever reason, the JobTracker continues with the default.  Having two different error-handling routes is a bit confusing; I propose to move one line so that it&apos;s consistent.  (On the TaskTracker instrumentation side, if any of the multiple instrumentations aren&apos;t available, the default is used.)<br>...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2379">MAPREDUCE-2379</a>.
     Major bug reported by tlipcon and fixed by tlipcon (distributed-cache, documentation)<br>
     <b>Distributed cache sizing configurations are missing from mapred-default.xml</b><br>
     <blockquote>* MAPREDUCE-1538 added {{mapreduce.tasktracker.cache.local.numberdirectories}} which is not documented in mapred-default.xml<br>* When MAPREDUCE-711 moved DistributedCache into the mapred project, the {{local.cache.size}} parameter was left in core-default.xml instead of moved to mapred-default.xml. It has since been renamed to {{mapreduce.tasktracker.cache.local.size}}</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2367">MAPREDUCE-2367</a>.
     Minor improvement reported by tlipcon and fixed by tlipcon <br>
     <b>Allow using a file to exclude certain tests from build</b><br>
     <blockquote>It would be nice to be able to exclude certain tests when running builds. For example, when a test is &quot;known flaky&quot;, you may want to exclude it from the main Hudson job, but not actually disable it in the codebase (so that it still runs as part of another Hudson job, for example).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2365">MAPREDUCE-2365</a>.
     Major bug reported by owen.omalley and fixed by sseth <br>
     <b>Add counters for FileInputFormat (BYTES_READ) and FileOutputFormat (BYTES_WRITTEN)</b><br>
     <blockquote>MAP_INPUT_BYTES and MAP_OUTPUT_BYTES will be computed using the difference between FileSystem<br>counters before and after each next(K,V) and collect/write op.<br><br>In case compression is being used, these counters will represent the compressed data sizes. The uncompressed size will<br>not be available.<br><br>This is not a direct back-port of 5710. (Counters will be computed in MapTask instead of in individual RecordReaders).<br><br>0.20.100 -&gt;<br>   New API -&gt; MAP_INPUT_BYTES will be computed using this method<br>   O...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2351">MAPREDUCE-2351</a>.
     Major improvement reported by tomwhite and fixed by tomwhite <br>
     <b>mapred.job.tracker.history.completed.location should support an arbitrary filesystem URI</b><br>
     <blockquote>Currently, mapred.job.tracker.history.completed.location is resolved relative to the default filesystem. If not set it defaults to history/done in the local log directory. There is no way to set it to another local filesystem location (with a file:// URI) or an arbitrary Hadoop filesystem.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2331">MAPREDUCE-2331</a>.
     Major test reported by tlipcon and fixed by tlipcon <br>
     <b>Add coverage of task graph servlet to fair scheduler system test</b><br>
     <blockquote>Would be useful to hit the TaskGraph servlet in the fair scheduler system test. This way, when run under JCarder, it will check for any lock inversions in this code.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2326">MAPREDUCE-2326</a>.
     Major improvement reported by acmurthy and fixed by  <br>
     <b>Port gridmix changes from hadoop-0.20.100 to trunk</b><br>
     <blockquote>We have some changes to gridmix in hadoop-0.20.100. Uber jira to track merges to trunk.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2323">MAPREDUCE-2323</a>.
     Major new feature reported by tlipcon and fixed by tlipcon (contrib/fair-share)<br>
     <b>Add metrics to the fair scheduler</b><br>
     <blockquote>It would be useful to be able to monitor various metrics in the fair scheduler, like demand, fair share, min share, and running task count.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2317">MAPREDUCE-2317</a>.
     Minor bug reported by devaraj.k and fixed by devaraj.k (harchive)<br>
     <b>HadoopArchives throwing NullPointerException while creating hadoop archives (.har files)</b><br>
     <blockquote>While we are trying to run hadoop archive tool in widows using this way, it is giving the below exception.<br><br>java org.apache.hadoop.tools.HadoopArchives -archiveName temp.har D:/test/in E:/temp<br><br>{code:xml} <br><br>java.lang.NullPointerException<br>	at org.apache.hadoop.tools.HadoopArchives.writeTopLevelDirs(HadoopArchives.java:320)<br>	at org.apache.hadoop.tools.HadoopArchives.archive(HadoopArchives.java:386)<br>	at org.apache.hadoop.tools.HadoopArchives.run(HadoopArchives.java:725)<br>	at org.apache.hadoop.uti...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2311">MAPREDUCE-2311</a>.
     Blocker bug reported by tlipcon and fixed by schen (contrib/fair-share)<br>
     <b>TestFairScheduler failing on trunk</b><br>
     <blockquote>Most of the test cases in this test are failing on trunk, unclear how long since the contrib tests weren&apos;t running while the core tests were failed.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2307">MAPREDUCE-2307</a>.
     Minor bug reported by devaraj.k and fixed by devaraj.k (contrib/fair-share)<br>
     <b>Exception thrown in Jobtracker logs, when the Scheduler configured is FairScheduler.</b><br>
     <blockquote>If we try to start the job tracker with fair scheduler using the default configuration, It is giving the below exception.<br><br><br>{code:xml} <br>2010-07-03 10:18:27,142 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9001: starting<br>2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9001: starting<br>2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9001: starting<br>2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Serv...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2302">MAPREDUCE-2302</a>.
     Major improvement reported by schen and fixed by schen (contrib/raid)<br>
     <b>Add static factory methods in GaloisField</b><br>
     <blockquote>GaloisField is immutable and should be kept reuse after creation to avoid redundant calculation of the multiplication and division tables.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2290">MAPREDUCE-2290</a>.
     Major bug reported by eli and fixed by eli (test)<br>
     <b>TestTaskCommit missing getProtocolSignature override</b><br>
     <blockquote>Fixes an MR compilation error, HADOOP-6904 added a new implementation of getProtocolSignature but TestTaskCommit doesn&apos;t override it.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2271">MAPREDUCE-2271</a>.
     Blocker bug reported by tlipcon and fixed by liangly (jobtracker)<br>
     <b>TestSetupTaskScheduling failing in trunk</b><br>
     <blockquote>This test case is failing in trunk after the commit of MAPREDUCE-2207</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2263">MAPREDUCE-2263</a>.
     Major improvement reported by hairong and fixed by hairong <br>
     <b>MapReduce side of HADOOP-6904</b><br>
     <blockquote>Make changes in Map/Reduce to incorporate HADOOP-6904.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2260">MAPREDUCE-2260</a>.
     Major improvement reported by rvs and fixed by rvs (build)<br>
     <b>Remove auto-generated native build files</b><br>
     <blockquote>                                              The native build run when from trunk now requires autotools, libtool and openssl dev libraries.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2258">MAPREDUCE-2258</a>.
     Major bug reported by tlipcon and fixed by tlipcon (task)<br>
     <b>IFile reader closes stream and compressor in wrong order</b><br>
     <blockquote>In IFile.Reader.close(), we return the decompressor to the pool and then call close() on the input stream. This is backwards and causes a rare race in the case of LzopCodec, since LzopInputStream makes a few calls on the decompressor object inside close(). If another thread pulls the decompressor out of the pool and starts to use it in the meantime, the first thread&apos;s close() will cause the second thread to potentially miss pieces of data.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2254">MAPREDUCE-2254</a>.
     Major improvement reported by ahmed.radwan and fixed by ahmed.radwan <br>
     <b>Allow setting of end-of-record delimiter for TextInputFormat</b><br>
     <blockquote>                                              TextInputFormat may now split lines with delimiters other than newline, by specifying a configuration parameter &amp;quot;textinputformat.record.delimiter&amp;quot;<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2250">MAPREDUCE-2250</a>.
     Trivial improvement reported by rvadali and fixed by rvadali (contrib/raid)<br>
     <b>Fix logging in raid code.</b><br>
     <blockquote>There are quite a few error messages being logged with a log level of info. That should be fixed to help debugging.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2249">MAPREDUCE-2249</a>.
     Major improvement reported by kam_iitkgp and fixed by devaraj.k <br>
     <b>Better to check the reflexive property of the object while overriding equals method of it</b><br>
     <blockquote>It is better to check the reflexive property of the object while overriding equals method of it.<br> <br>It improves the performance when a heavy object is compared to itself.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2248">MAPREDUCE-2248</a>.
     Major improvement reported by rvadali and fixed by rvadali <br>
     <b>DistributedRaidFileSystem should unraid only the corrupt block</b><br>
     <blockquote>DistributedRaidFileSystem unraids the entire file if it hits a corrupt block. It is better to unraid just the corrupt block and use the rest of the file as normal. This becomes really important when we have tera-byte sized files.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2243">MAPREDUCE-2243</a>.
     Minor improvement reported by kam_iitkgp and fixed by devaraj.k (jobtracker, tasktracker)<br>
     <b>Close all the file streams propely in a finally block to avoid their leakage.</b><br>
     <blockquote>In the following classes streams should be closed in finally block to avoid their leakage in the exceptional cases.<br><br>CompletedJobStatusStore.java<br>------------------------------------------<br>       dataOut.writeInt(events.length);<br>        for (TaskCompletionEvent event : events) {<br>          event.write(dataOut);<br>        }<br>       dataOut.close() ;<br><br>EventWriter.java<br>----------------------<br>   encoder.flush();<br>   out.close();<br><br>MapTask.java<br>-------------------<br>    splitMetaInfo.write(out);<br>     out....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2239">MAPREDUCE-2239</a>.
     Major improvement reported by schen and fixed by schen (contrib/raid)<br>
     <b>BlockPlacementPolicyRaid should call getBlockLocations only when necessary</b><br>
     <blockquote>Currently BlockPlacementPolicyRaid calls getBlockLocations for every chooseTarget().<br>This puts pressure on NameNode. We should avoid calling if this file is not raided or a parity file.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2225">MAPREDUCE-2225</a>.
     Blocker improvement reported by qwertymaniac and fixed by qwertymaniac (job submission)<br>
     <b>MultipleOutputs should not require the use of &apos;Writable&apos;</b><br>
     <blockquote>                                              MultipleOutputs should not require the use/check of &amp;#39;Writable&amp;#39; interfaces in key and value classes.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2215">MAPREDUCE-2215</a>.
     Major bug reported by pkling and fixed by pkling (contrib/raid)<br>
     <b>A more elegant FileSystem#listCorruptFileBlocks API (RAID changes)</b><br>
     <blockquote>Map/reduce changes related to HADOOP-7060 and HDFS-1533.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2207">MAPREDUCE-2207</a>.
     Major improvement reported by schen and fixed by liangly (jobtracker)<br>
     <b>Task-cleanup task should not be scheduled on the node that the task just failed</b><br>
     <blockquote>                                              Task-cleanup task should not be scheduled on the node that the task just failed<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2206">MAPREDUCE-2206</a>.
     Major improvement reported by schen and fixed by schen (jobtracker)<br>
     <b>The task-cleanup tasks should be optional</b><br>
     <blockquote>For job does not use OutputCommitter.abort(), this should be able to turn off.<br>This improves the latency of the job because failed tasks are often the bottleneck of the jobs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2203">MAPREDUCE-2203</a>.
     Trivial improvement reported by yaojingguo and fixed by yaojingguo <br>
     <b>Wong javadoc for TaskRunner&apos;s appendJobJarClasspaths method</b><br>
     <blockquote>&quot;{@link Configuration.getJar()})&quot; should be &quot;{@link JobConf.getJar()})&quot;</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2202">MAPREDUCE-2202</a>.
     Major improvement reported by cos and fixed by cos <br>
     <b>Generalize CLITest structure and interfaces to facilitate upstream adoption (e.g. for web or system testing)</b><br>
     <blockquote>Counterpart of HADOOP-7014 and HDFS-1486</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2199">MAPREDUCE-2199</a>.
     Major bug reported by cos and fixed by cos (build)<br>
     <b>build is broken 0.22 branch creation</b><br>
     <blockquote>hdfs and common dep versions weren&apos;t updated properly.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2185">MAPREDUCE-2185</a>.
     Major bug reported by hairong and fixed by rvadali (job submission)<br>
     <b>Infinite loop at creating splits using CombineFileInputFormat</b><br>
     <blockquote>This is caused by a missing block in HDFS. So the block&apos;s locations are empty. The following code adds the block to blockToNodes map but not to rackToBlocks map. Later on when generating splits, only blocks in rackToBlocks are removed from blockToNodes map. So blockToNodes map can never become empty therefore causing infinite loop<br><br>{code}<br>          // add this block to the block --&gt; node locations map<br>          blockToNodes.put(oneblock, oneblock.hosts);<br><br>          // add this block to the ra...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2172">MAPREDUCE-2172</a>.
     Major bug reported by pkling and fixed by nidaley <br>
     <b>test-patch.properties contains incorrect/version-dependent values of OK_FINDBUGS_WARNINGS and OK_RELEASEAUDIT_WARNINGS</b><br>
     <blockquote>Running ant test-patch with an empty patch yields 25 findbugs warning and 3 release audit warnings (rather than the 0 findbugs warnings and 1 release audit warning specified in test-patch.properties):<br><br>{code}<br>[exec] -1 overall.  <br>[exec] <br>[exec]     +1 @author.  The patch does not contain any @author tags.<br>[exec] <br>[exec]     -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.<br>[exec]                         Please justify why no new tests are needed for this patch...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2156">MAPREDUCE-2156</a>.
     Major improvement reported by pkling and fixed by pkling (contrib/raid)<br>
     <b>Raid-aware FSCK</b><br>
     <blockquote>Currently, FSCK reports files as corrupt even if they can be fixed using parity blocks. We need a tool that only reports files that are irreparably corrupt (i.e., files for which too many data or parity blocks belonging to the same stripe have been lost or corrupted).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2155">MAPREDUCE-2155</a>.
     Major improvement reported by pkling and fixed by pkling (contrib/raid)<br>
     <b>RaidNode should optionally dispatch map reduce jobs to fix corrupt blocks (instead of fixing locally)</b><br>
     <blockquote>Recomputing blocks based on parity information is expensive. Rather than doing this locally at the RaidNode, we should run map reduce jobs. This will allow us to quickly fix a large number of corrupt or missing blocks.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2153">MAPREDUCE-2153</a>.
     Major improvement reported by ravidotg and fixed by rajesh.balamohan (tools/rumen)<br>
     <b>Bring in more job configuration properties in to the trace file</b><br>
     <blockquote>                                              Adds job configuration parameters to the job trace. The configuration parameters are stored under the &amp;#39;jobProperties&amp;#39; field as key-value pairs.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2137">MAPREDUCE-2137</a>.
     Major bug reported by ravidotg and fixed by ravidotg (contrib/gridmix)<br>
     <b>Mapping between Gridmix jobs and the corresponding original MR jobs is needed</b><br>
     <blockquote>                                              New configuration properties gridmix.job.original-job-id and gridmix.job.original-job-name in the configuration of simulated job are exposed/documented to gridmix user for mapping between original cluster&amp;#39;s jobs and simulated jobs.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2127">MAPREDUCE-2127</a>.
     Major bug reported by gkesavan and fixed by bmahe (build, pipes)<br>
     <b>mapreduce trunk builds are failing on hudson</b><br>
     <blockquote>https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/507/console<br><br>[exec] checking for pthread.h... yes<br>     [exec] checking for pthread_create in -lpthread... yes<br>     [exec] checking for HMAC_Init in -lssl... no<br>     [exec] configure: error: Cannot find libssl.so<br>     [exec] /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/src/c++/pipes/configure: line 4250: exit: please: numeric argument required<br>     [exec] /grid/0/hudson/hudson-slave/workspace/Hadoop...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2107">MAPREDUCE-2107</a>.
     Major improvement reported by ranjit and fixed by amar_kamat (contrib/gridmix)<br>
     <b>Emulate Memory Usage of Tasks in GridMix3</b><br>
     <blockquote>                    Adds total heap usage emulation to Gridmix. Also, Gridmix can configure the simulated task&amp;#39;s JVM heap options with max heap options obtained from the original task (via Rumen). Use &amp;#39;gridmix.task.jvm-options.enable&amp;#39; to disable the task max heap options configuration. <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2106">MAPREDUCE-2106</a>.
     Major improvement reported by ranjit and fixed by amar_kamat (contrib/gridmix)<br>
     <b>Emulate CPU Usage of Tasks in GridMix3</b><br>
     <blockquote>                                              Adds cumulative cpu usage emulation to Gridmix<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2105">MAPREDUCE-2105</a>.
     Major improvement reported by ranjit and fixed by amar_kamat (contrib/gridmix)<br>
     <b>Simulate Load Incrementally and Adaptively in GridMix3</b><br>
     <blockquote>Tasks launched by GridMix3 should incrementally and adaptively simulate load (I/O, CPU, memory, etc.) rather than doing<br>everything upfront and then sleeping. This helps in evening out the load when fine-grained information from the original<br>Task is not available and greater accuracy when it is.<br><br>By &quot;incremental&quot; I mean having several iterations corresponding to appropriate phases/time-slices. By &quot;adaptive&quot; I mean<br>taking the existing load into account before inflicting additional load to meet ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2104">MAPREDUCE-2104</a>.
     Major bug reported by ranjit and fixed by amar_kamat (tools/rumen)<br>
     <b>Rumen TraceBuilder Does Not Emit CPU/Memory Usage Details in Traces</b><br>
     <blockquote>                                              Adds cpu, physical memory, virtual memory and heap usages to TraceBuilder&amp;#39;s output.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2081">MAPREDUCE-2081</a>.
     Major test reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>[GridMix3] Implement functionality for get the list of job traces which has different intervals.</b><br>
     <blockquote>Girdmix system tests should require different job traces with different time intervals for generate and submit the gridmix jobs. So, implement a functionaliy for getting the job traces and arrange them in hash table with time interval as key.Also getting the list of traces from resource location irrespective of time. The following methods needs to implement.<br><br>Method signature:<br>public static Map &lt;String, String&gt; getMRTraces(Configuration conf)  throws IOException; - it get the traces with time...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2074">MAPREDUCE-2074</a>.
     Minor bug reported by knoguchi and fixed by priyomustafi (distributed-cache)<br>
     <b>Task should fail when symlink creation fail</b><br>
     <blockquote>If I pass an invalid symlink as   -Dmapred.cache.files=/user/knoguchi/onerecord.txt#abc/abc<br><br>Task only reports a WARN and goes on.<br><br>{noformat} <br>2010-09-16 21:38:49,782 INFO org.apache.hadoop.mapred.TaskRunner: Creating symlink: /0/tmp/mapred-local/taskTracker/knoguchi/distcache/-5031501808205559510_-128488332_1354038698/abc-nn1.def.com/user/knoguchi/onerecord.txt &lt;- /0/tmp/mapred-local/taskTracker/knoguchi/jobcache/job_201008310107_15105/attempt_201008310107_15105_m_000000_0/work/./abc/abc<br>20...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2053">MAPREDUCE-2053</a>.
     Major task reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>[Herriot] Test Gridmix file pool for different input file sizes based on pool minimum size.</b><br>
     <blockquote>Scenario:<br>1. Generate 1.8G data with Gridmix data generator, such that the files can create under different folders inside the given input directory and also create the files directly in the given input directory with the following sizes {50 MB,100 MB,400 MB, 50 MB,300 MB,10 MB ,60 MB,40 MB,20 MB,10 MB,500 MB}.<br>2.Set the FilePool minimum size is 100 MB.<br>3. Verify the files count and sizes after excluding the files that are less than file pool minimum size.Also make sure, whether files are col...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2037">MAPREDUCE-2037</a>.
     Major new feature reported by dking and fixed by dking <br>
     <b>Capturing interim progress times, CPU usage, and memory usage, when tasks reach certain progress thresholds</b><br>
     <blockquote>                    Capture intermediate task resource consumption information:<br/><br><br>* Time taken so far<br/><br><br>* CPU load [either at the time the data are taken, or exponentially smoothed]<br/><br><br>* Memory load [also either at the time the data are taken, or exponentially smoothed]<br/><br><br><br/><br><br>This would be taken at intervals that depend on the task progress plateaus. For example, reducers have three progress ranges - [0-1/3], (1/3-2/3], and (2/3-3/3] - where fundamentally different activities happen. Mappers have different boundaries that are not symmetrically placed [0-9/10], (9/10-1]. Data capture boundaries should coincide with activity boundaries. For the state information capture [CPU and memory] we should average over the covered interval.<br/><br><br><br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2033">MAPREDUCE-2033</a>.
     Major task reported by vinaythota and fixed by vinaythota (contrib/gridmix)<br>
     <b>[Herriot] Gridmix generate data tests with various submission policies and different user resolvers.</b><br>
     <blockquote>Tests for submitting and verifying the gridmix generate input data in different submission policies and various user resolver modes. It covers the following scenarios.<br><br>1. Generate the data in a STRESS submission policy with SubmitterUserResolver mode and verify whether the generated data matches with given size of input or not.<br>2. Generate the data in a REPLAY submission policy with RoundRobinUserResolver mode and verify whether the generated data matches with the given input size or not.<br>3....</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-2026">MAPREDUCE-2026</a>.
     Major improvement reported by schen and fixed by jsensarma <br>
     <b>JobTracker.getJobCounters() should not hold JobTracker lock while calling JobInProgress.getCounters()</b><br>
     <blockquote>JobTracker.getJobCounter() will lock JobTracker and call JobInProgress.getCounters().<br>JobInProgress.getCounters() can be very expensive because it aggregates all the task counters.<br>We found that from the JobTracker jstacks that this method is one of the bottleneck of the JobTracker performance.<br><br>JobInProgress.getCounters() should be able to be called out side the JobTracker lock because it already has JobInProgress lock.<br>For example, it is used by jobdetails.jsp without a JobTracker lock.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1996">MAPREDUCE-1996</a>.
     Trivial bug reported by glynnbach and fixed by qwertymaniac (documentation)<br>
     <b>API: Reducer.reduce() method detail misstatement</b><br>
     <blockquote>                                              Fix a misleading documentation note about the usage of Reporter objects in Reducers.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1978">MAPREDUCE-1978</a>.
     Major improvement reported by amar_kamat and fixed by ravidotg (tools/rumen)<br>
     <b>[Rumen] TraceBuilder should provide recursive input folder scanning</b><br>
     <blockquote>                                              Adds -recursive option to TraceBuilder for scanning the input directories recursively.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1938">MAPREDUCE-1938</a>.
     Blocker new feature reported by devaraj and fixed by ramach (job submission, task, tasktracker)<br>
     <b>Ability for having user&apos;s classes take precedence over the system classes for tasks&apos; classpath</b><br>
     <blockquote>It would be nice to have the ability in MapReduce to allow users to specify for their jobs alternate implementations of classes that are already defined in the MapReduce libraries. For example, an alternate implementation for CombineFileInputFormat. </blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1927">MAPREDUCE-1927</a>.
     Minor test reported by roelofs and fixed by roelofs (test)<br>
     <b>unit test for HADOOP-6835 (concatenated gzip support)</b><br>
     <blockquote>More extensive test of concatenated gzip (and bzip2) decoding support for HADOOP-6835 (and HADOOP-4012 and HADOOP-6852).</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1906">MAPREDUCE-1906</a>.
     Major improvement reported by scott_carey and fixed by tlipcon (jobtracker, tasktracker)<br>
     <b>Lower minimum heartbeat interval for tasktracker &gt; Jobtracker</b><br>
     <blockquote>                                              The minimum heartbeat interval has been dropped from 3 seconds to 300ms to increase scheduling throughput on small clusters. Users may tune mapreduce.jobtracker.heartbeats.in.second to adjust this value.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1831">MAPREDUCE-1831</a>.
     Major improvement reported by schen and fixed by schen (contrib/raid)<br>
     <b>BlockPlacement policy for RAID</b><br>
     <blockquote>Raid introduce the new dependency between blocks within a file.<br>The blocks help decode each other. Therefore we should avoid put them on the same machine.<br><br>The proposed BlockPlacementPolicy does the following<br>1. When writing parity blocks, it avoid the parity blocks and source blocks sit together.<br>2. When reducing replication number, it deletes the blocks that sits with other dependent blocks.<br>3. It does not change the way we write normal files. It only has different behavior when processing ...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1811">MAPREDUCE-1811</a>.
     Minor bug reported by amareshwari and fixed by qwertymaniac (client)<br>
     <b>Job.monitorAndPrintJob() should print status of the job at completion</b><br>
     <blockquote>                                              Print the resultant status of a Job on completion instead of simply saying &amp;#39;Complete&amp;#39;.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1788">MAPREDUCE-1788</a>.
     Major bug reported by acmurthy and fixed by acmurthy (client)<br>
     <b>o.a.h.mapreduce.Job shouldn&apos;t make a copy of the JobConf</b><br>
     <blockquote>Having o.a.h.mapreduce.Job make a copy of the passed in JobConf has several issues: any modifications done by various pieces such as InputSplit etc. are not reflected back and causes issues for frameworks built on top.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1783">MAPREDUCE-1783</a>.
     Major improvement reported by rvadali and fixed by rvadali (contrib/fair-share)<br>
     <b>Task Initialization should be delayed till when a job can be run</b><br>
     <blockquote>The FairScheduler task scheduler uses PoolManager to impose limits on the number of jobs that can be running at a given time. However, jobs that are submitted are initiaiized immediately by EagerTaskInitializationListener by calling JobInProgress.initTasks. This causes the job split file to be read into memory. The split information is not needed until the number of running jobs is less than the maximum specified. If the amount of split information is large, this leads to unnecessary memory p...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1752">MAPREDUCE-1752</a>.
     Major improvement reported by dms and fixed by dms (harchive)<br>
     <b>Implement getFileBlockLocations in HarFilesystem</b><br>
     <blockquote>To efficiently run map reduce on the data that has been HAR&apos;ed it will be great to actually implement getFileBlockLocations for a given filename.<br>This way the JobTracker will have information about data locality and will schedule tasks appropriately.<br>I believe the overhead introduced by doing lookups in the index files can be smaller than that of copying data over the wire.<br>Will upload the patch shortly, but would love to get some feedback on this. And any ideas on how to test it are very wel...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1738">MAPREDUCE-1738</a>.
     Major improvement reported by vicaya and fixed by vicaya <br>
     <b>MapReduce portion of HADOOP-6728 (ovehaul metrics framework)</b><br>
     <blockquote></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1706">MAPREDUCE-1706</a>.
     Major improvement reported by rschmidt and fixed by schen (contrib/raid)<br>
     <b>Log RAID recoveries on HDFS</b><br>
     <blockquote>It would be good to have a way to centralize all the recovery logs, since recovery can be executed by any hdfs client. The best place to store this information is HDFS itself.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1702">MAPREDUCE-1702</a>.
     Minor improvement reported by jaideep and fixed by  (contrib/gridmix)<br>
     <b>CPU/Memory emulation for GridMix3</b><br>
     <blockquote>Currently GridMix3 can successfully recreate I/O workload of jobs from job traces. The goal of this feature is to emulate CPU and memory usage of jobs as well. For this we need to record cpu/memory usage of tasks on the cluster, save them to JobHistory so that they can be read by Rumen, and replay the cpu and memory usage in gridmix3 jobs.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1624">MAPREDUCE-1624</a>.
     Major improvement reported by devaraj and fixed by devaraj (documentation)<br>
     <b>Document the job credentials and associated details to do with delegation tokens (on the client side)</b><br>
     <blockquote>Document the job credentials and associated details to do with delegation tokens (on the client side)</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1461">MAPREDUCE-1461</a>.
     Major improvement reported by rajesh.balamohan and fixed by rajesh.balamohan (tools/rumen)<br>
     <b>Feature to instruct rumen-folder utility to skip jobs worth of specific duration</b><br>
     <blockquote>                    Added a &amp;#39;&amp;#39;-starts-after&amp;#39; option to Rumen&amp;#39;s Folder utility. The time duration specified after the &amp;#39;-starts-after&amp;#39; option is an offset with respect to the submit time of the first job in the input trace. Jobs in the input trace having a submit time (relative to the first job&amp;#39;s submit time) lesser than the specified offset will be ignored.<br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1334">MAPREDUCE-1334</a>.
     Major bug reported by kaykay.unique and fixed by kaykay.unique (contrib/index)<br>
     <b>contrib/index - test - TestIndexUpdater fails due to an additional presence of file _SUCCESS in hdfs </b><br>
     <blockquote>$ cd src/contrib/index<br>$ ant clean test <br><br>This fails the test TestIndexUpdater due to a mismatch in the - doneFileNames - data structure, when it is being run with different parameters. <br><br>(ArrayIndexOutOfBoundsException raised when inserting elements in doneFileNames, array ). <br><br>Debugging further - there seems to be an additional file called as - hdfs://localhost:36021/myoutput/_SUCCESS , taken into consideration in addition to those that begins with done* .  The presence of the extra file ca...</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1242">MAPREDUCE-1242</a>.
     Trivial bug reported by amogh and fixed by qwertymaniac <br>
     <b>Chain APIs error misleading</b><br>
     <blockquote>                                              Fix a misleading exception message in case the Chained Mappers have mismatch in input/output Key/Value pairs between them.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1207">MAPREDUCE-1207</a>.
     Blocker improvement reported by acmurthy and fixed by acmurthy (client, mrv2)<br>
     <b>Allow admins to set java options for map/reduce tasks</b><br>
     <blockquote>It will be useful for allow cluster-admins to set some java options for child map/reduce tasks. <br><br>E.g. We&apos;ve had to ask users to set -Djava.net.preferIPv4Stack=true in their jobs, it would be nice to do it for all users in such scenarios even when people override mapred.child.{map|reduce}.java.opts but forget to add this.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-1159">MAPREDUCE-1159</a>.
     Trivial improvement reported by zshao and fixed by qwertymaniac <br>
     <b>Limit Job name on jobtracker.jsp to be 80 char long</b><br>
     <blockquote>                                              Job names on jobtracker.jsp should be 80 characters long at most.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-993">MAPREDUCE-993</a>.
     Minor bug reported by iyappans and fixed by qwertymaniac (jobtracker)<br>
     <b>bin/hadoop job -events &lt;jobid&gt; &lt;from-event-#&gt; &lt;#-of-events&gt; help message is confusing</b><br>
     <blockquote>                                              Added a helpful description message to the `mapred job -events` command.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-901">MAPREDUCE-901</a>.
     Major improvement reported by owen.omalley and fixed by vicaya (task)<br>
     <b>Move Framework Counters into a TaskMetric structure</b><br>
     <blockquote>                                              Efficient implementation of MapReduce framework counters.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-587">MAPREDUCE-587</a>.
     Minor bug reported by stevel@apache.org and fixed by amar_kamat (contrib/streaming)<br>
     <b>Stream test TestStreamingExitStatus fails with Out of Memory</b><br>
     <blockquote>                                              Fixed the streaming test TestStreamingExitStatus&amp;#39;s failure due to an OutOfMemory error by reducing the testcase&amp;#39;s io.sort.mb.<br><br>      <br></blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-517">MAPREDUCE-517</a>.
     Critical bug reported by acmurthy and fixed by acmurthy <br>
     <b>The capacity-scheduler should assign multiple tasks per heartbeat</b><br>
     <blockquote>HADOOP-3136 changed the default o.a.h.mapred.JobQueueTaskScheduler to assign multiple tasks per TaskTracker heartbeat, the capacity-scheduler should do the same.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-461">MAPREDUCE-461</a>.
     Minor new feature reported by fhedberg and fixed by fhedberg <br>
     <b>Enable ServicePlugins for the JobTracker</b><br>
     <blockquote>Allow ServicePlugins (see HADOOP-5257) for the JobTracker.</blockquote></li>

<li> <a href="https://issues.apache.org/jira/browse/MAPREDUCE-279">MAPREDUCE-279</a>.
     Major improvement reported by acmurthy and fixed by  (mrv2)<br>
     <b>Map-Reduce 2.0</b><br>
     <blockquote>                    MapReduce has undergone a complete re-haul in hadoop-0.23 and we now have, what we call, MapReduce 2.0 (MRv2).<br/><br><br><br/><br><br>The fundamental idea of MRv2 is to split up the two major functionalities of the JobTracker, resource management and job scheduling/monitoring, into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM).  An application is either a single job in the classical sense of Map-Reduce jobs or a DAG of jobs. The ResourceManager and per-node slave, the NodeManager (NM), form the data-computation framework. The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. The per-application ApplicationMaster is, in effect, a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks.<br/><br><br><br/><br><br>The ResourceManager has two main components:<br/><br><br>* Scheduler (S)<br/><br><br>* ApplicationsManager (ASM)<br/><br><br><br/><br><br>The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. The Scheduler is pure scheduler in the sense that it performs no monitoring or tracking of status for the application. Also, it offers no guarantees on restarting failed tasks either due to application failure or hardware failures. The Scheduler performs its scheduling function based the resource requirements of the applications; it does so based on the abstract notion of a Resource Container which incorporates elements such as memory, cpu, disk, network etc. <br/><br><br><br/><br><br>The Scheduler has a pluggable policy plug-in, which is responsible for partitioning the cluster resources among the various queues, applications etc. The current Map-Reduce schedulers such as the CapacityScheduler and the FairScheduler would be some examples of the plug-in.<br/><br><br><br/><br><br>The CapacityScheduler supports hierarchical queues to allow for more predictable sharing of cluster resources.<br/><br><br>The ApplicationsManager is responsible for accepting job-submissions, negotiating the first container for executing the application specific ApplicationMaster and provides the service for restarting the ApplicationMaster container on failure.<br/><br><br><br/><br><br>The NodeManager is the per-machine framework agent who is responsible for launching the applications&amp;#39; containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the Scheduler.<br/><br><br><br/><br><br>The per-application ApplicationMaster has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress.<br/><br><br><br></blockquote></li>

</ul>

</body>
</html>
