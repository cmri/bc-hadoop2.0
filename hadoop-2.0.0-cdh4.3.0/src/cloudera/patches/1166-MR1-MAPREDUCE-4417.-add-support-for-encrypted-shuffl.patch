From b0c79dedffcb142d550728a150fb48afe32b7b9f Mon Sep 17 00:00:00 2001
From: Alejandro Abdelnur <tucu@cloudera.com>
Date: Thu, 26 Jul 2012 17:40:15 -0700
Subject: [PATCH 1166/1357] MR1: MAPREDUCE-4417. add support for encrypted shuffle (tucu)

  Reason: customer request
  Author: Alejandro Abdelnur
  Ref: CDH-6647
---
 .../hadoop/mapred/TestCapacityScheduler.java       |    6 +-
 .../hadoop/mapred/TestCapBasedLoadManager.java     |    2 +-
 .../apache/hadoop/mapred/TestFairScheduler.java    |    6 +-
 .../content/xdocs/Encrypted_Shuffle.xml            |  419 ++++++++++++++++++++
 src/docs/src/documentation/content/xdocs/site.xml  |    1 +
 src/mapred/mapred-default.xml                      |   26 ++
 .../org/apache/hadoop/mapred/JobInProgress.java    |    3 +-
 .../org/apache/hadoop/mapred/JobTracker.java       |   16 +-
 .../org/apache/hadoop/mapred/ReduceTask.java       |   43 ++-
 .../org/apache/hadoop/mapred/TaskTracker.java      |   97 +++++-
 .../apache/hadoop/mapred/TaskTrackerStatus.java    |   14 +-
 .../apache/hadoop/mapred/TestClusterStatus.java    |    6 +-
 .../hadoop/mapred/TestJobQueueTaskScheduler.java   |    4 +-
 .../hadoop/mapred/TestParallelInitialization.java  |    2 +-
 .../mapreduce/security/TestEncryptedShuffle.java   |  160 ++++++++
 15 files changed, 778 insertions(+), 27 deletions(-)
 create mode 100644 src/docs/src/documentation/content/xdocs/Encrypted_Shuffle.xml
 create mode 100644 src/test/org/apache/hadoop/mapreduce/security/TestEncryptedShuffle.java

diff --git a/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java b/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java
index 1052ea4..41fa7c1 100644
--- a/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java
+++ b/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java
@@ -468,7 +468,7 @@ public class TestCapacityScheduler extends TestCase {
       for (int i = 1; i < numTaskTrackers + 1; i++) {
         String ttName = "tt" + i;
         TaskTracker tt = new TaskTracker(ttName);
-        tt.setStatus(new TaskTrackerStatus(ttName, ttName + ".host", i,
+        tt.setStatus(new TaskTrackerStatus(ttName, "http", ttName + ".host", i,
                                            new ArrayList<TaskStatus>(), 0, 0,
                                            maxMapTasksPerTracker,
                                            maxReduceTasksPerTracker));
@@ -478,7 +478,7 @@ public class TestCapacityScheduler extends TestCase {
     
     public void addTaskTracker(String ttName) {
       TaskTracker tt = new TaskTracker(ttName);
-      tt.setStatus(new TaskTrackerStatus(ttName, ttName + ".host", 1,
+      tt.setStatus(new TaskTrackerStatus(ttName, "http", ttName + ".host", 1,
                                          new ArrayList<TaskStatus>(), 0, 0,
                                          maxMapTasksPerTracker, 
                                          maxReduceTasksPerTracker));
@@ -489,7 +489,7 @@ public class TestCapacityScheduler extends TestCase {
         int maxMapTasksPerTracker,
         int maxReduceTasksPerTracker) {
       TaskTracker tt = new TaskTracker(ttName);
-      tt.setStatus(new TaskTrackerStatus(ttName, ttName + ".host", 1,
+      tt.setStatus(new TaskTrackerStatus(ttName, "http", ttName + ".host", 1,
                                          new ArrayList<TaskStatus>(), 0, 0,
                                          maxMapTasksPerTracker,
                                          maxReduceTasksPerTracker));
diff --git a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java
index 45e29d3..5843a00 100644
--- a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java
+++ b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java
@@ -67,7 +67,7 @@ public class TestCapBasedLoadManager extends TestCase {
       ts.add(getRunningReduceTaskStatus());
     }
     TaskTrackerStatus tracker = new TaskTrackerStatus("tracker", 
-      "tracker_host", 1234, ts, 0, 0, mapCap, reduceCap);
+      "http", "tracker_host", 1234, ts, 0, 0, mapCap, reduceCap);
     return tracker;
   }
 
diff --git a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
index 64cf20f..22fc2c2 100644
--- a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
+++ b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
@@ -347,7 +347,7 @@ public class TestFairScheduler extends TestCase {
          String host = "rack" + rack + ".node" + node;
          System.out.println("Creating TaskTracker tt" + id + " on " + host);
          TaskTracker tt = new TaskTracker("tt" + id);
-         tt.setStatus(new TaskTrackerStatus("tt" + id, host, 0,
+         tt.setStatus(new TaskTrackerStatus("tt" + id, "http", host, 0,
              new ArrayList<TaskStatus>(), 0, 0,
              maxMapTasksPerTracker, maxReduceTasksPerTracker));
          trackers.put("tt" + id, tt);
@@ -357,14 +357,14 @@ public class TestFairScheduler extends TestCase {
 
     public FakeTaskTrackerManager() {
       TaskTracker tt1 = new TaskTracker("tt1");
-      tt1.setStatus(new TaskTrackerStatus("tt1", "tt1.host", 1,
+      tt1.setStatus(new TaskTrackerStatus("tt1", "http", "tt1.host", 1,
                                           new ArrayList<TaskStatus>(), 0, 0,
                                           maxMapTasksPerTracker, 
                                           maxReduceTasksPerTracker));
       trackers.put("tt1", tt1);
       
       TaskTracker tt2 = new TaskTracker("tt2");
-      tt2.setStatus(new TaskTrackerStatus("tt2", "tt2.host", 2,
+      tt2.setStatus(new TaskTrackerStatus("tt2", "http", "tt2.host", 2,
                                           new ArrayList<TaskStatus>(), 0, 0,
                                           maxMapTasksPerTracker, 
                                           maxReduceTasksPerTracker));
diff --git a/src/docs/src/documentation/content/xdocs/Encrypted_Shuffle.xml b/src/docs/src/documentation/content/xdocs/Encrypted_Shuffle.xml
new file mode 100644
index 0000000..ea04eb5
--- /dev/null
+++ b/src/docs/src/documentation/content/xdocs/Encrypted_Shuffle.xml
@@ -0,0 +1,419 @@
+<?xml version="1.0"?>
+<!--
+  Copyright 2002-2004 The Apache Software Foundation
+
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
+    "http://forrest.apache.org/dtd/document-v20.dtd">
+
+
+<document>
+
+  <header>
+    <title>
+      Hadoop Encrypted Shuffle
+    </title>
+  </header>
+
+  <body>
+    <section>
+      <title>Introduction</title>
+      <p>
+        The Encrypted Shuffle capability allows encryption of the MapReduce
+        shuffle
+        using HTTPS and with optional client authentication (also known as
+        bi-directional HTTPS, or HTTPS with client certificates). It comprises:
+      </p>
+
+        <ul>
+          <li>A Hadoop configuration setting for toggling the shuffle between
+            HTTP and
+            HTTPS.
+          </li>
+          <li>A Hadoop configuration settings for specifying the keystore and
+            truststore
+            properties (location, type, passwords) used by the shuffle service
+            and the
+            reducers tasks fetching shuffle data.
+          </li>
+          <li>A way to re-load truststores across the cluster (when a node is
+            added or
+            removed).
+          </li>
+        </ul>
+    </section>
+
+    <section>
+      <title>Configuration</title>
+
+      <p>
+        <strong>core-site.xml</strong>
+        Properties
+      </p>
+
+      <p>To enable encrypted shuffle, set the following properties in
+        <strong>core-site.xml</strong>
+        of all nodes in the cluster:
+      </p>
+
+      <ul>
+        <li><strong>hadoop.ssl.require.client.cert</strong>. Default:<strong>false</strong>.
+          Whether client certificates are required
+        </li>
+        <li><strong>hadoop.ssl.hostname.verifier</strong>. Default:<strong>DEFAULT</strong>.
+          The hostname verifier to provide for HttpsURLConnections. Valid
+          values are:<strong>DEFAULT</strong>,<strong>STRICT</strong>,<strong>STRICT_I6</strong>,
+          <strong>DEFAULT_AND_LOCALHOST</strong>
+          and
+          <strong>ALLOW_ALL</strong>
+        </li>
+        <li><strong>hadoop.ssl.keystores.factory.class</strong>. Default:
+          <strong>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</strong>
+          .
+          The KeyStoresFactory implementation to use
+        </li>
+        <li><strong>hadoop.ssl.server.conf</strong>. Default:<strong>ss-server.xml</strong>.
+          Resource file from which ssl server keystore information will be
+          extracted. This file is looked up in the classpath, typically it
+          should be in Hadoop conf/ directory
+        </li>
+        <li><strong>hadoop.ssl.client.conf</strong>. Default:<strong>ss-client.xml</strong>.
+          Resource file from which ssl server keystore information will be
+          extracted. This file is looked up in the classpath, typically it
+          should be in Hadoop conf/ directory
+        </li>
+      </ul>
+
+      <p>
+        <strong>IMPORTANT:</strong>
+        Currently requiring client certificates should be set
+        to false. Refer the Client Certificates section for details.
+      </p>
+
+      <p>
+        <strong>IMPORTANT:</strong>
+        All these properties should be marked as final in
+        the cluster configuration files.
+      </p>
+
+      <p>
+        <strong>Example:</strong>
+      </p>
+
+        <source>
+          ...
+          &lt;property&gt;
+            &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;
+            &lt;value&gt;false&lt;/value&gt;
+            &lt;final&gt;true&lt;/final&gt;
+          &lt;/property&gt;
+
+          &lt;property&gt;
+            &lt;name&gt;hadoop.ssl.hostname.verifier&lt;/name&gt;
+            &lt;value&gt;DEFAULT&lt;/value&gt;
+            &lt;final&gt;true&lt;/final&gt;
+          &lt;/property&gt;
+
+          &lt;property&gt;
+            &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;
+            &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
+            &lt;/value&gt;
+            &lt;final&gt;true&lt;/final&gt;
+          &lt;/property&gt;
+
+          &lt;property&gt;
+            &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;
+            &lt;value&gt;ssl-server.xml&lt;/value&gt;
+            &lt;final&gt;true&lt;/final&gt;
+          &lt;/property&gt;
+
+          &lt;property&gt;
+            &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;
+            &lt;value&gt;ssl-client.xml&lt;/value&gt;
+            &lt;final&gt;true&lt;/final&gt;
+          &lt;/property&gt;
+          ...
+        </source>
+
+      <p>
+        <strong>mapred-site.xml</strong>
+        Properties
+      </p>
+
+      <p>To enable encrypted shuffle, set the following property in
+        <strong>mapred-site.xml</strong>
+        of all nodes in the cluster:
+      </p>
+
+      <ul>
+        <li><strong>mapreduce.shuffle.ssl.enabled</strong>. Default:<strong>false</strong>.
+          Whether encrypted shuffle is enabled
+        </li>
+      </ul>
+
+
+      <p>
+        <strong>IMPORTANT:</strong>
+        All these properties should be marked as final in
+        the cluster configuration files.
+      </p>
+
+      <p>
+        <strong>Example:</strong>
+      </p>
+
+        <source>
+          ...
+          &lt;property&gt;
+            &lt;name&gt;mapreduce.shuffle.ssl.enabled&lt;/name&gt;
+            &lt;value&gt;true&lt;/value&gt;
+          &lt;/property&gt;
+          ...
+        </source>
+
+      <p>The Linux container executor should be set to prevent job tasks from
+      reading the server keystore information and gaining access to the shuffle
+      server certificates.
+      </p>
+
+      <p>Refer to Hadoop Kerberos configuration for details on how to do this.
+      </p>
+
+    </section>
+
+    <section>
+      <title>Keystore and Truststore Settings</title>
+
+      <p>Currently <strong>FileBasedKeyStoresFactory</strong> is the only
+        <strong>KeyStoresFactory</strong> implementation. The
+        <strong>FileBasedKeyStoresFactory</strong> implementation uses the following
+        properties, in the <strong>ssl-server.xml</strong> and <strong>ssl-client.xml</strong>
+        files, to configure the keystores and truststores.
+      </p>
+
+      <p><strong>ssl-server.xml</strong> (Shuffle server) Configuration:
+      </p>
+
+      <p>The mapred user should own the <strong>ssl-server.xml</strong> file and have
+      exclusive read access to it.
+      </p>
+
+      <ul>
+        <li><strong>ssl.server.keystore.type</strong>. Default <strong>jks</strong>.
+          Keystore file type</li>
+        <li><strong>ssl.server.keystore.location</strong>. Default <strong>NONE</strong>.
+          Keystore file location. The mapred user should own this file
+          and have exclusive read access to it</li>
+        <li><strong>ssl.server.keystore.password</strong>. Default <strong>NONE</strong>.
+          Keystore file password</li>
+        <li><strong>ssl.server.truststore.type</strong>. Default <strong>jks</strong>.
+          Truststore file type</li>
+        <li><strong>ssl.server.truststore.location</strong>. Default <strong>NONE</strong>.
+          Truststore file location. The mapred user should own this file
+          and have exclusive read access to it</li>
+        <li><strong>ssl.server.truststore.password</strong>. Default <strong>NONE</strong>.
+          Truststore file password</li>
+        <li><strong>ssl.server.truststore.reload.interval</strong>. Default <strong>10</strong>.
+          Truststore reload interval, in milliseconds</li>
+      </ul>
+
+      <p>
+        <strong>Example:</strong>
+      </p>
+
+        <source>
+          &lt;configuration&gt;
+
+            &lt;!-- Server Certificate Store --&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;
+              &lt;value&gt;jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;
+              &lt;value&gt;${user.home}/keystores/server-keystore.jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;
+              &lt;value&gt;serverfoo&lt;/value&gt;
+            &lt;/property&gt;
+
+            &lt;!-- Server Trust Store --&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.truststore.type&lt;/name&gt;
+              &lt;value&gt;jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.truststore.location&lt;/name&gt;
+              &lt;value&gt;${user.home}/keystores/truststore.jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.truststore.password&lt;/name&gt;
+              &lt;value&gt;clientserverbar&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.server.truststore.reload.interval&lt;/name&gt;
+              &lt;value&gt;10000&lt;/value&gt;
+            &lt;/property&gt;
+          &lt;/configuration&gt;
+        </source>
+
+      <p><strong>ssl-client.xml</strong> (Reducer/Fetcher) Configuration:
+      </p>
+
+      <p>The mapred user should own the <strong>ssl-server.xml</strong> file and it should
+        have default permissions.
+      </p>
+
+        <ul>
+          <li><strong>ssl.client.keystore.type</strong>. Default <strong>jks</strong>.
+            Keystore file type</li>
+          <li><strong>ssl.client.keystore.location</strong>. Default <strong>NONE</strong>.
+            Keystore file location. The mapred user should own this file
+            and have exclusive read access to it</li>
+          <li><strong>ssl.client.keystore.password</strong>. Default <strong>NONE</strong>.
+            Keystore file password</li>
+          <li><strong>ssl.client.truststore.type</strong>. Default <strong>jks</strong>.
+            Truststore file type</li>
+          <li><strong>ssl.client.truststore.location</strong>. Default <strong>NONE</strong>.
+            Truststore file location. The mapred user should own this file
+            and have exclusive read access to it</li>
+          <li><strong>ssl.client.truststore.password</strong>. Default <strong>NONE</strong>.
+            Truststore file password</li>
+          <li><strong>ssl.client.truststore.reload.interval</strong>. Default <strong>10</strong>.
+            Truststore reload interval, in milliseconds</li>
+        </ul>
+
+      <p>
+        <strong>Example:</strong>
+      </p>
+
+        <source>
+          &lt;configuration&gt;
+
+            &lt;!-- Clietn Certificate Store --&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.keystore.type&lt;/name&gt;
+              &lt;value&gt;jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.keystore.location&lt;/name&gt;
+              &lt;value&gt;${user.home}/keystores/client-keystore.jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.keystore.password&lt;/name&gt;
+              &lt;value&gt;clientfoo&lt;/value&gt;
+            &lt;/property&gt;
+
+            &lt;!-- Client Trust Store --&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.truststore.type&lt;/name&gt;
+              &lt;value&gt;jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.truststore.location&lt;/name&gt;
+              &lt;value&gt;${user.home}/keystores/truststore.jks&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.truststore.password&lt;/name&gt;
+              &lt;value&gt;clientbar&lt;/value&gt;
+            &lt;/property&gt;
+            &lt;property&gt;
+              &lt;name&gt;ssl.client.truststore.reload.interval&lt;/name&gt;
+              &lt;value&gt;10000&lt;/value&gt;
+            &lt;/property&gt;
+          &lt;/configuration&gt;
+        </source>
+
+    </section>
+
+    <section>
+      <title>Activating Encrypted Shuffle</title>
+
+      <p>When you have made the above configuration changes, activate Encrypted
+      Shuffle by re-starting all TaskTrackers.
+      </p>
+
+      <p><strong>IMPORTANT:</strong> Using encrypted shuffle will incur in a significant
+          performance impact. Users should profile this and potentially reserve
+          1 or more cores for encrypted shuffle.
+      </p>
+    </section>
+
+    <section>
+      <title>Client Certificates</title>
+
+      <p>
+        Using Client Certificates does not fully ensure that the client is a
+        reducer task for the job. Currently, Client Certificates (their private key)
+        keystore files must be readable by all users submitting jobs to the cluster.
+        This means that a rogue job could read such those keystore files and use
+        the client certificates in them to establish a secure connection with a
+        Shuffle server. However, unless the rogue job has a proper JobToken, it won't
+        be able to retrieve shuffle data from the Shuffle server. A job, using its
+        own JobToken, can only retrieve shuffle data that belongs to itself.
+      </p>
+    </section>
+
+    <section>
+      <title>Reloading Truststores</title>
+
+      <p>
+        By default the truststores will reload their configuration every 10 seconds.
+        If a new truststore file is copied over the old one, it will be re-read,
+        and its certificates will replace the old ones. This mechanism is useful for
+        adding or removing nodes from the cluster, or for adding or removing trusted
+        clients. In these cases, the client or TaskTracker certificate is added to
+        (or removed from) all the truststore files in the system, and the new
+        configuration will be picked up without you having to restart the TaskTracker
+      </p>
+    </section>
+
+    <section>
+      <title>Debugging</title>
+
+      <p><strong>NOTE:</strong> Enable debugging only for troubleshooting, and then only for jobs
+      running on small amounts of data. It is very verbose and slows down jobs by
+      several orders of magnitude. (You might need to increase mapred.task.timeout
+      to prevent jobs from failing because tasks run so slowly.)
+      </p>
+
+      <p>To enable SSL debugging in the reducers, set <strong>-Djavax.net.debug=all</strong>
+      in the <strong>mapreduce.reduce.child.java.opts</strong> property; for example:
+      </p>
+
+    <source>
+      &lt;property&gt;
+        &lt;name&gt;mapred.reduce.child.java.opts&lt;/name&gt;
+        &lt;value&gt;-Xmx-200m -Djavax.net.debug=all&lt;/value&gt;
+      &lt;/property&gt;
+    </source>
+
+      <p>You can do this on a per-job basis, or by means of a cluster-wide setting in
+      the <strong>mapred-site.xml</strong> file.
+      </p>
+
+      <p>To set this property in TaskTracker, set it in the <strong>hadoop-env.sh</strong>
+      file:
+      </p>
+
+      <source>
+      HADOOP_OPTS="-Djavax.net.debug=all $HADOOP_OPTS"
+      </source>
+
+    </section>
+
+  </body>
+</document>
diff --git a/src/docs/src/documentation/content/xdocs/site.xml b/src/docs/src/documentation/content/xdocs/site.xml
index 070a233..e0cad5d 100644
--- a/src/docs/src/documentation/content/xdocs/site.xml
+++ b/src/docs/src/documentation/content/xdocs/site.xml
@@ -72,6 +72,7 @@ See http://forrest.apache.org/docs/linking.html for more info.
   
   <docs label="Miscellaneous"> 
     <sec_impersonation label="Secure Impersonation" href="Secure_Impersonation.html"/>
+    <encrypted_shuffle label="Encrypted Shuffle" href="Encrypted_Shuffle.html"/>
     <api         label="API Docs"           href="ext:api/index" />
     <jdiff       label="API Changes"      href="ext:jdiff/changes" />
     <wiki        label="Wiki"                 href="ext:wiki" />
diff --git a/src/mapred/mapred-default.xml b/src/mapred/mapred-default.xml
index f6e3e78..1f9f266 100644
--- a/src/mapred/mapred-default.xml
+++ b/src/mapred/mapred-default.xml
@@ -1253,4 +1253,30 @@
   </description>
 </property>
 
+  <!-- Encrypted Shuffle Configuration -->
+
+<property>
+  <name>mapreduce.shuffle.ssl.enabled</name>
+  <value>false</value>
+  <description>
+    Whether to use SSL for for the Shuffle HTTP endpoints.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.shuffle.ssl.address</name>
+  <value>0.0.0.0</value>
+  <description>
+    IP Address to bind the SSL Shuffle endpoint.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.shuffle.ssl.port</name>
+  <value>50443</value>
+  <description>
+    Port to bind the SSL Shuffle endpoint.
+  </description>
+</property>
+  
 </configuration>
diff --git a/src/mapred/org/apache/hadoop/mapred/JobInProgress.java b/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
index 0e242d6..b3897e1 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
@@ -1109,7 +1109,8 @@ public class JobInProgress {
         } else {
           host = ttStatus.getHost();
         }
-        httpTaskLogLocation = "http://" + host + ":" + ttStatus.getHttpPort(); 
+        httpTaskLogLocation = ttStatus.getUrlScheme() + "://" + host + ":" +
+                              ttStatus.getHttpPort();
            //+ "/tasklog?plaintext=true&attemptid=" + status.getTaskID();
       }
 
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTracker.java b/src/mapred/org/apache/hadoop/mapred/JobTracker.java
index 2c6d7df..1651d40 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobTracker.java
@@ -172,7 +172,19 @@ public class JobTracker implements MRConstants, JTProtocols, JobTrackerMXBean {
     "mapreduce.cluster.delegation.token.max-lifetime";
   public static final long    DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT =  
     7*24*60*60*1000; // 7 days
-  
+
+  // Encrypted Shuffle constants.
+
+  public static final String SHUFFLE_SSL_ENABLED_KEY =
+    "mapreduce.shuffle.ssl.enabled";
+  public static final boolean SHUFFLE_SSL_ENABLED_DEFAULT = false;
+  public static final String SHUFFLE_SSL_ADDRESS_KEY =
+      "mapreduce.shuffle.ssl.address";
+  public static final String SHUFFLE_SSL_ADDRESS_DEFAULT = "0.0.0.0";
+  public static final String SHUFFLE_SSL_PORT_KEY =
+        "mapreduce.shuffle.ssl.port";
+  public static final int SHUFFLE_SSL_PORT_DEFAULT = 50443;
+
   // Approximate number of heartbeats that could arrive JobTracker
   // in a second
   static final String JT_HEARTBEATS_IN_SECOND = "mapred.heartbeats.in.second";
@@ -1284,7 +1296,7 @@ public class JobTracker implements MRConstants, JTProtocols, JobTrackerMXBean {
       
       // III. Create the dummy tasktracker status
       TaskTrackerStatus ttStatus = 
-        new TaskTrackerStatus(trackerName, trackerHostName, port, ttStatusList, 
+        new TaskTrackerStatus(trackerName, "http", trackerHostName, port, ttStatusList,
                               0 , 0, 0, 0);
       ttStatus.setLastSeen(clock.getTime());
 
diff --git a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
index 4481b0b..5788ce5 100644
--- a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
@@ -29,8 +29,10 @@ import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
 import java.net.URI;
 import java.net.URL;
+import java.net.HttpURLConnection;
 import java.net.URLClassLoader;
 import java.net.URLConnection;
+import java.security.GeneralSecurityException;
 import java.text.DecimalFormat;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -49,6 +51,7 @@ import java.util.TreeSet;
 import java.util.concurrent.ConcurrentHashMap;
 
 import javax.crypto.SecretKey;
+import javax.net.ssl.HttpsURLConnection;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -82,6 +85,7 @@ import org.apache.hadoop.metrics.MetricsContext;
 import org.apache.hadoop.metrics.MetricsRecord;
 import org.apache.hadoop.metrics.MetricsUtil;
 import org.apache.hadoop.metrics.Updater;
+import org.apache.hadoop.security.ssl.SSLFactory;
 import org.apache.hadoop.util.Progress;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.ReflectionUtils;
@@ -149,6 +153,9 @@ class ReduceTask extends Task {
   private final SortedSet<FileStatus> mapOutputFilesOnDisk = 
     new TreeSet<FileStatus>(mapOutputFileComparator);
 
+  private static boolean sslShuffle;
+  private static SSLFactory sslFactory;
+
   public ReduceTask() {
     super();
   }
@@ -372,7 +379,19 @@ class ReduceTask extends Task {
       runTaskCleanupTask(umbilical, reporter);
       return;
     }
-    
+
+    sslShuffle = job.getBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
+                                JobTracker.SHUFFLE_SSL_ENABLED_DEFAULT);
+    if (sslShuffle && sslFactory == null) {
+      sslFactory = new SSLFactory(SSLFactory.Mode.CLIENT, job);
+      try {
+        sslFactory.init();
+      } catch (Exception ex) {
+        sslFactory.destroy();
+        throw new RuntimeException(ex);
+      }
+    }
+
     // Initialize the codec
     codec = initCodec();
 
@@ -418,6 +437,10 @@ class ReduceTask extends Task {
                     keyClass, valueClass);
     }
     done(umbilical, reporter);
+
+    if (sslFactory != null) {
+      sslFactory.destroy();
+    }
   }
 
   @SuppressWarnings("unchecked")
@@ -1389,6 +1412,20 @@ class ReduceTask extends Task {
         ramManager.setNumCopiedMapOutputs(numMaps - copiedMapOutputs.size());
       }
 
+      protected HttpURLConnection openConnection(URL url) throws IOException {
+        HttpURLConnection conn = (HttpURLConnection)url.openConnection();
+        if (sslShuffle) {
+          HttpsURLConnection httpsConn = (HttpsURLConnection) conn;
+          try {
+            httpsConn.setSSLSocketFactory(sslFactory.createSSLSocketFactory());
+          } catch (GeneralSecurityException ex) {
+            throw new IOException(ex);
+          }
+          httpsConn.setHostnameVerifier(sslFactory.getHostnameVerifier());
+        }
+        return conn;
+      }
+
       /**
        * Get the map output into a local file (either in the inmemory fs or on the 
        * local fs) from the remote server.
@@ -1405,7 +1442,7 @@ class ReduceTask extends Task {
       throws IOException, InterruptedException {
         // Connect
         URL url = mapOutputLoc.getOutputLocation();
-        URLConnection connection = url.openConnection();
+        HttpURLConnection connection = openConnection(url);
         
         InputStream input = setupSecureConnection(mapOutputLoc, connection);
  
@@ -1583,7 +1620,7 @@ class ReduceTask extends Task {
         if (!createdNow) {
           // Reconnect
           try {
-            connection = mapOutputLoc.getOutputLocation().openConnection();
+            connection = openConnection(mapOutputLoc.getOutputLocation());
             input = setupSecureConnection(mapOutputLoc, connection);
           } catch (IOException ioe) {
             LOG.info("Failed reopen connection to fetch map-output from " + 
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
index 8e80be0..4706163 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
@@ -26,6 +26,7 @@ import java.io.RandomAccessFile;
 import java.net.InetSocketAddress;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.security.GeneralSecurityException;
 import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -48,6 +49,7 @@ import java.util.concurrent.LinkedBlockingQueue;
 import java.util.regex.Pattern;
 
 import javax.crypto.SecretKey;
+import javax.net.ssl.SSLServerSocketFactory;
 import javax.servlet.ServletContext;
 import javax.servlet.ServletException;
 import javax.servlet.http.HttpServlet;
@@ -101,10 +103,12 @@ import org.apache.hadoop.net.NetUtils;
 import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.authorize.PolicyProvider;
+import org.apache.hadoop.security.authorize.AccessControlList;
 import org.apache.hadoop.util.DiskChecker;
 import org.apache.hadoop.util.MemoryCalculatorPlugin;
 import org.apache.hadoop.util.ResourceCalculatorPlugin;
 import org.apache.hadoop.util.ProcfsBasedProcessTree;
+import org.apache.hadoop.security.ssl.SSLFactory;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.security.token.TokenIdentifier;
 import org.apache.hadoop.util.ReflectionUtils;
@@ -115,6 +119,7 @@ import org.apache.hadoop.util.Shell.ShellCommandExecutor;
 import org.apache.hadoop.util.MRAsyncDiskService;
 import org.apache.hadoop.mapreduce.security.TokenCache;
 import org.apache.hadoop.security.Credentials;
+import org.mortbay.jetty.security.SslSocketConnector;
 
 /*******************************************************
  * TaskTracker is a process that starts and tracks MR Tasks
@@ -149,6 +154,10 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
   static final long WAIT_FOR_DONE = 3 * 1000;
   private int httpPort;
 
+  private SSLFactory sslFactory;
+  private String shuffleScheme;
+  private int shufflePort;
+
   static enum State {NORMAL, STALE, INTERRUPTED, DENIED}
 
   static final FsPermission LOCAL_DIR_PERMISSION =
@@ -305,7 +314,7 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
   // The filesystem where job files are stored
   FileSystem systemFS = null;
   private LocalFileSystem localFs = null;
-  private final HttpServer server;
+  private final TTHttpServer server;
     
   volatile boolean shuttingDown = false;
     
@@ -1493,6 +1502,9 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
       try {
         LOG.info("Shutting down StatusHttpServer");
         this.server.stop();
+        if (sslFactory != null) {
+          sslFactory.destroy();
+        }
       } catch (Exception e) {
         LOG.warn("Exception shutting down TaskTracker", e);
       }
@@ -1599,7 +1611,40 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
   void setLocalDirAllocator(LocalDirAllocator in) {
     localDirAllocator = in;
   }
-  
+
+  private static class TTHttpServer extends HttpServer {
+
+    public TTHttpServer(String name, String bindAddress, int port,
+        boolean findPort, Configuration conf, AccessControlList adminsAcl)
+        throws IOException {
+      super(name, bindAddress, port, findPort, conf, adminsAcl, null, null);
+    }
+
+    /**
+     * Configure an ssl listener on the server for shuffle.
+     *
+     * @param addr address to listen on.
+     * @param sslFactory SSLFactory to use.
+     */
+    public void addSslListener(InetSocketAddress addr, final SSLFactory sslFactory)
+      throws IOException {
+      if (webServer.isStarted()) {
+        throw new IOException("Failed to add ssl listener");
+      }
+
+      SslSocketConnector sslListener = new SslSocketConnector() {
+        @Override
+        protected SSLServerSocketFactory createFactory() throws Exception {
+          return sslFactory.createSSLServerSocketFactory();
+        }
+      };
+
+      sslListener.setHost(addr.getHostName());
+      sslListener.setPort(addr.getPort());
+      webServer.addConnector(sslListener);
+    }
+
+  }
   /**
    * Start with the local machine name, and the default JobTracker
    */
@@ -1624,7 +1669,7 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
     InetSocketAddress infoSocAddr = NetUtils.createSocketAddr(infoAddr);
     String httpBindAddress = infoSocAddr.getHostName();
     int httpPort = infoSocAddr.getPort();
-    this.server = new HttpServer("task", httpBindAddress, httpPort,
+    this.server = new TTHttpServer("task", httpBindAddress, httpPort,
         httpPort == 0, conf, aclsManager.getAdminsAcl());
     this.shuffleServerMetrics = new ShuffleServerMetrics(conf);
     workerThreads = conf.getInt("tasktracker.http.threads", 40);
@@ -1664,8 +1709,32 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
     server.setAttribute("exceptionMsgRegex", exceptionMsgRegex);
     server.addInternalServlet("mapOutput", "/mapOutput", MapOutputServlet.class);
     server.addServlet("taskLog", "/tasklog", TaskLogServlet.class);
+
+    boolean shuffleSsl = conf.getBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
+                                         JobTracker.SHUFFLE_SSL_ENABLED_DEFAULT);
+    shuffleScheme = (shuffleSsl) ? "https" : "http";
+    server.setAttribute(JobTracker.SHUFFLE_SSL_ENABLED_KEY, shuffleSsl);
+    if (shuffleSsl) {
+      sslFactory = new SSLFactory(SSLFactory.Mode.SERVER, conf);
+      try {
+        sslFactory.init();
+      } catch (GeneralSecurityException ex) {
+        throw new RuntimeException(ex);
+      }
+      String sslHostname = conf.get(JobTracker.SHUFFLE_SSL_ADDRESS_KEY,
+                                    JobTracker.SHUFFLE_SSL_ADDRESS_DEFAULT);
+      int sslPort = conf.getInt(
+        JobTracker.SHUFFLE_SSL_PORT_KEY, JobTracker.SHUFFLE_SSL_PORT_DEFAULT);
+      InetSocketAddress sslAddr = new InetSocketAddress(sslHostname, sslPort);
+      server.addSslListener(sslAddr, sslFactory);
+      shufflePort = sslPort;
+    }
+
     server.start();
     this.httpPort = server.getPort();
+    if (sslFactory == null) {
+      shufflePort = this.httpPort;
+    }
     checkJettyPort(httpPort);
     LOG.info("FILE_CACHE_SIZE for mapOutputServlet set to : " + FILE_CACHE_SIZE);
     mapRetainSize = conf.getLong(TaskLogsTruncater.MAP_USERLOG_RETAIN_SIZE, 
@@ -1968,8 +2037,8 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
     //
     if (status == null) {
       synchronized (this) {
-        status = new TaskTrackerStatus(taskTrackerName, localHostname, 
-                                       httpPort, 
+        status = new TaskTrackerStatus(taskTrackerName, shuffleScheme, localHostname,
+                                       shufflePort,
                                        cloneAndResetRunningTaskStatuses(
                                          sendAllCounters), 
                                        taskFailures,
@@ -3981,11 +4050,27 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
     private static final int RESPONSE_BUFFER_SIZE = MAX_BYTES_TO_READ + 16;
     private static LRUCache<String, Path> fileCache = new LRUCache<String, Path>(FILE_CACHE_SIZE);
     private static LRUCache<String, Path> fileIndexCache = new LRUCache<String, Path>(FILE_CACHE_SIZE);
-    
+
+
+    private boolean shuffleSsl = false;
+
+    @Override
+    public void init() throws ServletException {
+      super.init();
+      shuffleSsl =
+        (Boolean) getServletContext().getAttribute(
+          JobTracker.SHUFFLE_SSL_ENABLED_KEY);
+    }
+
     @Override
     public void doGet(HttpServletRequest request, 
                       HttpServletResponse response
                       ) throws ServletException, IOException {
+      if (shuffleSsl && !request.isSecure()) {
+        response.sendError(HttpServletResponse.SC_FORBIDDEN,
+          "Encrypted Shuffle is enabled, shuffle is only served over HTTPS");
+        return;
+      }
       String mapId = request.getParameter("map");
       String reduceId = request.getParameter("reduce");
       String jobId = request.getParameter("job");
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java b/src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java
index d8aa7f7..46d98f7 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java
@@ -45,6 +45,7 @@ public class TaskTrackerStatus implements Writable {
   }
 
   String trackerName;
+  String urlScheme;
   String host;
   int httpPort;
   int taskFailures;
@@ -356,11 +357,11 @@ public class TaskTrackerStatus implements Writable {
 
   /**
    */
-  public TaskTrackerStatus(String trackerName, String host, 
-                           int httpPort, List<TaskStatus> taskReports, 
+  public TaskTrackerStatus(String trackerName, String urlScheme, String host,                           int httpPort, List<TaskStatus> taskReports,
                            int taskFailures, int dirFailures,
                            int maxMapTasks, int maxReduceTasks) {
     this.trackerName = trackerName;
+    this.urlScheme = urlScheme;
     this.host = host;
     this.httpPort = httpPort;
 
@@ -378,6 +379,13 @@ public class TaskTrackerStatus implements Writable {
   public String getTrackerName() {
     return trackerName;
   }
+
+  /**
+   */
+  public String getUrlScheme() {
+    return urlScheme;
+  }
+
   /**
    */
   public String getHost() {
@@ -660,6 +668,7 @@ public class TaskTrackerStatus implements Writable {
   ///////////////////////////////////////////
   public void write(DataOutput out) throws IOException {
     Text.writeString(out, trackerName);
+    Text.writeString(out, urlScheme);
     Text.writeString(out, host);
     out.writeInt(httpPort);
     out.writeInt(taskFailures);
@@ -677,6 +686,7 @@ public class TaskTrackerStatus implements Writable {
 
   public void readFields(DataInput in) throws IOException {
     this.trackerName = Text.readString(in);
+    this.urlScheme = Text.readString(in);
     this.host = Text.readString(in);
     this.httpPort = in.readInt();
     this.taskFailures = in.readInt();
diff --git a/src/test/org/apache/hadoop/mapred/TestClusterStatus.java b/src/test/org/apache/hadoop/mapred/TestClusterStatus.java
index 6620751..e6b3d2f 100644
--- a/src/test/org/apache/hadoop/mapred/TestClusterStatus.java
+++ b/src/test/org/apache/hadoop/mapred/TestClusterStatus.java
@@ -152,7 +152,7 @@ public class TestClusterStatus extends TestCase {
 
   private TaskTrackerStatus getTTStatus(String trackerName,
       List<TaskStatus> taskStatuses) {
-    return new TaskTrackerStatus(trackerName, 
+    return new TaskTrackerStatus(trackerName, "http",
       JobInProgress.convertTrackerNameToHostName(trackerName), 0,
       taskStatuses, 0, 0, mapSlotsPerTracker, reduceSlotsPerTracker);
   }
@@ -266,10 +266,10 @@ public class TestClusterStatus extends TestCase {
     TaskTracker tt1 = jobTracker.getTaskTracker(trackers[0]);
     TaskTracker tt2 = jobTracker.getTaskTracker(trackers[1]);
     TaskTrackerStatus status1 = new TaskTrackerStatus(
-        trackers[0],JobInProgress.convertTrackerNameToHostName(
+        trackers[0],"http", JobInProgress.convertTrackerNameToHostName(
             trackers[0]),0,new ArrayList<TaskStatus>(), 0, 0, 2, 2);
     TaskTrackerStatus status2 = new TaskTrackerStatus(
-        trackers[1],JobInProgress.convertTrackerNameToHostName(
+        trackers[1],"http", JobInProgress.convertTrackerNameToHostName(
             trackers[1]),0,new ArrayList<TaskStatus>(), 0, 0, 2, 2);
     tt1.setStatus(status1);
     tt2.setStatus(status2);
diff --git a/src/test/org/apache/hadoop/mapred/TestJobQueueTaskScheduler.java b/src/test/org/apache/hadoop/mapred/TestJobQueueTaskScheduler.java
index 2448415..45d2f05 100644
--- a/src/test/org/apache/hadoop/mapred/TestJobQueueTaskScheduler.java
+++ b/src/test/org/apache/hadoop/mapred/TestJobQueueTaskScheduler.java
@@ -130,13 +130,13 @@ public class TestJobQueueTaskScheduler extends TestCase {
       queueManager = new QueueManager(conf);
       
       TaskTracker tt1 = new TaskTracker("tt1");
-      tt1.setStatus(new TaskTrackerStatus("tt1", "tt1.host", 1,
+      tt1.setStatus(new TaskTrackerStatus("tt1", "http", "tt1.host", 1,
                     new ArrayList<TaskStatus>(), 0, 0,
                     maxMapTasksPerTracker, maxReduceTasksPerTracker));
       trackers.put("tt1", tt1);
       
       TaskTracker tt2 = new TaskTracker("tt2");
-      tt2.setStatus(new TaskTrackerStatus("tt2", "tt2.host", 2,
+      tt2.setStatus(new TaskTrackerStatus("tt2", "http", "tt2.host", 2,
                     new ArrayList<TaskStatus>(), 0, 0,
                     maxMapTasksPerTracker, maxReduceTasksPerTracker));
       trackers.put("tt2", tt2);
diff --git a/src/test/org/apache/hadoop/mapred/TestParallelInitialization.java b/src/test/org/apache/hadoop/mapred/TestParallelInitialization.java
index 88380c6..165d270 100644
--- a/src/test/org/apache/hadoop/mapred/TestParallelInitialization.java
+++ b/src/test/org/apache/hadoop/mapred/TestParallelInitialization.java
@@ -91,7 +91,7 @@ public class TestParallelInitialization extends TestCase {
     public FakeTaskTrackerManager() {
       JobConf conf = new JobConf();
       queueManager = new QueueManager(conf);
-      trackers.put("tt1", new TaskTrackerStatus("tt1", "tt1.host", 1,
+      trackers.put("tt1", new TaskTrackerStatus("tt1", "http", "tt1.host", 1,
                    new ArrayList<TaskStatus>(), 0, 0,
                    maxMapTasksPerTracker, maxReduceTasksPerTracker));
     }
diff --git a/src/test/org/apache/hadoop/mapreduce/security/TestEncryptedShuffle.java b/src/test/org/apache/hadoop/mapreduce/security/TestEncryptedShuffle.java
new file mode 100644
index 0000000..cdb9756
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapreduce/security/TestEncryptedShuffle.java
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.security;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobTracker;
+import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.security.ssl.KeyStoreTestUtil;
+import org.apache.hadoop.security.ssl.SSLFactory;
+import org.junit.After;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+
+public class TestEncryptedShuffle {
+
+  private static final String BASEDIR =
+    System.getProperty("test.build.dir", "build/test-dir") + "/" +
+    TestEncryptedShuffle.class.getSimpleName();
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    File base = new File(BASEDIR);
+    FileUtil.fullyDelete(base);
+    base.mkdirs();
+  }
+
+  @After
+  public void cleanUpMiniClusterSpecialConfig() throws Exception {
+    String classpathDir =
+      KeyStoreTestUtil.getClasspathDir(TestEncryptedShuffle.class);
+    String keystoresDir = new File(BASEDIR).getAbsolutePath();
+    KeyStoreTestUtil.cleanupSSLConfig(keystoresDir, classpathDir);
+  }
+
+  private MiniDFSCluster dfsCluster = null;
+  private MiniMRCluster mrCluster = null;
+
+  private void startCluster(Configuration  conf) throws Exception {
+    if (System.getProperty("hadoop.log.dir") == null) {
+      System.setProperty("hadoop.log.dir", "build/test-dir");
+    }
+    conf.set("dfs.block.access.token.enable", "false");
+    conf.set("dfs.permissions", "true");
+    conf.set("hadoop.security.authentication", "simple");
+    dfsCluster = new MiniDFSCluster(conf, 1, true, null);
+    FileSystem fileSystem = dfsCluster.getFileSystem();
+    fileSystem.mkdirs(new Path("/tmp"));
+    fileSystem.mkdirs(new Path("/user"));
+    fileSystem.mkdirs(new Path("/hadoop/mapred/system"));
+    fileSystem.setPermission(
+      new Path("/tmp"), FsPermission.valueOf("-rwxrwxrwx"));
+    fileSystem.setPermission(
+      new Path("/user"), FsPermission.valueOf("-rwxrwxrwx"));
+    fileSystem.setPermission(
+      new Path("/hadoop/mapred/system"), FsPermission.valueOf("-rwx------"));
+    FileSystem.setDefaultUri(conf, fileSystem.getUri());
+    mrCluster = new MiniMRCluster(1, fileSystem.getUri().toString(), 1, null, null, new JobConf(conf));
+  }
+
+  private void stopCluster() throws Exception {
+    if (mrCluster != null) {
+      mrCluster.shutdown();
+    }
+    if (dfsCluster != null) {
+      dfsCluster.shutdown();
+    }
+  }
+
+  protected JobConf getJobConf(Configuration sslConf) throws IOException {
+    JobConf conf = new JobConf(mrCluster.createJobConf());
+    //doing this because Hadoop1 minicluster does not use all mr cluster confs for the created jobconf
+    conf.set(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
+             sslConf.get(JobTracker.SHUFFLE_SSL_ENABLED_KEY));
+    conf.set(SSLFactory.SSL_HOSTNAME_VERIFIER_KEY, sslConf.get(SSLFactory.SSL_HOSTNAME_VERIFIER_KEY));
+    conf.set(SSLFactory.SSL_CLIENT_CONF_KEY, sslConf.get(SSLFactory.SSL_CLIENT_CONF_KEY));
+    conf.set(SSLFactory.SSL_SERVER_CONF_KEY, sslConf.get(SSLFactory.SSL_SERVER_CONF_KEY));
+    conf.set(SSLFactory.SSL_REQUIRE_CLIENT_CERT_KEY,
+             sslConf.get(SSLFactory.SSL_REQUIRE_CLIENT_CERT_KEY));
+    return conf;
+  }
+
+  private void encryptedShuffleWithCerts(boolean useClientCerts)
+    throws Exception {
+    try {
+      Configuration conf = new Configuration();
+      String keystoresDir = new File(BASEDIR).getAbsolutePath();
+      String sslConfsDir =
+        KeyStoreTestUtil.getClasspathDir(TestEncryptedShuffle.class);
+      KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfsDir, conf,
+                                      useClientCerts);
+      conf.setBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY, true);
+      startCluster(conf);
+      FileSystem fs = FileSystem.get(getJobConf(conf));
+      Path inputDir = new Path("input");
+      fs.mkdirs(inputDir);
+      Writer writer =
+        new OutputStreamWriter(fs.create(new Path(inputDir, "data.txt")));
+      writer.write("hello");
+      writer.close();
+
+      Path outputDir = new Path("output", "output");
+
+      JobConf jobConf = new JobConf(getJobConf(conf));
+      jobConf.setInt("mapred.map.tasks", 1);
+      jobConf.setInt("mapred.map.max.attempts", 1);
+      jobConf.setInt("mapred.reduce.max.attempts", 1);
+      jobConf.set("mapred.input.dir", inputDir.toString());
+      jobConf.set("mapred.output.dir", outputDir.toString());
+      JobClient jobClient = new JobClient(jobConf);
+      RunningJob runJob = jobClient.submitJob(jobConf);
+      runJob.waitForCompletion();
+      Assert.assertTrue(runJob.isComplete());
+      Assert.assertTrue(runJob.isSuccessful());
+    } finally {
+      stopCluster();
+    }
+  }
+
+  @Test
+  public void encryptedShuffleWithClientCerts() throws Exception {
+    encryptedShuffleWithCerts(true);
+  }
+
+  @Test
+  public void encryptedShuffleWithoutClientCerts() throws Exception {
+    encryptedShuffleWithCerts(false);
+  }
+
+}
+
-- 
1.7.0.4

