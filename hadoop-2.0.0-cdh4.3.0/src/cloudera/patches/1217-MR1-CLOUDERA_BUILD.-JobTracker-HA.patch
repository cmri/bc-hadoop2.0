From 88974b177887f61601b403b6f16613b26b452d0f Mon Sep 17 00:00:00 2001
From: Tom White <tom@cloudera.com>
Date: Thu, 4 Oct 2012 11:30:36 +0100
Subject: [PATCH 1217/1357] MR1: CLOUDERA_BUILD. JobTracker HA.

Reason: New feature
Ref: CDH-8379
Author: Tom White
---
 bin/hadoop                                         |   12 +
 ivy.xml                                            |    8 +-
 ivy/libraries.properties                           |    2 +
 .../mapred/ConfiguredFailoverProxyProvider.java    |  167 ++++++++++
 src/mapred/org/apache/hadoop/mapred/HAUtil.java    |  336 ++++++++++++++++++++
 .../apache/hadoop/mapred/InterTrackerProtocol.java |    8 +
 src/mapred/org/apache/hadoop/mapred/JobClient.java |   38 ++-
 .../hadoop/mapred/JobSubmissionProtocol.java       |   22 ++
 .../apache/hadoop/mapred/JobTrackerHADaemon.java   |  144 +++++++++
 .../hadoop/mapred/JobTrackerHAHttpRedirector.java  |  124 +++++++
 .../hadoop/mapred/JobTrackerHAServiceProtocol.java |  187 +++++++++++
 .../hadoop/mapred/JobTrackerHAServiceTarget.java   |  139 ++++++++
 .../apache/hadoop/mapred/JobTrackerProxies.java    |  189 +++++++++++
 .../hadoop/mapred/MapReducePolicyProvider.java     |    7 +
 .../apache/hadoop/mapred/SshFenceByTcpPort.java    |  324 +++++++++++++++++++
 .../org/apache/hadoop/mapred/StreamPumper.java     |   90 ++++++
 src/mapred/org/apache/hadoop/mapred/TaskLog.java   |    7 +
 .../org/apache/hadoop/mapred/TaskTracker.java      |   29 +-
 .../org/apache/hadoop/mapred/tools/MRHAAdmin.java  |   93 ++++++
 .../mapred/tools/MRZKFailoverController.java       |  192 +++++++++++
 .../hadoop/ha/TestMapredSshFenceByTcpPort.java     |  130 ++++++++
 .../org/apache/hadoop/mapred/MiniMRHACluster.java  |  308 ++++++++++++++++++
 .../mapred/TestHAStateTransitionFailure.java       |   80 +++++
 .../hadoop/mapred/TestHAStateTransitions.java      |  159 +++++++++
 src/test/org/apache/hadoop/mapred/TestHAUtil.java  |   91 ++++++
 src/test/org/apache/hadoop/mapred/TestHAWebUI.java |  105 ++++++
 .../hadoop/mapred/TestJobTrackerHealthCheck.java   |   71 ++++
 .../hadoop/mapred/TestMRZKFailoverController.java  |  229 +++++++++++++
 28 files changed, 3275 insertions(+), 16 deletions(-)
 create mode 100644 src/mapred/org/apache/hadoop/mapred/ConfiguredFailoverProxyProvider.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/HAUtil.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/JobTrackerHADaemon.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/JobTrackerHAHttpRedirector.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceProtocol.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceTarget.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/JobTrackerProxies.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/SshFenceByTcpPort.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/StreamPumper.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/tools/MRHAAdmin.java
 create mode 100644 src/mapred/org/apache/hadoop/mapred/tools/MRZKFailoverController.java
 create mode 100644 src/test/org/apache/hadoop/ha/TestMapredSshFenceByTcpPort.java
 create mode 100644 src/test/org/apache/hadoop/mapred/MiniMRHACluster.java
 create mode 100644 src/test/org/apache/hadoop/mapred/TestHAStateTransitionFailure.java
 create mode 100644 src/test/org/apache/hadoop/mapred/TestHAStateTransitions.java
 create mode 100644 src/test/org/apache/hadoop/mapred/TestHAUtil.java
 create mode 100644 src/test/org/apache/hadoop/mapred/TestHAWebUI.java
 create mode 100644 src/test/org/apache/hadoop/mapred/TestJobTrackerHealthCheck.java
 create mode 100644 src/test/org/apache/hadoop/mapred/TestMRZKFailoverController.java

diff --git a/bin/hadoop b/bin/hadoop
index 9bed6b0..ad45579 100755
--- a/bin/hadoop
+++ b/bin/hadoop
@@ -73,6 +73,9 @@ if [ $# = 0 ]; then
   echo "  balancer             run a cluster balancing utility"
   echo "  fetchdt              fetch a delegation token from the NameNode"
   echo "  jobtracker           run the MapReduce job Tracker node" 
+  echo "  jobtrackerhadaemon   run the Job Tracker HA daemon"
+  echo "  mrhaadmin            run a MapReduce HA admin client" 
+  echo "  mrzkfc               run the MapReduce ZK Failover Controller daemon" 
   echo "  pipes                run a Pipes job"
   echo "  tasktracker          run a MapReduce task Tracker node" 
   echo "  job                  manipulate MapReduce jobs"
@@ -241,6 +244,15 @@ elif [ "$COMMAND" = "fetchdt" ] ; then
 elif [ "$COMMAND" = "jobtracker" ] ; then
   CLASS=org.apache.hadoop.mapred.JobTracker
   HADOOP_OPTS="$HADOOP_OPTS $HADOOP_JOBTRACKER_OPTS"
+elif [ "$COMMAND" = "jobtrackerhadaemon" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobTrackerHADaemon
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_JOBTRACKER_OPTS"
+elif [ "$COMMAND" = "mrhaadmin" ] ; then
+  CLASS=org.apache.hadoop.mapred.tools.MRHAAdmin
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "mrzkfc" ] ; then
+  CLASS=org.apache.hadoop.mapred.tools.MRZKFailoverController
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_MRZKFC_OPTS"
 elif [ "$COMMAND" = "tasktracker" ] ; then
   CLASS=org.apache.hadoop.mapred.TaskTracker
   HADOOP_OPTS="$HADOOP_OPTS $HADOOP_TASKTRACKER_OPTS"
diff --git a/ivy.xml b/ivy.xml
index 64e4773..46276d9 100644
--- a/ivy.xml
+++ b/ivy.xml
@@ -126,7 +126,13 @@
                conf="compile->default"/>
    <dependency org="org.aspectj" name="aspectjtools" rev="${aspectj.version}"
                conf="compile->default"/>
-
+   <dependency org="org.apache.zookeeper" name="zookeeper" rev="${zookeeper.version}"
+               conf="compile->default"/>
+   <dependency org="org.apache.zookeeper" name="zookeeper" rev="${zookeeper.version}"
+               conf="test->default">
+     <artifact name="zookeeper" type="test-jar" ext="jar" m:classifier="tests"/>
+   </dependency>
+   
    <!-- Exclusions for transitive dependencies pulled in by log4j -->
    <exclude org="com.sun.jdmk"/>
    <exclude org="com.sun.jmx"/>
diff --git a/ivy/libraries.properties b/ivy/libraries.properties
index 6f599ad..dd851b9 100644
--- a/ivy/libraries.properties
+++ b/ivy/libraries.properties
@@ -81,3 +81,5 @@ slf4j-log4j12.version=1.5.11
 wagon-http.version=1.0-beta-2
 xmlenc.version=0.52
 xerces.version=1.4.4
+
+zookeeper.version=3.4.2
diff --git a/src/mapred/org/apache/hadoop/mapred/ConfiguredFailoverProxyProvider.java b/src/mapred/org/apache/hadoop/mapred/ConfiguredFailoverProxyProvider.java
new file mode 100644
index 0000000..8f4bf41
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/ConfiguredFailoverProxyProvider.java
@@ -0,0 +1,167 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.net.InetSocketAddress;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.ha.HAServiceProtocol;
+import org.apache.hadoop.io.retry.FailoverProxyProvider;
+import org.apache.hadoop.ipc.RPC;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Maps;
+
+/**
+ * A FailoverProxyProvider implementation which allows one to configure two URIs
+ * to connect to during fail-over. The first configured address is tried first,
+ * and on a fail-over event the other address is tried.
+ */
+public class ConfiguredFailoverProxyProvider<T> implements
+    FailoverProxyProvider<T> {
+  
+  private static final Log LOG =
+      LogFactory.getLog(ConfiguredFailoverProxyProvider.class);
+  
+  private final Configuration conf;
+  private final List<AddressRpcProxyPair<T>> proxies =
+      new ArrayList<AddressRpcProxyPair<T>>();
+  private final UserGroupInformation ugi;
+  private final Class<T> xface;
+  
+  private int currentProxyIndex = 0;
+
+  public ConfiguredFailoverProxyProvider(Configuration conf, String jtAddress,
+      Class<T> xface) {
+    Preconditions.checkArgument(
+        xface.isAssignableFrom(JTProtocols.class),
+        "Interface class %s is not a valid HAServiceProtocol protocol!", xface);
+    this.xface = xface;
+    
+    this.conf = new Configuration(conf);
+    int maxRetries = this.conf.getInt(
+        HAUtil.MR_CLIENT_FAILOVER_CONNECTION_RETRIES_KEY,
+        HAUtil.MR_CLIENT_FAILOVER_CONNECTION_RETRIES_DEFAULT);
+    this.conf.setInt(
+        CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,
+        maxRetries);
+    
+    int maxRetriesOnSocketTimeouts = this.conf.getInt(
+        HAUtil.MR_CLIENT_FAILOVER_CONNECTION_RETRIES_ON_SOCKET_TIMEOUTS_KEY,
+        HAUtil.MR_CLIENT_FAILOVER_CONNECTION_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT);
+    this.conf.setInt(
+        CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY,
+        maxRetriesOnSocketTimeouts);
+    
+    try {
+      ugi = UserGroupInformation.getCurrentUser();
+      
+      Map<String, Map<String, InetSocketAddress>> map = HAUtil.getHaJtRpcAddresses(
+          conf);
+      Map<String, InetSocketAddress> addressesInJT = map.get(jtAddress);
+      
+      if (addressesInJT == null || addressesInJT.size() == 0) {
+        throw new RuntimeException("Could not find any configured addresses " +
+            "for JT " + jtAddress);
+      }
+      
+      Collection<InetSocketAddress> addressesOfJTs = addressesInJT.values();
+      for (InetSocketAddress address : addressesOfJTs) {
+        proxies.add(new AddressRpcProxyPair<T>(address));
+      }
+
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+  
+
+
+  @Override
+  public Class<T> getInterface() {
+    return xface;
+  }
+
+  /**
+   * Lazily initialize the RPC proxy object.
+   */
+  @SuppressWarnings("unchecked")
+  @Override
+  public synchronized T getProxy() {
+    AddressRpcProxyPair current = proxies.get(currentProxyIndex);
+    if (current.jtHaDaemon == null) {
+      try {
+        current.jtHaDaemon = JobTrackerProxies.createNonHAProxy(conf,
+            current.address, xface, ugi, false).getProxy();
+      } catch (IOException e) {
+        LOG.error("Failed to create RPC proxy to JobTracker HA Daemon", e);
+        throw new RuntimeException(e);
+      }
+    }
+    return (T)current.jtHaDaemon;
+  }
+
+  @Override
+  public synchronized void performFailover(T currentProxy) {
+    currentProxyIndex = (currentProxyIndex + 1) % proxies.size();
+  }
+
+  /**
+   * A little pair object to store the address and connected RPC proxy object to
+   * a JT HA daemon. Note that {@link AddressRpcProxyPair#jtHaDaemon} may be
+   * null.
+   */
+  private static class AddressRpcProxyPair<T> {
+    public InetSocketAddress address;
+    public T jtHaDaemon;
+    
+    public AddressRpcProxyPair(InetSocketAddress address) {
+      this.address = address;
+    }
+  }
+
+  /**
+   * Close all the proxy objects which have been opened over the lifetime of
+   * this proxy provider.
+   */
+  @Override
+  public synchronized void close() throws IOException {
+    for (AddressRpcProxyPair<T> proxy : proxies) {
+      if (proxy.jtHaDaemon != null) {
+        if (proxy.jtHaDaemon instanceof Closeable) {
+          ((Closeable)proxy.jtHaDaemon).close();
+        } else {
+          RPC.stopProxy(proxy.jtHaDaemon);
+        }
+      }
+    }
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/HAUtil.java b/src/mapred/org/apache/hadoop/mapred/HAUtil.java
new file mode 100644
index 0000000..255de2c
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/HAUtil.java
@@ -0,0 +1,336 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+
+import java.net.InetSocketAddress;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.HadoopIllegalArgumentException;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.DFSUtil;
+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector;
+import org.apache.hadoop.net.NetUtils;
+
+public class HAUtil {
+  
+  private static final Log LOG = 
+    LogFactory.getLog(HAUtil.class);
+
+  public static final String  MR_JOBTRACKER_ADDRESS_KEY = "mapred.job.tracker";
+  public static final String  MR_JOBTRACKER_RPC_ADDRESS_KEY = "mapred.jobtracker.rpc-address";
+  public static final String  MR_JOBTRACKER_HTTP_ADDRESS_KEY = "mapred.job.tracker.http.address";
+  public static final String  MR_JOBTRACKER_OLD_HTTP_ADDRESS_KEY = "mapred.job.tracker.info.bindAddress";
+  public static final String  MR_JOBTRACKER_OLD_HTTP_PORT_KEY = "mapred.job.tracker.info.port";
+
+  // HA configuration
+  public static final String  MR_HA_JOBTRACKERS_KEY_PREFIX = "mapred.jobtrackers";
+  public static final String  MR_HA_JOBTRACKER_RPC_ADDRESS_KEY = "mapred.ha.jobtracker.rpc-address";
+  public static final String  MR_HA_JOBTRACKER_HTTP_REDIRECT_ADDRESS_KEY = "mapred.ha.jobtracker.http-redirect-address";
+  public static final String  MR_HA_JOBTRACKER_ID_KEY = "mapred.ha.jobtracker.id";
+  public static final String  MR_HA_FENCING_METHODS_KEY = "mapred.ha.fencing.methods";
+  public static final String  MR_HA_AUTO_FAILOVER_ENABLED_KEY = "mapred.ha.automatic-failover.enabled";
+  public static final boolean MR_HA_AUTO_FAILOVER_ENABLED_DEFAULT = false;
+  public static final String  MR_HA_ZKFC_PORT_KEY = "mapred.ha.zkfc.port";
+  public static final int     MR_HA_ZKFC_PORT_DEFAULT = 8019;
+
+  // Failover configuration
+  public static final String  MR_CLIENT_FAILOVER_PROXY_PROVIDER_KEY_PREFIX = "mapred.client.failover.proxy.provider";
+  public static final String  MR_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY = "mapred.client.failover.max.attempts";
+  public static final int     MR_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT = 15;
+  public static final String  MR_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY = "mapred.client.failover.sleep.base.millis";
+  public static final int     MR_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT = 500;
+  public static final String  MR_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY = "mapred.client.failover.sleep.max.millis";
+  public static final int     MR_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT = 15000;
+  public static final String  MR_CLIENT_FAILOVER_CONNECTION_RETRIES_KEY = "mapred.client.failover.connection.retries";
+  public static final int     MR_CLIENT_FAILOVER_CONNECTION_RETRIES_DEFAULT = 0;
+  public static final String  MR_CLIENT_FAILOVER_CONNECTION_RETRIES_ON_SOCKET_TIMEOUTS_KEY = "mapred.client.failover.connection.retries.on.timeouts";
+  public static final int     MR_CLIENT_FAILOVER_CONNECTION_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT = 0;
+
+  public static final String[] JOB_TRACKER_SPECIFIC_KEYS = {
+    MR_JOBTRACKER_RPC_ADDRESS_KEY,
+    MR_JOBTRACKER_HTTP_ADDRESS_KEY,
+    MR_JOBTRACKER_OLD_HTTP_ADDRESS_KEY,
+    MR_JOBTRACKER_OLD_HTTP_PORT_KEY,
+    MR_HA_ZKFC_PORT_KEY
+  };
+  
+  private interface AddressMatcher {
+    public boolean match(InetSocketAddress s);
+  }
+  
+  /**
+   * Address matcher for matching an address to local address
+   */
+  static final AddressMatcher LOCAL_ADDRESS_MATCHER = new AddressMatcher() {
+    @Override
+    public boolean match(InetSocketAddress s) {
+      return NetUtils.isLocalAddress(s.getAddress());
+    };
+  };
+
+  private HAUtil() { /* Hidden constructor */ }
+
+  /**
+   * Returns true if jobtracker HA is configured.
+   * 
+   * @return true if HA is configured in the configuration; else false.
+   */
+  public static boolean isHAEnabled() {
+    Configuration conf = new Configuration();
+    return isHAEnabled(conf, getLogicalName(conf));
+  }
+  
+  /**
+   * Returns true if jobtracker HA is configured.
+   * 
+   * @param conf Configuration
+   * @param jtAddress the (logical) jobtracker address
+   * @return true if HA is configured in the configuration; else false.
+   */
+  public static boolean isHAEnabled(Configuration conf, String jtAddress) {
+    Map<String, Map<String, InetSocketAddress>> addresses =
+      getHaJtRpcAddresses(conf);
+    if (addresses == null) return false;
+    Map<String, InetSocketAddress> jtMap = addresses.get(jtAddress);
+    return jtMap != null && jtMap.size() > 1;
+  }
+  
+  public static String getLogicalName(Configuration conf) {
+    return conf.get(MR_JOBTRACKER_ADDRESS_KEY);
+  }
+  
+  /**
+   * Get the jobtracker Id by matching the {@code addressKey}
+   * with the the address of the local node.
+   * 
+   * If {@link #MR_HA_JOBTRACKER_ID_KEY} is not specifically
+   * configured, this method determines the jobtracker Id by matching the local
+   * node's address with the configured addresses. When a match is found, it
+   * returns the JT Id from the corresponding configuration key.
+   * 
+   * @param conf Configuration
+   * @return jobtracker Id on success, null on failure.
+   * @throws HadoopIllegalArgumentException on error
+   */
+  public static String getJobTrackerId(Configuration conf) {
+    String jtId = conf.get(MR_HA_JOBTRACKER_ID_KEY);
+    if (jtId != null) {
+      return jtId;
+    }
+    String logicalName = getLogicalName(conf);
+    Collection<String> jtServiceIds = getJtServiceIds(conf, logicalName);
+    if (jtServiceIds.size() == 1) {
+      return jtServiceIds.toArray(new String[1])[0];
+    }
+    String suffixes[] = getSuffixIDs(conf, MR_JOBTRACKER_RPC_ADDRESS_KEY,
+        logicalName, LOCAL_ADDRESS_MATCHER);
+    if (suffixes == null) {
+      String msg = "Configuration " + MR_JOBTRACKER_RPC_ADDRESS_KEY + 
+          " must be suffixed with logicalName and jobtracker ID for HA " +
+          "configuration.";
+      throw new HadoopIllegalArgumentException(msg);
+    }
+    
+    return suffixes[1];
+  }
+  
+  /**
+   * Get the jobtracker Id of the other node in an HA setup.
+   * 
+   * If {@link #MR_HA_JOBTRACKER_ID_KEY} is not specifically
+   * configured, this method determines the jobtracker Id by matching the local
+   * node's address with the configured addresses. When a match is found, it
+   * returns the JT Id from the corresponding configuration key.
+   * 
+   * @param conf Configuration
+   * @return jobtracker Id on success, null on failure.
+   * @throws HadoopIllegalArgumentException on error
+   */
+  public static String getJobTrackerIdOfOtherNode(Configuration conf) {
+    String logicalName = getLogicalName(conf);
+    Collection<String> jtServiceIds = getJtServiceIds(conf, logicalName);
+    String myJtId = getJobTrackerId(conf);
+    ArrayList<String> jtSet = Lists.newArrayList(jtServiceIds);
+    jtSet.remove(myJtId);
+    assert jtSet.size() == 1;
+    return jtSet.get(0);
+  }
+
+  /**
+   * Returns logicalName and jobtracker Id when the local host matches the
+   * configuration parameter {@code addressKey}.<logical>.<jobtracker Id>
+   * 
+   * @param conf Configuration
+   * @param addressKey configuration key corresponding to the address.
+   * @param logicalName
+   * @param matcher matching criteria for matching the address
+   * @return Array with logical name and jobtracker Id on success. First element
+   *         in the array is logical name and second element is jobtracker Id.
+   *         Null value indicates that the configuration does not have the the
+   *         Id.
+   * @throws HadoopIllegalArgumentException on error
+   */
+  static String[] getSuffixIDs(final Configuration conf, final String addressKey,
+      String logicalName, final AddressMatcher matcher) {
+    String jobTrackerId = null;
+    int found = 0;
+    
+
+    Collection<String> jtIds = getJtServiceIds(conf, logicalName);
+    for (String jtId : emptyAsSingletonNull(jtIds)) {
+      if (LOG.isTraceEnabled()) {
+        LOG.trace(String.format("addressKey: %s logicalName: %s jtId: %s",
+            addressKey, logicalName, jtId));
+      }
+      String key = DFSUtil.addKeySuffixes(addressKey, logicalName, jtId);
+      String addr = conf.get(key);
+      if (addr == null) {
+        continue;
+      }
+      InetSocketAddress s = null;
+      try {
+        s = NetUtils.createSocketAddr(addr);
+      } catch (Exception e) {
+        LOG.warn("Exception in creating socket address " + addr, e);
+        continue;
+      }
+      if (!s.isUnresolved() && matcher.match(s)) {
+        jobTrackerId = jtId;
+        found++;
+      }
+    }
+
+    if (found > 1) { // Only one address must match the local address
+      String msg = "Configuration has multiple addresses that match "
+          + "local node's address. Please configure the system with "
+          + MR_HA_JOBTRACKER_ID_KEY;
+      throw new HadoopIllegalArgumentException(msg);
+    }
+    return new String[] { logicalName, jobTrackerId };
+  }
+  
+  /**
+   * @return <code>coll</code> if it is non-null and non-empty. Otherwise,
+   * returns a list with a single null value.
+   */
+  private static Collection<String> emptyAsSingletonNull(Collection<String> coll) {
+    if (coll == null || coll.isEmpty()) {
+      return Collections.singletonList(null);
+    } else {
+      return coll;
+    }
+  }
+
+  public static Map<String, Map<String, InetSocketAddress>> getHaJtRpcAddresses(
+      Configuration conf) {
+    
+    // For JT HA there can only be one logical name (unlike HDFS)
+    String logicalName = getLogicalName(conf);
+    Map<String, Map<String, InetSocketAddress>> ret = Maps.newHashMap();
+    Map<String, InetSocketAddress> map = Maps.newHashMap();
+    for (String jtId : getJtServiceIds(conf, logicalName)) {
+      String address = conf.get(
+          addKeySuffixes(MR_JOBTRACKER_RPC_ADDRESS_KEY, logicalName, jtId));
+      InetSocketAddress isa = NetUtils.createSocketAddr(address);
+      map.put(jtId, isa);
+    }
+    ret.put(logicalName, map);
+    return ret;
+  }
+
+  private static Collection<String> getJtServiceIds(Configuration conf,
+      String logicalName) {
+    return conf.getTrimmedStringCollection(MR_HA_JOBTRACKERS_KEY_PREFIX +
+        "." + logicalName);
+  }
+
+  
+  private static String getKey(String prefix, Configuration conf) {
+    String logicalName = getLogicalName(conf);
+    String jtId = conf.get(MR_HA_JOBTRACKER_ID_KEY);
+    if (jtId == null) {
+      throw new IllegalArgumentException(MR_HA_JOBTRACKER_ID_KEY + " not set.");
+    }
+    return addKeySuffixes(prefix, logicalName, jtId);
+  }
+  
+  public static InetSocketAddress getJtHaRpcAddress(Configuration conf) {
+    String address = conf.get(getKey(MR_HA_JOBTRACKER_RPC_ADDRESS_KEY, conf));
+    return NetUtils.createSocketAddr(address);
+  }
+  
+  public static void setJtHaRpcAddress(Configuration conf, String address) {
+    conf.set(getKey(MR_HA_JOBTRACKER_RPC_ADDRESS_KEY, conf), address);
+  }
+  
+  public static InetSocketAddress getJtHaRpcAddress(Configuration conf,
+      String jtId) {
+    String logicalName = getLogicalName(conf);
+    String address = conf.get(addKeySuffixes(MR_HA_JOBTRACKER_RPC_ADDRESS_KEY,
+        logicalName, jtId));
+    return NetUtils.createSocketAddr(address);
+  }
+
+  /**
+   * Set the JT address from the RPC address so that the wrapped JobTracker
+   * starts on the correct address.
+   */
+  public static void setJtRpcAddress(Configuration conf) {
+    String rpcAddress = conf.get(getKey(MR_JOBTRACKER_RPC_ADDRESS_KEY, conf));
+    conf.set(MR_JOBTRACKER_ADDRESS_KEY, rpcAddress);
+  }
+  
+  public static String getJtHaHttpRedirectAddress(Configuration conf,
+      String jtId) {
+    String logicalName = getLogicalName(conf);
+    return conf.get(addKeySuffixes(MR_HA_JOBTRACKER_HTTP_REDIRECT_ADDRESS_KEY,
+        logicalName, jtId));
+  }
+  
+  /**
+   * Return configuration key of format key.suffix1.suffix2...suffixN
+   */
+  public static String addKeySuffixes(String key, String... suffixes) {
+    return DFSUtil.addKeySuffixes(key, suffixes);
+  }
+  
+  public static void setGenericConf(Configuration conf,
+      String logicalName, String jtId, String... keys) {
+    for (String key : keys) {
+      String value = conf.get(addKeySuffixes(key, logicalName, jtId));
+      if (value != null) {
+        conf.set(key, value);
+        continue;
+      }
+      value = conf.get(addKeySuffixes(key, logicalName));
+      if (value != null) {
+        conf.set(key, value);
+      }
+    }
+  }
+
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 5961d7c..3aa3d76 100644
--- a/src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -20,6 +20,7 @@ package org.apache.hadoop.mapred;
 
 import java.io.IOException;
 
+import org.apache.hadoop.io.retry.Idempotent;
 import org.apache.hadoop.ipc.VersionedProtocol;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.security.KerberosInfo;
@@ -104,6 +105,7 @@ interface InterTrackerProtocol extends VersionedProtocol {
    * @return a {@link org.apache.hadoop.mapred.HeartbeatResponse} with 
    *         fresh instructions.
    */
+  @Idempotent
   HeartbeatResponse heartbeat(TaskTrackerStatus status, 
                               boolean restarted, 
                               boolean initialContact,
@@ -115,6 +117,7 @@ interface InterTrackerProtocol extends VersionedProtocol {
    * The task tracker calls this once, to discern where it can find
    * files referred to by the JobTracker
    */
+  @Idempotent
   public String getFilesystemName() throws IOException;
 
   /**
@@ -125,6 +128,7 @@ interface InterTrackerProtocol extends VersionedProtocol {
    * @throws IOException if there was a problem in communication or on the
    *                     remote side
    */
+  @Idempotent
   public void reportTaskTrackerError(String taskTracker,
                                      String errorClass,
                                      String errorMessage) throws IOException;
@@ -137,6 +141,7 @@ interface InterTrackerProtocol extends VersionedProtocol {
    * @return array of task completion events. 
    * @throws IOException
    */
+  @Idempotent
   TaskCompletionEvent[] getTaskCompletionEvents(JobID jobid, int fromEventId
       , int maxEvents) throws IOException;
 
@@ -145,15 +150,18 @@ interface InterTrackerProtocol extends VersionedProtocol {
    * 
    * @return the system directory where job-specific files are to be placed.
    */
+  @Idempotent
   public String getSystemDir();
   
   /**
    * Returns the VersionInfo build version of the JobTracker 
    */
+  @Idempotent
   public String getBuildVersion() throws IOException;
 
   /**
    * Returns the VersionInfo version of the JobTracker
    */
+  @Idempotent
   public String getVIVersion() throws IOException;
 }
diff --git a/src/mapred/org/apache/hadoop/mapred/JobClient.java b/src/mapred/org/apache/hadoop/mapred/JobClient.java
index 2f73196..20795b6 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobClient.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobClient.java
@@ -294,6 +294,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public float mapProgress() throws IOException {
       ensureFreshStatus();
+      if (status == null) {
+        return 0.0f;
+      }
       return status.mapProgress();
     }
 
@@ -303,6 +306,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public float reduceProgress() throws IOException {
       ensureFreshStatus();
+      if (status == null) {
+        return 0.0f;
+      }
       return status.reduceProgress();
     }
 
@@ -312,6 +318,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public float cleanupProgress() throws IOException {
       ensureFreshStatus();
+      if (status == null) {
+        return 0.0f;
+      }
       return status.cleanupProgress();
     }
 
@@ -321,6 +330,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public float setupProgress() throws IOException {
       ensureFreshStatus();
+      if (status == null) {
+        return 0.0f;
+      }
       return status.setupProgress();
     }
 
@@ -329,6 +341,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public synchronized boolean isComplete() throws IOException {
       updateStatus();
+      if (status == null) {
+        return false;
+      }
       return (status.getRunState() == JobStatus.SUCCEEDED ||
               status.getRunState() == JobStatus.FAILED ||
               status.getRunState() == JobStatus.KILLED);
@@ -339,6 +354,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public synchronized boolean isSuccessful() throws IOException {
       updateStatus();
+      if (status == null) {
+        return false;
+      }
       return status.getRunState() == JobStatus.SUCCEEDED;
     }
 
@@ -359,6 +377,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
      */
     public synchronized int getJobState() throws IOException {
       updateStatus();
+      if (status == null) {
+        return 0; // UNKNOWN
+      }
       return status.getRunState();
     }
     
@@ -415,8 +436,8 @@ public class JobClient extends Configured implements MRConstants, Tool  {
       return "Job: " + profile.getJobID() + "\n" + 
         "file: " + profile.getJobFile() + "\n" + 
         "tracking URL: " + profile.getURL() + "\n" + 
-        "map() completion: " + status.mapProgress() + "\n" + 
-        "reduce() completion: " + status.reduceProgress();
+        "map() completion: " + (status == null ? 0.0f : status.mapProgress()) + "\n" + 
+        "reduce() completion: " + (status == null ? 0.0f : status.reduceProgress());
     }
         
     /**
@@ -437,6 +458,9 @@ public class JobClient extends Configured implements MRConstants, Tool  {
       //we realized the job failed. SO we try avoiding 
       //a rpc by not calling updateStatus
       ensureFreshStatus();
+      if (status == null) {
+        return "NA";
+      }
       return status.getFailureInfo();
     }
 
@@ -501,8 +525,10 @@ public class JobClient extends Configured implements MRConstants, Tool  {
     if ("local".equals(tracker)) {
       conf.setNumMapTasks(1);
       this.jobSubmitClient = new LocalJobRunner(conf);
-    } else {
+    } else if (!HAUtil.isHAEnabled(conf, tracker)) {
       this.jobSubmitClient = createRPCProxy(JobTracker.getAddress(conf), conf);
+    } else {
+      this.jobSubmitClient = createRPCProxy(tracker, conf);
     }        
 
     // Read progress monitor poll interval from config. Default is 1 second.
@@ -523,6 +549,12 @@ public class JobClient extends Configured implements MRConstants, Tool  {
         NetUtils.getSocketFactory(conf, JobSubmissionProtocol.class));
   }
 
+  private static JobSubmissionProtocol createRPCProxy(String addr,
+      Configuration conf) throws IOException {
+    return JobTrackerProxies.createProxy(conf, addr,
+        JobSubmissionProtocol.class).getProxy();
+  }
+
   @InterfaceAudience.Private
   public static class Renewer extends TokenRenewer {
 
diff --git a/src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java b/src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java
index e9d7b4b..76cde1c 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java
@@ -21,6 +21,7 @@ package org.apache.hadoop.mapred;
 import java.io.IOException;
 
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.retry.Idempotent;
 import org.apache.hadoop.ipc.VersionedProtocol;
 import org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier;
 import org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSelector;
@@ -105,6 +106,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @param detailed if true then report tracker names and memory usage
    * @return summary of the state of the cluster
    */
+  @Idempotent
   public ClusterStatus getClusterStatus(boolean detailed) throws IOException;
 
   /**
@@ -115,6 +117,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    *         submitted to
    * @throws IOException
    */
+  @Idempotent
   public AccessControlList getQueueAdmins(String queueName) throws IOException;
 
   /**
@@ -141,37 +144,44 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * Grab a handle to a job that is already known to the JobTracker.
    * @return Profile of the job, or null if not found. 
    */
+  @Idempotent
   public JobProfile getJobProfile(JobID jobid) throws IOException;
 
   /**
    * Grab a handle to a job that is already known to the JobTracker.
    * @return Status of the job, or null if not found.
    */
+  @Idempotent
   public JobStatus getJobStatus(JobID jobid) throws IOException;
 
   /**
    * Grab the current job counters
    */
+  @Idempotent
   public Counters getJobCounters(JobID jobid) throws IOException;
     
   /**
    * Grab a bunch of info on the map tasks that make up the job
    */
+  @Idempotent
   public TaskReport[] getMapTaskReports(JobID jobid) throws IOException;
 
   /**
    * Grab a bunch of info on the reduce tasks that make up the job
    */
+  @Idempotent
   public TaskReport[] getReduceTaskReports(JobID jobid) throws IOException;
 
   /**
    * Grab a bunch of info on the cleanup tasks that make up the job
    */
+  @Idempotent
   public TaskReport[] getCleanupTaskReports(JobID jobid) throws IOException;
 
   /**
    * Grab a bunch of info on the setup tasks that make up the job
    */
+  @Idempotent
   public TaskReport[] getSetupTaskReports(JobID jobid) throws IOException;
 
   /**
@@ -180,6 +190,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * if dfs).  The client can then copy files into the right locations 
    * prior to submitting the job.
    */
+  @Idempotent
   public String getFilesystemName() throws IOException;
 
   /** 
@@ -187,12 +198,14 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @return array of JobStatus for the running/to-be-run
    * jobs.
    */
+  @Idempotent
   public JobStatus[] jobsToComplete() throws IOException;
     
   /** 
    * Get all the jobs submitted. 
    * @return array of JobStatus for the submitted jobs
    */
+  @Idempotent
   public JobStatus[] getAllJobs() throws IOException;
   
   /**
@@ -204,6 +217,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @return array of task completion events. 
    * @throws IOException
    */
+  @Idempotent
   public TaskCompletionEvent[] getTaskCompletionEvents(JobID jobid
       , int fromEventId, int maxEvents) throws IOException;
     
@@ -212,6 +226,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @param taskId the id of the task
    * @return an array of the diagnostic messages
    */
+  @Idempotent
   public String[] getTaskDiagnostics(TaskAttemptID taskId) 
   throws IOException;
 
@@ -220,6 +235,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * 
    * @return the system directory where job-specific files are to be placed.
    */
+  @Idempotent
   public String getSystemDir();  
   
   /**
@@ -228,6 +244,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * 
    * @return the directory where job-specific files are to be placed.
    */
+  @Idempotent
   public String getStagingAreaDir() throws IOException;
   
   /**
@@ -236,6 +253,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @return Array of the Job Queue Information Object
    * @throws IOException 
    */
+  @Idempotent
   public JobQueueInfo[] getQueues() throws IOException;
   
   /**
@@ -245,6 +263,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @return Scheduling Information of the Queue
    * @throws IOException 
    */
+  @Idempotent
   public JobQueueInfo getQueueInfo(String queue) throws IOException;
   
   /**
@@ -253,6 +272,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @return array of JobStatus for the submitted jobs
    * @throws IOException
    */
+  @Idempotent
   public JobStatus[] getJobsFromQueue(String queue) throws IOException;
   
   /**
@@ -260,6 +280,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @return array of QueueAclsInfo object for current user.
    * @throws IOException
    */
+  @Idempotent
   public QueueAclsInfo[] getQueueAclsForCurrentUser() throws IOException;
   
   /**
@@ -270,6 +291,7 @@ interface JobSubmissionProtocol extends VersionedProtocol {
    * @throws IOException
    * @throws InterruptedException
    */ 
+  @Idempotent
   public 
   Token<DelegationTokenIdentifier> getDelegationToken(Text renewer
                                                       ) throws IOException,
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTrackerHADaemon.java b/src/mapred/org/apache/hadoop/mapred/JobTrackerHADaemon.java
new file mode 100644
index 0000000..710c951
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/JobTrackerHADaemon.java
@@ -0,0 +1,144 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import com.google.protobuf.BlockingService;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ha.HAServiceProtocol;
+import org.apache.hadoop.ha.HAServiceStatus;
+import org.apache.hadoop.ha.proto.HAServiceProtocolProtos.HAServiceProtocolService;
+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB;
+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB;
+import org.apache.hadoop.ipc.ProtobufRpcEngine;
+import org.apache.hadoop.ipc.RPC;
+import org.apache.hadoop.ipc.WritableRpcEngine;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.util.StringUtils;
+
+public class JobTrackerHADaemon {
+  
+  static{
+    Configuration.addDefaultResource("mapred-default.xml");
+    Configuration.addDefaultResource("mapred-site.xml");
+  }
+  
+  private static final Log LOG =
+    LogFactory.getLog(JobTrackerHADaemon.class);
+  
+  private Configuration conf;
+  private JobTrackerHAServiceProtocol proto;
+  private RPC.Server rpcServer;
+  
+  public JobTrackerHADaemon(Configuration conf) {
+    this.conf = conf;
+  }
+  
+  public Configuration getConf() {
+    return conf;
+  }
+
+  public void start() throws IOException {
+    
+    Configuration jtConf = new Configuration(conf);
+    String logicalName = HAUtil.getLogicalName(jtConf);
+    String jtId = HAUtil.getJobTrackerId(jtConf);
+    HAUtil.setGenericConf(jtConf, logicalName, jtId, HAUtil.JOB_TRACKER_SPECIFIC_KEYS);
+    
+    this.proto = new JobTrackerHAServiceProtocol(jtConf);
+    
+    RPC.setProtocolEngine(conf, HAServiceProtocolPB.class,
+        ProtobufRpcEngine.class);
+    
+    HAServiceProtocolServerSideTranslatorPB haServiceProtocolXlator = 
+      new HAServiceProtocolServerSideTranslatorPB(proto);
+    BlockingService haPbService = HAServiceProtocolService
+        .newReflectiveBlockingService(haServiceProtocolXlator);
+    
+    WritableRpcEngine.ensureInitialized();
+    
+    InetSocketAddress rpcAddr = HAUtil.getJtHaRpcAddress(conf);
+
+    this.rpcServer = RPC.getServer(HAServiceProtocolPB.class, haPbService,
+        rpcAddr.getHostName(), rpcAddr.getPort(), conf);
+    
+    this.rpcServer.start();
+    
+    // set port in config
+    int port = rpcServer.getListenerAddress().getPort();
+    HAUtil.setJtHaRpcAddress(conf, rpcAddr.getHostName() + ":" + port);
+    LOG.info("Started " + getClass().getSimpleName() + " on port " + port);
+  }
+  
+  public void makeActive() throws IOException {
+    proto.transitionToActive(null);
+  }
+  
+  public JobTracker getJobTracker() {
+    return proto.getJobTracker();
+  }
+  
+  public JobTrackerHAServiceProtocol getJobTrackerHAServiceProtocol() {
+    return proto;
+  }
+  
+  public HAServiceStatus getServiceStatus() throws AccessControlException,
+      IOException {
+    return proto.getServiceStatus();
+  }
+  
+  public void stop() throws IOException {
+    proto.stop();
+    rpcServer.stop();
+  }
+  
+  /**
+   * Wait for service to finish.
+   * (Normally, it runs forever.)
+   */
+  public void join() {
+    try {
+      rpcServer.join();
+    } catch (InterruptedException ie) {
+      Thread.currentThread().interrupt();
+    }
+  }
+  
+  public static void startService() throws Exception {
+    JobTrackerHADaemon daemon = new JobTrackerHADaemon(new Configuration());
+    daemon.start();
+    daemon.join();
+  }
+  
+  public static void main(String[] args) {
+    try {
+      startService();
+    } catch (Throwable e) {
+      LOG.error(StringUtils.stringifyException(e));
+      System.exit(-1);
+    }
+  }
+  
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTrackerHAHttpRedirector.java b/src/mapred/org/apache/hadoop/mapred/JobTrackerHAHttpRedirector.java
new file mode 100644
index 0000000..d7bb374
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/JobTrackerHAHttpRedirector.java
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.http.HttpConfig;
+import org.apache.hadoop.http.HttpServer;
+import org.apache.hadoop.net.NetUtils;
+
+import javax.servlet.ServletException;
+import javax.servlet.http.HttpServlet;
+import javax.servlet.http.HttpServletRequest;
+import javax.servlet.http.HttpServletResponse;
+import java.io.IOException;
+import java.net.InetSocketAddress;
+
+public class JobTrackerHAHttpRedirector {
+  
+  private static final Log LOG =
+    LogFactory.getLog(JobTrackerHAHttpRedirector.class);
+
+  private static final String ACTIVE_JOBTRACKER_BASEURL =
+    "mapred.ha.active.jobtracker.baseurl";
+
+  private Configuration conf;
+  private String activeJobTrackerUrl;
+  private HttpServer server;
+
+  public JobTrackerHAHttpRedirector(Configuration conf) {
+    this.conf = conf;
+    this.activeJobTrackerUrl = getActiveJobTrackerUrl(conf);
+    if (activeJobTrackerUrl == null) {
+      LOG.warn("No redirect address configured. Set " +
+          HAUtil.MR_HA_JOBTRACKER_HTTP_REDIRECT_ADDRESS_KEY);
+    } else {
+      LOG.info("Redirect address is " + activeJobTrackerUrl);
+    }
+  }
+
+  private String getActiveJobTrackerUrl(Configuration conf) {
+    StringBuilder sb = new StringBuilder(HttpConfig.getSchemePrefix());
+    String otherJtId = HAUtil.getJobTrackerIdOfOtherNode(conf);
+    sb.append(HAUtil.getJtHaHttpRedirectAddress(conf, otherJtId));
+    return sb.toString();
+  }
+
+  public synchronized void start() throws Exception {
+    if (activeJobTrackerUrl == null) {
+      return;
+    }
+    if (server == null) {
+      @SuppressWarnings("deprecation")
+      String infoAddr =
+        NetUtils2.getServerAddress(conf,
+          HAUtil.MR_JOBTRACKER_OLD_HTTP_ADDRESS_KEY,
+          HAUtil.MR_JOBTRACKER_OLD_HTTP_PORT_KEY,
+          HAUtil.MR_JOBTRACKER_HTTP_ADDRESS_KEY);
+      LOG.info("Starting " + getClass().getSimpleName() + " on " + infoAddr);
+      InetSocketAddress infoSocAddr = NetUtils.createSocketAddr(infoAddr);
+      String infoBindAddress = infoSocAddr.getHostName();
+      int infoBindPort = infoSocAddr.getPort();
+      server = new HttpServer("static", infoBindAddress,
+        infoBindPort, false);
+      server.addInternalServlet("redirector", "/*", RedirectorServlet.class,
+        false);
+      server.setAttribute(ACTIVE_JOBTRACKER_BASEURL, activeJobTrackerUrl);
+      server.start();
+      LOG.info("Started");
+    } else {
+      throw new Exception("JobTrackerHAHttpRedirector already running");
+    }
+  }
+
+  public synchronized void stop() throws Exception {
+    if (server != null) {
+      LOG.info("Stopping " + getClass().getSimpleName() + " on port " +
+          server.getPort());
+      server.stop();
+      server = null;
+      LOG.info("Stopped");
+    }
+  }
+
+  public static class RedirectorServlet extends HttpServlet {
+    private String baseURL;
+
+    @Override
+    public void init() {
+      baseURL = (String)
+        getServletContext().getAttribute(ACTIVE_JOBTRACKER_BASEURL);
+    }
+
+    @Override
+    protected void doGet(HttpServletRequest req, HttpServletResponse resp)
+      throws ServletException, IOException {
+      StringBuilder sb = new StringBuilder(baseURL);
+      sb.append(req.getRequestURI());
+      String queryString = req.getQueryString();
+      if (queryString != null) {
+        sb.append("?").append(queryString);
+      }
+      resp.sendRedirect(sb.toString());
+    }
+  }
+
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceProtocol.java b/src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceProtocol.java
new file mode 100644
index 0000000..d53dad2
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceProtocol.java
@@ -0,0 +1,187 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static org.apache.hadoop.util.ExitUtil.terminate;
+
+import com.google.common.annotations.VisibleForTesting;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ha.HAServiceProtocol;
+import org.apache.hadoop.ha.HAServiceStatus;
+import org.apache.hadoop.ha.HealthCheckFailedException;
+import org.apache.hadoop.ha.ServiceFailedException;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.util.ExitUtil.ExitException;
+
+public class JobTrackerHAServiceProtocol implements HAServiceProtocol {
+  
+  private static final Log LOG =
+    LogFactory.getLog(JobTrackerHAServiceProtocol.class);
+
+  private Configuration conf;
+  private HAServiceState haState = HAServiceState.STANDBY;
+  private JobTracker jt;
+  private Thread jtThread;
+  private JobTrackerHAHttpRedirector httpRedirector;
+  
+  public JobTrackerHAServiceProtocol(Configuration conf) {
+    this.conf = conf;
+    this.httpRedirector = new JobTrackerHAHttpRedirector(conf);
+    try {
+      httpRedirector.start();
+    } catch (Throwable t) {
+      doImmediateShutdown(t);
+    }
+  }
+  
+  public JobTracker getJobTracker() {
+    return jt;
+  }
+  
+  @VisibleForTesting
+  Thread getJobTrackerThread() {
+    return jtThread;
+  }
+
+  @Override
+  public HAServiceStatus getServiceStatus() throws AccessControlException,
+      IOException {
+    HAServiceStatus ret = new HAServiceStatus(haState);
+    if (haState == HAServiceState.STANDBY || haState == HAServiceState.ACTIVE) {
+      ret.setReadyToBecomeActive();
+    } else {
+      ret.setNotReadyToBecomeActive("State is " + haState);
+    }
+    return ret;
+  }
+
+  @Override
+  public void monitorHealth() throws HealthCheckFailedException {
+    if (haState == HAServiceState.ACTIVE && jtThreadIsNotAlive()) {
+      throw new HealthCheckFailedException("The JobTracker thread is not running");
+    }
+  }
+
+  private boolean jtThreadIsNotAlive() {
+    return jtThread == null || !jtThread.isAlive();
+  }
+
+  @Override
+  public void transitionToActive(StateChangeRequestInfo reqInfo)
+      throws ServiceFailedException, AccessControlException, IOException {
+    if (haState == HAServiceState.ACTIVE) {
+      LOG.info("Already in active state.");
+      return;
+    }
+    LOG.info("Transitioning to active");
+    try {
+      httpRedirector.stop();
+      JobConf jtConf = new JobConf(conf);
+      // Update the conf for the JT so the address is resolved
+      HAUtil.setJtRpcAddress(jtConf);
+      jt = JobTracker.startTracker(jtConf);
+    } catch (Throwable t) {
+      doImmediateShutdown(t);
+    }
+    jtThread = new Thread(new Runnable() {
+      @Override
+      public void run() {
+        try {
+          jt.offerService();
+        } catch (Throwable t) {
+          doImmediateShutdown(t);
+        }
+      }
+    });
+    jtThread.start();
+    haState = HAServiceState.ACTIVE;
+    LOG.info("Transitioned to active");
+  }
+
+  @Override
+  public void transitionToStandby(StateChangeRequestInfo reqInfo)
+      throws ServiceFailedException, AccessControlException, IOException {
+    if (haState == HAServiceState.STANDBY) {
+      LOG.info("Already in standby state.");
+      return;
+    }
+    LOG.info("Transitioning to standby");
+    try {
+      if (jt != null) {
+        jt.close();
+      }
+      if (jtThread != null) {
+        jtThread.join();
+      }
+      httpRedirector.start();
+    } catch (Throwable t) {
+      doImmediateShutdown(t);
+    }
+    jt = null;
+    jtThread = null;
+    haState = HAServiceState.STANDBY;
+    LOG.info("Transitioned to standby");
+  }
+  
+  public void stop() {
+    LOG.info("Stopping");
+    try {
+      if (jt != null) {
+        jt.close();
+      }
+      if (jtThread != null) {
+        jtThread.join();
+      }
+      httpRedirector.stop();
+    } catch (Throwable t) {
+      doImmediateShutdown(t);
+    }
+    jt = null;
+    jtThread = null;
+    haState = HAServiceState.STANDBY;
+    LOG.info("Stopped");
+  }
+  
+  /**
+   * Shutdown the JT immediately in an ungraceful way. Used when it would be
+   * unsafe for the JT to continue operating, e.g. during a failed HA state
+   * transition.
+   * 
+   * @param t exception which warrants the shutdown. Printed to the JT log
+   *          before exit.
+   * @throws ExitException thrown only for testing.
+   */
+  private synchronized void doImmediateShutdown(Throwable t)
+      throws ExitException {
+    String message = "Error encountered requiring JT shutdown. " +
+        "Shutting down immediately.";
+    try {
+      LOG.fatal(message, t);
+    } catch (Throwable ignored) {
+      // This is unlikely to happen, but there's nothing we can do if it does.
+    }
+    terminate(1, t);
+  }
+
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceTarget.java b/src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceTarget.java
new file mode 100644
index 0000000..dc28e9d
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/JobTrackerHAServiceTarget.java
@@ -0,0 +1,139 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import com.google.common.base.Preconditions;
+
+import java.net.InetSocketAddress;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ha.BadFencingConfigurationException;
+import org.apache.hadoop.ha.FenceMethod;
+import org.apache.hadoop.ha.HAServiceTarget;
+import org.apache.hadoop.ha.NodeFencer;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.tools.DFSZKFailoverController;
+import org.apache.hadoop.mapred.tools.MRZKFailoverController;
+
+import com.google.common.base.Preconditions;
+
+public class JobTrackerHAServiceTarget extends HAServiceTarget {
+  
+  private static final Log LOG =
+    LogFactory.getLog(JobTrackerHAServiceTarget.class);
+  
+  private final InetSocketAddress addr;
+  private InetSocketAddress zkfcAddr;
+  private NodeFencer fencer;
+  private BadFencingConfigurationException fenceConfigError;
+  private String logicalName;
+  private String jtId;
+  private final boolean autoFailoverEnabled;
+  
+  public JobTrackerHAServiceTarget(Configuration conf) {
+    this(conf, HAUtil.getJobTrackerId(conf));
+  }
+  
+  public JobTrackerHAServiceTarget(Configuration conf, String jtId) {
+    this.logicalName = HAUtil.getLogicalName(conf);
+    this.jtId = jtId;
+
+    // Make a copy of the conf, and override configs based on the
+    // target node -- not the node we happen to be running on.
+    JobConf targetConf = new JobConf(conf);
+    HAUtil.setGenericConf(targetConf, logicalName, jtId, HAUtil.JOB_TRACKER_SPECIFIC_KEYS);
+    
+    this.addr = HAUtil.getJtHaRpcAddress(targetConf, jtId);
+
+    this.autoFailoverEnabled = targetConf.getBoolean(
+        HAUtil.MR_HA_AUTO_FAILOVER_ENABLED_KEY,
+        HAUtil.MR_HA_AUTO_FAILOVER_ENABLED_DEFAULT);
+    if (autoFailoverEnabled) {
+      int port = MRZKFailoverController.getZkfcPort(targetConf);
+      if (port != 0) {
+        setZkfcPort(port);
+      }
+    }
+    try {
+      fencer = NodeFencer.create(targetConf, HAUtil.MR_HA_FENCING_METHODS_KEY);
+    } catch (BadFencingConfigurationException e) {
+      this.fenceConfigError = e;
+    }
+  }
+
+  @Override
+  public void checkFencingConfigured() throws BadFencingConfigurationException {
+    if (fenceConfigError != null) {
+      throw fenceConfigError;
+    }
+    if (fencer == null) {
+      throw new BadFencingConfigurationException(
+          "No fencer configured for " + this);
+    }
+  }
+
+  @Override
+  public InetSocketAddress getAddress() {
+    return addr;
+  }
+
+  @Override
+  public NodeFencer getFencer() {
+    return fencer;
+  }
+
+  @Override
+  public InetSocketAddress getZKFCAddress() {
+    Preconditions.checkState(autoFailoverEnabled,
+        "ZKFC address not relevant when auto failover is off");
+    assert zkfcAddr != null;
+
+    return zkfcAddr;
+  }
+  
+  public void setZkfcPort(int port) {
+    assert autoFailoverEnabled;
+    
+    this.zkfcAddr = new InetSocketAddress(addr.getAddress(), port);
+  }
+  
+  @Override
+  public boolean isAutoFailoverEnabled() {
+    return autoFailoverEnabled;
+  }
+
+  public String getLogicalName() {
+    return logicalName;
+  }
+  
+  public String getJobTrackerId() {
+    return jtId;
+  }
+  
+  @Override
+  public String toString() {
+    return "JobTrackerHAServiceTarget at " + addr;
+  }
+  
+
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/JobTrackerProxies.java b/src/mapred/org/apache/hadoop/mapred/JobTrackerProxies.java
new file mode 100644
index 0000000..efb0fe6
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/JobTrackerProxies.java
@@ -0,0 +1,189 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import com.google.common.base.Preconditions;
+
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.net.InetSocketAddress;
+import java.net.URI;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.retry.DefaultFailoverProxyProvider;
+import org.apache.hadoop.io.retry.FailoverProxyProvider;
+import org.apache.hadoop.io.retry.RetryPolicies;
+import org.apache.hadoop.io.retry.RetryPolicy;
+import org.apache.hadoop.io.retry.RetryProxy;
+import org.apache.hadoop.ipc.*;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+
+public class JobTrackerProxies {
+  
+  private static final Log LOG = LogFactory.getLog(JobTrackerProxies.class);
+  
+  /**
+   * Wrapper for a client proxy as well as its associated service ID.
+   * This is simply used as a tuple-like return type for
+   * {@link JobTrackerProxies#createProxy} and
+   * {@link JobTrackerProxies#createNonHAProxy}.
+   */
+  public static class ProxyAndInfo<PROXYTYPE> {
+    private final PROXYTYPE proxy;
+    private final Text dtService;
+    
+    public ProxyAndInfo(PROXYTYPE proxy, Text dtService) {
+      this.proxy = proxy;
+      this.dtService = dtService;
+    }
+    
+    public PROXYTYPE getProxy() {
+      return proxy;
+    }
+    
+    public Text getDelegationTokenService() {
+      return dtService;
+    }
+  }
+  
+  @SuppressWarnings("unchecked")
+  public static <T> ProxyAndInfo<T> createProxy(Configuration conf,
+      String jtAddress, Class<T> xface) throws IOException {
+    Class<FailoverProxyProvider<T>> failoverProxyProviderClass =
+        getFailoverProxyProviderClass(conf, jtAddress, xface);
+  
+    if (failoverProxyProviderClass == null) {
+      // Non-HA case
+      return createNonHAProxy(conf, NetUtils.createSocketAddr(jtAddress), xface,
+          UserGroupInformation.getCurrentUser(), true);
+    } else {
+      // HA case
+      FailoverProxyProvider<T> failoverProxyProvider = 
+          createFailoverProxyProvider(conf, failoverProxyProviderClass, xface,
+              jtAddress);
+      int maxFailoverAttempts =
+        conf.getInt(HAUtil.MR_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,
+            HAUtil.MR_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);
+      long failoverSleepBaseMillis =
+        conf.getInt(HAUtil.MR_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,
+            HAUtil.MR_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);
+      long failoverSleepMaxMillis =
+        conf.getInt(HAUtil.MR_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,
+            HAUtil.MR_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);
+      T proxy = (T) RetryProxy.create(xface, failoverProxyProvider, RetryPolicies
+          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,
+              maxFailoverAttempts, failoverSleepBaseMillis,
+              failoverSleepMaxMillis));
+      
+      Text dtService = new Text(jtAddress);
+      return new ProxyAndInfo<T>(proxy, dtService);
+    }
+  }
+  
+  private static <T> Class<FailoverProxyProvider<T>> getFailoverProxyProviderClass(
+      Configuration conf, String jtAddress, Class<T> xface) throws IOException {
+    if (jtAddress == null) {
+      return null;
+    }
+    String configKey = HAUtil.MR_CLIENT_FAILOVER_PROXY_PROVIDER_KEY_PREFIX +
+      "." + jtAddress;
+    return (Class<FailoverProxyProvider<T>>)
+      conf.getClass(configKey, null, FailoverProxyProvider.class);
+  }
+  
+  public static <T> ProxyAndInfo<T> createNonHAProxy(
+      Configuration conf, InetSocketAddress jtAddr, Class<T> xface,
+      UserGroupInformation ugi, boolean withRetries) throws IOException {
+    
+    Text dtService = SecurityUtil.buildTokenService(jtAddr);
+    
+    T proxy;
+    if (xface == JobSubmissionProtocol.class) {
+      proxy = (T) createJTProxyWithJobSubmissionProtocol(jtAddr, conf, ugi,
+          withRetries);
+    } else if (xface == InterTrackerProtocol.class) {
+      proxy = (T) createJTProxyWithInterTrackerProtocol(jtAddr, conf, ugi,
+          withRetries);
+    } else {
+      throw new IOException("wrong protocol");
+    }
+    return new ProxyAndInfo<T>(proxy, dtService);
+  }
+  
+  private static JobSubmissionProtocol createJTProxyWithJobSubmissionProtocol(
+      InetSocketAddress address, Configuration conf, UserGroupInformation ugi,
+      boolean withRetries) throws IOException {
+    RPC.setProtocolEngine(conf, JobSubmissionProtocol.class, WritableRpcEngine.class);
+
+    final long version = RPC.getProtocolVersion(JobSubmissionProtocol.class);
+    
+    RPC.getProxy(JobSubmissionProtocol.class, version, address, ugi, conf,
+        NetUtils.getDefaultSocketFactory(conf), 0);
+    
+    JobSubmissionProtocol proxy = RPC.getProtocolProxy(
+        JobSubmissionProtocol.class, version, address, ugi, conf,
+        NetUtils.getDefaultSocketFactory(conf), 0, null).getProxy();
+    return proxy;
+  }
+  
+  private static InterTrackerProtocol createJTProxyWithInterTrackerProtocol(
+      InetSocketAddress address, Configuration conf, UserGroupInformation ugi,
+      boolean withRetries) throws IOException {
+    RPC.setProtocolEngine(conf, InterTrackerProtocol.class, WritableRpcEngine.class);
+
+    return RPC.waitForProtocolProxy(
+        InterTrackerProtocol.class, InterTrackerProtocol.versionID, address,
+        conf).getProxy();
+  }
+  
+  @SuppressWarnings("unchecked")
+  private static <T> FailoverProxyProvider<T> createFailoverProxyProvider(
+      Configuration conf, Class<FailoverProxyProvider<T>> failoverProxyProviderClass,
+      Class<T> xface, String jtAddress) throws IOException {
+    Preconditions.checkArgument(
+        xface.isAssignableFrom(JTProtocols.class),
+        "Interface %s is not a JobTracker protocol", xface);
+    try {
+      Constructor<FailoverProxyProvider<T>> ctor = failoverProxyProviderClass
+          .getConstructor(Configuration.class, String.class, Class.class);
+      FailoverProxyProvider<?> provider = ctor.newInstance(conf, jtAddress,
+          xface);
+      return (FailoverProxyProvider<T>) provider;
+    } catch (Exception e) {
+      String message = "Couldn't create proxy provider " + failoverProxyProviderClass;
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(message, e);
+      }
+      if (e.getCause() instanceof IOException) {
+        throw (IOException) e.getCause();
+      } else {
+        throw new IOException(message, e);
+      }
+    }
+  }
+
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/MapReducePolicyProvider.java b/src/mapred/org/apache/hadoop/mapred/MapReducePolicyProvider.java
index 7c01d9d..307543d 100644
--- a/src/mapred/org/apache/hadoop/mapred/MapReducePolicyProvider.java
+++ b/src/mapred/org/apache/hadoop/mapred/MapReducePolicyProvider.java
@@ -17,6 +17,9 @@
  */
 package org.apache.hadoop.mapred;
 
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.ha.HAServiceProtocol;
+import org.apache.hadoop.ha.ZKFCProtocol;
 import org.apache.hadoop.mr1security.RefreshUserMappingsProtocol;
 import org.apache.hadoop.security.authorize.PolicyProvider;
 import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;
@@ -43,6 +46,10 @@ public class MapReducePolicyProvider extends PolicyProvider {
                   AdminOperationsProtocol.class),
       new Service("security.get.user.mappings.protocol.acl",
                   GetUserMappingsProtocol.class),
+      new Service(CommonConfigurationKeys.SECURITY_HA_SERVICE_PROTOCOL_ACL,
+          HAServiceProtocol.class),
+      new Service(CommonConfigurationKeys.SECURITY_ZKFC_PROTOCOL_ACL,
+          ZKFCProtocol.class),
   };
   
   @Override
diff --git a/src/mapred/org/apache/hadoop/mapred/SshFenceByTcpPort.java b/src/mapred/org/apache/hadoop/mapred/SshFenceByTcpPort.java
new file mode 100644
index 0000000..7523aa0
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/SshFenceByTcpPort.java
@@ -0,0 +1,324 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.ha.BadFencingConfigurationException;
+import org.apache.hadoop.ha.FenceMethod;
+import org.apache.hadoop.ha.HAServiceTarget;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.jcraft.jsch.ChannelExec;
+import com.jcraft.jsch.JSch;
+import com.jcraft.jsch.JSchException;
+import com.jcraft.jsch.Session;
+
+/**
+ * NOTE: This is a copy of org.apache.hadoop.ha.SshFenceByTcpPort that uses
+ * MR-specific configuration options (since the original is hardcoded to HDFS
+ * configuration properties so there is no way to run MR and HDFS fencing using
+ * a single configuration file).
+ * 
+ * This fencing implementation sshes to the target node and uses 
+ * <code>fuser</code> to kill the process listening on the service's
+ * TCP port. This is more accurate than using "jps" since it doesn't 
+ * require parsing, and will work even if there are multiple service
+ * processes running on the same machine.<p>
+ * It returns a successful status code if:
+ * <ul>
+ * <li><code>fuser</code> indicates it successfully killed a process, <em>or</em>
+ * <li><code>nc -z</code> indicates that nothing is listening on the target port
+ * </ul>
+ * <p>
+ * This fencing mechanism is configured as following in the fencing method
+ * list:
+ * <code>sshfence([[username][:ssh-port]])</code>
+ * where the optional argument specifies the username and port to use
+ * with ssh.
+ * <p>
+ * In order to achieve passwordless SSH, the operator must also configure
+ * <code>mapred.ha.fencing.ssh.private-key-files<code> to point to an
+ * SSH key that has passphrase-less access to the given username and host.
+ */
+public class SshFenceByTcpPort extends Configured
+  implements FenceMethod {
+
+  public static final Log LOG = LogFactory.getLog(
+      SshFenceByTcpPort.class);
+  
+  public static final String CONF_CONNECT_TIMEOUT_KEY =
+    "mapred.ha.fencing.ssh.connect-timeout";
+  private static final int CONF_CONNECT_TIMEOUT_DEFAULT =
+    30*1000;
+  public static final String CONF_IDENTITIES_KEY =
+    "mapred.ha.fencing.ssh.private-key-files";
+
+  /**
+   * Verify that the argument, if given, in the conf is parseable.
+   */
+  @Override
+  public void checkArgs(String argStr) throws BadFencingConfigurationException {
+    if (argStr != null) {
+      new Args(argStr);
+    }
+  }
+
+  @Override
+  public boolean tryFence(HAServiceTarget target, String argsStr)
+      throws BadFencingConfigurationException {
+
+    Args args = new Args(argsStr);
+    InetSocketAddress serviceAddr = target.getAddress();
+    String host = serviceAddr.getHostName();
+    
+    Session session;
+    try {
+      session = createSession(serviceAddr.getHostName(), args);
+    } catch (JSchException e) {
+      LOG.warn("Unable to create SSH session", e);
+      return false;
+    }
+
+    LOG.info("Connecting to " + host + "...");
+    
+    try {
+      session.connect(getSshConnectTimeout());
+    } catch (JSchException e) {
+      LOG.warn("Unable to connect to " + host
+          + " as user " + args.user, e);
+      return false;
+    }
+    LOG.info("Connected to " + host);
+
+    try {
+      return doFence(session, serviceAddr);
+    } catch (JSchException e) {
+      LOG.warn("Unable to achieve fencing on remote host", e);
+      return false;
+    } finally {
+      session.disconnect();
+    }
+  }
+
+
+  private Session createSession(String host, Args args) throws JSchException {
+    JSch jsch = new JSch();
+    for (String keyFile : getKeyFiles()) {
+      jsch.addIdentity(keyFile);
+    }
+    JSch.setLogger(new LogAdapter());
+
+    Session session = jsch.getSession(args.user, host, args.sshPort);
+    session.setConfig("StrictHostKeyChecking", "no");
+    return session;
+  }
+
+  private boolean doFence(Session session, InetSocketAddress serviceAddr)
+      throws JSchException {
+    int port = serviceAddr.getPort();
+    try {
+      LOG.info("Looking for process running on port " + port);
+      int rc = execCommand(session,
+          "PATH=$PATH:/sbin:/usr/sbin fuser -v -k -n tcp " + port);
+      if (rc == 0) {
+        LOG.info("Successfully killed process that was " +
+            "listening on port " + port);
+        // exit code 0 indicates the process was successfully killed.
+        return true;
+      } else if (rc == 1) {
+        // exit code 1 indicates either that the process was not running
+        // or that fuser didn't have root privileges in order to find it
+        // (eg running as a different user)
+        LOG.info(
+            "Indeterminate response from trying to kill service. " +
+            "Verifying whether it is running using nc...");
+        rc = execCommand(session, "nc -z " + serviceAddr.getHostName() +
+            " " + serviceAddr.getPort());
+        if (rc == 0) {
+          // the service is still listening - we are unable to fence
+          LOG.warn("Unable to fence - it is running but we cannot kill it");
+          return false;
+        } else {
+          LOG.info("Verified that the service is down.");
+          return true;          
+        }
+      } else {
+        // other 
+      }
+      LOG.info("rc: " + rc);
+      return rc == 0;
+    } catch (InterruptedException e) {
+      LOG.warn("Interrupted while trying to fence via ssh", e);
+      return false;
+    } catch (IOException e) {
+      LOG.warn("Unknown failure while trying to fence via ssh", e);
+      return false;
+    }
+  }
+  
+  /**
+   * Execute a command through the ssh session, pumping its
+   * stderr and stdout to our own logs.
+   */
+  private int execCommand(Session session, String cmd)
+      throws JSchException, InterruptedException, IOException {
+    LOG.debug("Running cmd: " + cmd);
+    ChannelExec exec = null;
+    try {
+      exec = (ChannelExec)session.openChannel("exec");
+      exec.setCommand(cmd);
+      exec.setInputStream(null);
+      exec.connect();
+
+      // Pump stdout of the command to our WARN logs
+      StreamPumper outPumper = new StreamPumper(LOG, cmd + " via ssh",
+          exec.getInputStream(), StreamPumper.StreamType.STDOUT);
+      outPumper.start();
+      
+      // Pump stderr of the command to our WARN logs
+      StreamPumper errPumper = new StreamPumper(LOG, cmd + " via ssh",
+          exec.getErrStream(), StreamPumper.StreamType.STDERR);
+      errPumper.start();
+      
+      outPumper.join();
+      errPumper.join();
+      return exec.getExitStatus();
+    } finally {
+      cleanup(exec);
+    }
+  }
+
+  private static void cleanup(ChannelExec exec) {
+    if (exec != null) {
+      try {
+        exec.disconnect();
+      } catch (Throwable t) {
+        LOG.warn("Couldn't disconnect ssh channel", t);
+      }
+    }
+  }
+
+  private int getSshConnectTimeout() {
+    return getConf().getInt(
+        CONF_CONNECT_TIMEOUT_KEY, CONF_CONNECT_TIMEOUT_DEFAULT);
+  }
+
+  private Collection<String> getKeyFiles() {
+    return getConf().getTrimmedStringCollection(CONF_IDENTITIES_KEY);
+  }
+  
+  /**
+   * Container for the parsed arg line for this fencing method.
+   */
+  @VisibleForTesting
+  public static class Args {
+    private static final Pattern USER_PORT_RE = Pattern.compile(
+      "([^:]+?)?(?:\\:(\\d+))?");
+
+    private static final int DEFAULT_SSH_PORT = 22;
+
+    public String user;
+    public int sshPort;
+    
+    public Args(String arg) 
+        throws BadFencingConfigurationException {
+      user = System.getProperty("user.name");
+      sshPort = DEFAULT_SSH_PORT;
+
+      // Parse optional user and ssh port
+      if (arg != null && !arg.isEmpty()) {
+        Matcher m = USER_PORT_RE.matcher(arg);
+        if (!m.matches()) {
+          throw new BadFencingConfigurationException(
+              "Unable to parse user and SSH port: "+ arg);
+        }
+        if (m.group(1) != null) {
+          user = m.group(1);
+        }
+        if (m.group(2) != null) {
+          sshPort = parseConfiggedPort(m.group(2));
+        }
+      }
+    }
+
+    private Integer parseConfiggedPort(String portStr)
+        throws BadFencingConfigurationException {
+      try {
+        return Integer.valueOf(portStr);
+      } catch (NumberFormatException nfe) {
+        throw new BadFencingConfigurationException(
+            "Port number '" + portStr + "' invalid");
+      }
+    }
+  }
+
+  /**
+   * Adapter from JSch's logger interface to our log4j
+   */
+  private static class LogAdapter implements com.jcraft.jsch.Logger {
+    static final Log LOG = LogFactory.getLog(
+        SshFenceByTcpPort.class.getName() + ".jsch");
+
+    @Override
+    public boolean isEnabled(int level) {
+      switch (level) {
+      case com.jcraft.jsch.Logger.DEBUG:
+        return LOG.isDebugEnabled();
+      case com.jcraft.jsch.Logger.INFO:
+        return LOG.isInfoEnabled();
+      case com.jcraft.jsch.Logger.WARN:
+        return LOG.isWarnEnabled();
+      case com.jcraft.jsch.Logger.ERROR:
+        return LOG.isErrorEnabled();
+      case com.jcraft.jsch.Logger.FATAL:
+        return LOG.isFatalEnabled();
+      default:
+        return false;
+      }
+    }
+      
+    @Override
+    public void log(int level, String message) {
+      switch (level) {
+      case com.jcraft.jsch.Logger.DEBUG:
+        LOG.debug(message);
+        break;
+      case com.jcraft.jsch.Logger.INFO:
+        LOG.info(message);
+        break;
+      case com.jcraft.jsch.Logger.WARN:
+        LOG.warn(message);
+        break;
+      case com.jcraft.jsch.Logger.ERROR:
+        LOG.error(message);
+        break;
+      case com.jcraft.jsch.Logger.FATAL:
+        LOG.fatal(message);
+        break;
+      }
+    }
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/StreamPumper.java b/src/mapred/org/apache/hadoop/mapred/StreamPumper.java
new file mode 100644
index 0000000..3e3690a
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/StreamPumper.java
@@ -0,0 +1,90 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+
+import org.apache.commons.logging.Log;
+
+/**
+ * Class responsible for pumping the streams of the subprocess
+ * out to log4j. stderr is pumped to WARN level and stdout is
+ * pumped to INFO level
+ */
+class StreamPumper {
+  enum StreamType {
+    STDOUT, STDERR;
+  }
+
+  private final Log log;
+  
+  final Thread thread;
+  final String logPrefix;
+  final StreamPumper.StreamType type;
+  private final InputStream stream;
+  private boolean started = false;
+  
+  StreamPumper(final Log log, final String logPrefix,
+      final InputStream stream, final StreamType type) {
+    this.log = log;
+    this.logPrefix = logPrefix;
+    this.stream = stream;
+    this.type = type;
+    
+    thread = new Thread(new Runnable() {
+      @Override
+      public void run() {
+        try {
+          pump();
+        } catch (Throwable t) {
+          SshFenceByTcpPort.LOG.warn(logPrefix +
+              ": Unable to pump output from " + type,
+              t);
+        }
+      }
+    }, logPrefix + ": StreamPumper for " + type);
+    thread.setDaemon(true);
+  }
+  
+  void join() throws InterruptedException {
+    assert started;
+    thread.join();
+  }
+
+  void start() {
+    assert !started;
+    thread.start();
+    started = true;
+  }
+
+  protected void pump() throws IOException {
+    InputStreamReader inputStreamReader = new InputStreamReader(stream);
+    BufferedReader br = new BufferedReader(inputStreamReader);
+    String line = null;
+    while ((line = br.readLine()) != null) {
+      if (type == StreamType.STDOUT) {
+        log.info(logPrefix + ": " + line);
+      } else {
+        log.warn(logPrefix + ": " + line);          
+      }
+    }
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskLog.java b/src/mapred/org/apache/hadoop/mapred/TaskLog.java
index e0decfd..9b0d56a 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskLog.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskLog.java
@@ -109,6 +109,13 @@ public class TaskLog {
     String strAttemptLogDir = getTaskAttemptLogDir(taskID, 
         cleanupSuffix, localDirs);
     File attemptLogDir = new File(strAttemptLogDir);
+    if (attemptLogDir.exists()) {
+      // Delete attempt log dir if it already exists on this TT and we are doing
+      // job recovery
+      if (!FileUtil.fullyDelete(attemptLogDir)) {
+        throw new IOException("Deletion of existing " + attemptLogDir + " failed.");
+      }
+    }
     if (!attemptLogDir.mkdirs()) {
       throw new IOException("Creation of " + attemptLogDir + " failed.");
     }
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
index f9f2d3a..8a12b7e 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
@@ -288,7 +288,7 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
   private LocalDirAllocator localDirAllocator;
   String taskTrackerName;
   String localHostname;
-  InetSocketAddress jobTrackAddr;
+  String jobTrackAddr;
     
   InetSocketAddress taskReportAddress;
 
@@ -1000,15 +1000,20 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
         new TrackerDistributedCacheManager(this.fConf, taskController, asyncDiskService);
     this.distributedCacheManager.purgeCache(); // TODO(todd) purge here?
 
-    this.jobClient = (InterTrackerProtocol) 
-    UserGroupInformation.getLoginUser().doAs(
-        new PrivilegedExceptionAction<Object>() {
-      public Object run() throws IOException {
-        return RPC.waitForProxy(InterTrackerProtocol.class,
-            InterTrackerProtocol.versionID,
-            jobTrackAddr, fConf);
-      }
-    });
+    if (!HAUtil.isHAEnabled(fConf, jobTrackAddr)) {
+      this.jobClient = (InterTrackerProtocol) 
+      UserGroupInformation.getLoginUser().doAs(
+          new PrivilegedExceptionAction<Object>() {
+        public Object run() throws IOException {
+          return RPC.waitForProxy(InterTrackerProtocol.class,
+              InterTrackerProtocol.versionID,
+              NetUtils.createSocketAddr(jobTrackAddr), fConf);
+        }
+      });
+    } else {
+      this.jobClient = JobTrackerProxies.createProxy(fConf, 
+            jobTrackAddr, InterTrackerProtocol.class).getProxy();
+    }
     this.justInited = true;
     this.running = true;    
     // start the thread that will fetch map task completion events
@@ -1676,7 +1681,7 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
     diskHealthCheckInterval = conf.getLong(DISK_HEALTH_CHECK_INTERVAL_PROPERTY,
                                            DEFAULT_DISK_HEALTH_CHECK_INTERVAL);
     aclsManager = new ACLsManager(conf, new JobACLsManager(conf), null);
-    this.jobTrackAddr = JobTracker.getAddress(conf);
+    this.jobTrackAddr = conf.get("mapred.job.tracker", "localhost:8012");
     String infoAddr = 
       NetUtils2.getServerAddress(conf,
                                 "tasktracker.http.bindAddress", 
@@ -2080,7 +2085,7 @@ public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
                                        maxReduceSlots); 
       }
     } else {
-      LOG.info("Resending 'status' to '" + jobTrackAddr.getHostName() +
+      LOG.info("Resending 'status' to '" + jobTrackAddr +
                "' with reponseId '" + heartbeatResponseId);
     }
       
diff --git a/src/mapred/org/apache/hadoop/mapred/tools/MRHAAdmin.java b/src/mapred/org/apache/hadoop/mapred/tools/MRHAAdmin.java
new file mode 100644
index 0000000..f56066a
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/tools/MRHAAdmin.java
@@ -0,0 +1,93 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.tools;
+
+import java.io.PrintStream;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.ha.HAAdmin;
+import org.apache.hadoop.ha.HAServiceTarget;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.tools.DFSHAAdmin;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobTracker;
+import org.apache.hadoop.mapred.JobTrackerHAServiceTarget;
+import org.apache.hadoop.util.ToolRunner;
+
+public class MRHAAdmin extends HAAdmin {
+  
+  private static final Log LOG = LogFactory.getLog(MRHAAdmin.class);
+
+  protected void setErrOut(PrintStream errOut) {
+    this.errOut = errOut;
+  }
+  
+  protected void setOut(PrintStream out) {
+    this.out = out;
+  }
+  
+  @Override
+  public void setConf(Configuration conf) {
+    if (conf != null) {
+      conf = addSecurityConfiguration(conf);
+    }
+    super.setConf(conf);
+  }
+  
+  /**
+   * Add the requisite security principal settings to the given Configuration,
+   * returning a copy.
+   * @param conf the original config
+   * @return a copy with the security settings added
+   */
+  public static Configuration addSecurityConfiguration(Configuration conf) {
+    // Make a copy so we don't mutate it. Also use a JobConf to
+    // force loading of mapred-site.xml.
+    conf = new JobConf(conf);
+    String jobTrackerPrincipal = conf.get(
+        JobTracker.JT_USER_NAME, "");
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Using JT principal: " + jobTrackerPrincipal);
+    }
+
+    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY,
+        jobTrackerPrincipal);
+    return conf;
+  }
+  
+  @Override
+  protected HAServiceTarget resolveTarget(String jtId) {
+    JobConf conf = new JobConf(getConf()); // force load mapred-site.xml
+    return new JobTrackerHAServiceTarget(conf, jtId);
+  }
+  
+  @Override
+  protected String getUsageString() {
+    return "Usage: MRHAAdmin";
+  }
+
+  public static void main(String[] argv) throws Exception {
+    int res = ToolRunner.run(new MRHAAdmin(), argv);
+    System.exit(res);
+  }
+}
diff --git a/src/mapred/org/apache/hadoop/mapred/tools/MRZKFailoverController.java b/src/mapred/org/apache/hadoop/mapred/tools/MRZKFailoverController.java
new file mode 100644
index 0000000..1f8721f
--- /dev/null
+++ b/src/mapred/org/apache/hadoop/mapred/tools/MRZKFailoverController.java
@@ -0,0 +1,192 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.tools;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.HadoopIllegalArgumentException;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ha.HAServiceTarget;
+import org.apache.hadoop.ha.ZKFailoverController;
+import org.apache.hadoop.hdfs.DFSUtil;
+import org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos.ActiveNodeInfo;
+import org.apache.hadoop.ipc.Server;
+import org.apache.hadoop.mapred.HAUtil;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobTracker;
+import org.apache.hadoop.mapred.JobTrackerHAServiceTarget;
+import org.apache.hadoop.mapred.MapReducePolicyProvider;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.authorize.AccessControlList;
+import org.apache.hadoop.security.authorize.PolicyProvider;
+import org.apache.hadoop.security.authorize.Service;
+import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.StringUtils;
+
+import com.google.protobuf.InvalidProtocolBufferException;
+
+@InterfaceAudience.Private
+public class MRZKFailoverController extends ZKFailoverController {
+
+  private static final Log LOG =
+    LogFactory.getLog(MRZKFailoverController.class);
+  private AccessControlList adminAcl;
+  /* the same as superclass's localTarget, but with the more specfic JT type */
+  private final JobTrackerHAServiceTarget localJTTarget;
+
+  @Override
+  protected HAServiceTarget dataToTarget(byte[] data) {
+    ActiveNodeInfo proto;
+    try {
+      proto = ActiveNodeInfo.parseFrom(data);
+    } catch (InvalidProtocolBufferException e) {
+      throw new RuntimeException("Invalid data in ZK: " +
+          StringUtils.byteToHexString(data));
+    }
+    JobTrackerHAServiceTarget ret = new JobTrackerHAServiceTarget(
+        conf, proto.getNamenodeId());
+    InetSocketAddress addressFromProtobuf = new InetSocketAddress(
+        proto.getHostname(), proto.getPort());
+    
+    if (!addressFromProtobuf.equals(ret.getAddress())) {
+      throw new RuntimeException("Mismatched address stored in ZK for " +
+          ret + ": Stored protobuf was " + addressFromProtobuf + ", address from our own " +
+          "configuration for this JobTracker was " + ret.getAddress());
+    }
+    
+    ret.setZkfcPort(proto.getZkfcPort());
+    return ret;
+  }
+
+  @Override
+  protected byte[] targetToData(HAServiceTarget target) {
+    InetSocketAddress addr = target.getAddress();
+
+    return ActiveNodeInfo.newBuilder()
+      .setHostname(addr.getHostName())
+      .setPort(addr.getPort())
+      .setZkfcPort(target.getZKFCAddress().getPort())
+      .setNameserviceId(localJTTarget.getLogicalName())
+      .setNamenodeId(localJTTarget.getJobTrackerId())
+      .build()
+      .toByteArray();
+  }
+  
+  @Override
+  protected InetSocketAddress getRpcAddressToBindTo() {
+    int zkfcPort = getZkfcPort(conf);
+    return new InetSocketAddress(localTarget.getAddress().getAddress(),
+          zkfcPort);
+  }
+  
+
+  @Override
+  protected PolicyProvider getPolicyProvider() {
+    return new MapReducePolicyProvider();
+  }
+  
+  public static int getZkfcPort(Configuration conf) {
+    return conf.getInt(HAUtil.MR_HA_ZKFC_PORT_KEY,
+        HAUtil.MR_HA_ZKFC_PORT_DEFAULT);
+  }
+  
+  public static MRZKFailoverController create(Configuration conf) {
+    Configuration localJTConf = MRHAAdmin.addSecurityConfiguration(conf);
+    
+    if (!HAUtil.isHAEnabled(localJTConf, HAUtil.getLogicalName(localJTConf))) {
+      throw new HadoopIllegalArgumentException(
+          "HA is not enabled for this jobtracker.");
+    }
+    String jtId = HAUtil.getJobTrackerId(localJTConf);
+    HAUtil.setGenericConf(localJTConf, "logicaljt", jtId, HAUtil.JOB_TRACKER_SPECIFIC_KEYS);
+    HAUtil.setGenericConf(localJTConf, "logicaljt", jtId, ZKFC_CONF_KEYS);
+    
+    JobTrackerHAServiceTarget localTarget = new JobTrackerHAServiceTarget(
+        localJTConf, jtId);
+    return new MRZKFailoverController(localJTConf, localTarget);
+  }
+
+  private MRZKFailoverController(Configuration conf,
+      JobTrackerHAServiceTarget localTarget) {
+    super(conf, localTarget);
+    this.localJTTarget = localTarget;
+    // Setup ACLs
+    adminAcl = new AccessControlList(
+        conf.get("mapreduce.cluster.administrators", " "));
+    LOG.info("Failover controller configured for JobTracker " +
+        localTarget);
+  }
+  
+  
+  @Override
+  protected void initRPC() throws IOException {
+    super.initRPC();
+    localJTTarget.setZkfcPort(rpcServer.getAddress().getPort());
+  }
+
+  @Override
+  public void loginAsFCUser() throws IOException {
+    InetSocketAddress socAddr = NetUtils.createSocketAddr(
+        conf.get(HAUtil.MR_JOBTRACKER_RPC_ADDRESS_KEY));
+    SecurityUtil.login(conf, JobTracker.JT_KEYTAB_FILE,
+        JobTracker.JT_USER_NAME, socAddr.getHostName());
+  }
+  
+  @Override
+  protected String getScopeInsideParentNode() {
+    return HAUtil.getLogicalName(conf);
+  }
+
+  public static void main(String args[])
+      throws Exception {
+    if (DFSUtil.parseHelpArgument(args, 
+        ZKFailoverController.USAGE, System.out, true)) {
+      System.exit(0);
+    }
+    
+    GenericOptionsParser parser = new GenericOptionsParser(
+        new JobConf(), args);
+    MRZKFailoverController zkfc = MRZKFailoverController.create(
+        parser.getConfiguration());
+    
+    System.exit(zkfc.run(parser.getRemainingArgs()));
+  }
+
+  @Override
+  protected void checkRpcAdminAccess() throws IOException, AccessControlException {
+    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
+    UserGroupInformation zkfcUgi = UserGroupInformation.getLoginUser();
+    if (adminAcl.isUserAllowed(ugi) ||
+        ugi.getShortUserName().equals(zkfcUgi.getShortUserName())) {
+      LOG.info("Allowed RPC access from " + ugi + " at " + Server.getRemoteAddress());
+      return;
+    }
+    String msg = "Disallowed RPC access from " + ugi + " at " +
+        Server.getRemoteAddress() + ". Not listed in " +
+        "mapreduce.cluster.administrators"; 
+    LOG.warn(msg);
+    throw new AccessControlException(msg);
+  }
+}
diff --git a/src/test/org/apache/hadoop/ha/TestMapredSshFenceByTcpPort.java b/src/test/org/apache/hadoop/ha/TestMapredSshFenceByTcpPort.java
new file mode 100644
index 0000000..1752360
--- /dev/null
+++ b/src/test/org/apache/hadoop/ha/TestMapredSshFenceByTcpPort.java
@@ -0,0 +1,130 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.ha;
+
+import static org.junit.Assert.*;
+
+import java.net.InetSocketAddress;
+
+import org.apache.commons.logging.impl.Log4JLogger;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;
+import org.apache.hadoop.mapred.SshFenceByTcpPort;
+import org.apache.hadoop.mapred.SshFenceByTcpPort.Args;
+import org.apache.log4j.Level;
+import org.junit.Assume;
+import org.junit.Test;
+
+public class TestMapredSshFenceByTcpPort {
+
+  static {
+    ((Log4JLogger)SshFenceByTcpPort.LOG).getLogger().setLevel(Level.ALL);
+  }
+  
+  private static String TEST_FENCING_HOST = System.getProperty(
+      "test.TestSshFenceByTcpPort.host", "localhost");
+  private static final String TEST_FENCING_PORT = System.getProperty(
+      "test.TestSshFenceByTcpPort.port", "8020");
+  private static final String TEST_KEYFILE = System.getProperty(
+      "test.TestSshFenceByTcpPort.key");
+  
+  private static final InetSocketAddress TEST_ADDR =
+    new InetSocketAddress(TEST_FENCING_HOST,
+      Integer.valueOf(TEST_FENCING_PORT));
+  private static final HAServiceTarget TEST_TARGET =
+    new DummyHAService(HAServiceState.ACTIVE, TEST_ADDR);
+  
+  /**
+   *  Connect to Google's DNS server - not running ssh!
+   */
+  private static final HAServiceTarget UNFENCEABLE_TARGET =
+    new DummyHAService(HAServiceState.ACTIVE,
+        new InetSocketAddress("8.8.8.8", 1234));
+
+  @Test(timeout=20000)
+  public void testFence() throws BadFencingConfigurationException {
+    Assume.assumeTrue(isConfigured());
+    Configuration conf = new Configuration();
+    conf.set(SshFenceByTcpPort.CONF_IDENTITIES_KEY, TEST_KEYFILE);
+    SshFenceByTcpPort fence = new SshFenceByTcpPort();
+    fence.setConf(conf);
+    assertTrue(fence.tryFence(
+        TEST_TARGET,
+        null));
+  }
+
+  /**
+   * Test connecting to a host which definitely won't respond.
+   * Make sure that it times out and returns false, but doesn't throw
+   * any exception
+   */
+  @Test(timeout=20000)
+  public void testConnectTimeout() throws BadFencingConfigurationException {
+    Configuration conf = new Configuration();
+    conf.setInt(SshFenceByTcpPort.CONF_CONNECT_TIMEOUT_KEY, 3000);
+    SshFenceByTcpPort fence = new SshFenceByTcpPort();
+    fence.setConf(conf);
+    assertFalse(fence.tryFence(UNFENCEABLE_TARGET, ""));
+  }
+  
+  @Test
+  public void testArgsParsing() throws BadFencingConfigurationException {
+    Args args = new SshFenceByTcpPort.Args(null);
+    assertEquals(System.getProperty("user.name"), args.user);
+    assertEquals(22, args.sshPort);
+    
+    args = new SshFenceByTcpPort.Args("");
+    assertEquals(System.getProperty("user.name"), args.user);
+    assertEquals(22, args.sshPort);
+
+    args = new SshFenceByTcpPort.Args("12345");
+    assertEquals("12345", args.user);
+    assertEquals(22, args.sshPort);
+
+    args = new SshFenceByTcpPort.Args(":12345");
+    assertEquals(System.getProperty("user.name"), args.user);
+    assertEquals(12345, args.sshPort);
+
+    args = new SshFenceByTcpPort.Args("foo:2222");
+    assertEquals("foo", args.user);
+    assertEquals(2222, args.sshPort);
+  }
+  
+  @Test
+  public void testBadArgsParsing() throws BadFencingConfigurationException {
+    assertBadArgs(":");          // No port specified
+    assertBadArgs("bar.com:");   // "
+    assertBadArgs(":xx");        // Port does not parse
+    assertBadArgs("bar.com:xx"); // "
+  }
+  
+  private void assertBadArgs(String argStr) {
+    try {
+      new Args(argStr);
+      fail("Did not fail on bad args: " + argStr);
+    } catch (BadFencingConfigurationException e) {
+      // Expected
+    }
+  }
+
+  private boolean isConfigured() {
+    return (TEST_FENCING_HOST != null && !TEST_FENCING_HOST.isEmpty()) &&
+           (TEST_FENCING_PORT != null && !TEST_FENCING_PORT.isEmpty()) &&
+           (TEST_KEYFILE != null && !TEST_KEYFILE.isEmpty());
+  }
+}
diff --git a/src/test/org/apache/hadoop/mapred/MiniMRHACluster.java b/src/test/org/apache/hadoop/mapred/MiniMRHACluster.java
new file mode 100644
index 0000000..26f3c3d
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/MiniMRHACluster.java
@@ -0,0 +1,308 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.MiniMRHACluster.TaskTrackerRunner;
+import org.apache.hadoop.mapreduce.Cluster.JobTrackerStatus;
+import org.apache.hadoop.util.ExitUtil;
+
+public class MiniMRHACluster {
+  
+  private static final Log LOG = 
+    LogFactory.getLog(MiniMRHACluster.class);
+
+  private Configuration conf; // client and tt configuration
+  private List<JobTrackerHADaemon> jtHaDaemonList = new ArrayList<JobTrackerHADaemon>();
+
+  private List<TaskTrackerRunner> taskTrackerList = new ArrayList<TaskTrackerRunner>();
+  private List<Thread> taskTrackerThreadList = new ArrayList<Thread>();
+  
+  public MiniMRHACluster() throws IOException, InterruptedException {
+    this(new Configuration());
+  }
+  public MiniMRHACluster(Configuration conf) throws IOException, InterruptedException {
+    this(conf, 2, 1);
+  }
+  public MiniMRHACluster(Configuration conf, int numJobTrackers, int numTaskTrackers) throws IOException, InterruptedException {
+    this.conf = conf;
+    ExitUtil.disableSystemExit();
+    configureLogicalName(conf);
+    
+    for (int i = 0; i < numJobTrackers; i++) {
+      JobTrackerHADaemon jtHaDaemon = createJobTrackerHADaemon(conf, "jt" + (i + 1));
+      jtHaDaemon.start();
+      jtHaDaemonList.add(jtHaDaemon);
+      Thread.sleep(1000); // wait so jt identifiers are different
+    }
+  }
+  
+  public JobTrackerHADaemon getJobTrackerHaDaemon(int index) {
+    return jtHaDaemonList.get(index);
+  }
+  
+  public Configuration getClientConf() {
+    return conf;
+  }
+  
+  private static JobTrackerHADaemon createJobTrackerHADaemon(Configuration conf, String jtId) {
+    Configuration c = new Configuration(conf);
+    c.setBoolean("mapred.jobtracker.restart.recover", true); // recover jobs
+    configureLogicalName(c);
+    c.set(HAUtil.MR_HA_JOBTRACKER_ID_KEY, jtId);
+    return new JobTrackerHADaemon(c);
+  }
+  
+  private static void configureLogicalName(Configuration conf) {
+    String logicalName = "logicaljt";
+    String jt1Id = "jt1";
+    String jt2Id = "jt2";
+    
+    // Use hardcoded port numbers as using ephemeral ports for JTs doesn't work
+    // since they are not
+    // resolved until the JTs are started, and the standby JT will
+    // not start until failover, which is too late for the client
+    String jt1Address = "localhost:1234";
+    String jt2Address = "localhost:5678";
+    String jt1HaAddress = "localhost:12340";
+    String jt2HaAddress = "localhost:56780";
+    String jt1HttpAddress = "0.0.0.0:50030";
+    String jt2HttpAddress = "0.0.0.0:50031";
+    String jt1HttpRedirectAddress = "localhost:50030";
+    String jt2HttpRedirectAddress = "localhost:50031";
+    
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_JOBTRACKER_RPC_ADDRESS_KEY, logicalName, jt1Id), jt1Address);
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_JOBTRACKER_RPC_ADDRESS_KEY, logicalName, jt2Id), jt2Address);
+    
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_HA_JOBTRACKER_RPC_ADDRESS_KEY, logicalName, jt1Id), jt1HaAddress);
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_HA_JOBTRACKER_RPC_ADDRESS_KEY, logicalName, jt2Id), jt2HaAddress);
+
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_JOBTRACKER_HTTP_ADDRESS_KEY, logicalName, jt1Id), jt1HttpAddress);
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_JOBTRACKER_HTTP_ADDRESS_KEY, logicalName, jt2Id), jt2HttpAddress);
+
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_HA_JOBTRACKER_HTTP_REDIRECT_ADDRESS_KEY, logicalName, jt1Id), jt1HttpRedirectAddress);
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_HA_JOBTRACKER_HTTP_REDIRECT_ADDRESS_KEY, logicalName, jt2Id), jt2HttpRedirectAddress);
+
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_HA_JOBTRACKERS_KEY_PREFIX, logicalName), jt1Id + "," + jt2Id);
+    conf.set(HAUtil.addKeySuffixes(HAUtil.MR_CLIENT_FAILOVER_PROXY_PROVIDER_KEY_PREFIX, logicalName), ConfiguredFailoverProxyProvider.class.getName());
+    
+    conf.set(HAUtil.MR_JOBTRACKER_ADDRESS_KEY, logicalName);
+  }
+  
+  // Wait until at least one JT is active
+  public void waitActive() throws IOException {
+    while (true) {
+      for (JobTrackerHADaemon jtHaDaemon : jtHaDaemonList) {
+        JobTracker jt = jtHaDaemon.getJobTracker();
+        if (jt != null) {
+          if (jt.getClusterStatus().getJobTrackerStatus() == JobTrackerStatus.RUNNING) {
+            return;
+          }
+        }
+      }
+      try {
+        Thread.sleep(1000);
+      } catch (InterruptedException ie) {}
+    }
+  }
+  
+  public void startTaskTracker(int idx, int numDir) throws IOException {
+    TaskTrackerRunner taskTracker;
+    taskTracker = new TaskTrackerRunner(idx, numDir, null, new JobConf(conf));
+    
+    addTaskTracker(taskTracker);
+  }
+  
+  void addTaskTracker(TaskTrackerRunner taskTracker) {
+    Thread taskTrackerThread = new Thread(taskTracker);
+    taskTrackerList.add(taskTracker);
+    taskTrackerThreadList.add(taskTrackerThread);
+    taskTrackerThread.start();
+  }
+
+  private void waitTaskTrackers() {
+    for(Iterator<TaskTrackerRunner> itr= taskTrackerList.iterator(); itr.hasNext();) {
+      TaskTrackerRunner runner = itr.next();
+      while (!runner.isDead && (!runner.isInitialized || !runner.tt.isIdle())) {
+        if (!runner.isInitialized) {
+          LOG.info("Waiting for task tracker to start.");
+        } else {
+          LOG.info("Waiting for task tracker " + runner.tt.getName() +
+                   " to be idle.");
+        }
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException ie) {}
+      }
+    }
+  }
+  
+  public void shutdownJobTracker(int index) {
+    JobTrackerHADaemon jtHaDaemon = jtHaDaemonList.get(index);
+    try {
+      jtHaDaemon.stop();
+    } catch (IOException ex) {
+      LOG.error("Problem shutting down jobtracker HA daemon", ex);
+    }
+    jtHaDaemon.join();
+  }
+  
+  public void shutdown() {
+    waitTaskTrackers();
+    for (int idx = 0; idx < taskTrackerList.size(); idx++) {
+      TaskTrackerRunner taskTracker = taskTrackerList.get(idx);
+      Thread taskTrackerThread = taskTrackerThreadList.get(idx);
+      taskTracker.shutdown();
+      taskTrackerThread.interrupt();
+      try {
+        taskTrackerThread.join();
+      } catch (InterruptedException ex) {
+        LOG.error("Problem shutting down task tracker", ex);
+      }
+    }
+    for (JobTrackerHADaemon jtHaDaemon : jtHaDaemonList) {
+      try {
+        jtHaDaemon.stop();
+      } catch (IOException ex) {
+        LOG.error("Problem shutting down jobtracker HA daemon", ex);
+      }
+      jtHaDaemon.join();
+    }
+  }
+
+  static class TaskTrackerRunner implements Runnable {
+    volatile TaskTracker tt;
+    int trackerId;
+    // the localDirs for this taskTracker
+    String[] localDirs;
+    volatile boolean isInitialized = false;
+    volatile boolean isDead = false;
+    volatile boolean exited = false;
+    int numDir;
+
+    public TaskTrackerRunner(int trackerId, int numDir, String hostname, 
+                                    JobConf cfg) 
+    throws IOException {
+      this.trackerId = trackerId;
+      this.numDir = numDir;
+      localDirs = new String[numDir];
+      final JobConf conf = cfg;
+
+      if (hostname != null) {
+        conf.set("slave.host.name", hostname);
+      }
+      conf.set("mapred.task.tracker.http.address", "0.0.0.0:0");
+      conf.set("mapred.task.tracker.report.address", 
+                "127.0.0.1:0");
+      File localDirBase = 
+        new File(conf.get("mapred.local.dir")).getAbsoluteFile();
+      localDirBase.mkdirs();
+      StringBuffer localPath = new StringBuffer();
+      for(int i=0; i < numDir; ++i) {
+        File ttDir = new File(localDirBase, 
+                              Integer.toString(trackerId) + "_" + i);
+        if (!ttDir.mkdirs()) {
+          if (!ttDir.isDirectory()) {
+            throw new IOException("Mkdirs failed to create " + ttDir);
+          }
+        }
+        localDirs[i] = ttDir.toString();
+        if (i != 0) {
+          localPath.append(",");
+        }
+        localPath.append(localDirs[i]);
+      }
+      conf.set("mapred.local.dir", localPath.toString());
+      try {
+        tt = createTaskTracker(conf);
+
+        isInitialized = true;
+      } catch (Throwable e) {
+        isDead = true;
+        tt = null;
+        e.printStackTrace();
+      }
+    }
+     
+    /**
+     * Creates a default {@link TaskTracker} using the conf passed. 
+     */
+    TaskTracker createTaskTracker(JobConf conf)
+        throws IOException, InterruptedException {
+      return new TaskTracker(conf);
+    }
+    
+    /**
+     * Create and run the task tracker.
+     */
+    public void run() {
+      try {
+        if (tt != null) {
+          tt.run();
+        }
+      } catch (Throwable e) {
+        isDead = true;
+        tt = null;
+        e.printStackTrace();
+      }
+      exited = true;
+    }
+ 
+    /**
+     * Get the local dir for this TaskTracker.
+     * This is there so that we do not break
+     * previous tests. 
+     * @return the absolute pathname
+     */
+    public String getLocalDir() {
+      return localDirs[0];
+    }
+       
+    public String[] getLocalDirs(){
+      return localDirs;
+    } 
+    
+    public TaskTracker getTaskTracker() {
+      return tt;
+    }
+    
+    /**
+     * Shut down the server and wait for it to finish.
+     */
+    public void shutdown() {
+      if (tt != null) {
+        try {
+          tt.shutdown();
+        } catch (Throwable e) {
+          e.printStackTrace();
+
+        }
+      }
+    }
+  }
+
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestHAStateTransitionFailure.java b/src/test/org/apache/hadoop/mapred/TestHAStateTransitionFailure.java
new file mode 100644
index 0000000..19395ce
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/TestHAStateTransitionFailure.java
@@ -0,0 +1,80 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import static org.apache.hadoop.test.GenericTestUtils.assertExceptionContains;
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.util.ExitUtil.ExitException;
+import org.junit.Test;
+
+/**
+ * Tests to verify the behavior of failing to fully start transition HA states.
+ */
+public class TestHAStateTransitionFailure {
+
+  /**
+   * Ensure that a failure to fully transition to the active state causes a
+   * shutdown of the jobtracker.
+   */
+  @Test
+  public void testFailureToTransitionCausesShutdown() throws Exception {
+    MiniDFSCluster dfs = null;
+    MiniMRHACluster cluster = null;
+    try {
+      Configuration conf = new Configuration();
+      dfs = new MiniDFSCluster.Builder(conf)
+          .numDataNodes(1)
+          .format(true)
+          .checkExitOnShutdown(false)
+          .build();
+      
+      // Set the owner of the system directory to a different user to the one
+      // that starts the JT. This will cause the JT to fail to transition to
+      // the active state.
+      FileSystem fs = dfs.getFileSystem();
+      Path mapredSysDir = new Path(conf.get("mapred.system.dir"));
+      fs.mkdirs(mapredSysDir);
+      fs.setOwner(mapredSysDir, "mr", "mrgroup");
+
+      cluster = new MiniMRHACluster(fs.getConf());
+      try {
+        cluster.getJobTrackerHaDaemon(0).makeActive();
+        fail("Transitioned to active but should not have been able to.");
+      } catch (ExitException ee) {
+        assertExceptionContains("is not owned by", ee);
+      }
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+      if (dfs != null) {
+        dfs.shutdown();
+      }
+    }
+  }
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestHAStateTransitions.java b/src/test/org/apache/hadoop/mapred/TestHAStateTransitions.java
new file mode 100644
index 0000000..f192519
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/TestHAStateTransitions.java
@@ -0,0 +1,159 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static org.junit.Assert.*;
+
+import java.io.File;
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.ha.FailoverController;
+import org.apache.hadoop.ha.HAServiceProtocol.RequestSource;
+import org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo;
+import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;
+import org.apache.hadoop.mapred.ConfiguredFailoverProxyProvider;
+import org.apache.hadoop.mapreduce.Cluster.JobTrackerStatus;
+import org.junit.*;
+
+/**
+ * Tests state transition from active->standby, and manual failover
+ * and failback between two jobtrackers.
+ */
+public class TestHAStateTransitions {
+  
+  private static final Log LOG = 
+    LogFactory.getLog(TestHAStateTransitions.class);
+
+  private static final Path TEST_DIR = new Path("/tmp/tst");
+  
+  private static final StateChangeRequestInfo REQ_INFO = new StateChangeRequestInfo(
+      RequestSource.REQUEST_BY_USER_FORCED);
+
+  private MiniMRHACluster cluster;
+  private JobTrackerHADaemon jt1;
+  private JobTrackerHADaemon jt2;
+  private JobTrackerHAServiceTarget target1;
+  private JobTrackerHAServiceTarget target2;
+  private Configuration conf;
+  
+  @Before
+  public void setUp() throws Exception {
+    conf = new Configuration();
+    conf.set(HAUtil.MR_HA_FENCING_METHODS_KEY,
+        AlwaysSucceedFencer.class.getName());
+    cluster = new MiniMRHACluster(conf);
+    cluster.getJobTrackerHaDaemon(0).makeActive();
+    cluster.startTaskTracker(0, 1);
+    cluster.waitActive();
+    
+    jt1 = cluster.getJobTrackerHaDaemon(0);
+    jt2 = cluster.getJobTrackerHaDaemon(1);
+    target1 = new JobTrackerHAServiceTarget(jt1.getConf());
+    target2 = new JobTrackerHAServiceTarget(jt2.getConf());
+  }
+  
+  @After
+  public void tearDown() throws Exception {
+    cluster.shutdown();
+  }
+  
+  @Test(timeout=60000)
+  public void testClientFailover() throws Exception {
+    LOG.info("Running testClientFailover");
+
+    // Test with client. c.f. HATestUtil.setFailoverConfigurations
+    JobClient jc = new JobClient(conf);
+    assertEquals("client sees jt running", JobTrackerStatus.RUNNING,
+        jc.getClusterStatus().getJobTrackerStatus());
+
+    // failover to jt2
+    FailoverController fc = new FailoverController(conf, 
+        RequestSource.REQUEST_BY_USER);
+    fc.failover(target1, target2, false, false);
+    
+    cluster.waitActive();
+    
+    assertEquals("jt2 running", JobTrackerStatus.RUNNING,
+        jt2.getJobTracker().getClusterStatus().getJobTrackerStatus());
+    assertNull("jt1 not running", jt1.getJobTracker());
+    
+    assertEquals("client still sees jt running", JobTrackerStatus.RUNNING,
+        jc.getClusterStatus().getJobTrackerStatus());
+  }
+  
+  @Test(timeout=60000)
+  public void testFailoverWhileRunningJob() throws Exception {
+    LOG.info("Running testFailoverWhileRunningJob");
+
+    // Inspired by TestRecoveryManager#testJobResubmission
+    
+    FileUtil.fullyDelete(new File("/tmp/tst"));
+    
+    // start a job on jt1
+    JobConf job1 = new JobConf(conf);
+    String signalFile = new Path(TEST_DIR, "signal").toString();
+    UtilsForTests.configureWaitingJobConf(job1, new Path(TEST_DIR, "input"),
+        new Path(TEST_DIR, "output3"), 2, 0, "test-resubmission", signalFile,
+        signalFile);
+    JobClient jc = new JobClient(job1);
+    RunningJob rJob1 = jc.submitJob(job1);
+    while (rJob1.mapProgress() < 0.5f) {
+      LOG.info("Waiting for job " + rJob1.getID() + " to be 50% done: " +
+          rJob1.mapProgress());
+      UtilsForTests.waitFor(500);
+    }
+    LOG.info("Waiting for job " + rJob1.getID() + " to be 50% done: " +
+        rJob1.mapProgress());
+    
+    // failover to jt2
+    FailoverController fc = new FailoverController(conf, 
+        RequestSource.REQUEST_BY_USER);
+    fc.failover(target1, target2, false, false);
+    
+    // allow job to complete
+    FileSystem fs = FileSystem.getLocal(conf);
+    fs.create(new Path(TEST_DIR, "signal"));
+    while (!rJob1.isComplete()) {
+      LOG.info("Waiting for job " + rJob1.getID() + " to be successful: " +
+          rJob1.mapProgress());
+      UtilsForTests.waitFor(500);
+    }
+    assertTrue("Job should be successful", rJob1.isSuccessful());
+  }
+  
+  @Test(timeout=60000)
+  public void testTransitionToCurrentStateIsANop() throws Exception {
+    LOG.info("Running testTransitionToCurrentStateIsANop");
+
+    JobTracker existingJt = jt1.getJobTracker();
+    jt1.getJobTrackerHAServiceProtocol().transitionToActive(REQ_INFO);
+    assertSame("Should not create a new JobTracker", existingJt,
+        jt1.getJobTracker());
+    jt1.getJobTrackerHAServiceProtocol().transitionToStandby(REQ_INFO);
+    // Transitioning to standby for a second time should not throw an exception
+    jt1.getJobTrackerHAServiceProtocol().transitionToStandby(REQ_INFO);
+  }
+
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestHAUtil.java b/src/test/org/apache/hadoop/mapred/TestHAUtil.java
new file mode 100644
index 0000000..c9910a9
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/TestHAUtil.java
@@ -0,0 +1,91 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static org.apache.hadoop.mapred.HAUtil.MR_HA_JOBTRACKERS_KEY_PREFIX;
+import static org.apache.hadoop.mapred.HAUtil.MR_JOBTRACKER_ADDRESS_KEY;
+import static org.apache.hadoop.mapred.HAUtil.MR_HA_JOBTRACKER_ID_KEY;
+import static org.apache.hadoop.mapred.HAUtil.MR_JOBTRACKER_RPC_ADDRESS_KEY;
+import static org.apache.hadoop.mapred.HAUtil.addKeySuffixes;
+import static org.apache.hadoop.mapred.HAUtil.getHaJtRpcAddresses;
+import static org.apache.hadoop.mapred.HAUtil.getJobTrackerId;
+import static org.apache.hadoop.mapred.HAUtil.getJobTrackerIdOfOtherNode;
+
+import static org.junit.Assert.assertEquals;
+
+import java.net.InetSocketAddress;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.net.NetUtils;
+
+import org.junit.Before;
+import org.junit.Test;
+
+public class TestHAUtil {
+  
+  private Configuration conf;
+  
+  @Before
+  public void setUp() {
+    conf = new Configuration();
+    conf.set(MR_JOBTRACKER_ADDRESS_KEY, "logicalName");
+    conf.set(addKeySuffixes(MR_HA_JOBTRACKERS_KEY_PREFIX, "logicalName"), "jt1,jt2");
+    final String JT1_ADDRESS = "1.2.3.4:8021";
+    final String JT2_ADDRESS = "localhost:8022";
+    conf.set(addKeySuffixes(MR_JOBTRACKER_RPC_ADDRESS_KEY, "logicalName", "jt1"), JT1_ADDRESS);
+    conf.set(addKeySuffixes(MR_JOBTRACKER_RPC_ADDRESS_KEY, "logicalName", "jt2"), JT2_ADDRESS);
+  }
+  
+  @Test
+  public void testGetHaJtRpcAddresses() throws Exception {
+    Map<String, Map<String, InetSocketAddress>> map =
+      getHaJtRpcAddresses(conf);
+    assertEquals(1, map.size());
+    
+    Map<String, InetSocketAddress> jtMap = map.get("logicalName");
+    assertEquals(2, jtMap.size());
+    InetSocketAddress addr = jtMap.get("jt1");
+    assertEquals("1.2.3.4", addr.getHostName());
+    assertEquals(8021, addr.getPort());
+
+    addr = jtMap.get("jt2");
+    assertEquals("localhost", addr.getHostName());
+    assertEquals(8022, addr.getPort());
+  }
+  
+  @Test
+  public void testGetJobTrackerId() throws Exception {
+    assertEquals("Matches localhost when " + MR_HA_JOBTRACKER_ID_KEY +
+        " not set", "jt2", getJobTrackerId(conf));
+    conf.set(MR_HA_JOBTRACKER_ID_KEY, "jt1");
+    assertEquals("Honors " + MR_HA_JOBTRACKER_ID_KEY + " if set",
+        "jt1", getJobTrackerId(conf));
+  }
+  
+  @Test
+  public void testGetJobTrackerIdOfOtherNode() throws Exception {
+    assertEquals(MR_HA_JOBTRACKER_ID_KEY + " not set", "jt1",
+        getJobTrackerIdOfOtherNode(conf));
+    conf.set(MR_HA_JOBTRACKER_ID_KEY, "jt1");
+    assertEquals(MR_HA_JOBTRACKER_ID_KEY + " set", "jt2",
+        getJobTrackerIdOfOtherNode(conf));
+  }
+
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestHAWebUI.java b/src/test/org/apache/hadoop/mapred/TestHAWebUI.java
new file mode 100644
index 0000000..35bb869
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/TestHAWebUI.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static org.junit.Assert.*;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.URL;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.ha.FailoverController;
+import org.apache.hadoop.ha.HAServiceProtocol.RequestSource;
+import org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo;
+import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;
+import org.apache.hadoop.hdfs.DFSTestUtil;
+import org.apache.hadoop.mapred.ConfiguredFailoverProxyProvider;
+import org.apache.hadoop.mapreduce.Cluster.JobTrackerStatus;
+import org.junit.*;
+
+/**
+ * Tests web UI redirect from standby to active jobtracker.
+ */
+public class TestHAWebUI {
+  
+  private static final Log LOG = 
+    LogFactory.getLog(TestHAWebUI.class);
+
+  private MiniMRHACluster cluster;
+  private JobTrackerHADaemon jt1;
+  private JobTrackerHADaemon jt2;
+  private JobTrackerHAServiceTarget target1;
+  private JobTrackerHAServiceTarget target2;
+  private Configuration conf;
+  
+  @Before
+  public void setUp() throws Exception {
+    conf = new Configuration();
+    conf.set(HAUtil.MR_HA_FENCING_METHODS_KEY,
+        AlwaysSucceedFencer.class.getName());
+    cluster = new MiniMRHACluster(conf);
+    cluster.getJobTrackerHaDaemon(0).makeActive();
+    cluster.startTaskTracker(0, 1);
+    cluster.waitActive();
+    
+    jt1 = cluster.getJobTrackerHaDaemon(0);
+    jt2 = cluster.getJobTrackerHaDaemon(1);
+    target1 = new JobTrackerHAServiceTarget(jt1.getConf());
+    target2 = new JobTrackerHAServiceTarget(jt2.getConf());
+  }
+  
+  @After
+  public void tearDown() throws Exception {
+    cluster.shutdown();
+  }
+  
+  @Test(timeout=60000)
+  public void testRedirect() throws Exception {
+
+    // both jobtracker addresses should serve up the jobtracker page
+    // regardless of state
+    checkJobTrackerPage("jt1");
+    checkJobTrackerPage("jt2");
+
+    // failover to jt2
+    FailoverController fc = new FailoverController(conf, 
+        RequestSource.REQUEST_BY_USER);
+    fc.failover(target1, target2, false, false);
+    
+    cluster.waitActive();
+    
+    checkJobTrackerPage("jt1");
+    checkJobTrackerPage("jt2");
+  }
+  
+  private void checkJobTrackerPage(String jtId) throws IOException {
+    String redirectAddress = conf.get(HAUtil.addKeySuffixes(
+        HAUtil.MR_HA_JOBTRACKER_HTTP_REDIRECT_ADDRESS_KEY, "logicaljt", jtId));
+    URL url = new URL("http://" + redirectAddress + "/jobtracker.jsp");
+    String page = DFSTestUtil.urlGet(url);
+    assertTrue(page.contains("Hadoop Map/Reduce Administration"));
+  }
+
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestJobTrackerHealthCheck.java b/src/test/org/apache/hadoop/mapred/TestJobTrackerHealthCheck.java
new file mode 100644
index 0000000..b948ba7
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/TestJobTrackerHealthCheck.java
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ha.HealthCheckFailedException;
+import org.apache.hadoop.test.GenericTestUtils;
+import org.junit.*;
+
+public class TestJobTrackerHealthCheck {
+  
+  private Configuration conf;
+  private MiniMRHACluster cluster;
+  
+  @Before
+  public void setUp() throws Exception {
+    conf = new Configuration();
+    cluster = new MiniMRHACluster(conf);
+    cluster.getJobTrackerHaDaemon(0).makeActive();
+    cluster.waitActive();
+  }
+
+  @After
+  public void tearDown() {
+    cluster.shutdown();
+  }
+  
+  @SuppressWarnings("deprecation")
+  @Test(timeout=60000)
+  public void test() throws Exception {
+    JobTrackerHAServiceProtocol haServiceProtocol =
+      cluster.getJobTrackerHaDaemon(0).getJobTrackerHAServiceProtocol();
+    
+    // Should not throw error, which indicates healthy.
+    haServiceProtocol.monitorHealth();
+    
+    // Forcibly stop JT thread
+    haServiceProtocol.getJobTrackerThread().stop();
+    haServiceProtocol.getJobTrackerThread().join();
+    
+    try {
+      // Should throw error - JT is unhealthy.
+      haServiceProtocol.monitorHealth();
+      fail("Should not have succeeded in calling monitorHealth");
+    } catch (HealthCheckFailedException hcfe) {
+      GenericTestUtils.assertExceptionContains(
+          "The JobTracker thread is not running", hcfe);
+    }
+  }
+
+}
diff --git a/src/test/org/apache/hadoop/mapred/TestMRZKFailoverController.java b/src/test/org/apache/hadoop/mapred/TestMRZKFailoverController.java
new file mode 100644
index 0000000..87b5199
--- /dev/null
+++ b/src/test/org/apache/hadoop/mapred/TestMRZKFailoverController.java
@@ -0,0 +1,229 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.util.concurrent.TimeoutException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.ha.ClientBaseWithFixes;
+import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;
+import org.apache.hadoop.ha.HAServiceProtocol.RequestSource;
+import org.apache.hadoop.ha.HealthMonitor;
+import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;
+import org.apache.hadoop.ha.FailoverController;
+import org.apache.hadoop.ha.ZKFCTestUtil;
+import org.apache.hadoop.ha.ZKFailoverController;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.mapred.HAUtil;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobTrackerHADaemon;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.tools.MRHAAdmin;
+import org.apache.hadoop.mapred.tools.MRZKFailoverController;
+import org.apache.hadoop.test.GenericTestUtils;
+import org.apache.hadoop.test.MultithreadedTestUtil.TestContext;
+import org.apache.hadoop.test.MultithreadedTestUtil.TestingThread;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import com.google.common.base.Supplier;
+
+public class TestMRZKFailoverController extends ClientBaseWithFixes {
+  
+  private static final Log LOG = 
+    LogFactory.getLog(TestMRZKFailoverController.class);
+  
+  private static final Path TEST_DIR = new Path("/tmp/tst");
+
+  private Configuration conf;
+  private MiniMRHACluster cluster;
+  private TestContext ctx;
+  private ZKFCThread thr1, thr2;
+  
+  @Before
+  public void setup() throws Exception {
+    conf = new Configuration();
+    conf.set(ZKFailoverController.ZK_QUORUM_KEY + ".logicaljt", hostPort);
+    conf.set(HAUtil.MR_HA_FENCING_METHODS_KEY,
+        AlwaysSucceedFencer.class.getName());
+    conf.setBoolean(HAUtil.MR_HA_AUTO_FAILOVER_ENABLED_KEY, true);
+
+    // Turn off IPC client caching, so that the suite can handle
+    // the restart of the daemons between test cases.
+    conf.setInt(
+        CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,
+        0);
+    
+    conf.setInt(HAUtil.MR_HA_ZKFC_PORT_KEY + ".logicaljt.jt1", 10003);
+    conf.setInt(HAUtil.MR_HA_ZKFC_PORT_KEY + ".logicaljt.jt2", 10004);
+
+    cluster = new MiniMRHACluster(conf);
+
+    ctx = new TestContext();
+    ctx.addThread(thr1 = new ZKFCThread(ctx, 0));
+    assertEquals(0, thr1.zkfc.run(new String[]{"-formatZK"}));
+
+    thr1.start();
+    waitForHAState(0, HAServiceState.ACTIVE);
+    
+    ctx.addThread(thr2 = new ZKFCThread(ctx, 1));
+    thr2.start();
+    
+    cluster.startTaskTracker(0, 1);
+    cluster.waitActive();
+    
+    // Wait for the ZKFCs to fully start up
+    ZKFCTestUtil.waitForHealthState(thr1.zkfc,
+        HealthMonitor.State.SERVICE_HEALTHY, ctx);
+    ZKFCTestUtil.waitForHealthState(thr2.zkfc,
+        HealthMonitor.State.SERVICE_HEALTHY, ctx);
+  }
+  
+  @After
+  public void shutdown() throws Exception {
+    cluster.shutdown();
+    
+    if (thr1 != null) {
+      thr1.interrupt();
+    }
+    if (thr2 != null) {
+      thr2.interrupt();
+    }
+    if (ctx != null) {
+      ctx.stop();
+    }
+  }
+  
+  @Test(timeout=60000)
+  public void testFailoverWhileRunningJob() throws Exception {
+    LOG.info("Running job failover test");
+
+    // Inspired by TestRecoveryManager#testJobResubmission
+    
+    FileUtil.fullyDelete(new File("/tmp/tst"));
+    
+    // start a job on jt1
+    JobConf job1 = new JobConf(conf);
+    String signalFile = new Path(TEST_DIR, "signal").toString();
+    UtilsForTests.configureWaitingJobConf(job1, new Path(TEST_DIR, "input"),
+        new Path(TEST_DIR, "output3"), 2, 0, "test-resubmission", signalFile,
+        signalFile);
+    JobClient jc = new JobClient(job1);
+    RunningJob rJob1 = jc.submitJob(job1);
+    while (rJob1.mapProgress() < 0.5f) {
+      LOG.info("Waiting for job " + rJob1.getID() + " to be 50% done: " +
+          rJob1.mapProgress());
+      UtilsForTests.waitFor(500);
+    }
+    LOG.info("Waiting for job " + rJob1.getID() + " to be 50% done: " +
+        rJob1.mapProgress());
+    
+    // Shut the first JT down, causing automatic failover
+    LOG.info("Shutting down jt1");
+    cluster.shutdownJobTracker(0);
+    
+    // allow job to complete
+    FileSystem fs = FileSystem.getLocal(conf);
+    fs.create(new Path(TEST_DIR, "signal"));
+    while (!rJob1.isComplete()) {
+      LOG.info("Waiting for job " + rJob1.getID() + " to be successful: " +
+          rJob1.mapProgress());
+      UtilsForTests.waitFor(500);
+    }
+    assertTrue("Job should be successful", rJob1.isSuccessful());
+  }
+  
+  @Test(timeout=60000)
+  public void testManualFailover() throws Exception {
+    LOG.info("Running manual failover test");
+    thr2.zkfc.getLocalTarget().getZKFCProxy(conf, 15000).gracefulFailover();
+    waitForHAState(0, HAServiceState.STANDBY);
+    waitForHAState(1, HAServiceState.ACTIVE);
+
+    thr1.zkfc.getLocalTarget().getZKFCProxy(conf, 15000).gracefulFailover();
+    waitForHAState(0, HAServiceState.ACTIVE);
+    waitForHAState(1, HAServiceState.STANDBY);
+  }
+  
+  @Test(timeout=60000)
+  public void testManualFailoverWithMRHAAdmin() throws Exception {
+    LOG.info("Running manual failover test with MRHAAdmin");
+    MRHAAdmin tool = new MRHAAdmin();
+    tool.setConf(conf);
+    assertEquals(0, 
+        tool.run(new String[]{"-failover", "jt1", "jt2"}));
+    waitForHAState(0, HAServiceState.STANDBY);
+    waitForHAState(1, HAServiceState.ACTIVE);
+    assertEquals(0,
+        tool.run(new String[]{"-failover", "jt2", "jt1"}));
+    waitForHAState(0, HAServiceState.ACTIVE);
+    waitForHAState(1, HAServiceState.STANDBY);
+  }
+  
+  private void waitForHAState(int jtidx, final HAServiceState state)
+      throws TimeoutException, InterruptedException {
+    final JobTrackerHADaemon jtHaDaemon = cluster.getJobTrackerHaDaemon(jtidx);
+    GenericTestUtils.waitFor(new Supplier<Boolean>() {
+      @Override
+      public Boolean get() {
+        try {
+          return jtHaDaemon.getServiceStatus().getState() == state;
+        } catch (Exception e) {
+          e.printStackTrace();
+          return false;
+        }
+      }
+    }, 50, 5000);
+  }
+
+  /**
+   * Test-thread which runs a ZK Failover Controller corresponding
+   * to a given JobTracker in the minicluster.
+   */
+  private class ZKFCThread extends TestingThread {
+    private final MRZKFailoverController zkfc;
+
+    public ZKFCThread(TestContext ctx, int idx) {
+      super(ctx);
+      this.zkfc = MRZKFailoverController.create(
+          cluster.getJobTrackerHaDaemon(idx).getConf());
+    }
+
+    @Override
+    public void doWork() throws Exception {
+      try {
+        assertEquals(0, zkfc.run(new String[0]));
+      } catch (InterruptedException ie) {
+        // Interrupted by main thread, that's OK.
+      }
+    }
+  }
+
+}
-- 
1.7.0.4

