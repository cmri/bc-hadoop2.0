From 46e6f5ffe9ad9f60e8e3e82bd0059a160bf158fa Mon Sep 17 00:00:00 2001
From: Harsh J <harsh@apache.org>
Date: Sat, 30 Jun 2012 16:49:24 +0000
Subject: [PATCH 0283/1357] MAPREDUCE-4253. Tests for mapreduce-client-core are lying under mapreduce-client-jobclient. Contributed by Tsuyoshi Ozawa. (harsh)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1355766 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 57915e47f35ba70ff5acf09d5d9adf0e1e50217f)
---
 .../org/apache/hadoop/mapred/TestAuditLogger.java  |  157 +++++
 .../java/org/apache/hadoop/mapred/TestIFile.java   |   63 ++
 .../org/apache/hadoop/mapred/TestIndexCache.java   |  324 ++++++++++
 .../java/org/apache/hadoop/mapred/TestJobConf.java |   85 +++
 .../hadoop/mapred/TestKeyValueTextInputFormat.java |  233 +++++++
 .../hadoop/mapred/TestMultiFileInputFormat.java    |  145 +++++
 .../apache/hadoop/mapred/TestMultiFileSplit.java   |   61 ++
 .../org/apache/hadoop/mapred/TestReduceTask.java   |  142 ++++
 .../TestSequenceFileAsBinaryInputFormat.java       |  101 +++
 .../TestSequenceFileAsBinaryOutputFormat.java      |  212 ++++++
 .../mapred/TestSequenceFileAsTextInputFormat.java  |  119 ++++
 .../hadoop/mapred/TestSequenceFileInputFilter.java |  176 +++++
 .../org/apache/hadoop/mapred/TestSortedRanges.java |   99 +++
 .../hadoop/mapred/TestStatisticsCollector.java     |   83 +++
 .../org/apache/hadoop/mapred/TestTaskStatus.java   |  207 ++++++
 .../java/org/apache/hadoop/mapred/TestUtils.java   |   79 +++
 .../org/apache/hadoop/mapreduce/TestCounters.java  |  154 +++++
 .../lib/jobcontrol/TestControlledJob.java          |   46 ++
 .../mapreduce/util/TestProcfsBasedProcessTree.java |  677 ++++++++++++++++++++
 .../org/apache/hadoop/mapred/TestAuditLogger.java  |  157 -----
 .../java/org/apache/hadoop/mapred/TestIFile.java   |   63 --
 .../org/apache/hadoop/mapred/TestIndexCache.java   |  324 ----------
 .../java/org/apache/hadoop/mapred/TestJobConf.java |   85 ---
 .../hadoop/mapred/TestKeyValueTextInputFormat.java |  233 -------
 .../hadoop/mapred/TestMultiFileInputFormat.java    |  145 -----
 .../apache/hadoop/mapred/TestMultiFileSplit.java   |   61 --
 .../org/apache/hadoop/mapred/TestReduceTask.java   |  142 ----
 .../TestSequenceFileAsBinaryInputFormat.java       |  101 ---
 .../TestSequenceFileAsBinaryOutputFormat.java      |  212 ------
 .../mapred/TestSequenceFileAsTextInputFormat.java  |  119 ----
 .../hadoop/mapred/TestSequenceFileInputFilter.java |  176 -----
 .../org/apache/hadoop/mapred/TestSortedRanges.java |   99 ---
 .../hadoop/mapred/TestStatisticsCollector.java     |   83 ---
 .../org/apache/hadoop/mapred/TestTaskStatus.java   |  207 ------
 .../java/org/apache/hadoop/mapred/TestUtils.java   |   79 ---
 .../org/apache/hadoop/mapreduce/TestCounters.java  |  154 -----
 .../lib/jobcontrol/TestControlledJob.java          |   46 --
 .../mapreduce/util/TestProcfsBasedProcessTree.java |  677 --------------------
 38 files changed, 3163 insertions(+), 3163 deletions(-)
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIFile.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobConf.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestUtils.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java
 create mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIFile.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobConf.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestUtils.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java
 delete mode 100644 hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java

diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java
new file mode 100644
index 0000000..a6aebb0
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java
@@ -0,0 +1,157 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.ipc.ProtocolInfo;
+import org.apache.hadoop.ipc.RPC;
+import org.apache.hadoop.ipc.Server;
+import org.apache.hadoop.ipc.TestRPC.TestImpl;
+import org.apache.hadoop.ipc.TestRPC.TestProtocol;
+import org.apache.hadoop.mapred.AuditLogger.Keys;
+import org.apache.hadoop.net.NetUtils;
+
+import junit.framework.TestCase;
+
+/**
+ * Tests {@link AuditLogger}.
+ */
+public class TestAuditLogger extends TestCase {
+  private static final String USER = "test";
+  private static final String OPERATION = "oper";
+  private static final String TARGET = "tgt";
+  private static final String PERM = "admin group";
+  private static final String DESC = "description of an audit log";
+
+  /**
+   * Test the AuditLog format with key-val pair.
+   */
+  public void testKeyValLogFormat() {
+    StringBuilder actLog = new StringBuilder();
+    StringBuilder expLog = new StringBuilder();
+    // add the first k=v pair and check
+    AuditLogger.start(Keys.USER, USER, actLog);
+    expLog.append("USER=test");
+    assertEquals(expLog.toString(), actLog.toString());
+
+    // append another k1=v1 pair to already added k=v and test
+    AuditLogger.add(Keys.OPERATION, OPERATION, actLog);
+    expLog.append("\tOPERATION=oper");
+    assertEquals(expLog.toString(), actLog.toString());
+
+    // append another k1=null pair and test
+    AuditLogger.add(Keys.PERMISSIONS, (String)null, actLog);
+    expLog.append("\tPERMISSIONS=null");
+    assertEquals(expLog.toString(), actLog.toString());
+
+    // now add the target and check of the final string
+    AuditLogger.add(Keys.TARGET, TARGET, actLog);
+    expLog.append("\tTARGET=tgt");
+    assertEquals(expLog.toString(), actLog.toString());
+  }
+
+  /**
+   * Test the AuditLog format for successful events.
+   */
+  private void testSuccessLogFormat(boolean checkIP) {
+    // check without the IP
+    String sLog = AuditLogger.createSuccessLog(USER, OPERATION, TARGET);
+    StringBuilder expLog = new StringBuilder();
+    expLog.append("USER=test\t");
+    if (checkIP) {
+      InetAddress ip = Server.getRemoteIp();
+      expLog.append(Keys.IP.name() + "=" + ip.getHostAddress() + "\t");
+    }
+    expLog.append("OPERATION=oper\tTARGET=tgt\tRESULT=SUCCESS");
+    assertEquals(expLog.toString(), sLog);
+
+  }
+
+  /**
+   * Test the AuditLog format for failure events.
+   */
+  private void testFailureLogFormat(boolean checkIP, String perm) {
+    String fLog =
+      AuditLogger.createFailureLog(USER, OPERATION, perm, TARGET, DESC);
+    StringBuilder expLog = new StringBuilder();
+    expLog.append("USER=test\t");
+    if (checkIP) {
+      InetAddress ip = Server.getRemoteIp();
+      expLog.append(Keys.IP.name() + "=" + ip.getHostAddress() + "\t");
+    }
+    expLog.append("OPERATION=oper\tTARGET=tgt\tRESULT=FAILURE\t");
+    expLog.append("DESCRIPTION=description of an audit log\t");
+    expLog.append("PERMISSIONS=" + perm);
+    assertEquals(expLog.toString(), fLog);
+  }
+
+  /**
+   * Test the AuditLog format for failure events.
+   */
+  private void testFailureLogFormat(boolean checkIP) {
+    testFailureLogFormat(checkIP, PERM);
+    testFailureLogFormat(checkIP, null);
+  }
+
+  /**
+   * Test {@link AuditLogger} without IP set.
+   */
+  public void testAuditLoggerWithoutIP() throws Exception {
+    // test without ip
+    testSuccessLogFormat(false);
+    testFailureLogFormat(false);
+  }
+
+  /**
+   * A special extension of {@link TestImpl} RPC server with
+   * {@link TestImpl#ping()} testing the audit logs.
+   */
+  @ProtocolInfo(protocolName = "org.apache.hadoop.ipc.TestRPC$TestProtocol")
+  private class MyTestRPCServer extends TestImpl {
+    @Override
+    public void ping() {
+      // test with ip set
+      testSuccessLogFormat(true);
+      testFailureLogFormat(true);
+    }
+  }
+
+  /**
+   * Test {@link AuditLogger} with IP set.
+   */
+  @SuppressWarnings("deprecation")
+  public void testAuditLoggerWithIP() throws Exception {
+    Configuration conf = new Configuration();
+    // start the IPC server
+    Server server = RPC.getServer(new MyTestRPCServer(), "0.0.0.0", 0, conf);
+    server.start();
+
+    InetSocketAddress addr = NetUtils.getConnectAddress(server);
+
+    // Make a client connection and test the audit log
+    TestProtocol proxy = (TestProtocol)RPC.getProxy(TestProtocol.class,
+                           TestProtocol.versionID, addr, conf);
+    // Start the testcase
+    proxy.ping();
+
+    server.stop();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIFile.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIFile.java
new file mode 100644
index 0000000..0411711
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIFile.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.io.compress.GzipCodec;
+
+import org.junit.Test;
+
+public class TestIFile {
+
+  @Test
+  /**
+   * Create an IFile.Writer using GzipCodec since this codec does not
+   * have a compressor when run via the tests (ie no native libraries).
+   */
+  public void testIFileWriterWithCodec() throws Exception {
+    Configuration conf = new Configuration();
+    FileSystem localFs = FileSystem.getLocal(conf);
+    FileSystem rfs = ((LocalFileSystem)localFs).getRaw();
+    Path path = new Path(new Path("build/test.ifile"), "data");
+    DefaultCodec codec = new GzipCodec();
+    codec.setConf(conf);
+    IFile.Writer<Text, Text> writer =
+      new IFile.Writer<Text, Text>(conf, rfs, path, Text.class, Text.class,
+                                   codec, null);
+    writer.close();
+  }
+
+  @Test
+  /** Same as above but create a reader. */
+  public void testIFileReaderWithCodec() throws Exception {
+    Configuration conf = new Configuration();
+    FileSystem localFs = FileSystem.getLocal(conf);
+    FileSystem rfs = ((LocalFileSystem)localFs).getRaw();
+    Path path = new Path(new Path("build/test.ifile"), "data");
+    DefaultCodec codec = new GzipCodec();
+    codec.setConf(conf);
+    IFile.Reader<Text, Text> reader =
+      new IFile.Reader<Text, Text>(conf, rfs, path, codec, null);
+    reader.close();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java
new file mode 100644
index 0000000..b6a2df0
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java
@@ -0,0 +1,324 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.DataOutputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.Random;
+import java.util.zip.CRC32;
+import java.util.zip.CheckedOutputStream;
+
+import org.apache.hadoop.fs.ChecksumException;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
+
+import junit.framework.TestCase;
+
+public class TestIndexCache extends TestCase {
+  private JobConf conf;
+  private FileSystem fs;
+  private Path p;
+
+  @Override
+  public void setUp() throws IOException {
+    conf = new JobConf();
+    fs = FileSystem.getLocal(conf).getRaw();
+    p =  new Path(System.getProperty("test.build.data", "/tmp"),
+        "cache").makeQualified(fs.getUri(), fs.getWorkingDirectory());
+  }
+
+  public void testLRCPolicy() throws Exception {
+    Random r = new Random();
+    long seed = r.nextLong();
+    r.setSeed(seed);
+    System.out.println("seed: " + seed);
+    fs.delete(p, true);
+    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
+    final int partsPerMap = 1000;
+    final int bytesPerFile = partsPerMap * 24;
+    IndexCache cache = new IndexCache(conf);
+
+    // fill cache
+    int totalsize = bytesPerFile;
+    for (; totalsize < 1024 * 1024; totalsize += bytesPerFile) {
+      Path f = new Path(p, Integer.toString(totalsize, 36));
+      writeFile(fs, f, totalsize, partsPerMap);
+      IndexRecord rec = cache.getIndexInformation(
+        Integer.toString(totalsize, 36), r.nextInt(partsPerMap), f,
+        UserGroupInformation.getCurrentUser().getShortUserName());
+      checkRecord(rec, totalsize);
+    }
+
+    // delete files, ensure cache retains all elem
+    for (FileStatus stat : fs.listStatus(p)) {
+      fs.delete(stat.getPath(),true);
+    }
+    for (int i = bytesPerFile; i < 1024 * 1024; i += bytesPerFile) {
+      Path f = new Path(p, Integer.toString(i, 36));
+      IndexRecord rec = cache.getIndexInformation(Integer.toString(i, 36),
+        r.nextInt(partsPerMap), f,
+        UserGroupInformation.getCurrentUser().getShortUserName());
+      checkRecord(rec, i);
+    }
+
+    // push oldest (bytesPerFile) out of cache
+    Path f = new Path(p, Integer.toString(totalsize, 36));
+    writeFile(fs, f, totalsize, partsPerMap);
+    cache.getIndexInformation(Integer.toString(totalsize, 36),
+        r.nextInt(partsPerMap), f,
+        UserGroupInformation.getCurrentUser().getShortUserName());
+    fs.delete(f, false);
+
+    // oldest fails to read, or error
+    boolean fnf = false;
+    try {
+      cache.getIndexInformation(Integer.toString(bytesPerFile, 36),
+        r.nextInt(partsPerMap), new Path(p, Integer.toString(bytesPerFile)),
+        UserGroupInformation.getCurrentUser().getShortUserName());
+    } catch (IOException e) {
+      if (e.getCause() == null ||
+          !(e.getCause()  instanceof FileNotFoundException)) {
+        throw e;
+      }
+      else {
+        fnf = true;
+      }
+    }
+    if (!fnf)
+      fail("Failed to push out last entry");
+    // should find all the other entries
+    for (int i = bytesPerFile << 1; i < 1024 * 1024; i += bytesPerFile) {
+      IndexRecord rec = cache.getIndexInformation(Integer.toString(i, 36),
+          r.nextInt(partsPerMap), new Path(p, Integer.toString(i, 36)),
+          UserGroupInformation.getCurrentUser().getShortUserName());
+      checkRecord(rec, i);
+    }
+    IndexRecord rec = cache.getIndexInformation(Integer.toString(totalsize, 36),
+      r.nextInt(partsPerMap), f,
+      UserGroupInformation.getCurrentUser().getShortUserName());
+
+    checkRecord(rec, totalsize);
+  }
+
+  public void testBadIndex() throws Exception {
+    final int parts = 30;
+    fs.delete(p, true);
+    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
+    IndexCache cache = new IndexCache(conf);
+
+    Path f = new Path(p, "badindex");
+    FSDataOutputStream out = fs.create(f, false);
+    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());
+    DataOutputStream dout = new DataOutputStream(iout);
+    for (int i = 0; i < parts; ++i) {
+      for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {
+        if (0 == (i % 3)) {
+          dout.writeLong(i);
+        } else {
+          out.writeLong(i);
+        }
+      }
+    }
+    out.writeLong(iout.getChecksum().getValue());
+    dout.close();
+    try {
+      cache.getIndexInformation("badindex", 7, f,
+        UserGroupInformation.getCurrentUser().getShortUserName());
+      fail("Did not detect bad checksum");
+    } catch (IOException e) {
+      if (!(e.getCause() instanceof ChecksumException)) {
+        throw e;
+      }
+    }
+  }
+
+  public void testInvalidReduceNumberOrLength() throws Exception {
+    fs.delete(p, true);
+    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
+    final int partsPerMap = 1000;
+    final int bytesPerFile = partsPerMap * 24;
+    IndexCache cache = new IndexCache(conf);
+
+    // fill cache
+    Path feq = new Path(p, "invalidReduceOrPartsPerMap");
+    writeFile(fs, feq, bytesPerFile, partsPerMap);
+
+    // Number of reducers should always be less than partsPerMap as reducer
+    // numbers start from 0 and there cannot be more reducer than parts
+
+    try {
+      // Number of reducers equal to partsPerMap
+      cache.getIndexInformation("reduceEqualPartsPerMap", 
+               partsPerMap, // reduce number == partsPerMap
+               feq, UserGroupInformation.getCurrentUser().getShortUserName());
+      fail("Number of reducers equal to partsPerMap did not fail");
+    } catch (Exception e) {
+      if (!(e instanceof IOException)) {
+        throw e;
+      }
+    }
+
+    try {
+      // Number of reducers more than partsPerMap
+      cache.getIndexInformation(
+      "reduceMorePartsPerMap", 
+      partsPerMap + 1, // reduce number > partsPerMap
+      feq, UserGroupInformation.getCurrentUser().getShortUserName());
+      fail("Number of reducers more than partsPerMap did not fail");
+    } catch (Exception e) {
+      if (!(e instanceof IOException)) {
+        throw e;
+      }
+    }
+  }
+
+  public void testRemoveMap() throws Exception {
+    // This test case use two thread to call getIndexInformation and 
+    // removeMap concurrently, in order to construct race condition.
+    // This test case may not repeatable. But on my macbook this test 
+    // fails with probability of 100% on code before MAPREDUCE-2541,
+    // so it is repeatable in practice.
+    fs.delete(p, true);
+    conf.setInt(TTConfig.TT_INDEX_CACHE, 10);
+    // Make a big file so removeMapThread almost surely runs faster than 
+    // getInfoThread 
+    final int partsPerMap = 100000;
+    final int bytesPerFile = partsPerMap * 24;
+    final IndexCache cache = new IndexCache(conf);
+
+    final Path big = new Path(p, "bigIndex");
+    final String user = 
+      UserGroupInformation.getCurrentUser().getShortUserName();
+    writeFile(fs, big, bytesPerFile, partsPerMap);
+    
+    // run multiple times
+    for (int i = 0; i < 20; ++i) {
+      Thread getInfoThread = new Thread() {
+        @Override
+        public void run() {
+          try {
+            cache.getIndexInformation("bigIndex", partsPerMap, big, user);
+          } catch (Exception e) {
+            // should not be here
+          }
+        }
+      };
+      Thread removeMapThread = new Thread() {
+        @Override
+        public void run() {
+          cache.removeMap("bigIndex");
+        }
+      };
+      if (i%2==0) {
+        getInfoThread.start();
+        removeMapThread.start();        
+      } else {
+        removeMapThread.start();        
+        getInfoThread.start();
+      }
+      getInfoThread.join();
+      removeMapThread.join();
+      assertEquals(true, cache.checkTotalMemoryUsed());
+    }      
+  }
+  
+  public void testCreateRace() throws Exception {
+    fs.delete(p, true);
+    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
+    final int partsPerMap = 1000;
+    final int bytesPerFile = partsPerMap * 24;
+    final IndexCache cache = new IndexCache(conf);
+    
+    final Path racy = new Path(p, "racyIndex");
+    final String user =  
+      UserGroupInformation.getCurrentUser().getShortUserName();
+    writeFile(fs, racy, bytesPerFile, partsPerMap);
+
+    // run multiple instances
+    Thread[] getInfoThreads = new Thread[50];
+    for (int i = 0; i < 50; i++) {
+      getInfoThreads[i] = new Thread() {
+        @Override
+        public void run() {
+          try {
+            cache.getIndexInformation("racyIndex", partsPerMap, racy, user);
+            cache.removeMap("racyIndex");
+          } catch (Exception e) {
+            // should not be here
+          }
+        }
+      };
+    }
+
+    for (int i = 0; i < 50; i++) {
+      getInfoThreads[i].start();
+    }
+
+    final Thread mainTestThread = Thread.currentThread();
+
+    Thread timeoutThread = new Thread() {
+      @Override
+      public void run() {
+        try {
+          Thread.sleep(15000);
+          mainTestThread.interrupt();
+        } catch (InterruptedException ie) {
+          // we are done;
+        }
+      }
+    };
+
+    for (int i = 0; i < 50; i++) {
+      try {
+        getInfoThreads[i].join();
+      } catch (InterruptedException ie) {
+        // we haven't finished in time. Potential deadlock/race.
+        fail("Unexpectedly long delay during concurrent cache entry creations");
+      }
+    }
+    // stop the timeoutThread. If we get interrupted before stopping, there
+    // must be something wrong, although it wasn't a deadlock. No need to
+    // catch and swallow.
+    timeoutThread.interrupt();
+  }
+
+  private static void checkRecord(IndexRecord rec, long fill) {
+    assertEquals(fill, rec.startOffset);
+    assertEquals(fill, rec.rawLength);
+    assertEquals(fill, rec.partLength);
+  }
+
+  private static void writeFile(FileSystem fs, Path f, long fill, int parts)
+      throws IOException {
+    FSDataOutputStream out = fs.create(f, false);
+    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());
+    DataOutputStream dout = new DataOutputStream(iout);
+    for (int i = 0; i < parts; ++i) {
+      for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {
+        dout.writeLong(fill);
+      }
+    }
+    out.writeLong(iout.getChecksum().getValue());
+    dout.close();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobConf.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobConf.java
new file mode 100644
index 0000000..3bd2c78
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobConf.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.junit.Ignore;
+import org.junit.Test;
+import java.io.File;
+import java.net.URLClassLoader;
+import java.net.URL;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+
+
+import static org.junit.Assert.*;
+@Ignore
+public class TestJobConf {
+  private static final String JAR_RELATIVE_PATH =
+    "build/test/mapred/testjar/testjob.jar";
+  private static final String CLASSNAME = "testjar.ClassWordCount";
+
+  private static String TEST_DIR_WITH_SPECIAL_CHARS =
+    System.getProperty("test.build.data","/tmp") +
+    File.separator + "test jobconf with + and spaces";
+
+  @Test
+  public void testFindContainingJar() throws Exception {
+    testJarAtPath(JAR_RELATIVE_PATH);
+  }
+
+  /**
+   * Test that findContainingJar works correctly even if the
+   * path has a "+" sign or spaces in it
+   */
+  @Test
+  public void testFindContainingJarWithPlus() throws Exception {
+    new File(TEST_DIR_WITH_SPECIAL_CHARS).mkdirs();
+    Configuration conf = new Configuration();
+
+    FileSystem localfs = FileSystem.getLocal(conf);
+
+    FileUtil.copy(localfs, new Path(JAR_RELATIVE_PATH),
+                  localfs, new Path(TEST_DIR_WITH_SPECIAL_CHARS, "test.jar"),
+                  false, true, conf);
+    testJarAtPath(TEST_DIR_WITH_SPECIAL_CHARS + File.separator + "test.jar");
+  }
+
+  /**
+   * Given a path with a jar, make a classloader with that jar on the
+   * classpath, and check that findContainingJar can correctly
+   * identify the path of the jar.
+   */
+  private void testJarAtPath(String path) throws Exception {
+    File jar = new File(path).getAbsoluteFile();
+    assertTrue(jar.exists());
+
+    URL urls[] = new URL[] {
+      jar.toURI().toURL()
+    };
+
+    ClassLoader cl = new URLClassLoader(urls);
+    Class clazz = Class.forName(CLASSNAME, true, cl);
+    assertNotNull(clazz);
+
+    String containingJar = JobConf.findContainingJar(clazz);
+    assertEquals(jar.getAbsolutePath(), containingJar);
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java
new file mode 100644
index 0000000..3846bbe
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java
@@ -0,0 +1,233 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.*;
+import java.util.*;
+import junit.framework.TestCase;
+
+import org.apache.commons.logging.*;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.io.compress.*;
+import org.apache.hadoop.util.LineReader;
+import org.apache.hadoop.util.ReflectionUtils;
+
+public class TestKeyValueTextInputFormat extends TestCase {
+  private static final Log LOG =
+    LogFactory.getLog(TestKeyValueTextInputFormat.class.getName());
+
+  private static int MAX_LENGTH = 10000;
+  
+  private static JobConf defaultConf = new JobConf();
+  private static FileSystem localFs = null; 
+  static {
+    try {
+      localFs = FileSystem.getLocal(defaultConf);
+    } catch (IOException e) {
+      throw new RuntimeException("init failure", e);
+    }
+  }
+  private static Path workDir = 
+    new Path(new Path(System.getProperty("test.build.data", "."), "data"),
+             "TestKeyValueTextInputFormat");
+  
+  public void testFormat() throws Exception {
+    JobConf job = new JobConf();
+    Path file = new Path(workDir, "test.txt");
+
+    // A reporter that does nothing
+    Reporter reporter = Reporter.NULL;
+    
+    int seed = new Random().nextInt();
+    LOG.info("seed = "+seed);
+    Random random = new Random(seed);
+
+    localFs.delete(workDir, true);
+    FileInputFormat.setInputPaths(job, workDir);
+
+    // for a variety of lengths
+    for (int length = 0; length < MAX_LENGTH;
+         length+= random.nextInt(MAX_LENGTH/10)+1) {
+
+      LOG.debug("creating; entries = " + length);
+
+      // create a file with length entries
+      Writer writer = new OutputStreamWriter(localFs.create(file));
+      try {
+        for (int i = 0; i < length; i++) {
+          writer.write(Integer.toString(i*2));
+          writer.write("\t");
+          writer.write(Integer.toString(i));
+          writer.write("\n");
+        }
+      } finally {
+        writer.close();
+      }
+
+      // try splitting the file in a variety of sizes
+      KeyValueTextInputFormat format = new KeyValueTextInputFormat();
+      format.configure(job);
+      for (int i = 0; i < 3; i++) {
+        int numSplits = random.nextInt(MAX_LENGTH/20)+1;
+        LOG.debug("splitting: requesting = " + numSplits);
+        InputSplit[] splits = format.getSplits(job, numSplits);
+        LOG.debug("splitting: got =        " + splits.length);
+
+        // check each split
+        BitSet bits = new BitSet(length);
+        for (int j = 0; j < splits.length; j++) {
+          LOG.debug("split["+j+"]= " + splits[j]);
+          RecordReader<Text, Text> reader =
+            format.getRecordReader(splits[j], job, reporter);
+          Class readerClass = reader.getClass();
+          assertEquals("reader class is KeyValueLineRecordReader.", KeyValueLineRecordReader.class, readerClass);        
+
+          Text key = reader.createKey();
+          Class keyClass = key.getClass();
+          Text value = reader.createValue();
+          Class valueClass = value.getClass();
+          assertEquals("Key class is Text.", Text.class, keyClass);
+          assertEquals("Value class is Text.", Text.class, valueClass);
+          try {
+            int count = 0;
+            while (reader.next(key, value)) {
+              int v = Integer.parseInt(value.toString());
+              LOG.debug("read " + v);
+              if (bits.get(v)) {
+                LOG.warn("conflict with " + v + 
+                         " in split " + j +
+                         " at position "+reader.getPos());
+              }
+              assertFalse("Key in multiple partitions.", bits.get(v));
+              bits.set(v);
+              count++;
+            }
+            LOG.debug("splits["+j+"]="+splits[j]+" count=" + count);
+          } finally {
+            reader.close();
+          }
+        }
+        assertEquals("Some keys in no partition.", length, bits.cardinality());
+      }
+
+    }
+  }
+  private LineReader makeStream(String str) throws IOException {
+    return new LineReader(new ByteArrayInputStream
+                                           (str.getBytes("UTF-8")), 
+                                           defaultConf);
+  }
+  
+  public void testUTF8() throws Exception {
+    LineReader in = makeStream("abcd\u20acbdcd\u20ac");
+    Text line = new Text();
+    in.readLine(line);
+    assertEquals("readLine changed utf8 characters", 
+                 "abcd\u20acbdcd\u20ac", line.toString());
+    in = makeStream("abc\u200axyz");
+    in.readLine(line);
+    assertEquals("split on fake newline", "abc\u200axyz", line.toString());
+  }
+
+  public void testNewLines() throws Exception {
+    LineReader in = makeStream("a\nbb\n\nccc\rdddd\r\neeeee");
+    Text out = new Text();
+    in.readLine(out);
+    assertEquals("line1 length", 1, out.getLength());
+    in.readLine(out);
+    assertEquals("line2 length", 2, out.getLength());
+    in.readLine(out);
+    assertEquals("line3 length", 0, out.getLength());
+    in.readLine(out);
+    assertEquals("line4 length", 3, out.getLength());
+    in.readLine(out);
+    assertEquals("line5 length", 4, out.getLength());
+    in.readLine(out);
+    assertEquals("line5 length", 5, out.getLength());
+    assertEquals("end of file", 0, in.readLine(out));
+  }
+  
+  private static void writeFile(FileSystem fs, Path name, 
+                                CompressionCodec codec,
+                                String contents) throws IOException {
+    OutputStream stm;
+    if (codec == null) {
+      stm = fs.create(name);
+    } else {
+      stm = codec.createOutputStream(fs.create(name));
+    }
+    stm.write(contents.getBytes());
+    stm.close();
+  }
+  
+  private static final Reporter voidReporter = Reporter.NULL;
+  
+  private static List<Text> readSplit(KeyValueTextInputFormat format, 
+                                      InputSplit split, 
+                                      JobConf job) throws IOException {
+    List<Text> result = new ArrayList<Text>();
+    RecordReader<Text, Text> reader = format.getRecordReader(split, job,
+                                                 voidReporter);
+    Text key = reader.createKey();
+    Text value = reader.createValue();
+    while (reader.next(key, value)) {
+      result.add(value);
+      value = reader.createValue();
+    }
+    return result;
+  }
+  
+  /**
+   * Test using the gzip codec for reading
+   */
+  public static void testGzip() throws IOException {
+    JobConf job = new JobConf();
+    CompressionCodec gzip = new GzipCodec();
+    ReflectionUtils.setConf(gzip, job);
+    localFs.delete(workDir, true);
+    writeFile(localFs, new Path(workDir, "part1.txt.gz"), gzip, 
+              "line-1\tthe quick\nline-2\tbrown\nline-3\tfox jumped\nline-4\tover\nline-5\t the lazy\nline-6\t dog\n");
+    writeFile(localFs, new Path(workDir, "part2.txt.gz"), gzip,
+              "line-1\tthis is a test\nline-1\tof gzip\n");
+    FileInputFormat.setInputPaths(job, workDir);
+    KeyValueTextInputFormat format = new KeyValueTextInputFormat();
+    format.configure(job);
+    InputSplit[] splits = format.getSplits(job, 100);
+    assertEquals("compressed splits == 2", 2, splits.length);
+    FileSplit tmp = (FileSplit) splits[0];
+    if (tmp.getPath().getName().equals("part2.txt.gz")) {
+      splits[0] = splits[1];
+      splits[1] = tmp;
+    }
+    List<Text> results = readSplit(format, splits[0], job);
+    assertEquals("splits[0] length", 6, results.size());
+    assertEquals("splits[0][5]", " dog", results.get(5).toString());
+    results = readSplit(format, splits[1], job);
+    assertEquals("splits[1] length", 2, results.size());
+    assertEquals("splits[1][0]", "this is a test", 
+                 results.get(0).toString());    
+    assertEquals("splits[1][1]", "of gzip", 
+                 results.get(1).toString());    
+  }
+  
+  public static void main(String[] args) throws Exception {
+    new TestKeyValueTextInputFormat().testFormat();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java
new file mode 100644
index 0000000..ff7a632
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java
@@ -0,0 +1,145 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.BitSet;
+import java.util.HashMap;
+import java.util.Random;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+
+public class TestMultiFileInputFormat extends TestCase{
+
+  private static JobConf job = new JobConf();
+
+  private static final Log LOG = LogFactory.getLog(TestMultiFileInputFormat.class);
+  
+  private static final int MAX_SPLIT_COUNT  = 10000;
+  private static final int SPLIT_COUNT_INCR = 6000;
+  private static final int MAX_BYTES = 1024;
+  private static final int MAX_NUM_FILES = 10000;
+  private static final int NUM_FILES_INCR = 8000;
+  
+  private Random rand = new Random(System.currentTimeMillis());
+  private HashMap<String, Long> lengths = new HashMap<String, Long>();
+  
+  /** Dummy class to extend MultiFileInputFormat*/
+  private class DummyMultiFileInputFormat extends MultiFileInputFormat<Text, Text> {
+    @Override
+    public RecordReader<Text,Text> getRecordReader(InputSplit split, JobConf job
+        , Reporter reporter) throws IOException {
+      return null;
+    }
+  }
+  
+  private Path initFiles(FileSystem fs, int numFiles, int numBytes) throws IOException{
+    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
+    Path multiFileDir = new Path(dir, "test.multifile");
+    fs.delete(multiFileDir, true);
+    fs.mkdirs(multiFileDir);
+    LOG.info("Creating " + numFiles + " file(s) in " + multiFileDir);
+    for(int i=0; i<numFiles ;i++) {
+      Path path = new Path(multiFileDir, "file_" + i);
+       FSDataOutputStream out = fs.create(path);
+       if (numBytes == -1) {
+         numBytes = rand.nextInt(MAX_BYTES);
+       }
+       for(int j=0; j< numBytes; j++) {
+         out.write(rand.nextInt());
+       }
+       out.close();
+       if(LOG.isDebugEnabled()) {
+         LOG.debug("Created file " + path + " with length " + numBytes);
+       }
+       lengths.put(path.getName(), new Long(numBytes));
+    }
+    FileInputFormat.setInputPaths(job, multiFileDir);
+    return multiFileDir;
+  }
+  
+  public void testFormat() throws IOException {
+    if(LOG.isInfoEnabled()) {
+      LOG.info("Test started");
+      LOG.info("Max split count           = " + MAX_SPLIT_COUNT);
+      LOG.info("Split count increment     = " + SPLIT_COUNT_INCR);
+      LOG.info("Max bytes per file        = " + MAX_BYTES);
+      LOG.info("Max number of files       = " + MAX_NUM_FILES);
+      LOG.info("Number of files increment = " + NUM_FILES_INCR);
+    }
+    
+    MultiFileInputFormat<Text,Text> format = new DummyMultiFileInputFormat();
+    FileSystem fs = FileSystem.getLocal(job);
+    
+    for(int numFiles = 1; numFiles< MAX_NUM_FILES ; 
+      numFiles+= (NUM_FILES_INCR / 2) + rand.nextInt(NUM_FILES_INCR / 2)) {
+      
+      Path dir = initFiles(fs, numFiles, -1);
+      BitSet bits = new BitSet(numFiles);
+      for(int i=1;i< MAX_SPLIT_COUNT ;i+= rand.nextInt(SPLIT_COUNT_INCR) + 1) {
+        LOG.info("Running for Num Files=" + numFiles + ", split count=" + i);
+        
+        MultiFileSplit[] splits = (MultiFileSplit[])format.getSplits(job, i);
+        bits.clear();
+        
+        for(MultiFileSplit split : splits) {
+          long splitLength = 0;
+          for(Path p : split.getPaths()) {
+            long length = fs.getContentSummary(p).getLength();
+            assertEquals(length, lengths.get(p.getName()).longValue());
+            splitLength += length;
+            String name = p.getName();
+            int index = Integer.parseInt(
+                name.substring(name.lastIndexOf("file_") + 5));
+            assertFalse(bits.get(index));
+            bits.set(index);
+          }
+          assertEquals(splitLength, split.getLength());
+        }
+      }
+      assertEquals(bits.cardinality(), numFiles);
+      fs.delete(dir, true);
+    }
+    LOG.info("Test Finished");
+  }
+  
+  public void testFormatWithLessPathsThanSplits() throws Exception {
+    MultiFileInputFormat<Text,Text> format = new DummyMultiFileInputFormat();
+    FileSystem fs = FileSystem.getLocal(job);     
+    
+    // Test with no path
+    initFiles(fs, 0, -1);    
+    assertEquals(0, format.getSplits(job, 2).length);
+    
+    // Test with 2 path and 4 splits
+    initFiles(fs, 2, 500);
+    assertEquals(2, format.getSplits(job, 4).length);
+  }
+  
+  public static void main(String[] args) throws Exception{
+    TestMultiFileInputFormat test = new TestMultiFileInputFormat();
+    test.testFormat();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java
new file mode 100644
index 0000000..af0f399
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.util.Arrays;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+
+public class TestMultiFileSplit extends TestCase{
+
+    public void testReadWrite() throws Exception {
+      MultiFileSplit split = new MultiFileSplit(new JobConf(), new Path[] {new Path("/test/path/1"), new Path("/test/path/2")}, new long[] {100,200});
+        
+      ByteArrayOutputStream bos = null;
+      byte[] result = null;
+      try {    
+        bos = new ByteArrayOutputStream();
+        split.write(new DataOutputStream(bos));
+        result = bos.toByteArray();
+      } finally {
+        IOUtils.closeStream(bos);
+      }
+      
+      MultiFileSplit readSplit = new MultiFileSplit();
+      ByteArrayInputStream bis = null;
+      try {
+        bis = new ByteArrayInputStream(result);
+        readSplit.readFields(new DataInputStream(bis));
+      } finally {
+        IOUtils.closeStream(bis);
+      }
+      
+      assertTrue(split.getLength() != 0);
+      assertEquals(split.getLength(), readSplit.getLength());
+      assertTrue(Arrays.equals(split.getPaths(), readSplit.getPaths()));
+      assertTrue(Arrays.equals(split.getLengths(), readSplit.getLengths()));
+      System.out.println(split.toString());
+    }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java
new file mode 100644
index 0000000..d3a0844
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java
@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * This test exercises the ValueIterator.
+ */
+public class TestReduceTask extends TestCase {
+
+  static class NullProgress implements Progressable {
+    public void progress() { }
+  }
+
+  private static class Pair {
+    String key;
+    String value;
+    Pair(String k, String v) {
+      key = k;
+      value = v;
+    }
+  }
+  private static Pair[][] testCases =
+    new Pair[][]{
+      new Pair[]{
+                 new Pair("k1", "v1"),
+                 new Pair("k2", "v2"),
+                 new Pair("k3", "v3"),
+                 new Pair("k3", "v4"),
+                 new Pair("k4", "v5"),
+                 new Pair("k5", "v6"),
+      },
+      new Pair[]{
+                 new Pair("", "v1"),
+                 new Pair("k1", "v2"),
+                 new Pair("k2", "v3"),
+                 new Pair("k2", "v4"),
+      },
+      new Pair[] {},
+      new Pair[]{
+                 new Pair("k1", "v1"),
+                 new Pair("k1", "v2"),
+                 new Pair("k1", "v3"),
+                 new Pair("k1", "v4"),
+      }
+    };
+  
+  public void runValueIterator(Path tmpDir, Pair[] vals, 
+                               Configuration conf, 
+                               CompressionCodec codec) throws IOException {
+    FileSystem localFs = FileSystem.getLocal(conf);
+    FileSystem rfs = ((LocalFileSystem)localFs).getRaw();
+    Path path = new Path(tmpDir, "data.in");
+    IFile.Writer<Text, Text> writer = 
+      new IFile.Writer<Text, Text>(conf, rfs, path, Text.class, Text.class,
+                                   codec, null);
+    for(Pair p: vals) {
+      writer.append(new Text(p.key), new Text(p.value));
+    }
+    writer.close();
+    
+    @SuppressWarnings("unchecked")
+    RawKeyValueIterator rawItr = 
+      Merger.merge(conf, rfs, Text.class, Text.class, codec, new Path[]{path}, 
+                   false, conf.getInt(JobContext.IO_SORT_FACTOR, 100), tmpDir, 
+                   new Text.Comparator(), new NullProgress(), null, null, null);
+    @SuppressWarnings("unchecked") // WritableComparators are not generic
+    ReduceTask.ValuesIterator valItr = 
+      new ReduceTask.ValuesIterator<Text,Text>(rawItr,
+          WritableComparator.get(Text.class), Text.class, Text.class,
+          conf, new NullProgress());
+    int i = 0;
+    while (valItr.more()) {
+      Object key = valItr.getKey();
+      String keyString = key.toString();
+      // make sure it matches!
+      assertEquals(vals[i].key, keyString);
+      // must have at least 1 value!
+      assertTrue(valItr.hasNext());
+      while (valItr.hasNext()) {
+        String valueString = valItr.next().toString();
+        // make sure the values match
+        assertEquals(vals[i].value, valueString);
+        // make sure the keys match
+        assertEquals(vals[i].key, valItr.getKey().toString());
+        i += 1;
+      }
+      // make sure the key hasn't changed under the hood
+      assertEquals(keyString, valItr.getKey().toString());
+      valItr.nextKey();
+    }
+    assertEquals(vals.length, i);
+    // make sure we have progress equal to 1.0
+    assertEquals(1.0f, rawItr.getProgress().get());
+  }
+
+  public void testValueIterator() throws Exception {
+    Path tmpDir = new Path("build/test/test.reduce.task");
+    Configuration conf = new Configuration();
+    for (Pair[] testCase: testCases) {
+      runValueIterator(tmpDir, testCase, conf, null);
+    }
+  }
+  
+  public void testValueIteratorWithCompression() throws Exception {
+    Path tmpDir = new Path("build/test/test.reduce.task.compression");
+    Configuration conf = new Configuration();
+    DefaultCodec codec = new DefaultCodec();
+    codec.setConf(conf);
+    for (Pair[] testCase: testCases) {
+      runValueIterator(tmpDir, testCase, conf, codec);
+    }
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java
new file mode 100644
index 0000000..b8be740
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.io.*;
+
+import junit.framework.TestCase;
+import org.apache.commons.logging.*;
+
+public class TestSequenceFileAsBinaryInputFormat extends TestCase {
+  private static final Log LOG = FileInputFormat.LOG;
+  private static final int RECORDS = 10000;
+
+  public void testBinary() throws IOException {
+    JobConf job = new JobConf();
+    FileSystem fs = FileSystem.getLocal(job);
+    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
+    Path file = new Path(dir, "testbinary.seq");
+    Random r = new Random();
+    long seed = r.nextLong();
+    r.setSeed(seed);
+
+    fs.delete(dir, true);
+    FileInputFormat.setInputPaths(job, dir);
+
+    Text tkey = new Text();
+    Text tval = new Text();
+
+    SequenceFile.Writer writer =
+      new SequenceFile.Writer(fs, job, file, Text.class, Text.class);
+    try {
+      for (int i = 0; i < RECORDS; ++i) {
+        tkey.set(Integer.toString(r.nextInt(), 36));
+        tval.set(Long.toString(r.nextLong(), 36));
+        writer.append(tkey, tval);
+      }
+    } finally {
+      writer.close();
+    }
+
+    InputFormat<BytesWritable,BytesWritable> bformat =
+      new SequenceFileAsBinaryInputFormat();
+
+    int count = 0;
+    r.setSeed(seed);
+    BytesWritable bkey = new BytesWritable();
+    BytesWritable bval = new BytesWritable();
+    Text cmpkey = new Text();
+    Text cmpval = new Text();
+    DataInputBuffer buf = new DataInputBuffer();
+    final int NUM_SPLITS = 3;
+    FileInputFormat.setInputPaths(job, file);
+    for (InputSplit split : bformat.getSplits(job, NUM_SPLITS)) {
+      RecordReader<BytesWritable,BytesWritable> reader =
+        bformat.getRecordReader(split, job, Reporter.NULL);
+      try {
+        while (reader.next(bkey, bval)) {
+          tkey.set(Integer.toString(r.nextInt(), 36));
+          tval.set(Long.toString(r.nextLong(), 36));
+          buf.reset(bkey.getBytes(), bkey.getLength());
+          cmpkey.readFields(buf);
+          buf.reset(bval.getBytes(), bval.getLength());
+          cmpval.readFields(buf);
+          assertTrue(
+              "Keys don't match: " + "*" + cmpkey.toString() + ":" +
+                                           tkey.toString() + "*",
+              cmpkey.toString().equals(tkey.toString()));
+          assertTrue(
+              "Vals don't match: " + "*" + cmpval.toString() + ":" +
+                                           tval.toString() + "*",
+              cmpval.toString().equals(tval.toString()));
+          ++count;
+        }
+      } finally {
+        reader.close();
+      }
+    }
+    assertEquals("Some records not found", RECORDS, count);
+  }
+
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java
new file mode 100644
index 0000000..abe21f2
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java
@@ -0,0 +1,212 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+
+import junit.framework.TestCase;
+import org.apache.commons.logging.*;
+
+public class TestSequenceFileAsBinaryOutputFormat extends TestCase {
+  private static final Log LOG =
+      LogFactory.getLog(TestSequenceFileAsBinaryOutputFormat.class.getName());
+
+  private static final int RECORDS = 10000;
+  // A random task attempt id for testing.
+  private static final String attempt = "attempt_200707121733_0001_m_000000_0";
+
+  public void testBinary() throws IOException {
+    JobConf job = new JobConf();
+    FileSystem fs = FileSystem.getLocal(job);
+    
+    Path dir = 
+      new Path(new Path(new Path(System.getProperty("test.build.data",".")), 
+                        FileOutputCommitter.TEMP_DIR_NAME), "_" + attempt);
+    Path file = new Path(dir, "testbinary.seq");
+    Random r = new Random();
+    long seed = r.nextLong();
+    r.setSeed(seed);
+
+    fs.delete(dir, true);
+    if (!fs.mkdirs(dir)) { 
+      fail("Failed to create output directory");
+    }
+
+    job.set(JobContext.TASK_ATTEMPT_ID, attempt);
+    FileOutputFormat.setOutputPath(job, dir.getParent().getParent());
+    FileOutputFormat.setWorkOutputPath(job, dir);
+
+    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, 
+                                          IntWritable.class );
+    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, 
+                                          DoubleWritable.class ); 
+
+    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);
+    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, 
+                                                       CompressionType.BLOCK);
+
+    BytesWritable bkey = new BytesWritable();
+    BytesWritable bval = new BytesWritable();
+
+
+    RecordWriter <BytesWritable, BytesWritable> writer = 
+      new SequenceFileAsBinaryOutputFormat().getRecordWriter(fs, 
+                                                       job, file.toString(),
+                                                       Reporter.NULL);
+
+    IntWritable iwritable = new IntWritable();
+    DoubleWritable dwritable = new DoubleWritable();
+    DataOutputBuffer outbuf = new DataOutputBuffer();
+    LOG.info("Creating data by SequenceFileAsBinaryOutputFormat");
+    try {
+      for (int i = 0; i < RECORDS; ++i) {
+        iwritable = new IntWritable(r.nextInt());
+        iwritable.write(outbuf);
+        bkey.set(outbuf.getData(), 0, outbuf.getLength());
+        outbuf.reset();
+        dwritable = new DoubleWritable(r.nextDouble());
+        dwritable.write(outbuf);
+        bval.set(outbuf.getData(), 0, outbuf.getLength());
+        outbuf.reset();
+        writer.write(bkey, bval);
+      }
+    } finally {
+      writer.close(Reporter.NULL);
+    }
+
+    InputFormat<IntWritable,DoubleWritable> iformat =
+                    new SequenceFileInputFormat<IntWritable,DoubleWritable>();
+    int count = 0;
+    r.setSeed(seed);
+    DataInputBuffer buf = new DataInputBuffer();
+    final int NUM_SPLITS = 3;
+    SequenceFileInputFormat.addInputPath(job, file);
+    LOG.info("Reading data by SequenceFileInputFormat");
+    for (InputSplit split : iformat.getSplits(job, NUM_SPLITS)) {
+      RecordReader<IntWritable,DoubleWritable> reader =
+        iformat.getRecordReader(split, job, Reporter.NULL);
+      try {
+        int sourceInt;
+        double sourceDouble;
+        while (reader.next(iwritable, dwritable)) {
+          sourceInt = r.nextInt();
+          sourceDouble = r.nextDouble();
+          assertEquals(
+              "Keys don't match: " + "*" + iwritable.get() + ":" + 
+                                           sourceInt + "*",
+              sourceInt, iwritable.get());
+          assertTrue(
+              "Vals don't match: " + "*" + dwritable.get() + ":" +
+                                           sourceDouble + "*",
+              Double.compare(dwritable.get(), sourceDouble) == 0 );
+          ++count;
+        }
+      } finally {
+        reader.close();
+      }
+    }
+    assertEquals("Some records not found", RECORDS, count);
+  }
+
+  public void testSequenceOutputClassDefaultsToMapRedOutputClass() 
+         throws IOException {
+    JobConf job = new JobConf();
+    FileSystem fs = FileSystem.getLocal(job);
+
+    // Setting Random class to test getSequenceFileOutput{Key,Value}Class
+    job.setOutputKeyClass(FloatWritable.class);
+    job.setOutputValueClass(BooleanWritable.class);
+
+    assertEquals("SequenceFileOutputKeyClass should default to ouputKeyClass", 
+             FloatWritable.class,
+             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(
+                                                                         job));
+    assertEquals("SequenceFileOutputValueClass should default to " 
+             + "ouputValueClass", 
+             BooleanWritable.class,
+             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(
+                                                                         job));
+
+    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, 
+                                          IntWritable.class );
+    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, 
+                                          DoubleWritable.class ); 
+
+    assertEquals("SequenceFileOutputKeyClass not updated", 
+             IntWritable.class,
+             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(
+                                                                         job));
+    assertEquals("SequenceFileOutputValueClass not updated", 
+             DoubleWritable.class,
+             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(
+                                                                         job));
+  }
+
+  public void testcheckOutputSpecsForbidRecordCompression() throws IOException {
+    JobConf job = new JobConf();
+    FileSystem fs = FileSystem.getLocal(job);
+    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
+    Path outputdir = new Path(System.getProperty("test.build.data",".") 
+                              + "/output");
+
+    fs.delete(dir, true);
+    fs.delete(outputdir, true);
+    if (!fs.mkdirs(dir)) { 
+      fail("Failed to create output directory");
+    }
+
+    FileOutputFormat.setWorkOutputPath(job, dir);
+
+    // Without outputpath, FileOutputFormat.checkoutputspecs will throw 
+    // InvalidJobConfException
+    FileOutputFormat.setOutputPath(job, outputdir);
+
+    // SequenceFileAsBinaryOutputFormat doesn't support record compression
+    // It should throw an exception when checked by checkOutputSpecs
+    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);
+
+    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, 
+                                                       CompressionType.BLOCK);
+    try {
+      new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(fs, job);
+    } catch (Exception e) {
+      fail("Block compression should be allowed for " 
+                       + "SequenceFileAsBinaryOutputFormat:" 
+                       + "Caught " + e.getClass().getName());
+    }
+
+    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, 
+                                                       CompressionType.RECORD);
+    try {
+      new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(fs, job);
+      fail("Record compression should not be allowed for " 
+                           +"SequenceFileAsBinaryOutputFormat");
+    } catch (InvalidJobConfException ie) {
+      // expected
+    } catch (Exception e) {
+      fail("Expected " + InvalidJobConfException.class.getName() 
+                       + "but caught " + e.getClass().getName() );
+    }
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java
new file mode 100644
index 0000000..4cfd59a
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java
@@ -0,0 +1,119 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.*;
+import java.util.*;
+import junit.framework.TestCase;
+
+import org.apache.commons.logging.*;
+
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.conf.*;
+
+public class TestSequenceFileAsTextInputFormat extends TestCase {
+  private static final Log LOG = FileInputFormat.LOG;
+
+  private static int MAX_LENGTH = 10000;
+  private static Configuration conf = new Configuration();
+
+  public void testFormat() throws Exception {
+    JobConf job = new JobConf(conf);
+    FileSystem fs = FileSystem.getLocal(conf);
+    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
+    Path file = new Path(dir, "test.seq");
+    
+    Reporter reporter = Reporter.NULL;
+    
+    int seed = new Random().nextInt();
+    //LOG.info("seed = "+seed);
+    Random random = new Random(seed);
+
+    fs.delete(dir, true);
+
+    FileInputFormat.setInputPaths(job, dir);
+
+    // for a variety of lengths
+    for (int length = 0; length < MAX_LENGTH;
+         length+= random.nextInt(MAX_LENGTH/10)+1) {
+
+      //LOG.info("creating; entries = " + length);
+
+      // create a file with length entries
+      SequenceFile.Writer writer =
+        SequenceFile.createWriter(fs, conf, file,
+                                  IntWritable.class, LongWritable.class);
+      try {
+        for (int i = 0; i < length; i++) {
+          IntWritable key = new IntWritable(i);
+          LongWritable value = new LongWritable(10 * i);
+          writer.append(key, value);
+        }
+      } finally {
+        writer.close();
+      }
+
+      // try splitting the file in a variety of sizes
+      InputFormat<Text, Text> format =
+        new SequenceFileAsTextInputFormat();
+      
+      for (int i = 0; i < 3; i++) {
+        int numSplits =
+          random.nextInt(MAX_LENGTH/(SequenceFile.SYNC_INTERVAL/20))+1;
+        //LOG.info("splitting: requesting = " + numSplits);
+        InputSplit[] splits = format.getSplits(job, numSplits);
+        //LOG.info("splitting: got =        " + splits.length);
+
+        // check each split
+        BitSet bits = new BitSet(length);
+        for (int j = 0; j < splits.length; j++) {
+          RecordReader<Text, Text> reader =
+            format.getRecordReader(splits[j], job, reporter);
+          Class readerClass = reader.getClass();
+          assertEquals("reader class is SequenceFileAsTextRecordReader.", SequenceFileAsTextRecordReader.class, readerClass);        
+          Text value = reader.createValue();
+          Text key = reader.createKey();
+          try {
+            int count = 0;
+            while (reader.next(key, value)) {
+              // if (bits.get(key.get())) {
+              // LOG.info("splits["+j+"]="+splits[j]+" : " + key.get());
+              // LOG.info("@"+reader.getPos());
+              // }
+              int keyInt = Integer.parseInt(key.toString());
+              assertFalse("Key in multiple partitions.", bits.get(keyInt));
+              bits.set(keyInt);
+              count++;
+            }
+            //LOG.info("splits["+j+"]="+splits[j]+" count=" + count);
+          } finally {
+            reader.close();
+          }
+        }
+        assertEquals("Some keys in no partition.", length, bits.cardinality());
+      }
+
+    }
+  }
+
+  public static void main(String[] args) throws Exception {
+    new TestSequenceFileAsTextInputFormat().testFormat();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java
new file mode 100644
index 0000000..e50c396
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java
@@ -0,0 +1,176 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.*;
+import java.util.*;
+import junit.framework.TestCase;
+
+import org.apache.commons.logging.*;
+
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.conf.*;
+
+public class TestSequenceFileInputFilter extends TestCase {
+  private static final Log LOG = FileInputFormat.LOG;
+
+  private static final int MAX_LENGTH = 15000;
+  private static final Configuration conf = new Configuration();
+  private static final JobConf job = new JobConf(conf);
+  private static final FileSystem fs;
+  private static final Path inDir = new Path(System.getProperty("test.build.data",".") + "/mapred");
+  private static final Path inFile = new Path(inDir, "test.seq");
+  private static final Random random = new Random(1);
+  private static final Reporter reporter = Reporter.NULL;
+  
+  static {
+    FileInputFormat.setInputPaths(job, inDir);
+    try {
+      fs = FileSystem.getLocal(conf);
+    } catch (IOException e) {
+      e.printStackTrace();
+      throw new RuntimeException(e);
+    }
+  }
+
+  private static void createSequenceFile(int numRecords) throws Exception {
+    // create a file with length entries
+    SequenceFile.Writer writer =
+      SequenceFile.createWriter(fs, conf, inFile,
+                                Text.class, BytesWritable.class);
+    try {
+      for (int i = 1; i <= numRecords; i++) {
+        Text key = new Text(Integer.toString(i));
+        byte[] data = new byte[random.nextInt(10)];
+        random.nextBytes(data);
+        BytesWritable value = new BytesWritable(data);
+        writer.append(key, value);
+      }
+    } finally {
+      writer.close();
+    }
+  }
+
+
+  private int countRecords(int numSplits) throws IOException {
+    InputFormat<Text, BytesWritable> format =
+      new SequenceFileInputFilter<Text, BytesWritable>();
+    Text key = new Text();
+    BytesWritable value = new BytesWritable();
+    if (numSplits==0) {
+      numSplits =
+        random.nextInt(MAX_LENGTH/(SequenceFile.SYNC_INTERVAL/20))+1;
+    }
+    InputSplit[] splits = format.getSplits(job, numSplits);
+      
+    // check each split
+    int count = 0;
+    LOG.info("Generated " + splits.length + " splits.");
+    for (int j = 0; j < splits.length; j++) {
+      RecordReader<Text, BytesWritable> reader =
+        format.getRecordReader(splits[j], job, reporter);
+      try {
+        while (reader.next(key, value)) {
+          LOG.info("Accept record "+key.toString());
+          count++;
+        }
+      } finally {
+        reader.close();
+      }
+    }
+    return count;
+  }
+  
+  public void testRegexFilter() throws Exception {
+    // set the filter class
+    LOG.info("Testing Regex Filter with patter: \\A10*");
+    SequenceFileInputFilter.setFilterClass(job, 
+                                           SequenceFileInputFilter.RegexFilter.class);
+    SequenceFileInputFilter.RegexFilter.setPattern(job, "\\A10*");
+    
+    // clean input dir
+    fs.delete(inDir, true);
+  
+    // for a variety of lengths
+    for (int length = 1; length < MAX_LENGTH;
+         length+= random.nextInt(MAX_LENGTH/10)+1) {
+      LOG.info("******Number of records: "+length);
+      createSequenceFile(length);
+      int count = countRecords(0);
+      assertEquals(count, length==0?0:(int)Math.log10(length)+1);
+    }
+    
+    // clean up
+    fs.delete(inDir, true);
+  }
+
+  public void testPercentFilter() throws Exception {
+    LOG.info("Testing Percent Filter with frequency: 1000");
+    // set the filter class
+    SequenceFileInputFilter.setFilterClass(job, 
+                                           SequenceFileInputFilter.PercentFilter.class);
+    SequenceFileInputFilter.PercentFilter.setFrequency(job, 1000);
+      
+    // clean input dir
+    fs.delete(inDir, true);
+    
+    // for a variety of lengths
+    for (int length = 0; length < MAX_LENGTH;
+         length+= random.nextInt(MAX_LENGTH/10)+1) {
+      LOG.info("******Number of records: "+length);
+      createSequenceFile(length);
+      int count = countRecords(1);
+      LOG.info("Accepted "+count+" records");
+      int expectedCount = length/1000;
+      if (expectedCount*1000!=length)
+        expectedCount++;
+      assertEquals(count, expectedCount);
+    }
+      
+    // clean up
+    fs.delete(inDir, true);
+  }
+  
+  public void testMD5Filter() throws Exception {
+    // set the filter class
+    LOG.info("Testing MD5 Filter with frequency: 1000");
+    SequenceFileInputFilter.setFilterClass(job, 
+                                           SequenceFileInputFilter.MD5Filter.class);
+    SequenceFileInputFilter.MD5Filter.setFrequency(job, 1000);
+      
+    // clean input dir
+    fs.delete(inDir, true);
+    
+    // for a variety of lengths
+    for (int length = 0; length < MAX_LENGTH;
+         length+= random.nextInt(MAX_LENGTH/10)+1) {
+      LOG.info("******Number of records: "+length);
+      createSequenceFile(length);
+      LOG.info("Accepted "+countRecords(0)+" records");
+    }
+    // clean up
+    fs.delete(inDir, true);
+  }
+
+  public static void main(String[] args) throws Exception {
+    TestSequenceFileInputFilter filter = new TestSequenceFileInputFilter();
+    filter.testRegexFilter();
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java
new file mode 100644
index 0000000..ad4d4ce
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.util.Iterator;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapred.SortedRanges.Range;
+
+public class TestSortedRanges extends TestCase {
+  private static final Log LOG = 
+    LogFactory.getLog(TestSortedRanges.class);
+  
+  public void testAdd() {
+    SortedRanges sr = new SortedRanges();
+    sr.add(new Range(2,9));
+    assertEquals(9, sr.getIndicesCount());
+    
+    sr.add(new SortedRanges.Range(3,5));
+    assertEquals(9, sr.getIndicesCount());
+    
+    sr.add(new SortedRanges.Range(7,1));
+    assertEquals(9, sr.getIndicesCount());
+    
+    sr.add(new Range(1,12));
+    assertEquals(12, sr.getIndicesCount());
+    
+    sr.add(new Range(7,9));
+    assertEquals(15, sr.getIndicesCount());
+    
+    sr.add(new Range(31,10));
+    sr.add(new Range(51,10));
+    sr.add(new Range(66,10));
+    assertEquals(45, sr.getIndicesCount());
+    
+    sr.add(new Range(21,50));
+    assertEquals(70, sr.getIndicesCount());
+    
+    LOG.debug(sr);
+    
+    Iterator<Long> it = sr.skipRangeIterator();
+    int i = 0;
+    assertEquals(i, it.next().longValue());
+    for(i=16;i<21;i++) {
+      assertEquals(i, it.next().longValue());
+    }
+    assertEquals(76, it.next().longValue());
+    assertEquals(77, it.next().longValue());
+    
+  }
+  
+  public void testRemove() {
+    SortedRanges sr = new SortedRanges();
+    sr.add(new Range(2,19));
+    assertEquals(19, sr.getIndicesCount());
+    
+    sr.remove(new SortedRanges.Range(15,8));
+    assertEquals(13, sr.getIndicesCount());
+    
+    sr.remove(new SortedRanges.Range(6,5));
+    assertEquals(8, sr.getIndicesCount());
+    
+    sr.remove(new SortedRanges.Range(8,4));
+    assertEquals(7, sr.getIndicesCount());
+    
+    sr.add(new Range(18,5));
+    assertEquals(12, sr.getIndicesCount());
+    
+    sr.add(new Range(25,1));
+    assertEquals(13, sr.getIndicesCount());
+    
+    sr.remove(new SortedRanges.Range(7,24));
+    assertEquals(4, sr.getIndicesCount());
+    
+    sr.remove(new SortedRanges.Range(5,1));
+    assertEquals(3, sr.getIndicesCount());
+    
+    LOG.debug(sr);
+  }
+
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java
new file mode 100644
index 0000000..87ab6b2
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java
@@ -0,0 +1,83 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.mapred.StatisticsCollector.TimeWindow;
+import org.apache.hadoop.mapred.StatisticsCollector.Stat;
+
+public class TestStatisticsCollector extends TestCase{
+
+  public void testMovingWindow() throws Exception {
+    StatisticsCollector collector = new StatisticsCollector(1);
+    TimeWindow window = new TimeWindow("test", 6, 2);
+    TimeWindow sincStart = StatisticsCollector.SINCE_START;
+    TimeWindow[] windows = {sincStart, window};
+    
+    Stat stat = collector.createStat("m1", windows);
+    
+    stat.inc(3);
+    collector.update();
+    assertEquals(0, stat.getValues().get(window).getValue());
+    assertEquals(3, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(3);
+    collector.update();
+    assertEquals((3+3), stat.getValues().get(window).getValue());
+    assertEquals(6, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(10);
+    collector.update();
+    assertEquals((3+3), stat.getValues().get(window).getValue());
+    assertEquals(16, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(10);
+    collector.update();
+    assertEquals((3+3+10+10), stat.getValues().get(window).getValue());
+    assertEquals(26, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(10);
+    collector.update();
+    stat.inc(10);
+    collector.update();
+    assertEquals((3+3+10+10+10+10), stat.getValues().get(window).getValue());
+    assertEquals(46, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(10);
+    collector.update();
+    assertEquals((3+3+10+10+10+10), stat.getValues().get(window).getValue());
+    assertEquals(56, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(12);
+    collector.update();
+    assertEquals((10+10+10+10+10+12), stat.getValues().get(window).getValue());
+    assertEquals(68, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(13);
+    collector.update();
+    assertEquals((10+10+10+10+10+12), stat.getValues().get(window).getValue());
+    assertEquals(81, stat.getValues().get(sincStart).getValue());
+    
+    stat.inc(14);
+    collector.update();
+    assertEquals((10+10+10+12+13+14), stat.getValues().get(window).getValue());
+    assertEquals(95, stat.getValues().get(sincStart).getValue());
+  }
+
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java
new file mode 100644
index 0000000..e71103d
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java
@@ -0,0 +1,207 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+public class TestTaskStatus {
+  private static final Log LOG = LogFactory.getLog(TestTaskStatus.class);
+
+  @Test
+  public void testMapTaskStatusStartAndFinishTimes() {
+    checkTaskStatues(true);
+  }
+
+  @Test
+  public void testReduceTaskStatusStartAndFinishTimes() {
+    checkTaskStatues(false);
+  }
+
+  /**
+   * Private utility method which ensures uniform testing of newly created
+   * TaskStatus object.
+   * 
+   * @param isMap
+   *          true to test map task status, false for reduce.
+   */
+  private void checkTaskStatues(boolean isMap) {
+
+    TaskStatus status = null;
+    if (isMap) {
+      status = new MapTaskStatus();
+    } else {
+      status = new ReduceTaskStatus();
+    }
+    long currentTime = System.currentTimeMillis();
+    // first try to set the finish time before
+    // start time is set.
+    status.setFinishTime(currentTime);
+    assertEquals("Finish time of the task status set without start time", 0,
+        status.getFinishTime());
+    // Now set the start time to right time.
+    status.setStartTime(currentTime);
+    assertEquals("Start time of the task status not set correctly.",
+        currentTime, status.getStartTime());
+    // try setting wrong start time to task status.
+    long wrongTime = -1;
+    status.setStartTime(wrongTime);
+    assertEquals(
+        "Start time of the task status is set to wrong negative value",
+        currentTime, status.getStartTime());
+    // finally try setting wrong finish time i.e. negative value.
+    status.setFinishTime(wrongTime);
+    assertEquals("Finish time of task status is set to wrong negative value",
+        0, status.getFinishTime());
+    status.setFinishTime(currentTime);
+    assertEquals("Finish time of the task status not set correctly.",
+        currentTime, status.getFinishTime());
+    
+    // test with null task-diagnostics
+    TaskStatus ts = ((TaskStatus)status.clone());
+    ts.setDiagnosticInfo(null);
+    ts.setDiagnosticInfo("");
+    ts.setStateString(null);
+    ts.setStateString("");
+    ((TaskStatus)status.clone()).statusUpdate(ts);
+    
+    // test with null state-string
+    ((TaskStatus)status.clone()).statusUpdate(0, null, null);
+    ((TaskStatus)status.clone()).statusUpdate(0, "", null);
+    ((TaskStatus)status.clone()).statusUpdate(null, 0, "", null, 1);
+  }
+  
+  /**
+   * Test the {@link TaskStatus} against large sized task-diagnostic-info and 
+   * state-string. Does the following
+   *  - create Map/Reduce TaskStatus such that the task-diagnostic-info and 
+   *    state-string are small strings and check their contents
+   *  - append them with small string and check their contents
+   *  - append them with large string and check their size
+   *  - update the status using statusUpdate() calls and check the size/contents
+   *  - create Map/Reduce TaskStatus with large string and check their size
+   */
+  @Test
+  public void testTaskDiagnosticsAndStateString() {
+    // check the default case
+    String test = "hi";
+    final int maxSize = 16;
+    TaskStatus status = new TaskStatus(null, 0, 0, null, test, test, null, null, 
+                                       null) {
+      @Override
+      protected int getMaxStringSize() {
+        return maxSize;
+      }
+
+      @Override
+      public void addFetchFailedMap(TaskAttemptID mapTaskId) {
+      }
+
+      @Override
+      public boolean getIsMap() {
+        return false;
+      }
+    };
+    assertEquals("Small diagnostic info test failed", 
+                 status.getDiagnosticInfo(), test);
+    assertEquals("Small state string test failed", status.getStateString(), 
+                 test);
+    
+    // now append some small string and check
+    String newDInfo = test.concat(test);
+    status.setDiagnosticInfo(test);
+    status.setStateString(newDInfo);
+    assertEquals("Small diagnostic info append failed", 
+                 newDInfo, status.getDiagnosticInfo());
+    assertEquals("Small state-string append failed", 
+                 newDInfo, status.getStateString());
+    
+    // update the status with small state strings
+    TaskStatus newStatus = (TaskStatus)status.clone();
+    String newSInfo = "hi1";
+    newStatus.setStateString(newSInfo);
+    status.statusUpdate(newStatus);
+    newDInfo = newDInfo.concat(newStatus.getDiagnosticInfo());
+    
+    assertEquals("Status-update on diagnostic-info failed", 
+                 newDInfo, status.getDiagnosticInfo());
+    assertEquals("Status-update on state-string failed", 
+                 newSInfo, status.getStateString());
+    
+    newSInfo = "hi2";
+    status.statusUpdate(0, newSInfo, null);
+    assertEquals("Status-update on state-string failed", 
+                 newSInfo, status.getStateString());
+    
+    newSInfo = "hi3";
+    status.statusUpdate(null, 0, newSInfo, null, 0);
+    assertEquals("Status-update on state-string failed", 
+                 newSInfo, status.getStateString());
+    
+    
+    // now append each with large string
+    String large = "hihihihihihihihihihi"; // 20 chars
+    status.setDiagnosticInfo(large);
+    status.setStateString(large);
+    assertEquals("Large diagnostic info append test failed", 
+                 maxSize, status.getDiagnosticInfo().length());
+    assertEquals("Large state-string append test failed",
+                 maxSize, status.getStateString().length());
+    
+    // update a large status with large strings
+    newStatus.setDiagnosticInfo(large + "0");
+    newStatus.setStateString(large + "1");
+    status.statusUpdate(newStatus);
+    assertEquals("Status-update on diagnostic info failed",
+                 maxSize, status.getDiagnosticInfo().length());
+    assertEquals("Status-update on state-string failed", 
+                 maxSize, status.getStateString().length());
+    
+    status.statusUpdate(0, large + "2", null);
+    assertEquals("Status-update on state-string failed", 
+                 maxSize, status.getStateString().length());
+    
+    status.statusUpdate(null, 0, large + "3", null, 0);
+    assertEquals("Status-update on state-string failed", 
+                 maxSize, status.getStateString().length());
+    
+    // test passing large string in constructor
+    status = new TaskStatus(null, 0, 0, null, large, large, null, null, 
+        null) {
+      @Override
+      protected int getMaxStringSize() {
+        return maxSize;
+      }
+
+      @Override
+      public void addFetchFailedMap(TaskAttemptID mapTaskId) {
+      }
+
+      @Override
+      public boolean getIsMap() {
+        return false;
+      }
+    };
+    assertEquals("Large diagnostic info test failed", 
+                maxSize, status.getDiagnosticInfo().length());
+    assertEquals("Large state-string test failed", 
+                 maxSize, status.getStateString().length());
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestUtils.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestUtils.java
new file mode 100644
index 0000000..0c43704
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestUtils.java
@@ -0,0 +1,79 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+public class TestUtils {
+  private static final Path[] LOG_PATHS = new Path[] {
+    new Path("file:///foo/_logs"),
+    new Path("file:///foo/_logs/"),
+    new Path("_logs/"),
+    new Path("_logs")
+  };
+
+  private static final Path[] SUCCEEDED_PATHS = new Path[] {
+    new Path("file:///blah/" + FileOutputCommitter.SUCCEEDED_FILE_NAME)
+  };
+
+  private static final Path[] PASS_PATHS = new Path[] {
+    new Path("file:///my_logs/blah"),
+    new Path("file:///a/b/c"),
+    new Path("file:///foo/_logs/blah"),
+    new Path("_logs/foo"),
+    new Path("file:///blah/" +
+             FileOutputCommitter.SUCCEEDED_FILE_NAME +
+             "/bar")
+  };
+
+  @Test
+  public void testOutputFilesFilter() {
+    PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();
+    for (Path p : LOG_PATHS) {
+      assertFalse(filter.accept(p));
+    }
+
+    for (Path p : SUCCEEDED_PATHS) {
+      assertFalse(filter.accept(p));
+    }
+
+    for (Path p : PASS_PATHS) {
+      assertTrue(filter.accept(p));
+    }
+  }
+
+  @Test
+  public void testLogFilter() {
+    PathFilter filter = new Utils.OutputFileUtils.OutputLogFilter();
+    for (Path p : LOG_PATHS) {
+      assertFalse(filter.accept(p));
+    }
+
+    for (Path p : SUCCEEDED_PATHS) {
+      assertTrue(filter.accept(p));
+    }
+
+    for (Path p : PASS_PATHS) {
+      assertTrue(filter.accept(p));
+    }
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java
new file mode 100644
index 0000000..7b85bd4
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java
@@ -0,0 +1,154 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapreduce;
+
+import java.util.Random;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapreduce.counters.LimitExceededException;
+import org.apache.hadoop.mapreduce.counters.Limits;
+
+/**
+ * TestCounters checks the sanity and recoverability of {@code Counters}
+ */
+public class TestCounters {
+
+  static final Log LOG = LogFactory.getLog(TestCounters.class);
+
+  /**
+   * Verify counter value works
+   */
+  @Test
+  public void testCounterValue() {
+    final int NUMBER_TESTS = 100;
+    final int NUMBER_INC = 10;
+    final Random rand = new Random();
+    for (int i = 0; i < NUMBER_TESTS; i++) {
+      long initValue = rand.nextInt();
+      long expectedValue = initValue;
+      Counter counter = new Counters().findCounter("test", "foo");
+      counter.setValue(initValue);
+      assertEquals("Counter value is not initialized correctly",
+          expectedValue, counter.getValue());
+      for (int j = 0; j < NUMBER_INC; j++) {
+        int incValue = rand.nextInt();
+        counter.increment(incValue);
+        expectedValue += incValue;
+        assertEquals("Counter value is not incremented correctly",
+            expectedValue, counter.getValue());
+      }
+      expectedValue = rand.nextInt();
+      counter.setValue(expectedValue);
+      assertEquals("Counter value is not set correctly",
+          expectedValue, counter.getValue());
+    }
+  }
+
+  @Test public void testLimits() {
+    for (int i = 0; i < 3; ++i) {
+      // make sure limits apply to separate containers
+      testMaxCounters(new Counters());
+      testMaxGroups(new Counters());
+    }
+  }
+  
+  @Test
+  public void testCountersIncrement() {
+    Counters fCounters = new Counters();
+    Counter fCounter = fCounters.findCounter(FRAMEWORK_COUNTER);
+    fCounter.setValue(100);
+    Counter gCounter = fCounters.findCounter("test", "foo");
+    gCounter.setValue(200);
+
+    Counters counters = new Counters();
+    counters.incrAllCounters(fCounters);
+    Counter counter;
+    for (CounterGroup cg : fCounters) {
+      CounterGroup group = counters.getGroup(cg.getName());
+      if (group.getName().equals("test")) {
+        counter = counters.findCounter("test", "foo");
+        assertEquals(200, counter.getValue());
+      } else {
+        counter = counters.findCounter(FRAMEWORK_COUNTER);
+        assertEquals(100, counter.getValue());
+      }
+    }
+  }
+
+  static final Enum<?> FRAMEWORK_COUNTER = TaskCounter.CPU_MILLISECONDS;
+  static final long FRAMEWORK_COUNTER_VALUE = 8;
+  static final String FS_SCHEME = "HDFS";
+  static final FileSystemCounter FS_COUNTER = FileSystemCounter.BYTES_READ;
+  static final long FS_COUNTER_VALUE = 10;
+
+  private void testMaxCounters(final Counters counters) {
+    LOG.info("counters max="+ Limits.COUNTERS_MAX);
+    for (int i = 0; i < Limits.COUNTERS_MAX; ++i) {
+      counters.findCounter("test", "test"+ i);
+    }
+    setExpected(counters);
+    shouldThrow(LimitExceededException.class, new Runnable() {
+      public void run() {
+        counters.findCounter("test", "bad");
+      }
+    });
+    checkExpected(counters);
+  }
+
+  private void testMaxGroups(final Counters counters) {
+    LOG.info("counter groups max="+ Limits.GROUPS_MAX);
+    for (int i = 0; i < Limits.GROUPS_MAX; ++i) {
+      // assuming COUNTERS_MAX > GROUPS_MAX
+      counters.findCounter("test"+ i, "test");
+    }
+    setExpected(counters);
+    shouldThrow(LimitExceededException.class, new Runnable() {
+      public void run() {
+        counters.findCounter("bad", "test");
+      }
+    });
+    checkExpected(counters);
+  }
+
+  private void setExpected(Counters counters) {
+    counters.findCounter(FRAMEWORK_COUNTER).setValue(FRAMEWORK_COUNTER_VALUE);
+    counters.findCounter(FS_SCHEME, FS_COUNTER).setValue(FS_COUNTER_VALUE);
+  }
+
+  private void checkExpected(Counters counters) {
+    assertEquals(FRAMEWORK_COUNTER_VALUE,
+                 counters.findCounter(FRAMEWORK_COUNTER).getValue());
+    assertEquals(FS_COUNTER_VALUE,
+                 counters.findCounter(FS_SCHEME, FS_COUNTER).getValue());
+  }
+
+  private void shouldThrow(Class<? extends Exception> ecls, Runnable runnable) {
+    try {
+      runnable.run();
+    } catch (Exception e) {
+      assertSame(ecls, e.getClass());
+      LOG.info("got expected: "+ e);
+      return;
+    }
+    assertTrue("Should've thrown "+ ecls.getSimpleName(), false);
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java
new file mode 100644
index 0000000..b893fe1
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.lib.jobcontrol;
+
+import static org.junit.Assert.assertFalse;
+
+import org.apache.hadoop.conf.Configuration;
+import org.junit.Test;
+
+/**
+ */
+public class TestControlledJob {
+  
+  @Test
+  public void testAddingDependingJobToRunningJobFails() throws Exception {
+    Configuration conf = new Configuration();
+    ControlledJob job1 = new ControlledJob(conf);
+    job1.setJobState(ControlledJob.State.RUNNING);
+    assertFalse(job1.addDependingJob(new ControlledJob(conf)));
+  }
+
+  @Test
+  public void testAddingDependingJobToCompletedJobFails() throws Exception {
+    Configuration conf = new Configuration();
+    ControlledJob job1 = new ControlledJob(conf);
+    job1.setJobState(ControlledJob.State.SUCCESS);
+    assertFalse(job1.addDependingJob(new ControlledJob(conf)));
+  }
+
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java
new file mode 100644
index 0000000..54e1302
--- /dev/null
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java
@@ -0,0 +1,677 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.util;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Random;
+import java.util.Vector;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.Shell.ExitCodeException;
+import org.apache.hadoop.util.Shell.ShellCommandExecutor;
+
+import junit.framework.TestCase;
+
+/**
+ * A JUnit test to test ProcfsBasedProcessTree.
+ */
+public class TestProcfsBasedProcessTree extends TestCase {
+
+  private static final Log LOG = LogFactory
+      .getLog(TestProcfsBasedProcessTree.class);
+  private static String TEST_ROOT_DIR = new Path(System.getProperty(
+         "test.build.data", "/tmp")).toString().replace(' ', '+');
+
+  private ShellCommandExecutor shexec = null;
+  private String pidFile, lowestDescendant;
+  private String shellScript;
+  private static final int N = 6; // Controls the RogueTask
+
+  private class RogueTaskThread extends Thread {
+    public void run() {
+      try {
+        Vector<String> args = new Vector<String>();
+        if(ProcessTree.isSetsidAvailable) {
+          args.add("setsid");
+        }
+        args.add("bash");
+        args.add("-c");
+        args.add(" echo $$ > " + pidFile + "; sh " +
+                          shellScript + " " + N + ";") ;
+        shexec = new ShellCommandExecutor(args.toArray(new String[0]));
+        shexec.execute();
+      } catch (ExitCodeException ee) {
+        LOG.info("Shell Command exit with a non-zero exit code. This is" +
+                 " expected as we are killing the subprocesses of the" +
+                 " task intentionally. " + ee);
+      } catch (IOException ioe) {
+        LOG.info("Error executing shell command " + ioe);
+      } finally {
+        LOG.info("Exit code: " + shexec.getExitCode());
+      }
+    }
+  }
+
+  private String getRogueTaskPID() {
+    File f = new File(pidFile);
+    while (!f.exists()) {
+      try {
+        Thread.sleep(500);
+      } catch (InterruptedException ie) {
+        break;
+      }
+    }
+
+    // read from pidFile
+    return getPidFromPidFile(pidFile);
+  }
+
+  public void testProcessTree() {
+
+    try {
+      if (!ProcfsBasedProcessTree.isAvailable()) {
+        System.out
+            .println("ProcfsBasedProcessTree is not available on this system. Not testing");
+        return;
+      }
+    } catch (Exception e) {
+      LOG.info(StringUtils.stringifyException(e));
+      return;
+    }
+    // create shell script
+    Random rm = new Random();
+    File tempFile = new File(TEST_ROOT_DIR, this.getName() + "_shellScript_" +
+                             rm.nextInt() + ".sh");
+    tempFile.deleteOnExit();
+    shellScript = TEST_ROOT_DIR + File.separator + tempFile.getName();
+
+    // create pid file
+    tempFile = new File(TEST_ROOT_DIR,  this.getName() + "_pidFile_" +
+                        rm.nextInt() + ".pid");
+    tempFile.deleteOnExit();
+    pidFile = TEST_ROOT_DIR + File.separator + tempFile.getName();
+
+    lowestDescendant = TEST_ROOT_DIR + File.separator + "lowestDescendantPidFile";
+
+    // write to shell-script
+    try {
+      FileWriter fWriter = new FileWriter(shellScript);
+      fWriter.write(
+          "# rogue task\n" +
+          "sleep 1\n" +
+          "echo hello\n" +
+          "if [ $1 -ne 0 ]\n" +
+          "then\n" +
+          " sh " + shellScript + " $(($1-1))\n" +
+          "else\n" +
+          " echo $$ > " + lowestDescendant + "\n" +
+          " while true\n do\n" +
+          "  sleep 5\n" +
+          " done\n" +
+          "fi");
+      fWriter.close();
+    } catch (IOException ioe) {
+      LOG.info("Error: " + ioe);
+      return;
+    }
+
+    Thread t = new RogueTaskThread();
+    t.start();
+    String pid = getRogueTaskPID();
+    LOG.info("Root process pid: " + pid);
+    ProcfsBasedProcessTree p = new ProcfsBasedProcessTree(pid,
+                               ProcessTree.isSetsidAvailable,
+                               ProcessTree.DEFAULT_SLEEPTIME_BEFORE_SIGKILL);
+    p = p.getProcessTree(); // initialize
+    LOG.info("ProcessTree: " + p.toString());
+
+    File leaf = new File(lowestDescendant);
+    //wait till lowest descendant process of Rougue Task starts execution
+    while (!leaf.exists()) {
+      try {
+        Thread.sleep(500);
+      } catch (InterruptedException ie) {
+        break;
+      }
+    }
+
+    p = p.getProcessTree(); // reconstruct
+    LOG.info("ProcessTree: " + p.toString());
+
+    // Get the process-tree dump
+    String processTreeDump = p.getProcessTreeDump();
+
+    // destroy the process and all its subprocesses
+    p.destroy(true/*in the background*/);
+
+    if(ProcessTree.isSetsidAvailable) {// whole processtree should be gone
+      assertEquals(false, p.isAnyProcessInTreeAlive());
+    }
+    else {// process should be gone
+      assertFalse("ProcessTree must have been gone", p.isAlive());
+    }
+
+    LOG.info("Process-tree dump follows: \n" + processTreeDump);
+    assertTrue("Process-tree dump doesn't start with a proper header",
+        processTreeDump.startsWith("\t|- PID PPID PGRPID SESSID CMD_NAME " +
+        "USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) " +
+        "RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n"));
+    for (int i = N; i >= 0; i--) {
+      String cmdLineDump = "\\|- [0-9]+ [0-9]+ [0-9]+ [0-9]+ \\(sh\\)" +
+          " [0-9]+ [0-9]+ [0-9]+ [0-9]+ sh " + shellScript + " " + i;
+      Pattern pat = Pattern.compile(cmdLineDump);
+      Matcher mat = pat.matcher(processTreeDump);
+      assertTrue("Process-tree dump doesn't contain the cmdLineDump of " + i
+          + "th process!", mat.find());
+    }
+
+    // Not able to join thread sometimes when forking with large N.
+    try {
+      t.join(2000);
+      LOG.info("RogueTaskThread successfully joined.");
+    } catch (InterruptedException ie) {
+      LOG.info("Interrupted while joining RogueTaskThread.");
+    }
+
+    // ProcessTree is gone now. Any further calls should be sane.
+    p = p.getProcessTree();
+    assertFalse("ProcessTree must have been gone", p.isAlive());
+    assertTrue("Cumulative vmem for the gone-process is "
+        + p.getCumulativeVmem() + " . It should be zero.", p
+        .getCumulativeVmem() == 0);
+    assertTrue(p.toString().equals("[ ]"));
+  }
+
+  /**
+   * Get PID from a pid-file.
+   * 
+   * @param pidFileName
+   *          Name of the pid-file.
+   * @return the PID string read from the pid-file. Returns null if the
+   *         pidFileName points to a non-existing file or if read fails from the
+   *         file.
+   */
+  public static String getPidFromPidFile(String pidFileName) {
+    BufferedReader pidFile = null;
+    FileReader fReader = null;
+    String pid = null;
+
+    try {
+      fReader = new FileReader(pidFileName);
+      pidFile = new BufferedReader(fReader);
+    } catch (FileNotFoundException f) {
+      LOG.debug("PidFile doesn't exist : " + pidFileName);
+      return pid;
+    }
+
+    try {
+      pid = pidFile.readLine();
+    } catch (IOException i) {
+      LOG.error("Failed to read from " + pidFileName);
+    } finally {
+      try {
+        if (fReader != null) {
+          fReader.close();
+        }
+        try {
+          if (pidFile != null) {
+            pidFile.close();
+          }
+        } catch (IOException i) {
+          LOG.warn("Error closing the stream " + pidFile);
+        }
+      } catch (IOException i) {
+        LOG.warn("Error closing the stream " + fReader);
+      }
+    }
+    return pid;
+  }
+  
+  public static class ProcessStatInfo {
+    // sample stat in a single line : 3910 (gpm) S 1 3910 3910 0 -1 4194624 
+    // 83 0 0 0 0 0 0 0 16 0 1 0 7852 2408448 88 4294967295 134512640 
+    // 134590050 3220521392 3220520036 10975138 0 0 4096 134234626 
+    // 4294967295 0 0 17 1 0 0
+    String pid;
+    String name;
+    String ppid;
+    String pgrpId;
+    String session;
+    String vmem = "0";
+    String rssmemPage = "0";
+    String utime = "0";
+    String stime = "0";
+    
+    public ProcessStatInfo(String[] statEntries) {
+      pid = statEntries[0];
+      name = statEntries[1];
+      ppid = statEntries[2];
+      pgrpId = statEntries[3];
+      session = statEntries[4];
+      vmem = statEntries[5];
+      if (statEntries.length > 6) {
+        rssmemPage = statEntries[6];
+      }
+      if (statEntries.length > 7) {
+        utime = statEntries[7];
+        stime = statEntries[8];
+      }
+    }
+    
+    // construct a line that mimics the procfs stat file.
+    // all unused numerical entries are set to 0.
+    public String getStatLine() {
+      return String.format("%s (%s) S %s %s %s 0 0 0" +
+                      " 0 0 0 0 %s %s 0 0 0 0 0 0 0 %s %s 0 0" +
+                      " 0 0 0 0 0 0 0 0" +
+                      " 0 0 0 0 0", 
+                      pid, name, ppid, pgrpId, session,
+                      utime, stime, vmem, rssmemPage);
+    }
+  }
+  
+  /**
+   * A basic test that creates a few process directories and writes
+   * stat files. Verifies that the cpu time and memory is correctly
+   * computed.
+   * @throws IOException if there was a problem setting up the
+   *                      fake procfs directories or files.
+   */
+  public void testCpuAndMemoryForProcessTree() throws IOException {
+
+    // test processes
+    String[] pids = { "100", "200", "300", "400" };
+    // create the fake procfs root directory. 
+    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
+
+    try {
+      setupProcfsRootDir(procfsRootDir);
+      setupPidDirs(procfsRootDir, pids);
+      
+      // create stat objects.
+      // assuming processes 100, 200, 300 are in tree and 400 is not.
+      ProcessStatInfo[] procInfos = new ProcessStatInfo[4];
+      procInfos[0] = new ProcessStatInfo(new String[] 
+          {"100", "proc1", "1", "100", "100", "100000", "100", "1000", "200"});
+      procInfos[1] = new ProcessStatInfo(new String[] 
+          {"200", "proc2", "100", "100", "100", "200000", "200", "2000", "400"});
+      procInfos[2] = new ProcessStatInfo(new String[] 
+          {"300", "proc3", "200", "100", "100", "300000", "300", "3000", "600"});
+      procInfos[3] = new ProcessStatInfo(new String[] 
+          {"400", "proc4", "1", "400", "400", "400000", "400", "4000", "800"});
+      
+      writeStatFiles(procfsRootDir, pids, procInfos);
+      
+      // crank up the process tree class.
+      ProcfsBasedProcessTree processTree = 
+          new ProcfsBasedProcessTree("100", true, 100L, 
+                                  procfsRootDir.getAbsolutePath());
+      // build the process tree.
+      processTree.getProcessTree();
+      
+      // verify cumulative memory
+      assertEquals("Cumulative virtual memory does not match", 600000L,
+                   processTree.getCumulativeVmem());
+
+      // verify rss memory
+      long cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
+                        600L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
+      assertEquals("Cumulative rss memory does not match",
+                   cumuRssMem, processTree.getCumulativeRssmem());
+
+      // verify cumulative cpu time
+      long cumuCpuTime = ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS > 0 ?
+             7200L * ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS : 0L;
+      assertEquals("Cumulative cpu time does not match",
+                   cumuCpuTime, processTree.getCumulativeCpuTime());
+
+      // test the cpu time again to see if it cumulates
+      procInfos[0] = new ProcessStatInfo(new String[]
+          {"100", "proc1", "1", "100", "100", "100000", "100", "2000", "300"});
+      procInfos[1] = new ProcessStatInfo(new String[]
+          {"200", "proc2", "100", "100", "100", "200000", "200", "3000", "500"});
+      writeStatFiles(procfsRootDir, pids, procInfos);
+
+      // build the process tree.
+      processTree.getProcessTree();
+
+      // verify cumulative cpu time again
+      cumuCpuTime = ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS > 0 ?
+             9400L * ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS : 0L;
+      assertEquals("Cumulative cpu time does not match",
+                   cumuCpuTime, processTree.getCumulativeCpuTime());
+    } finally {
+      FileUtil.fullyDelete(procfsRootDir);
+    }
+  }
+  
+  /**
+   * Tests that cumulative memory is computed only for
+   * processes older than a given age.
+   * @throws IOException if there was a problem setting up the
+   *                      fake procfs directories or files.
+   */
+  public void testMemForOlderProcesses() throws IOException {
+    // initial list of processes
+    String[] pids = { "100", "200", "300", "400" };
+    // create the fake procfs root directory. 
+    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
+
+    try {
+      setupProcfsRootDir(procfsRootDir);
+      setupPidDirs(procfsRootDir, pids);
+      
+      // create stat objects.
+      // assuming 100, 200 and 400 are in tree, 300 is not.
+      ProcessStatInfo[] procInfos = new ProcessStatInfo[4];
+      procInfos[0] = new ProcessStatInfo(new String[] 
+                        {"100", "proc1", "1", "100", "100", "100000", "100"});
+      procInfos[1] = new ProcessStatInfo(new String[] 
+                        {"200", "proc2", "100", "100", "100", "200000", "200"});
+      procInfos[2] = new ProcessStatInfo(new String[] 
+                        {"300", "proc3", "1", "300", "300", "300000", "300"});
+      procInfos[3] = new ProcessStatInfo(new String[] 
+                        {"400", "proc4", "100", "100", "100", "400000", "400"});
+      
+      writeStatFiles(procfsRootDir, pids, procInfos);
+      
+      // crank up the process tree class.
+      ProcfsBasedProcessTree processTree = 
+          new ProcfsBasedProcessTree("100", true, 100L, 
+                                  procfsRootDir.getAbsolutePath());
+      // build the process tree.
+      processTree.getProcessTree();
+      
+      // verify cumulative memory
+      assertEquals("Cumulative memory does not match",
+                   700000L, processTree.getCumulativeVmem());
+
+      // write one more process as child of 100.
+      String[] newPids = { "500" };
+      setupPidDirs(procfsRootDir, newPids);
+      
+      ProcessStatInfo[] newProcInfos = new ProcessStatInfo[1];
+      newProcInfos[0] = new ProcessStatInfo(new String[]
+                      {"500", "proc5", "100", "100", "100", "500000", "500"});
+      writeStatFiles(procfsRootDir, newPids, newProcInfos);
+      
+      // check memory includes the new process.
+      processTree.getProcessTree();
+      assertEquals("Cumulative vmem does not include new process",
+                   1200000L, processTree.getCumulativeVmem());
+      long cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
+                        1200L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
+      assertEquals("Cumulative rssmem does not include new process",
+                   cumuRssMem, processTree.getCumulativeRssmem());
+      
+      // however processes older than 1 iteration will retain the older value
+      assertEquals("Cumulative vmem shouldn't have included new process",
+                   700000L, processTree.getCumulativeVmem(1));
+      cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
+                   700L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
+      assertEquals("Cumulative rssmem shouldn't have included new process",
+                   cumuRssMem, processTree.getCumulativeRssmem(1));
+
+      // one more process
+      newPids = new String[]{ "600" };
+      setupPidDirs(procfsRootDir, newPids);
+      
+      newProcInfos = new ProcessStatInfo[1];
+      newProcInfos[0] = new ProcessStatInfo(new String[]
+                      {"600", "proc6", "100", "100", "100", "600000", "600"});
+      writeStatFiles(procfsRootDir, newPids, newProcInfos);
+
+      // refresh process tree
+      processTree.getProcessTree();
+      
+      // processes older than 2 iterations should be same as before.
+      assertEquals("Cumulative vmem shouldn't have included new processes",
+                   700000L, processTree.getCumulativeVmem(2));
+      cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
+                   700L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
+      assertEquals("Cumulative rssmem shouldn't have included new processes",
+                   cumuRssMem, processTree.getCumulativeRssmem(2));
+
+      // processes older than 1 iteration should not include new process,
+      // but include process 500
+      assertEquals("Cumulative vmem shouldn't have included new processes",
+                   1200000L, processTree.getCumulativeVmem(1));
+      cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
+                   1200L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
+      assertEquals("Cumulative rssmem shouldn't have included new processes",
+                   cumuRssMem, processTree.getCumulativeRssmem(1));
+
+      // no processes older than 3 iterations, this should be 0
+      assertEquals("Getting non-zero vmem for processes older than 3 iterations",
+                    0L, processTree.getCumulativeVmem(3));
+      assertEquals("Getting non-zero rssmem for processes older than 3 iterations",
+                    0L, processTree.getCumulativeRssmem(3));
+    } finally {
+      FileUtil.fullyDelete(procfsRootDir);
+    }
+  }
+
+  /**
+   * Verifies ProcfsBasedProcessTree.checkPidPgrpidForMatch() in case of
+   * 'constructProcessInfo() returning null' by not writing stat file for the
+   * mock process
+   * @throws IOException if there was a problem setting up the
+   *                      fake procfs directories or files.
+   */
+  public void testDestroyProcessTree() throws IOException {
+    // test process
+    String pid = "100";
+    // create the fake procfs root directory. 
+    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
+
+    try {
+      setupProcfsRootDir(procfsRootDir);
+      
+      // crank up the process tree class.
+      ProcfsBasedProcessTree processTree = new ProcfsBasedProcessTree(
+                        pid, true, 100L, procfsRootDir.getAbsolutePath());
+
+      // Let us not create stat file for pid 100.
+      assertTrue(ProcfsBasedProcessTree.checkPidPgrpidForMatch(
+                            pid, procfsRootDir.getAbsolutePath()));
+    } finally {
+      FileUtil.fullyDelete(procfsRootDir);
+    }
+  }
+  
+  /**
+   * Test the correctness of process-tree dump.
+   * 
+   * @throws IOException
+   */
+  public void testProcessTreeDump()
+      throws IOException {
+
+    String[] pids = { "100", "200", "300", "400", "500", "600" };
+
+    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
+
+    try {
+      setupProcfsRootDir(procfsRootDir);
+      setupPidDirs(procfsRootDir, pids);
+
+      int numProcesses = pids.length;
+      // Processes 200, 300, 400 and 500 are descendants of 100. 600 is not.
+      ProcessStatInfo[] procInfos = new ProcessStatInfo[numProcesses];
+      procInfos[0] = new ProcessStatInfo(new String[] {
+          "100", "proc1", "1", "100", "100", "100000", "100", "1000", "200"});
+      procInfos[1] = new ProcessStatInfo(new String[] {
+          "200", "proc2", "100", "100", "100", "200000", "200", "2000", "400"});
+      procInfos[2] = new ProcessStatInfo(new String[] {
+          "300", "proc3", "200", "100", "100", "300000", "300", "3000", "600"});
+      procInfos[3] = new ProcessStatInfo(new String[] {
+          "400", "proc4", "200", "100", "100", "400000", "400", "4000", "800"});
+      procInfos[4] = new ProcessStatInfo(new String[] {
+          "500", "proc5", "400", "100", "100", "400000", "400", "4000", "800"});
+      procInfos[5] = new ProcessStatInfo(new String[] {
+          "600", "proc6", "1", "1", "1", "400000", "400", "4000", "800"});
+
+      String[] cmdLines = new String[numProcesses];
+      cmdLines[0] = "proc1 arg1 arg2";
+      cmdLines[1] = "proc2 arg3 arg4";
+      cmdLines[2] = "proc3 arg5 arg6";
+      cmdLines[3] = "proc4 arg7 arg8";
+      cmdLines[4] = "proc5 arg9 arg10";
+      cmdLines[5] = "proc6 arg11 arg12";
+
+      writeStatFiles(procfsRootDir, pids, procInfos);
+      writeCmdLineFiles(procfsRootDir, pids, cmdLines);
+
+      ProcfsBasedProcessTree processTree =
+          new ProcfsBasedProcessTree("100", true, 100L, procfsRootDir
+              .getAbsolutePath());
+      // build the process tree.
+      processTree.getProcessTree();
+
+      // Get the process-tree dump
+      String processTreeDump = processTree.getProcessTreeDump();
+
+      LOG.info("Process-tree dump follows: \n" + processTreeDump);
+      assertTrue("Process-tree dump doesn't start with a proper header",
+          processTreeDump.startsWith("\t|- PID PPID PGRPID SESSID CMD_NAME " +
+          "USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) " +
+          "RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n"));
+      for (int i = 0; i < 5; i++) {
+        ProcessStatInfo p = procInfos[i];
+        assertTrue(
+            "Process-tree dump doesn't contain the cmdLineDump of process "
+                + p.pid, processTreeDump.contains("\t|- " + p.pid + " "
+                + p.ppid + " " + p.pgrpId + " " + p.session + " (" + p.name
+                + ") " + p.utime + " " + p.stime + " " + p.vmem + " "
+                + p.rssmemPage + " " + cmdLines[i]));
+      }
+
+      // 600 should not be in the dump
+      ProcessStatInfo p = procInfos[5];
+      assertFalse(
+          "Process-tree dump shouldn't contain the cmdLineDump of process "
+              + p.pid, processTreeDump.contains("\t|- " + p.pid + " " + p.ppid
+              + " " + p.pgrpId + " " + p.session + " (" + p.name + ") "
+              + p.utime + " " + p.stime + " " + p.vmem + " " + cmdLines[5]));
+    } finally {
+      FileUtil.fullyDelete(procfsRootDir);
+    }
+  }
+
+  /**
+   * Create a directory to mimic the procfs file system's root.
+   * @param procfsRootDir root directory to create.
+   * @throws IOException if could not delete the procfs root directory
+   */
+  public static void setupProcfsRootDir(File procfsRootDir) {
+    // cleanup any existing process root dir.
+    if (procfsRootDir.exists()) {
+      assertTrue(FileUtil.fullyDelete(procfsRootDir));  
+    }
+
+    // create afresh
+    assertTrue(procfsRootDir.mkdirs());
+  }
+
+  /**
+   * Create PID directories under the specified procfs root directory
+   * @param procfsRootDir root directory of procfs file system
+   * @param pids the PID directories to create.
+   * @throws IOException If PID dirs could not be created
+   */
+  public static void setupPidDirs(File procfsRootDir, String[] pids) 
+                      throws IOException {
+    for (String pid : pids) {
+      File pidDir = new File(procfsRootDir, pid);
+      pidDir.mkdir();
+      if (!pidDir.exists()) {
+        throw new IOException ("couldn't make process directory under " +
+            "fake procfs");
+      } else {
+        LOG.info("created pid dir");
+      }
+    }
+  }
+  
+  /**
+   * Write stat files under the specified pid directories with data
+   * setup in the corresponding ProcessStatInfo objects
+   * @param procfsRootDir root directory of procfs file system
+   * @param pids the PID directories under which to create the stat file
+   * @param procs corresponding ProcessStatInfo objects whose data should be
+   *              written to the stat files.
+   * @throws IOException if stat files could not be written
+   */
+  public static void writeStatFiles(File procfsRootDir, String[] pids, 
+                              ProcessStatInfo[] procs) throws IOException {
+    for (int i=0; i<pids.length; i++) {
+      File statFile =
+          new File(new File(procfsRootDir, pids[i]),
+              ProcfsBasedProcessTree.PROCFS_STAT_FILE);
+      BufferedWriter bw = null;
+      try {
+        FileWriter fw = new FileWriter(statFile);
+        bw = new BufferedWriter(fw);
+        bw.write(procs[i].getStatLine());
+        LOG.info("wrote stat file for " + pids[i] + 
+                  " with contents: " + procs[i].getStatLine());
+      } finally {
+        // not handling exception - will throw an error and fail the test.
+        if (bw != null) {
+          bw.close();
+        }
+      }
+    }
+  }
+
+  private static void writeCmdLineFiles(File procfsRootDir, String[] pids,
+      String[] cmdLines)
+      throws IOException {
+    for (int i = 0; i < pids.length; i++) {
+      File statFile =
+          new File(new File(procfsRootDir, pids[i]),
+              ProcfsBasedProcessTree.PROCFS_CMDLINE_FILE);
+      BufferedWriter bw = null;
+      try {
+        bw = new BufferedWriter(new FileWriter(statFile));
+        bw.write(cmdLines[i]);
+        LOG.info("wrote command-line file for " + pids[i] + " with contents: "
+            + cmdLines[i]);
+      } finally {
+        // not handling exception - will throw an error and fail the test.
+        if (bw != null) {
+          bw.close();
+        }
+      }
+    }
+  }
+}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java
deleted file mode 100644
index a6aebb0..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestAuditLogger.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.ipc.ProtocolInfo;
-import org.apache.hadoop.ipc.RPC;
-import org.apache.hadoop.ipc.Server;
-import org.apache.hadoop.ipc.TestRPC.TestImpl;
-import org.apache.hadoop.ipc.TestRPC.TestProtocol;
-import org.apache.hadoop.mapred.AuditLogger.Keys;
-import org.apache.hadoop.net.NetUtils;
-
-import junit.framework.TestCase;
-
-/**
- * Tests {@link AuditLogger}.
- */
-public class TestAuditLogger extends TestCase {
-  private static final String USER = "test";
-  private static final String OPERATION = "oper";
-  private static final String TARGET = "tgt";
-  private static final String PERM = "admin group";
-  private static final String DESC = "description of an audit log";
-
-  /**
-   * Test the AuditLog format with key-val pair.
-   */
-  public void testKeyValLogFormat() {
-    StringBuilder actLog = new StringBuilder();
-    StringBuilder expLog = new StringBuilder();
-    // add the first k=v pair and check
-    AuditLogger.start(Keys.USER, USER, actLog);
-    expLog.append("USER=test");
-    assertEquals(expLog.toString(), actLog.toString());
-
-    // append another k1=v1 pair to already added k=v and test
-    AuditLogger.add(Keys.OPERATION, OPERATION, actLog);
-    expLog.append("\tOPERATION=oper");
-    assertEquals(expLog.toString(), actLog.toString());
-
-    // append another k1=null pair and test
-    AuditLogger.add(Keys.PERMISSIONS, (String)null, actLog);
-    expLog.append("\tPERMISSIONS=null");
-    assertEquals(expLog.toString(), actLog.toString());
-
-    // now add the target and check of the final string
-    AuditLogger.add(Keys.TARGET, TARGET, actLog);
-    expLog.append("\tTARGET=tgt");
-    assertEquals(expLog.toString(), actLog.toString());
-  }
-
-  /**
-   * Test the AuditLog format for successful events.
-   */
-  private void testSuccessLogFormat(boolean checkIP) {
-    // check without the IP
-    String sLog = AuditLogger.createSuccessLog(USER, OPERATION, TARGET);
-    StringBuilder expLog = new StringBuilder();
-    expLog.append("USER=test\t");
-    if (checkIP) {
-      InetAddress ip = Server.getRemoteIp();
-      expLog.append(Keys.IP.name() + "=" + ip.getHostAddress() + "\t");
-    }
-    expLog.append("OPERATION=oper\tTARGET=tgt\tRESULT=SUCCESS");
-    assertEquals(expLog.toString(), sLog);
-
-  }
-
-  /**
-   * Test the AuditLog format for failure events.
-   */
-  private void testFailureLogFormat(boolean checkIP, String perm) {
-    String fLog =
-      AuditLogger.createFailureLog(USER, OPERATION, perm, TARGET, DESC);
-    StringBuilder expLog = new StringBuilder();
-    expLog.append("USER=test\t");
-    if (checkIP) {
-      InetAddress ip = Server.getRemoteIp();
-      expLog.append(Keys.IP.name() + "=" + ip.getHostAddress() + "\t");
-    }
-    expLog.append("OPERATION=oper\tTARGET=tgt\tRESULT=FAILURE\t");
-    expLog.append("DESCRIPTION=description of an audit log\t");
-    expLog.append("PERMISSIONS=" + perm);
-    assertEquals(expLog.toString(), fLog);
-  }
-
-  /**
-   * Test the AuditLog format for failure events.
-   */
-  private void testFailureLogFormat(boolean checkIP) {
-    testFailureLogFormat(checkIP, PERM);
-    testFailureLogFormat(checkIP, null);
-  }
-
-  /**
-   * Test {@link AuditLogger} without IP set.
-   */
-  public void testAuditLoggerWithoutIP() throws Exception {
-    // test without ip
-    testSuccessLogFormat(false);
-    testFailureLogFormat(false);
-  }
-
-  /**
-   * A special extension of {@link TestImpl} RPC server with
-   * {@link TestImpl#ping()} testing the audit logs.
-   */
-  @ProtocolInfo(protocolName = "org.apache.hadoop.ipc.TestRPC$TestProtocol")
-  private class MyTestRPCServer extends TestImpl {
-    @Override
-    public void ping() {
-      // test with ip set
-      testSuccessLogFormat(true);
-      testFailureLogFormat(true);
-    }
-  }
-
-  /**
-   * Test {@link AuditLogger} with IP set.
-   */
-  @SuppressWarnings("deprecation")
-  public void testAuditLoggerWithIP() throws Exception {
-    Configuration conf = new Configuration();
-    // start the IPC server
-    Server server = RPC.getServer(new MyTestRPCServer(), "0.0.0.0", 0, conf);
-    server.start();
-
-    InetSocketAddress addr = NetUtils.getConnectAddress(server);
-
-    // Make a client connection and test the audit log
-    TestProtocol proxy = (TestProtocol)RPC.getProxy(TestProtocol.class,
-                           TestProtocol.versionID, addr, conf);
-    // Start the testcase
-    proxy.ping();
-
-    server.stop();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIFile.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIFile.java
deleted file mode 100644
index 0411711..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIFile.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.hadoop.io.compress.GzipCodec;
-
-import org.junit.Test;
-
-public class TestIFile {
-
-  @Test
-  /**
-   * Create an IFile.Writer using GzipCodec since this codec does not
-   * have a compressor when run via the tests (ie no native libraries).
-   */
-  public void testIFileWriterWithCodec() throws Exception {
-    Configuration conf = new Configuration();
-    FileSystem localFs = FileSystem.getLocal(conf);
-    FileSystem rfs = ((LocalFileSystem)localFs).getRaw();
-    Path path = new Path(new Path("build/test.ifile"), "data");
-    DefaultCodec codec = new GzipCodec();
-    codec.setConf(conf);
-    IFile.Writer<Text, Text> writer =
-      new IFile.Writer<Text, Text>(conf, rfs, path, Text.class, Text.class,
-                                   codec, null);
-    writer.close();
-  }
-
-  @Test
-  /** Same as above but create a reader. */
-  public void testIFileReaderWithCodec() throws Exception {
-    Configuration conf = new Configuration();
-    FileSystem localFs = FileSystem.getLocal(conf);
-    FileSystem rfs = ((LocalFileSystem)localFs).getRaw();
-    Path path = new Path(new Path("build/test.ifile"), "data");
-    DefaultCodec codec = new GzipCodec();
-    codec.setConf(conf);
-    IFile.Reader<Text, Text> reader =
-      new IFile.Reader<Text, Text>(conf, rfs, path, codec, null);
-    reader.close();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java
deleted file mode 100644
index b6a2df0..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestIndexCache.java
+++ /dev/null
@@ -1,324 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import java.io.DataOutputStream;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.Random;
-import java.util.zip.CRC32;
-import java.util.zip.CheckedOutputStream;
-
-import org.apache.hadoop.fs.ChecksumException;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-
-import junit.framework.TestCase;
-
-public class TestIndexCache extends TestCase {
-  private JobConf conf;
-  private FileSystem fs;
-  private Path p;
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new JobConf();
-    fs = FileSystem.getLocal(conf).getRaw();
-    p =  new Path(System.getProperty("test.build.data", "/tmp"),
-        "cache").makeQualified(fs.getUri(), fs.getWorkingDirectory());
-  }
-
-  public void testLRCPolicy() throws Exception {
-    Random r = new Random();
-    long seed = r.nextLong();
-    r.setSeed(seed);
-    System.out.println("seed: " + seed);
-    fs.delete(p, true);
-    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
-    final int partsPerMap = 1000;
-    final int bytesPerFile = partsPerMap * 24;
-    IndexCache cache = new IndexCache(conf);
-
-    // fill cache
-    int totalsize = bytesPerFile;
-    for (; totalsize < 1024 * 1024; totalsize += bytesPerFile) {
-      Path f = new Path(p, Integer.toString(totalsize, 36));
-      writeFile(fs, f, totalsize, partsPerMap);
-      IndexRecord rec = cache.getIndexInformation(
-        Integer.toString(totalsize, 36), r.nextInt(partsPerMap), f,
-        UserGroupInformation.getCurrentUser().getShortUserName());
-      checkRecord(rec, totalsize);
-    }
-
-    // delete files, ensure cache retains all elem
-    for (FileStatus stat : fs.listStatus(p)) {
-      fs.delete(stat.getPath(),true);
-    }
-    for (int i = bytesPerFile; i < 1024 * 1024; i += bytesPerFile) {
-      Path f = new Path(p, Integer.toString(i, 36));
-      IndexRecord rec = cache.getIndexInformation(Integer.toString(i, 36),
-        r.nextInt(partsPerMap), f,
-        UserGroupInformation.getCurrentUser().getShortUserName());
-      checkRecord(rec, i);
-    }
-
-    // push oldest (bytesPerFile) out of cache
-    Path f = new Path(p, Integer.toString(totalsize, 36));
-    writeFile(fs, f, totalsize, partsPerMap);
-    cache.getIndexInformation(Integer.toString(totalsize, 36),
-        r.nextInt(partsPerMap), f,
-        UserGroupInformation.getCurrentUser().getShortUserName());
-    fs.delete(f, false);
-
-    // oldest fails to read, or error
-    boolean fnf = false;
-    try {
-      cache.getIndexInformation(Integer.toString(bytesPerFile, 36),
-        r.nextInt(partsPerMap), new Path(p, Integer.toString(bytesPerFile)),
-        UserGroupInformation.getCurrentUser().getShortUserName());
-    } catch (IOException e) {
-      if (e.getCause() == null ||
-          !(e.getCause()  instanceof FileNotFoundException)) {
-        throw e;
-      }
-      else {
-        fnf = true;
-      }
-    }
-    if (!fnf)
-      fail("Failed to push out last entry");
-    // should find all the other entries
-    for (int i = bytesPerFile << 1; i < 1024 * 1024; i += bytesPerFile) {
-      IndexRecord rec = cache.getIndexInformation(Integer.toString(i, 36),
-          r.nextInt(partsPerMap), new Path(p, Integer.toString(i, 36)),
-          UserGroupInformation.getCurrentUser().getShortUserName());
-      checkRecord(rec, i);
-    }
-    IndexRecord rec = cache.getIndexInformation(Integer.toString(totalsize, 36),
-      r.nextInt(partsPerMap), f,
-      UserGroupInformation.getCurrentUser().getShortUserName());
-
-    checkRecord(rec, totalsize);
-  }
-
-  public void testBadIndex() throws Exception {
-    final int parts = 30;
-    fs.delete(p, true);
-    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
-    IndexCache cache = new IndexCache(conf);
-
-    Path f = new Path(p, "badindex");
-    FSDataOutputStream out = fs.create(f, false);
-    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());
-    DataOutputStream dout = new DataOutputStream(iout);
-    for (int i = 0; i < parts; ++i) {
-      for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {
-        if (0 == (i % 3)) {
-          dout.writeLong(i);
-        } else {
-          out.writeLong(i);
-        }
-      }
-    }
-    out.writeLong(iout.getChecksum().getValue());
-    dout.close();
-    try {
-      cache.getIndexInformation("badindex", 7, f,
-        UserGroupInformation.getCurrentUser().getShortUserName());
-      fail("Did not detect bad checksum");
-    } catch (IOException e) {
-      if (!(e.getCause() instanceof ChecksumException)) {
-        throw e;
-      }
-    }
-  }
-
-  public void testInvalidReduceNumberOrLength() throws Exception {
-    fs.delete(p, true);
-    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
-    final int partsPerMap = 1000;
-    final int bytesPerFile = partsPerMap * 24;
-    IndexCache cache = new IndexCache(conf);
-
-    // fill cache
-    Path feq = new Path(p, "invalidReduceOrPartsPerMap");
-    writeFile(fs, feq, bytesPerFile, partsPerMap);
-
-    // Number of reducers should always be less than partsPerMap as reducer
-    // numbers start from 0 and there cannot be more reducer than parts
-
-    try {
-      // Number of reducers equal to partsPerMap
-      cache.getIndexInformation("reduceEqualPartsPerMap", 
-               partsPerMap, // reduce number == partsPerMap
-               feq, UserGroupInformation.getCurrentUser().getShortUserName());
-      fail("Number of reducers equal to partsPerMap did not fail");
-    } catch (Exception e) {
-      if (!(e instanceof IOException)) {
-        throw e;
-      }
-    }
-
-    try {
-      // Number of reducers more than partsPerMap
-      cache.getIndexInformation(
-      "reduceMorePartsPerMap", 
-      partsPerMap + 1, // reduce number > partsPerMap
-      feq, UserGroupInformation.getCurrentUser().getShortUserName());
-      fail("Number of reducers more than partsPerMap did not fail");
-    } catch (Exception e) {
-      if (!(e instanceof IOException)) {
-        throw e;
-      }
-    }
-  }
-
-  public void testRemoveMap() throws Exception {
-    // This test case use two thread to call getIndexInformation and 
-    // removeMap concurrently, in order to construct race condition.
-    // This test case may not repeatable. But on my macbook this test 
-    // fails with probability of 100% on code before MAPREDUCE-2541,
-    // so it is repeatable in practice.
-    fs.delete(p, true);
-    conf.setInt(TTConfig.TT_INDEX_CACHE, 10);
-    // Make a big file so removeMapThread almost surely runs faster than 
-    // getInfoThread 
-    final int partsPerMap = 100000;
-    final int bytesPerFile = partsPerMap * 24;
-    final IndexCache cache = new IndexCache(conf);
-
-    final Path big = new Path(p, "bigIndex");
-    final String user = 
-      UserGroupInformation.getCurrentUser().getShortUserName();
-    writeFile(fs, big, bytesPerFile, partsPerMap);
-    
-    // run multiple times
-    for (int i = 0; i < 20; ++i) {
-      Thread getInfoThread = new Thread() {
-        @Override
-        public void run() {
-          try {
-            cache.getIndexInformation("bigIndex", partsPerMap, big, user);
-          } catch (Exception e) {
-            // should not be here
-          }
-        }
-      };
-      Thread removeMapThread = new Thread() {
-        @Override
-        public void run() {
-          cache.removeMap("bigIndex");
-        }
-      };
-      if (i%2==0) {
-        getInfoThread.start();
-        removeMapThread.start();        
-      } else {
-        removeMapThread.start();        
-        getInfoThread.start();
-      }
-      getInfoThread.join();
-      removeMapThread.join();
-      assertEquals(true, cache.checkTotalMemoryUsed());
-    }      
-  }
-  
-  public void testCreateRace() throws Exception {
-    fs.delete(p, true);
-    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);
-    final int partsPerMap = 1000;
-    final int bytesPerFile = partsPerMap * 24;
-    final IndexCache cache = new IndexCache(conf);
-    
-    final Path racy = new Path(p, "racyIndex");
-    final String user =  
-      UserGroupInformation.getCurrentUser().getShortUserName();
-    writeFile(fs, racy, bytesPerFile, partsPerMap);
-
-    // run multiple instances
-    Thread[] getInfoThreads = new Thread[50];
-    for (int i = 0; i < 50; i++) {
-      getInfoThreads[i] = new Thread() {
-        @Override
-        public void run() {
-          try {
-            cache.getIndexInformation("racyIndex", partsPerMap, racy, user);
-            cache.removeMap("racyIndex");
-          } catch (Exception e) {
-            // should not be here
-          }
-        }
-      };
-    }
-
-    for (int i = 0; i < 50; i++) {
-      getInfoThreads[i].start();
-    }
-
-    final Thread mainTestThread = Thread.currentThread();
-
-    Thread timeoutThread = new Thread() {
-      @Override
-      public void run() {
-        try {
-          Thread.sleep(15000);
-          mainTestThread.interrupt();
-        } catch (InterruptedException ie) {
-          // we are done;
-        }
-      }
-    };
-
-    for (int i = 0; i < 50; i++) {
-      try {
-        getInfoThreads[i].join();
-      } catch (InterruptedException ie) {
-        // we haven't finished in time. Potential deadlock/race.
-        fail("Unexpectedly long delay during concurrent cache entry creations");
-      }
-    }
-    // stop the timeoutThread. If we get interrupted before stopping, there
-    // must be something wrong, although it wasn't a deadlock. No need to
-    // catch and swallow.
-    timeoutThread.interrupt();
-  }
-
-  private static void checkRecord(IndexRecord rec, long fill) {
-    assertEquals(fill, rec.startOffset);
-    assertEquals(fill, rec.rawLength);
-    assertEquals(fill, rec.partLength);
-  }
-
-  private static void writeFile(FileSystem fs, Path f, long fill, int parts)
-      throws IOException {
-    FSDataOutputStream out = fs.create(f, false);
-    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());
-    DataOutputStream dout = new DataOutputStream(iout);
-    for (int i = 0; i < parts; ++i) {
-      for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {
-        dout.writeLong(fill);
-      }
-    }
-    out.writeLong(iout.getChecksum().getValue());
-    dout.close();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobConf.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobConf.java
deleted file mode 100644
index 3bd2c78..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobConf.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.junit.Ignore;
-import org.junit.Test;
-import java.io.File;
-import java.net.URLClassLoader;
-import java.net.URL;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-
-
-import static org.junit.Assert.*;
-@Ignore
-public class TestJobConf {
-  private static final String JAR_RELATIVE_PATH =
-    "build/test/mapred/testjar/testjob.jar";
-  private static final String CLASSNAME = "testjar.ClassWordCount";
-
-  private static String TEST_DIR_WITH_SPECIAL_CHARS =
-    System.getProperty("test.build.data","/tmp") +
-    File.separator + "test jobconf with + and spaces";
-
-  @Test
-  public void testFindContainingJar() throws Exception {
-    testJarAtPath(JAR_RELATIVE_PATH);
-  }
-
-  /**
-   * Test that findContainingJar works correctly even if the
-   * path has a "+" sign or spaces in it
-   */
-  @Test
-  public void testFindContainingJarWithPlus() throws Exception {
-    new File(TEST_DIR_WITH_SPECIAL_CHARS).mkdirs();
-    Configuration conf = new Configuration();
-
-    FileSystem localfs = FileSystem.getLocal(conf);
-
-    FileUtil.copy(localfs, new Path(JAR_RELATIVE_PATH),
-                  localfs, new Path(TEST_DIR_WITH_SPECIAL_CHARS, "test.jar"),
-                  false, true, conf);
-    testJarAtPath(TEST_DIR_WITH_SPECIAL_CHARS + File.separator + "test.jar");
-  }
-
-  /**
-   * Given a path with a jar, make a classloader with that jar on the
-   * classpath, and check that findContainingJar can correctly
-   * identify the path of the jar.
-   */
-  private void testJarAtPath(String path) throws Exception {
-    File jar = new File(path).getAbsoluteFile();
-    assertTrue(jar.exists());
-
-    URL urls[] = new URL[] {
-      jar.toURI().toURL()
-    };
-
-    ClassLoader cl = new URLClassLoader(urls);
-    Class clazz = Class.forName(CLASSNAME, true, cl);
-    assertNotNull(clazz);
-
-    String containingJar = JobConf.findContainingJar(clazz);
-    assertEquals(jar.getAbsolutePath(), containingJar);
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java
deleted file mode 100644
index 3846bbe..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestKeyValueTextInputFormat.java
+++ /dev/null
@@ -1,233 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.*;
-import java.util.*;
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.*;
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.io.compress.*;
-import org.apache.hadoop.util.LineReader;
-import org.apache.hadoop.util.ReflectionUtils;
-
-public class TestKeyValueTextInputFormat extends TestCase {
-  private static final Log LOG =
-    LogFactory.getLog(TestKeyValueTextInputFormat.class.getName());
-
-  private static int MAX_LENGTH = 10000;
-  
-  private static JobConf defaultConf = new JobConf();
-  private static FileSystem localFs = null; 
-  static {
-    try {
-      localFs = FileSystem.getLocal(defaultConf);
-    } catch (IOException e) {
-      throw new RuntimeException("init failure", e);
-    }
-  }
-  private static Path workDir = 
-    new Path(new Path(System.getProperty("test.build.data", "."), "data"),
-             "TestKeyValueTextInputFormat");
-  
-  public void testFormat() throws Exception {
-    JobConf job = new JobConf();
-    Path file = new Path(workDir, "test.txt");
-
-    // A reporter that does nothing
-    Reporter reporter = Reporter.NULL;
-    
-    int seed = new Random().nextInt();
-    LOG.info("seed = "+seed);
-    Random random = new Random(seed);
-
-    localFs.delete(workDir, true);
-    FileInputFormat.setInputPaths(job, workDir);
-
-    // for a variety of lengths
-    for (int length = 0; length < MAX_LENGTH;
-         length+= random.nextInt(MAX_LENGTH/10)+1) {
-
-      LOG.debug("creating; entries = " + length);
-
-      // create a file with length entries
-      Writer writer = new OutputStreamWriter(localFs.create(file));
-      try {
-        for (int i = 0; i < length; i++) {
-          writer.write(Integer.toString(i*2));
-          writer.write("\t");
-          writer.write(Integer.toString(i));
-          writer.write("\n");
-        }
-      } finally {
-        writer.close();
-      }
-
-      // try splitting the file in a variety of sizes
-      KeyValueTextInputFormat format = new KeyValueTextInputFormat();
-      format.configure(job);
-      for (int i = 0; i < 3; i++) {
-        int numSplits = random.nextInt(MAX_LENGTH/20)+1;
-        LOG.debug("splitting: requesting = " + numSplits);
-        InputSplit[] splits = format.getSplits(job, numSplits);
-        LOG.debug("splitting: got =        " + splits.length);
-
-        // check each split
-        BitSet bits = new BitSet(length);
-        for (int j = 0; j < splits.length; j++) {
-          LOG.debug("split["+j+"]= " + splits[j]);
-          RecordReader<Text, Text> reader =
-            format.getRecordReader(splits[j], job, reporter);
-          Class readerClass = reader.getClass();
-          assertEquals("reader class is KeyValueLineRecordReader.", KeyValueLineRecordReader.class, readerClass);        
-
-          Text key = reader.createKey();
-          Class keyClass = key.getClass();
-          Text value = reader.createValue();
-          Class valueClass = value.getClass();
-          assertEquals("Key class is Text.", Text.class, keyClass);
-          assertEquals("Value class is Text.", Text.class, valueClass);
-          try {
-            int count = 0;
-            while (reader.next(key, value)) {
-              int v = Integer.parseInt(value.toString());
-              LOG.debug("read " + v);
-              if (bits.get(v)) {
-                LOG.warn("conflict with " + v + 
-                         " in split " + j +
-                         " at position "+reader.getPos());
-              }
-              assertFalse("Key in multiple partitions.", bits.get(v));
-              bits.set(v);
-              count++;
-            }
-            LOG.debug("splits["+j+"]="+splits[j]+" count=" + count);
-          } finally {
-            reader.close();
-          }
-        }
-        assertEquals("Some keys in no partition.", length, bits.cardinality());
-      }
-
-    }
-  }
-  private LineReader makeStream(String str) throws IOException {
-    return new LineReader(new ByteArrayInputStream
-                                           (str.getBytes("UTF-8")), 
-                                           defaultConf);
-  }
-  
-  public void testUTF8() throws Exception {
-    LineReader in = makeStream("abcd\u20acbdcd\u20ac");
-    Text line = new Text();
-    in.readLine(line);
-    assertEquals("readLine changed utf8 characters", 
-                 "abcd\u20acbdcd\u20ac", line.toString());
-    in = makeStream("abc\u200axyz");
-    in.readLine(line);
-    assertEquals("split on fake newline", "abc\u200axyz", line.toString());
-  }
-
-  public void testNewLines() throws Exception {
-    LineReader in = makeStream("a\nbb\n\nccc\rdddd\r\neeeee");
-    Text out = new Text();
-    in.readLine(out);
-    assertEquals("line1 length", 1, out.getLength());
-    in.readLine(out);
-    assertEquals("line2 length", 2, out.getLength());
-    in.readLine(out);
-    assertEquals("line3 length", 0, out.getLength());
-    in.readLine(out);
-    assertEquals("line4 length", 3, out.getLength());
-    in.readLine(out);
-    assertEquals("line5 length", 4, out.getLength());
-    in.readLine(out);
-    assertEquals("line5 length", 5, out.getLength());
-    assertEquals("end of file", 0, in.readLine(out));
-  }
-  
-  private static void writeFile(FileSystem fs, Path name, 
-                                CompressionCodec codec,
-                                String contents) throws IOException {
-    OutputStream stm;
-    if (codec == null) {
-      stm = fs.create(name);
-    } else {
-      stm = codec.createOutputStream(fs.create(name));
-    }
-    stm.write(contents.getBytes());
-    stm.close();
-  }
-  
-  private static final Reporter voidReporter = Reporter.NULL;
-  
-  private static List<Text> readSplit(KeyValueTextInputFormat format, 
-                                      InputSplit split, 
-                                      JobConf job) throws IOException {
-    List<Text> result = new ArrayList<Text>();
-    RecordReader<Text, Text> reader = format.getRecordReader(split, job,
-                                                 voidReporter);
-    Text key = reader.createKey();
-    Text value = reader.createValue();
-    while (reader.next(key, value)) {
-      result.add(value);
-      value = reader.createValue();
-    }
-    return result;
-  }
-  
-  /**
-   * Test using the gzip codec for reading
-   */
-  public static void testGzip() throws IOException {
-    JobConf job = new JobConf();
-    CompressionCodec gzip = new GzipCodec();
-    ReflectionUtils.setConf(gzip, job);
-    localFs.delete(workDir, true);
-    writeFile(localFs, new Path(workDir, "part1.txt.gz"), gzip, 
-              "line-1\tthe quick\nline-2\tbrown\nline-3\tfox jumped\nline-4\tover\nline-5\t the lazy\nline-6\t dog\n");
-    writeFile(localFs, new Path(workDir, "part2.txt.gz"), gzip,
-              "line-1\tthis is a test\nline-1\tof gzip\n");
-    FileInputFormat.setInputPaths(job, workDir);
-    KeyValueTextInputFormat format = new KeyValueTextInputFormat();
-    format.configure(job);
-    InputSplit[] splits = format.getSplits(job, 100);
-    assertEquals("compressed splits == 2", 2, splits.length);
-    FileSplit tmp = (FileSplit) splits[0];
-    if (tmp.getPath().getName().equals("part2.txt.gz")) {
-      splits[0] = splits[1];
-      splits[1] = tmp;
-    }
-    List<Text> results = readSplit(format, splits[0], job);
-    assertEquals("splits[0] length", 6, results.size());
-    assertEquals("splits[0][5]", " dog", results.get(5).toString());
-    results = readSplit(format, splits[1], job);
-    assertEquals("splits[1] length", 2, results.size());
-    assertEquals("splits[1][0]", "this is a test", 
-                 results.get(0).toString());    
-    assertEquals("splits[1][1]", "of gzip", 
-                 results.get(1).toString());    
-  }
-  
-  public static void main(String[] args) throws Exception {
-    new TestKeyValueTextInputFormat().testFormat();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java
deleted file mode 100644
index ff7a632..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import java.io.IOException;
-import java.util.BitSet;
-import java.util.HashMap;
-import java.util.Random;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-
-public class TestMultiFileInputFormat extends TestCase{
-
-  private static JobConf job = new JobConf();
-
-  private static final Log LOG = LogFactory.getLog(TestMultiFileInputFormat.class);
-  
-  private static final int MAX_SPLIT_COUNT  = 10000;
-  private static final int SPLIT_COUNT_INCR = 6000;
-  private static final int MAX_BYTES = 1024;
-  private static final int MAX_NUM_FILES = 10000;
-  private static final int NUM_FILES_INCR = 8000;
-  
-  private Random rand = new Random(System.currentTimeMillis());
-  private HashMap<String, Long> lengths = new HashMap<String, Long>();
-  
-  /** Dummy class to extend MultiFileInputFormat*/
-  private class DummyMultiFileInputFormat extends MultiFileInputFormat<Text, Text> {
-    @Override
-    public RecordReader<Text,Text> getRecordReader(InputSplit split, JobConf job
-        , Reporter reporter) throws IOException {
-      return null;
-    }
-  }
-  
-  private Path initFiles(FileSystem fs, int numFiles, int numBytes) throws IOException{
-    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
-    Path multiFileDir = new Path(dir, "test.multifile");
-    fs.delete(multiFileDir, true);
-    fs.mkdirs(multiFileDir);
-    LOG.info("Creating " + numFiles + " file(s) in " + multiFileDir);
-    for(int i=0; i<numFiles ;i++) {
-      Path path = new Path(multiFileDir, "file_" + i);
-       FSDataOutputStream out = fs.create(path);
-       if (numBytes == -1) {
-         numBytes = rand.nextInt(MAX_BYTES);
-       }
-       for(int j=0; j< numBytes; j++) {
-         out.write(rand.nextInt());
-       }
-       out.close();
-       if(LOG.isDebugEnabled()) {
-         LOG.debug("Created file " + path + " with length " + numBytes);
-       }
-       lengths.put(path.getName(), new Long(numBytes));
-    }
-    FileInputFormat.setInputPaths(job, multiFileDir);
-    return multiFileDir;
-  }
-  
-  public void testFormat() throws IOException {
-    if(LOG.isInfoEnabled()) {
-      LOG.info("Test started");
-      LOG.info("Max split count           = " + MAX_SPLIT_COUNT);
-      LOG.info("Split count increment     = " + SPLIT_COUNT_INCR);
-      LOG.info("Max bytes per file        = " + MAX_BYTES);
-      LOG.info("Max number of files       = " + MAX_NUM_FILES);
-      LOG.info("Number of files increment = " + NUM_FILES_INCR);
-    }
-    
-    MultiFileInputFormat<Text,Text> format = new DummyMultiFileInputFormat();
-    FileSystem fs = FileSystem.getLocal(job);
-    
-    for(int numFiles = 1; numFiles< MAX_NUM_FILES ; 
-      numFiles+= (NUM_FILES_INCR / 2) + rand.nextInt(NUM_FILES_INCR / 2)) {
-      
-      Path dir = initFiles(fs, numFiles, -1);
-      BitSet bits = new BitSet(numFiles);
-      for(int i=1;i< MAX_SPLIT_COUNT ;i+= rand.nextInt(SPLIT_COUNT_INCR) + 1) {
-        LOG.info("Running for Num Files=" + numFiles + ", split count=" + i);
-        
-        MultiFileSplit[] splits = (MultiFileSplit[])format.getSplits(job, i);
-        bits.clear();
-        
-        for(MultiFileSplit split : splits) {
-          long splitLength = 0;
-          for(Path p : split.getPaths()) {
-            long length = fs.getContentSummary(p).getLength();
-            assertEquals(length, lengths.get(p.getName()).longValue());
-            splitLength += length;
-            String name = p.getName();
-            int index = Integer.parseInt(
-                name.substring(name.lastIndexOf("file_") + 5));
-            assertFalse(bits.get(index));
-            bits.set(index);
-          }
-          assertEquals(splitLength, split.getLength());
-        }
-      }
-      assertEquals(bits.cardinality(), numFiles);
-      fs.delete(dir, true);
-    }
-    LOG.info("Test Finished");
-  }
-  
-  public void testFormatWithLessPathsThanSplits() throws Exception {
-    MultiFileInputFormat<Text,Text> format = new DummyMultiFileInputFormat();
-    FileSystem fs = FileSystem.getLocal(job);     
-    
-    // Test with no path
-    initFiles(fs, 0, -1);    
-    assertEquals(0, format.getSplits(job, 2).length);
-    
-    // Test with 2 path and 4 splits
-    initFiles(fs, 2, 500);
-    assertEquals(2, format.getSplits(job, 4).length);
-  }
-  
-  public static void main(String[] args) throws Exception{
-    TestMultiFileInputFormat test = new TestMultiFileInputFormat();
-    test.testFormat();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java
deleted file mode 100644
index af0f399..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.util.Arrays;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-
-public class TestMultiFileSplit extends TestCase{
-
-    public void testReadWrite() throws Exception {
-      MultiFileSplit split = new MultiFileSplit(new JobConf(), new Path[] {new Path("/test/path/1"), new Path("/test/path/2")}, new long[] {100,200});
-        
-      ByteArrayOutputStream bos = null;
-      byte[] result = null;
-      try {    
-        bos = new ByteArrayOutputStream();
-        split.write(new DataOutputStream(bos));
-        result = bos.toByteArray();
-      } finally {
-        IOUtils.closeStream(bos);
-      }
-      
-      MultiFileSplit readSplit = new MultiFileSplit();
-      ByteArrayInputStream bis = null;
-      try {
-        bis = new ByteArrayInputStream(result);
-        readSplit.readFields(new DataInputStream(bis));
-      } finally {
-        IOUtils.closeStream(bis);
-      }
-      
-      assertTrue(split.getLength() != 0);
-      assertEquals(split.getLength(), readSplit.getLength());
-      assertTrue(Arrays.equals(split.getPaths(), readSplit.getPaths()));
-      assertTrue(Arrays.equals(split.getLengths(), readSplit.getLengths()));
-      System.out.println(split.toString());
-    }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java
deleted file mode 100644
index d3a0844..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import java.io.IOException;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * This test exercises the ValueIterator.
- */
-public class TestReduceTask extends TestCase {
-
-  static class NullProgress implements Progressable {
-    public void progress() { }
-  }
-
-  private static class Pair {
-    String key;
-    String value;
-    Pair(String k, String v) {
-      key = k;
-      value = v;
-    }
-  }
-  private static Pair[][] testCases =
-    new Pair[][]{
-      new Pair[]{
-                 new Pair("k1", "v1"),
-                 new Pair("k2", "v2"),
-                 new Pair("k3", "v3"),
-                 new Pair("k3", "v4"),
-                 new Pair("k4", "v5"),
-                 new Pair("k5", "v6"),
-      },
-      new Pair[]{
-                 new Pair("", "v1"),
-                 new Pair("k1", "v2"),
-                 new Pair("k2", "v3"),
-                 new Pair("k2", "v4"),
-      },
-      new Pair[] {},
-      new Pair[]{
-                 new Pair("k1", "v1"),
-                 new Pair("k1", "v2"),
-                 new Pair("k1", "v3"),
-                 new Pair("k1", "v4"),
-      }
-    };
-  
-  public void runValueIterator(Path tmpDir, Pair[] vals, 
-                               Configuration conf, 
-                               CompressionCodec codec) throws IOException {
-    FileSystem localFs = FileSystem.getLocal(conf);
-    FileSystem rfs = ((LocalFileSystem)localFs).getRaw();
-    Path path = new Path(tmpDir, "data.in");
-    IFile.Writer<Text, Text> writer = 
-      new IFile.Writer<Text, Text>(conf, rfs, path, Text.class, Text.class,
-                                   codec, null);
-    for(Pair p: vals) {
-      writer.append(new Text(p.key), new Text(p.value));
-    }
-    writer.close();
-    
-    @SuppressWarnings("unchecked")
-    RawKeyValueIterator rawItr = 
-      Merger.merge(conf, rfs, Text.class, Text.class, codec, new Path[]{path}, 
-                   false, conf.getInt(JobContext.IO_SORT_FACTOR, 100), tmpDir, 
-                   new Text.Comparator(), new NullProgress(), null, null, null);
-    @SuppressWarnings("unchecked") // WritableComparators are not generic
-    ReduceTask.ValuesIterator valItr = 
-      new ReduceTask.ValuesIterator<Text,Text>(rawItr,
-          WritableComparator.get(Text.class), Text.class, Text.class,
-          conf, new NullProgress());
-    int i = 0;
-    while (valItr.more()) {
-      Object key = valItr.getKey();
-      String keyString = key.toString();
-      // make sure it matches!
-      assertEquals(vals[i].key, keyString);
-      // must have at least 1 value!
-      assertTrue(valItr.hasNext());
-      while (valItr.hasNext()) {
-        String valueString = valItr.next().toString();
-        // make sure the values match
-        assertEquals(vals[i].value, valueString);
-        // make sure the keys match
-        assertEquals(vals[i].key, valItr.getKey().toString());
-        i += 1;
-      }
-      // make sure the key hasn't changed under the hood
-      assertEquals(keyString, valItr.getKey().toString());
-      valItr.nextKey();
-    }
-    assertEquals(vals.length, i);
-    // make sure we have progress equal to 1.0
-    assertEquals(1.0f, rawItr.getProgress().get());
-  }
-
-  public void testValueIterator() throws Exception {
-    Path tmpDir = new Path("build/test/test.reduce.task");
-    Configuration conf = new Configuration();
-    for (Pair[] testCase: testCases) {
-      runValueIterator(tmpDir, testCase, conf, null);
-    }
-  }
-  
-  public void testValueIteratorWithCompression() throws Exception {
-    Path tmpDir = new Path("build/test/test.reduce.task.compression");
-    Configuration conf = new Configuration();
-    DefaultCodec codec = new DefaultCodec();
-    codec.setConf(conf);
-    for (Pair[] testCase: testCases) {
-      runValueIterator(tmpDir, testCase, conf, codec);
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java
deleted file mode 100644
index b8be740..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryInputFormat.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.io.*;
-
-import junit.framework.TestCase;
-import org.apache.commons.logging.*;
-
-public class TestSequenceFileAsBinaryInputFormat extends TestCase {
-  private static final Log LOG = FileInputFormat.LOG;
-  private static final int RECORDS = 10000;
-
-  public void testBinary() throws IOException {
-    JobConf job = new JobConf();
-    FileSystem fs = FileSystem.getLocal(job);
-    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
-    Path file = new Path(dir, "testbinary.seq");
-    Random r = new Random();
-    long seed = r.nextLong();
-    r.setSeed(seed);
-
-    fs.delete(dir, true);
-    FileInputFormat.setInputPaths(job, dir);
-
-    Text tkey = new Text();
-    Text tval = new Text();
-
-    SequenceFile.Writer writer =
-      new SequenceFile.Writer(fs, job, file, Text.class, Text.class);
-    try {
-      for (int i = 0; i < RECORDS; ++i) {
-        tkey.set(Integer.toString(r.nextInt(), 36));
-        tval.set(Long.toString(r.nextLong(), 36));
-        writer.append(tkey, tval);
-      }
-    } finally {
-      writer.close();
-    }
-
-    InputFormat<BytesWritable,BytesWritable> bformat =
-      new SequenceFileAsBinaryInputFormat();
-
-    int count = 0;
-    r.setSeed(seed);
-    BytesWritable bkey = new BytesWritable();
-    BytesWritable bval = new BytesWritable();
-    Text cmpkey = new Text();
-    Text cmpval = new Text();
-    DataInputBuffer buf = new DataInputBuffer();
-    final int NUM_SPLITS = 3;
-    FileInputFormat.setInputPaths(job, file);
-    for (InputSplit split : bformat.getSplits(job, NUM_SPLITS)) {
-      RecordReader<BytesWritable,BytesWritable> reader =
-        bformat.getRecordReader(split, job, Reporter.NULL);
-      try {
-        while (reader.next(bkey, bval)) {
-          tkey.set(Integer.toString(r.nextInt(), 36));
-          tval.set(Long.toString(r.nextLong(), 36));
-          buf.reset(bkey.getBytes(), bkey.getLength());
-          cmpkey.readFields(buf);
-          buf.reset(bval.getBytes(), bval.getLength());
-          cmpval.readFields(buf);
-          assertTrue(
-              "Keys don't match: " + "*" + cmpkey.toString() + ":" +
-                                           tkey.toString() + "*",
-              cmpkey.toString().equals(tkey.toString()));
-          assertTrue(
-              "Vals don't match: " + "*" + cmpval.toString() + ":" +
-                                           tval.toString() + "*",
-              cmpval.toString().equals(tval.toString()));
-          ++count;
-        }
-      } finally {
-        reader.close();
-      }
-    }
-    assertEquals("Some records not found", RECORDS, count);
-  }
-
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java
deleted file mode 100644
index abe21f2..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsBinaryOutputFormat.java
+++ /dev/null
@@ -1,212 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.io.SequenceFile.CompressionType;
-
-import junit.framework.TestCase;
-import org.apache.commons.logging.*;
-
-public class TestSequenceFileAsBinaryOutputFormat extends TestCase {
-  private static final Log LOG =
-      LogFactory.getLog(TestSequenceFileAsBinaryOutputFormat.class.getName());
-
-  private static final int RECORDS = 10000;
-  // A random task attempt id for testing.
-  private static final String attempt = "attempt_200707121733_0001_m_000000_0";
-
-  public void testBinary() throws IOException {
-    JobConf job = new JobConf();
-    FileSystem fs = FileSystem.getLocal(job);
-    
-    Path dir = 
-      new Path(new Path(new Path(System.getProperty("test.build.data",".")), 
-                        FileOutputCommitter.TEMP_DIR_NAME), "_" + attempt);
-    Path file = new Path(dir, "testbinary.seq");
-    Random r = new Random();
-    long seed = r.nextLong();
-    r.setSeed(seed);
-
-    fs.delete(dir, true);
-    if (!fs.mkdirs(dir)) { 
-      fail("Failed to create output directory");
-    }
-
-    job.set(JobContext.TASK_ATTEMPT_ID, attempt);
-    FileOutputFormat.setOutputPath(job, dir.getParent().getParent());
-    FileOutputFormat.setWorkOutputPath(job, dir);
-
-    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, 
-                                          IntWritable.class );
-    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, 
-                                          DoubleWritable.class ); 
-
-    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);
-    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, 
-                                                       CompressionType.BLOCK);
-
-    BytesWritable bkey = new BytesWritable();
-    BytesWritable bval = new BytesWritable();
-
-
-    RecordWriter <BytesWritable, BytesWritable> writer = 
-      new SequenceFileAsBinaryOutputFormat().getRecordWriter(fs, 
-                                                       job, file.toString(),
-                                                       Reporter.NULL);
-
-    IntWritable iwritable = new IntWritable();
-    DoubleWritable dwritable = new DoubleWritable();
-    DataOutputBuffer outbuf = new DataOutputBuffer();
-    LOG.info("Creating data by SequenceFileAsBinaryOutputFormat");
-    try {
-      for (int i = 0; i < RECORDS; ++i) {
-        iwritable = new IntWritable(r.nextInt());
-        iwritable.write(outbuf);
-        bkey.set(outbuf.getData(), 0, outbuf.getLength());
-        outbuf.reset();
-        dwritable = new DoubleWritable(r.nextDouble());
-        dwritable.write(outbuf);
-        bval.set(outbuf.getData(), 0, outbuf.getLength());
-        outbuf.reset();
-        writer.write(bkey, bval);
-      }
-    } finally {
-      writer.close(Reporter.NULL);
-    }
-
-    InputFormat<IntWritable,DoubleWritable> iformat =
-                    new SequenceFileInputFormat<IntWritable,DoubleWritable>();
-    int count = 0;
-    r.setSeed(seed);
-    DataInputBuffer buf = new DataInputBuffer();
-    final int NUM_SPLITS = 3;
-    SequenceFileInputFormat.addInputPath(job, file);
-    LOG.info("Reading data by SequenceFileInputFormat");
-    for (InputSplit split : iformat.getSplits(job, NUM_SPLITS)) {
-      RecordReader<IntWritable,DoubleWritable> reader =
-        iformat.getRecordReader(split, job, Reporter.NULL);
-      try {
-        int sourceInt;
-        double sourceDouble;
-        while (reader.next(iwritable, dwritable)) {
-          sourceInt = r.nextInt();
-          sourceDouble = r.nextDouble();
-          assertEquals(
-              "Keys don't match: " + "*" + iwritable.get() + ":" + 
-                                           sourceInt + "*",
-              sourceInt, iwritable.get());
-          assertTrue(
-              "Vals don't match: " + "*" + dwritable.get() + ":" +
-                                           sourceDouble + "*",
-              Double.compare(dwritable.get(), sourceDouble) == 0 );
-          ++count;
-        }
-      } finally {
-        reader.close();
-      }
-    }
-    assertEquals("Some records not found", RECORDS, count);
-  }
-
-  public void testSequenceOutputClassDefaultsToMapRedOutputClass() 
-         throws IOException {
-    JobConf job = new JobConf();
-    FileSystem fs = FileSystem.getLocal(job);
-
-    // Setting Random class to test getSequenceFileOutput{Key,Value}Class
-    job.setOutputKeyClass(FloatWritable.class);
-    job.setOutputValueClass(BooleanWritable.class);
-
-    assertEquals("SequenceFileOutputKeyClass should default to ouputKeyClass", 
-             FloatWritable.class,
-             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(
-                                                                         job));
-    assertEquals("SequenceFileOutputValueClass should default to " 
-             + "ouputValueClass", 
-             BooleanWritable.class,
-             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(
-                                                                         job));
-
-    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, 
-                                          IntWritable.class );
-    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, 
-                                          DoubleWritable.class ); 
-
-    assertEquals("SequenceFileOutputKeyClass not updated", 
-             IntWritable.class,
-             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(
-                                                                         job));
-    assertEquals("SequenceFileOutputValueClass not updated", 
-             DoubleWritable.class,
-             SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(
-                                                                         job));
-  }
-
-  public void testcheckOutputSpecsForbidRecordCompression() throws IOException {
-    JobConf job = new JobConf();
-    FileSystem fs = FileSystem.getLocal(job);
-    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
-    Path outputdir = new Path(System.getProperty("test.build.data",".") 
-                              + "/output");
-
-    fs.delete(dir, true);
-    fs.delete(outputdir, true);
-    if (!fs.mkdirs(dir)) { 
-      fail("Failed to create output directory");
-    }
-
-    FileOutputFormat.setWorkOutputPath(job, dir);
-
-    // Without outputpath, FileOutputFormat.checkoutputspecs will throw 
-    // InvalidJobConfException
-    FileOutputFormat.setOutputPath(job, outputdir);
-
-    // SequenceFileAsBinaryOutputFormat doesn't support record compression
-    // It should throw an exception when checked by checkOutputSpecs
-    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);
-
-    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, 
-                                                       CompressionType.BLOCK);
-    try {
-      new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(fs, job);
-    } catch (Exception e) {
-      fail("Block compression should be allowed for " 
-                       + "SequenceFileAsBinaryOutputFormat:" 
-                       + "Caught " + e.getClass().getName());
-    }
-
-    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, 
-                                                       CompressionType.RECORD);
-    try {
-      new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(fs, job);
-      fail("Record compression should not be allowed for " 
-                           +"SequenceFileAsBinaryOutputFormat");
-    } catch (InvalidJobConfException ie) {
-      // expected
-    } catch (Exception e) {
-      fail("Expected " + InvalidJobConfException.class.getName() 
-                       + "but caught " + e.getClass().getName() );
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java
deleted file mode 100644
index 4cfd59a..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileAsTextInputFormat.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.*;
-import java.util.*;
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.*;
-
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.conf.*;
-
-public class TestSequenceFileAsTextInputFormat extends TestCase {
-  private static final Log LOG = FileInputFormat.LOG;
-
-  private static int MAX_LENGTH = 10000;
-  private static Configuration conf = new Configuration();
-
-  public void testFormat() throws Exception {
-    JobConf job = new JobConf(conf);
-    FileSystem fs = FileSystem.getLocal(conf);
-    Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");
-    Path file = new Path(dir, "test.seq");
-    
-    Reporter reporter = Reporter.NULL;
-    
-    int seed = new Random().nextInt();
-    //LOG.info("seed = "+seed);
-    Random random = new Random(seed);
-
-    fs.delete(dir, true);
-
-    FileInputFormat.setInputPaths(job, dir);
-
-    // for a variety of lengths
-    for (int length = 0; length < MAX_LENGTH;
-         length+= random.nextInt(MAX_LENGTH/10)+1) {
-
-      //LOG.info("creating; entries = " + length);
-
-      // create a file with length entries
-      SequenceFile.Writer writer =
-        SequenceFile.createWriter(fs, conf, file,
-                                  IntWritable.class, LongWritable.class);
-      try {
-        for (int i = 0; i < length; i++) {
-          IntWritable key = new IntWritable(i);
-          LongWritable value = new LongWritable(10 * i);
-          writer.append(key, value);
-        }
-      } finally {
-        writer.close();
-      }
-
-      // try splitting the file in a variety of sizes
-      InputFormat<Text, Text> format =
-        new SequenceFileAsTextInputFormat();
-      
-      for (int i = 0; i < 3; i++) {
-        int numSplits =
-          random.nextInt(MAX_LENGTH/(SequenceFile.SYNC_INTERVAL/20))+1;
-        //LOG.info("splitting: requesting = " + numSplits);
-        InputSplit[] splits = format.getSplits(job, numSplits);
-        //LOG.info("splitting: got =        " + splits.length);
-
-        // check each split
-        BitSet bits = new BitSet(length);
-        for (int j = 0; j < splits.length; j++) {
-          RecordReader<Text, Text> reader =
-            format.getRecordReader(splits[j], job, reporter);
-          Class readerClass = reader.getClass();
-          assertEquals("reader class is SequenceFileAsTextRecordReader.", SequenceFileAsTextRecordReader.class, readerClass);        
-          Text value = reader.createValue();
-          Text key = reader.createKey();
-          try {
-            int count = 0;
-            while (reader.next(key, value)) {
-              // if (bits.get(key.get())) {
-              // LOG.info("splits["+j+"]="+splits[j]+" : " + key.get());
-              // LOG.info("@"+reader.getPos());
-              // }
-              int keyInt = Integer.parseInt(key.toString());
-              assertFalse("Key in multiple partitions.", bits.get(keyInt));
-              bits.set(keyInt);
-              count++;
-            }
-            //LOG.info("splits["+j+"]="+splits[j]+" count=" + count);
-          } finally {
-            reader.close();
-          }
-        }
-        assertEquals("Some keys in no partition.", length, bits.cardinality());
-      }
-
-    }
-  }
-
-  public static void main(String[] args) throws Exception {
-    new TestSequenceFileAsTextInputFormat().testFormat();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java
deleted file mode 100644
index e50c396..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java
+++ /dev/null
@@ -1,176 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred;
-
-import java.io.*;
-import java.util.*;
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.*;
-
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.conf.*;
-
-public class TestSequenceFileInputFilter extends TestCase {
-  private static final Log LOG = FileInputFormat.LOG;
-
-  private static final int MAX_LENGTH = 15000;
-  private static final Configuration conf = new Configuration();
-  private static final JobConf job = new JobConf(conf);
-  private static final FileSystem fs;
-  private static final Path inDir = new Path(System.getProperty("test.build.data",".") + "/mapred");
-  private static final Path inFile = new Path(inDir, "test.seq");
-  private static final Random random = new Random(1);
-  private static final Reporter reporter = Reporter.NULL;
-  
-  static {
-    FileInputFormat.setInputPaths(job, inDir);
-    try {
-      fs = FileSystem.getLocal(conf);
-    } catch (IOException e) {
-      e.printStackTrace();
-      throw new RuntimeException(e);
-    }
-  }
-
-  private static void createSequenceFile(int numRecords) throws Exception {
-    // create a file with length entries
-    SequenceFile.Writer writer =
-      SequenceFile.createWriter(fs, conf, inFile,
-                                Text.class, BytesWritable.class);
-    try {
-      for (int i = 1; i <= numRecords; i++) {
-        Text key = new Text(Integer.toString(i));
-        byte[] data = new byte[random.nextInt(10)];
-        random.nextBytes(data);
-        BytesWritable value = new BytesWritable(data);
-        writer.append(key, value);
-      }
-    } finally {
-      writer.close();
-    }
-  }
-
-
-  private int countRecords(int numSplits) throws IOException {
-    InputFormat<Text, BytesWritable> format =
-      new SequenceFileInputFilter<Text, BytesWritable>();
-    Text key = new Text();
-    BytesWritable value = new BytesWritable();
-    if (numSplits==0) {
-      numSplits =
-        random.nextInt(MAX_LENGTH/(SequenceFile.SYNC_INTERVAL/20))+1;
-    }
-    InputSplit[] splits = format.getSplits(job, numSplits);
-      
-    // check each split
-    int count = 0;
-    LOG.info("Generated " + splits.length + " splits.");
-    for (int j = 0; j < splits.length; j++) {
-      RecordReader<Text, BytesWritable> reader =
-        format.getRecordReader(splits[j], job, reporter);
-      try {
-        while (reader.next(key, value)) {
-          LOG.info("Accept record "+key.toString());
-          count++;
-        }
-      } finally {
-        reader.close();
-      }
-    }
-    return count;
-  }
-  
-  public void testRegexFilter() throws Exception {
-    // set the filter class
-    LOG.info("Testing Regex Filter with patter: \\A10*");
-    SequenceFileInputFilter.setFilterClass(job, 
-                                           SequenceFileInputFilter.RegexFilter.class);
-    SequenceFileInputFilter.RegexFilter.setPattern(job, "\\A10*");
-    
-    // clean input dir
-    fs.delete(inDir, true);
-  
-    // for a variety of lengths
-    for (int length = 1; length < MAX_LENGTH;
-         length+= random.nextInt(MAX_LENGTH/10)+1) {
-      LOG.info("******Number of records: "+length);
-      createSequenceFile(length);
-      int count = countRecords(0);
-      assertEquals(count, length==0?0:(int)Math.log10(length)+1);
-    }
-    
-    // clean up
-    fs.delete(inDir, true);
-  }
-
-  public void testPercentFilter() throws Exception {
-    LOG.info("Testing Percent Filter with frequency: 1000");
-    // set the filter class
-    SequenceFileInputFilter.setFilterClass(job, 
-                                           SequenceFileInputFilter.PercentFilter.class);
-    SequenceFileInputFilter.PercentFilter.setFrequency(job, 1000);
-      
-    // clean input dir
-    fs.delete(inDir, true);
-    
-    // for a variety of lengths
-    for (int length = 0; length < MAX_LENGTH;
-         length+= random.nextInt(MAX_LENGTH/10)+1) {
-      LOG.info("******Number of records: "+length);
-      createSequenceFile(length);
-      int count = countRecords(1);
-      LOG.info("Accepted "+count+" records");
-      int expectedCount = length/1000;
-      if (expectedCount*1000!=length)
-        expectedCount++;
-      assertEquals(count, expectedCount);
-    }
-      
-    // clean up
-    fs.delete(inDir, true);
-  }
-  
-  public void testMD5Filter() throws Exception {
-    // set the filter class
-    LOG.info("Testing MD5 Filter with frequency: 1000");
-    SequenceFileInputFilter.setFilterClass(job, 
-                                           SequenceFileInputFilter.MD5Filter.class);
-    SequenceFileInputFilter.MD5Filter.setFrequency(job, 1000);
-      
-    // clean input dir
-    fs.delete(inDir, true);
-    
-    // for a variety of lengths
-    for (int length = 0; length < MAX_LENGTH;
-         length+= random.nextInt(MAX_LENGTH/10)+1) {
-      LOG.info("******Number of records: "+length);
-      createSequenceFile(length);
-      LOG.info("Accepted "+countRecords(0)+" records");
-    }
-    // clean up
-    fs.delete(inDir, true);
-  }
-
-  public static void main(String[] args) throws Exception {
-    TestSequenceFileInputFilter filter = new TestSequenceFileInputFilter();
-    filter.testRegexFilter();
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java
deleted file mode 100644
index ad4d4ce..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSortedRanges.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import java.util.Iterator;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapred.SortedRanges.Range;
-
-public class TestSortedRanges extends TestCase {
-  private static final Log LOG = 
-    LogFactory.getLog(TestSortedRanges.class);
-  
-  public void testAdd() {
-    SortedRanges sr = new SortedRanges();
-    sr.add(new Range(2,9));
-    assertEquals(9, sr.getIndicesCount());
-    
-    sr.add(new SortedRanges.Range(3,5));
-    assertEquals(9, sr.getIndicesCount());
-    
-    sr.add(new SortedRanges.Range(7,1));
-    assertEquals(9, sr.getIndicesCount());
-    
-    sr.add(new Range(1,12));
-    assertEquals(12, sr.getIndicesCount());
-    
-    sr.add(new Range(7,9));
-    assertEquals(15, sr.getIndicesCount());
-    
-    sr.add(new Range(31,10));
-    sr.add(new Range(51,10));
-    sr.add(new Range(66,10));
-    assertEquals(45, sr.getIndicesCount());
-    
-    sr.add(new Range(21,50));
-    assertEquals(70, sr.getIndicesCount());
-    
-    LOG.debug(sr);
-    
-    Iterator<Long> it = sr.skipRangeIterator();
-    int i = 0;
-    assertEquals(i, it.next().longValue());
-    for(i=16;i<21;i++) {
-      assertEquals(i, it.next().longValue());
-    }
-    assertEquals(76, it.next().longValue());
-    assertEquals(77, it.next().longValue());
-    
-  }
-  
-  public void testRemove() {
-    SortedRanges sr = new SortedRanges();
-    sr.add(new Range(2,19));
-    assertEquals(19, sr.getIndicesCount());
-    
-    sr.remove(new SortedRanges.Range(15,8));
-    assertEquals(13, sr.getIndicesCount());
-    
-    sr.remove(new SortedRanges.Range(6,5));
-    assertEquals(8, sr.getIndicesCount());
-    
-    sr.remove(new SortedRanges.Range(8,4));
-    assertEquals(7, sr.getIndicesCount());
-    
-    sr.add(new Range(18,5));
-    assertEquals(12, sr.getIndicesCount());
-    
-    sr.add(new Range(25,1));
-    assertEquals(13, sr.getIndicesCount());
-    
-    sr.remove(new SortedRanges.Range(7,24));
-    assertEquals(4, sr.getIndicesCount());
-    
-    sr.remove(new SortedRanges.Range(5,1));
-    assertEquals(3, sr.getIndicesCount());
-    
-    LOG.debug(sr);
-  }
-
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java
deleted file mode 100644
index 87ab6b2..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestStatisticsCollector.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.mapred.StatisticsCollector.TimeWindow;
-import org.apache.hadoop.mapred.StatisticsCollector.Stat;
-
-public class TestStatisticsCollector extends TestCase{
-
-  public void testMovingWindow() throws Exception {
-    StatisticsCollector collector = new StatisticsCollector(1);
-    TimeWindow window = new TimeWindow("test", 6, 2);
-    TimeWindow sincStart = StatisticsCollector.SINCE_START;
-    TimeWindow[] windows = {sincStart, window};
-    
-    Stat stat = collector.createStat("m1", windows);
-    
-    stat.inc(3);
-    collector.update();
-    assertEquals(0, stat.getValues().get(window).getValue());
-    assertEquals(3, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(3);
-    collector.update();
-    assertEquals((3+3), stat.getValues().get(window).getValue());
-    assertEquals(6, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(10);
-    collector.update();
-    assertEquals((3+3), stat.getValues().get(window).getValue());
-    assertEquals(16, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(10);
-    collector.update();
-    assertEquals((3+3+10+10), stat.getValues().get(window).getValue());
-    assertEquals(26, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(10);
-    collector.update();
-    stat.inc(10);
-    collector.update();
-    assertEquals((3+3+10+10+10+10), stat.getValues().get(window).getValue());
-    assertEquals(46, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(10);
-    collector.update();
-    assertEquals((3+3+10+10+10+10), stat.getValues().get(window).getValue());
-    assertEquals(56, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(12);
-    collector.update();
-    assertEquals((10+10+10+10+10+12), stat.getValues().get(window).getValue());
-    assertEquals(68, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(13);
-    collector.update();
-    assertEquals((10+10+10+10+10+12), stat.getValues().get(window).getValue());
-    assertEquals(81, stat.getValues().get(sincStart).getValue());
-    
-    stat.inc(14);
-    collector.update();
-    assertEquals((10+10+10+12+13+14), stat.getValues().get(window).getValue());
-    assertEquals(95, stat.getValues().get(sincStart).getValue());
-  }
-
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java
deleted file mode 100644
index e71103d..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTaskStatus.java
+++ /dev/null
@@ -1,207 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestTaskStatus {
-  private static final Log LOG = LogFactory.getLog(TestTaskStatus.class);
-
-  @Test
-  public void testMapTaskStatusStartAndFinishTimes() {
-    checkTaskStatues(true);
-  }
-
-  @Test
-  public void testReduceTaskStatusStartAndFinishTimes() {
-    checkTaskStatues(false);
-  }
-
-  /**
-   * Private utility method which ensures uniform testing of newly created
-   * TaskStatus object.
-   * 
-   * @param isMap
-   *          true to test map task status, false for reduce.
-   */
-  private void checkTaskStatues(boolean isMap) {
-
-    TaskStatus status = null;
-    if (isMap) {
-      status = new MapTaskStatus();
-    } else {
-      status = new ReduceTaskStatus();
-    }
-    long currentTime = System.currentTimeMillis();
-    // first try to set the finish time before
-    // start time is set.
-    status.setFinishTime(currentTime);
-    assertEquals("Finish time of the task status set without start time", 0,
-        status.getFinishTime());
-    // Now set the start time to right time.
-    status.setStartTime(currentTime);
-    assertEquals("Start time of the task status not set correctly.",
-        currentTime, status.getStartTime());
-    // try setting wrong start time to task status.
-    long wrongTime = -1;
-    status.setStartTime(wrongTime);
-    assertEquals(
-        "Start time of the task status is set to wrong negative value",
-        currentTime, status.getStartTime());
-    // finally try setting wrong finish time i.e. negative value.
-    status.setFinishTime(wrongTime);
-    assertEquals("Finish time of task status is set to wrong negative value",
-        0, status.getFinishTime());
-    status.setFinishTime(currentTime);
-    assertEquals("Finish time of the task status not set correctly.",
-        currentTime, status.getFinishTime());
-    
-    // test with null task-diagnostics
-    TaskStatus ts = ((TaskStatus)status.clone());
-    ts.setDiagnosticInfo(null);
-    ts.setDiagnosticInfo("");
-    ts.setStateString(null);
-    ts.setStateString("");
-    ((TaskStatus)status.clone()).statusUpdate(ts);
-    
-    // test with null state-string
-    ((TaskStatus)status.clone()).statusUpdate(0, null, null);
-    ((TaskStatus)status.clone()).statusUpdate(0, "", null);
-    ((TaskStatus)status.clone()).statusUpdate(null, 0, "", null, 1);
-  }
-  
-  /**
-   * Test the {@link TaskStatus} against large sized task-diagnostic-info and 
-   * state-string. Does the following
-   *  - create Map/Reduce TaskStatus such that the task-diagnostic-info and 
-   *    state-string are small strings and check their contents
-   *  - append them with small string and check their contents
-   *  - append them with large string and check their size
-   *  - update the status using statusUpdate() calls and check the size/contents
-   *  - create Map/Reduce TaskStatus with large string and check their size
-   */
-  @Test
-  public void testTaskDiagnosticsAndStateString() {
-    // check the default case
-    String test = "hi";
-    final int maxSize = 16;
-    TaskStatus status = new TaskStatus(null, 0, 0, null, test, test, null, null, 
-                                       null) {
-      @Override
-      protected int getMaxStringSize() {
-        return maxSize;
-      }
-
-      @Override
-      public void addFetchFailedMap(TaskAttemptID mapTaskId) {
-      }
-
-      @Override
-      public boolean getIsMap() {
-        return false;
-      }
-    };
-    assertEquals("Small diagnostic info test failed", 
-                 status.getDiagnosticInfo(), test);
-    assertEquals("Small state string test failed", status.getStateString(), 
-                 test);
-    
-    // now append some small string and check
-    String newDInfo = test.concat(test);
-    status.setDiagnosticInfo(test);
-    status.setStateString(newDInfo);
-    assertEquals("Small diagnostic info append failed", 
-                 newDInfo, status.getDiagnosticInfo());
-    assertEquals("Small state-string append failed", 
-                 newDInfo, status.getStateString());
-    
-    // update the status with small state strings
-    TaskStatus newStatus = (TaskStatus)status.clone();
-    String newSInfo = "hi1";
-    newStatus.setStateString(newSInfo);
-    status.statusUpdate(newStatus);
-    newDInfo = newDInfo.concat(newStatus.getDiagnosticInfo());
-    
-    assertEquals("Status-update on diagnostic-info failed", 
-                 newDInfo, status.getDiagnosticInfo());
-    assertEquals("Status-update on state-string failed", 
-                 newSInfo, status.getStateString());
-    
-    newSInfo = "hi2";
-    status.statusUpdate(0, newSInfo, null);
-    assertEquals("Status-update on state-string failed", 
-                 newSInfo, status.getStateString());
-    
-    newSInfo = "hi3";
-    status.statusUpdate(null, 0, newSInfo, null, 0);
-    assertEquals("Status-update on state-string failed", 
-                 newSInfo, status.getStateString());
-    
-    
-    // now append each with large string
-    String large = "hihihihihihihihihihi"; // 20 chars
-    status.setDiagnosticInfo(large);
-    status.setStateString(large);
-    assertEquals("Large diagnostic info append test failed", 
-                 maxSize, status.getDiagnosticInfo().length());
-    assertEquals("Large state-string append test failed",
-                 maxSize, status.getStateString().length());
-    
-    // update a large status with large strings
-    newStatus.setDiagnosticInfo(large + "0");
-    newStatus.setStateString(large + "1");
-    status.statusUpdate(newStatus);
-    assertEquals("Status-update on diagnostic info failed",
-                 maxSize, status.getDiagnosticInfo().length());
-    assertEquals("Status-update on state-string failed", 
-                 maxSize, status.getStateString().length());
-    
-    status.statusUpdate(0, large + "2", null);
-    assertEquals("Status-update on state-string failed", 
-                 maxSize, status.getStateString().length());
-    
-    status.statusUpdate(null, 0, large + "3", null, 0);
-    assertEquals("Status-update on state-string failed", 
-                 maxSize, status.getStateString().length());
-    
-    // test passing large string in constructor
-    status = new TaskStatus(null, 0, 0, null, large, large, null, null, 
-        null) {
-      @Override
-      protected int getMaxStringSize() {
-        return maxSize;
-      }
-
-      @Override
-      public void addFetchFailedMap(TaskAttemptID mapTaskId) {
-      }
-
-      @Override
-      public boolean getIsMap() {
-        return false;
-      }
-    };
-    assertEquals("Large diagnostic info test failed", 
-                maxSize, status.getDiagnosticInfo().length());
-    assertEquals("Large state-string test failed", 
-                 maxSize, status.getStateString().length());
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestUtils.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestUtils.java
deleted file mode 100644
index 0c43704..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestUtils.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestUtils {
-  private static final Path[] LOG_PATHS = new Path[] {
-    new Path("file:///foo/_logs"),
-    new Path("file:///foo/_logs/"),
-    new Path("_logs/"),
-    new Path("_logs")
-  };
-
-  private static final Path[] SUCCEEDED_PATHS = new Path[] {
-    new Path("file:///blah/" + FileOutputCommitter.SUCCEEDED_FILE_NAME)
-  };
-
-  private static final Path[] PASS_PATHS = new Path[] {
-    new Path("file:///my_logs/blah"),
-    new Path("file:///a/b/c"),
-    new Path("file:///foo/_logs/blah"),
-    new Path("_logs/foo"),
-    new Path("file:///blah/" +
-             FileOutputCommitter.SUCCEEDED_FILE_NAME +
-             "/bar")
-  };
-
-  @Test
-  public void testOutputFilesFilter() {
-    PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();
-    for (Path p : LOG_PATHS) {
-      assertFalse(filter.accept(p));
-    }
-
-    for (Path p : SUCCEEDED_PATHS) {
-      assertFalse(filter.accept(p));
-    }
-
-    for (Path p : PASS_PATHS) {
-      assertTrue(filter.accept(p));
-    }
-  }
-
-  @Test
-  public void testLogFilter() {
-    PathFilter filter = new Utils.OutputFileUtils.OutputLogFilter();
-    for (Path p : LOG_PATHS) {
-      assertFalse(filter.accept(p));
-    }
-
-    for (Path p : SUCCEEDED_PATHS) {
-      assertTrue(filter.accept(p));
-    }
-
-    for (Path p : PASS_PATHS) {
-      assertTrue(filter.accept(p));
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java
deleted file mode 100644
index 7b85bd4..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java
+++ /dev/null
@@ -1,154 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapreduce;
-
-import java.util.Random;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapreduce.counters.LimitExceededException;
-import org.apache.hadoop.mapreduce.counters.Limits;
-
-/**
- * TestCounters checks the sanity and recoverability of {@code Counters}
- */
-public class TestCounters {
-
-  static final Log LOG = LogFactory.getLog(TestCounters.class);
-
-  /**
-   * Verify counter value works
-   */
-  @Test
-  public void testCounterValue() {
-    final int NUMBER_TESTS = 100;
-    final int NUMBER_INC = 10;
-    final Random rand = new Random();
-    for (int i = 0; i < NUMBER_TESTS; i++) {
-      long initValue = rand.nextInt();
-      long expectedValue = initValue;
-      Counter counter = new Counters().findCounter("test", "foo");
-      counter.setValue(initValue);
-      assertEquals("Counter value is not initialized correctly",
-          expectedValue, counter.getValue());
-      for (int j = 0; j < NUMBER_INC; j++) {
-        int incValue = rand.nextInt();
-        counter.increment(incValue);
-        expectedValue += incValue;
-        assertEquals("Counter value is not incremented correctly",
-            expectedValue, counter.getValue());
-      }
-      expectedValue = rand.nextInt();
-      counter.setValue(expectedValue);
-      assertEquals("Counter value is not set correctly",
-          expectedValue, counter.getValue());
-    }
-  }
-
-  @Test public void testLimits() {
-    for (int i = 0; i < 3; ++i) {
-      // make sure limits apply to separate containers
-      testMaxCounters(new Counters());
-      testMaxGroups(new Counters());
-    }
-  }
-  
-  @Test
-  public void testCountersIncrement() {
-    Counters fCounters = new Counters();
-    Counter fCounter = fCounters.findCounter(FRAMEWORK_COUNTER);
-    fCounter.setValue(100);
-    Counter gCounter = fCounters.findCounter("test", "foo");
-    gCounter.setValue(200);
-
-    Counters counters = new Counters();
-    counters.incrAllCounters(fCounters);
-    Counter counter;
-    for (CounterGroup cg : fCounters) {
-      CounterGroup group = counters.getGroup(cg.getName());
-      if (group.getName().equals("test")) {
-        counter = counters.findCounter("test", "foo");
-        assertEquals(200, counter.getValue());
-      } else {
-        counter = counters.findCounter(FRAMEWORK_COUNTER);
-        assertEquals(100, counter.getValue());
-      }
-    }
-  }
-
-  static final Enum<?> FRAMEWORK_COUNTER = TaskCounter.CPU_MILLISECONDS;
-  static final long FRAMEWORK_COUNTER_VALUE = 8;
-  static final String FS_SCHEME = "HDFS";
-  static final FileSystemCounter FS_COUNTER = FileSystemCounter.BYTES_READ;
-  static final long FS_COUNTER_VALUE = 10;
-
-  private void testMaxCounters(final Counters counters) {
-    LOG.info("counters max="+ Limits.COUNTERS_MAX);
-    for (int i = 0; i < Limits.COUNTERS_MAX; ++i) {
-      counters.findCounter("test", "test"+ i);
-    }
-    setExpected(counters);
-    shouldThrow(LimitExceededException.class, new Runnable() {
-      public void run() {
-        counters.findCounter("test", "bad");
-      }
-    });
-    checkExpected(counters);
-  }
-
-  private void testMaxGroups(final Counters counters) {
-    LOG.info("counter groups max="+ Limits.GROUPS_MAX);
-    for (int i = 0; i < Limits.GROUPS_MAX; ++i) {
-      // assuming COUNTERS_MAX > GROUPS_MAX
-      counters.findCounter("test"+ i, "test");
-    }
-    setExpected(counters);
-    shouldThrow(LimitExceededException.class, new Runnable() {
-      public void run() {
-        counters.findCounter("bad", "test");
-      }
-    });
-    checkExpected(counters);
-  }
-
-  private void setExpected(Counters counters) {
-    counters.findCounter(FRAMEWORK_COUNTER).setValue(FRAMEWORK_COUNTER_VALUE);
-    counters.findCounter(FS_SCHEME, FS_COUNTER).setValue(FS_COUNTER_VALUE);
-  }
-
-  private void checkExpected(Counters counters) {
-    assertEquals(FRAMEWORK_COUNTER_VALUE,
-                 counters.findCounter(FRAMEWORK_COUNTER).getValue());
-    assertEquals(FS_COUNTER_VALUE,
-                 counters.findCounter(FS_SCHEME, FS_COUNTER).getValue());
-  }
-
-  private void shouldThrow(Class<? extends Exception> ecls, Runnable runnable) {
-    try {
-      runnable.run();
-    } catch (Exception e) {
-      assertSame(ecls, e.getClass());
-      LOG.info("got expected: "+ e);
-      return;
-    }
-    assertTrue("Should've thrown "+ ecls.getSimpleName(), false);
-  }
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java
deleted file mode 100644
index b893fe1..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapreduce.lib.jobcontrol;
-
-import static org.junit.Assert.assertFalse;
-
-import org.apache.hadoop.conf.Configuration;
-import org.junit.Test;
-
-/**
- */
-public class TestControlledJob {
-  
-  @Test
-  public void testAddingDependingJobToRunningJobFails() throws Exception {
-    Configuration conf = new Configuration();
-    ControlledJob job1 = new ControlledJob(conf);
-    job1.setJobState(ControlledJob.State.RUNNING);
-    assertFalse(job1.addDependingJob(new ControlledJob(conf)));
-  }
-
-  @Test
-  public void testAddingDependingJobToCompletedJobFails() throws Exception {
-    Configuration conf = new Configuration();
-    ControlledJob job1 = new ControlledJob(conf);
-    job1.setJobState(ControlledJob.State.SUCCESS);
-    assertFalse(job1.addDependingJob(new ControlledJob(conf)));
-  }
-
-}
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java
deleted file mode 100644
index 54e1302..0000000
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java
+++ /dev/null
@@ -1,677 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapreduce.util;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.FileReader;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Random;
-import java.util.Vector;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.util.Shell.ExitCodeException;
-import org.apache.hadoop.util.Shell.ShellCommandExecutor;
-
-import junit.framework.TestCase;
-
-/**
- * A JUnit test to test ProcfsBasedProcessTree.
- */
-public class TestProcfsBasedProcessTree extends TestCase {
-
-  private static final Log LOG = LogFactory
-      .getLog(TestProcfsBasedProcessTree.class);
-  private static String TEST_ROOT_DIR = new Path(System.getProperty(
-         "test.build.data", "/tmp")).toString().replace(' ', '+');
-
-  private ShellCommandExecutor shexec = null;
-  private String pidFile, lowestDescendant;
-  private String shellScript;
-  private static final int N = 6; // Controls the RogueTask
-
-  private class RogueTaskThread extends Thread {
-    public void run() {
-      try {
-        Vector<String> args = new Vector<String>();
-        if(ProcessTree.isSetsidAvailable) {
-          args.add("setsid");
-        }
-        args.add("bash");
-        args.add("-c");
-        args.add(" echo $$ > " + pidFile + "; sh " +
-                          shellScript + " " + N + ";") ;
-        shexec = new ShellCommandExecutor(args.toArray(new String[0]));
-        shexec.execute();
-      } catch (ExitCodeException ee) {
-        LOG.info("Shell Command exit with a non-zero exit code. This is" +
-                 " expected as we are killing the subprocesses of the" +
-                 " task intentionally. " + ee);
-      } catch (IOException ioe) {
-        LOG.info("Error executing shell command " + ioe);
-      } finally {
-        LOG.info("Exit code: " + shexec.getExitCode());
-      }
-    }
-  }
-
-  private String getRogueTaskPID() {
-    File f = new File(pidFile);
-    while (!f.exists()) {
-      try {
-        Thread.sleep(500);
-      } catch (InterruptedException ie) {
-        break;
-      }
-    }
-
-    // read from pidFile
-    return getPidFromPidFile(pidFile);
-  }
-
-  public void testProcessTree() {
-
-    try {
-      if (!ProcfsBasedProcessTree.isAvailable()) {
-        System.out
-            .println("ProcfsBasedProcessTree is not available on this system. Not testing");
-        return;
-      }
-    } catch (Exception e) {
-      LOG.info(StringUtils.stringifyException(e));
-      return;
-    }
-    // create shell script
-    Random rm = new Random();
-    File tempFile = new File(TEST_ROOT_DIR, this.getName() + "_shellScript_" +
-                             rm.nextInt() + ".sh");
-    tempFile.deleteOnExit();
-    shellScript = TEST_ROOT_DIR + File.separator + tempFile.getName();
-
-    // create pid file
-    tempFile = new File(TEST_ROOT_DIR,  this.getName() + "_pidFile_" +
-                        rm.nextInt() + ".pid");
-    tempFile.deleteOnExit();
-    pidFile = TEST_ROOT_DIR + File.separator + tempFile.getName();
-
-    lowestDescendant = TEST_ROOT_DIR + File.separator + "lowestDescendantPidFile";
-
-    // write to shell-script
-    try {
-      FileWriter fWriter = new FileWriter(shellScript);
-      fWriter.write(
-          "# rogue task\n" +
-          "sleep 1\n" +
-          "echo hello\n" +
-          "if [ $1 -ne 0 ]\n" +
-          "then\n" +
-          " sh " + shellScript + " $(($1-1))\n" +
-          "else\n" +
-          " echo $$ > " + lowestDescendant + "\n" +
-          " while true\n do\n" +
-          "  sleep 5\n" +
-          " done\n" +
-          "fi");
-      fWriter.close();
-    } catch (IOException ioe) {
-      LOG.info("Error: " + ioe);
-      return;
-    }
-
-    Thread t = new RogueTaskThread();
-    t.start();
-    String pid = getRogueTaskPID();
-    LOG.info("Root process pid: " + pid);
-    ProcfsBasedProcessTree p = new ProcfsBasedProcessTree(pid,
-                               ProcessTree.isSetsidAvailable,
-                               ProcessTree.DEFAULT_SLEEPTIME_BEFORE_SIGKILL);
-    p = p.getProcessTree(); // initialize
-    LOG.info("ProcessTree: " + p.toString());
-
-    File leaf = new File(lowestDescendant);
-    //wait till lowest descendant process of Rougue Task starts execution
-    while (!leaf.exists()) {
-      try {
-        Thread.sleep(500);
-      } catch (InterruptedException ie) {
-        break;
-      }
-    }
-
-    p = p.getProcessTree(); // reconstruct
-    LOG.info("ProcessTree: " + p.toString());
-
-    // Get the process-tree dump
-    String processTreeDump = p.getProcessTreeDump();
-
-    // destroy the process and all its subprocesses
-    p.destroy(true/*in the background*/);
-
-    if(ProcessTree.isSetsidAvailable) {// whole processtree should be gone
-      assertEquals(false, p.isAnyProcessInTreeAlive());
-    }
-    else {// process should be gone
-      assertFalse("ProcessTree must have been gone", p.isAlive());
-    }
-
-    LOG.info("Process-tree dump follows: \n" + processTreeDump);
-    assertTrue("Process-tree dump doesn't start with a proper header",
-        processTreeDump.startsWith("\t|- PID PPID PGRPID SESSID CMD_NAME " +
-        "USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) " +
-        "RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n"));
-    for (int i = N; i >= 0; i--) {
-      String cmdLineDump = "\\|- [0-9]+ [0-9]+ [0-9]+ [0-9]+ \\(sh\\)" +
-          " [0-9]+ [0-9]+ [0-9]+ [0-9]+ sh " + shellScript + " " + i;
-      Pattern pat = Pattern.compile(cmdLineDump);
-      Matcher mat = pat.matcher(processTreeDump);
-      assertTrue("Process-tree dump doesn't contain the cmdLineDump of " + i
-          + "th process!", mat.find());
-    }
-
-    // Not able to join thread sometimes when forking with large N.
-    try {
-      t.join(2000);
-      LOG.info("RogueTaskThread successfully joined.");
-    } catch (InterruptedException ie) {
-      LOG.info("Interrupted while joining RogueTaskThread.");
-    }
-
-    // ProcessTree is gone now. Any further calls should be sane.
-    p = p.getProcessTree();
-    assertFalse("ProcessTree must have been gone", p.isAlive());
-    assertTrue("Cumulative vmem for the gone-process is "
-        + p.getCumulativeVmem() + " . It should be zero.", p
-        .getCumulativeVmem() == 0);
-    assertTrue(p.toString().equals("[ ]"));
-  }
-
-  /**
-   * Get PID from a pid-file.
-   * 
-   * @param pidFileName
-   *          Name of the pid-file.
-   * @return the PID string read from the pid-file. Returns null if the
-   *         pidFileName points to a non-existing file or if read fails from the
-   *         file.
-   */
-  public static String getPidFromPidFile(String pidFileName) {
-    BufferedReader pidFile = null;
-    FileReader fReader = null;
-    String pid = null;
-
-    try {
-      fReader = new FileReader(pidFileName);
-      pidFile = new BufferedReader(fReader);
-    } catch (FileNotFoundException f) {
-      LOG.debug("PidFile doesn't exist : " + pidFileName);
-      return pid;
-    }
-
-    try {
-      pid = pidFile.readLine();
-    } catch (IOException i) {
-      LOG.error("Failed to read from " + pidFileName);
-    } finally {
-      try {
-        if (fReader != null) {
-          fReader.close();
-        }
-        try {
-          if (pidFile != null) {
-            pidFile.close();
-          }
-        } catch (IOException i) {
-          LOG.warn("Error closing the stream " + pidFile);
-        }
-      } catch (IOException i) {
-        LOG.warn("Error closing the stream " + fReader);
-      }
-    }
-    return pid;
-  }
-  
-  public static class ProcessStatInfo {
-    // sample stat in a single line : 3910 (gpm) S 1 3910 3910 0 -1 4194624 
-    // 83 0 0 0 0 0 0 0 16 0 1 0 7852 2408448 88 4294967295 134512640 
-    // 134590050 3220521392 3220520036 10975138 0 0 4096 134234626 
-    // 4294967295 0 0 17 1 0 0
-    String pid;
-    String name;
-    String ppid;
-    String pgrpId;
-    String session;
-    String vmem = "0";
-    String rssmemPage = "0";
-    String utime = "0";
-    String stime = "0";
-    
-    public ProcessStatInfo(String[] statEntries) {
-      pid = statEntries[0];
-      name = statEntries[1];
-      ppid = statEntries[2];
-      pgrpId = statEntries[3];
-      session = statEntries[4];
-      vmem = statEntries[5];
-      if (statEntries.length > 6) {
-        rssmemPage = statEntries[6];
-      }
-      if (statEntries.length > 7) {
-        utime = statEntries[7];
-        stime = statEntries[8];
-      }
-    }
-    
-    // construct a line that mimics the procfs stat file.
-    // all unused numerical entries are set to 0.
-    public String getStatLine() {
-      return String.format("%s (%s) S %s %s %s 0 0 0" +
-                      " 0 0 0 0 %s %s 0 0 0 0 0 0 0 %s %s 0 0" +
-                      " 0 0 0 0 0 0 0 0" +
-                      " 0 0 0 0 0", 
-                      pid, name, ppid, pgrpId, session,
-                      utime, stime, vmem, rssmemPage);
-    }
-  }
-  
-  /**
-   * A basic test that creates a few process directories and writes
-   * stat files. Verifies that the cpu time and memory is correctly
-   * computed.
-   * @throws IOException if there was a problem setting up the
-   *                      fake procfs directories or files.
-   */
-  public void testCpuAndMemoryForProcessTree() throws IOException {
-
-    // test processes
-    String[] pids = { "100", "200", "300", "400" };
-    // create the fake procfs root directory. 
-    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
-
-    try {
-      setupProcfsRootDir(procfsRootDir);
-      setupPidDirs(procfsRootDir, pids);
-      
-      // create stat objects.
-      // assuming processes 100, 200, 300 are in tree and 400 is not.
-      ProcessStatInfo[] procInfos = new ProcessStatInfo[4];
-      procInfos[0] = new ProcessStatInfo(new String[] 
-          {"100", "proc1", "1", "100", "100", "100000", "100", "1000", "200"});
-      procInfos[1] = new ProcessStatInfo(new String[] 
-          {"200", "proc2", "100", "100", "100", "200000", "200", "2000", "400"});
-      procInfos[2] = new ProcessStatInfo(new String[] 
-          {"300", "proc3", "200", "100", "100", "300000", "300", "3000", "600"});
-      procInfos[3] = new ProcessStatInfo(new String[] 
-          {"400", "proc4", "1", "400", "400", "400000", "400", "4000", "800"});
-      
-      writeStatFiles(procfsRootDir, pids, procInfos);
-      
-      // crank up the process tree class.
-      ProcfsBasedProcessTree processTree = 
-          new ProcfsBasedProcessTree("100", true, 100L, 
-                                  procfsRootDir.getAbsolutePath());
-      // build the process tree.
-      processTree.getProcessTree();
-      
-      // verify cumulative memory
-      assertEquals("Cumulative virtual memory does not match", 600000L,
-                   processTree.getCumulativeVmem());
-
-      // verify rss memory
-      long cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
-                        600L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
-      assertEquals("Cumulative rss memory does not match",
-                   cumuRssMem, processTree.getCumulativeRssmem());
-
-      // verify cumulative cpu time
-      long cumuCpuTime = ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS > 0 ?
-             7200L * ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS : 0L;
-      assertEquals("Cumulative cpu time does not match",
-                   cumuCpuTime, processTree.getCumulativeCpuTime());
-
-      // test the cpu time again to see if it cumulates
-      procInfos[0] = new ProcessStatInfo(new String[]
-          {"100", "proc1", "1", "100", "100", "100000", "100", "2000", "300"});
-      procInfos[1] = new ProcessStatInfo(new String[]
-          {"200", "proc2", "100", "100", "100", "200000", "200", "3000", "500"});
-      writeStatFiles(procfsRootDir, pids, procInfos);
-
-      // build the process tree.
-      processTree.getProcessTree();
-
-      // verify cumulative cpu time again
-      cumuCpuTime = ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS > 0 ?
-             9400L * ProcfsBasedProcessTree.JIFFY_LENGTH_IN_MILLIS : 0L;
-      assertEquals("Cumulative cpu time does not match",
-                   cumuCpuTime, processTree.getCumulativeCpuTime());
-    } finally {
-      FileUtil.fullyDelete(procfsRootDir);
-    }
-  }
-  
-  /**
-   * Tests that cumulative memory is computed only for
-   * processes older than a given age.
-   * @throws IOException if there was a problem setting up the
-   *                      fake procfs directories or files.
-   */
-  public void testMemForOlderProcesses() throws IOException {
-    // initial list of processes
-    String[] pids = { "100", "200", "300", "400" };
-    // create the fake procfs root directory. 
-    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
-
-    try {
-      setupProcfsRootDir(procfsRootDir);
-      setupPidDirs(procfsRootDir, pids);
-      
-      // create stat objects.
-      // assuming 100, 200 and 400 are in tree, 300 is not.
-      ProcessStatInfo[] procInfos = new ProcessStatInfo[4];
-      procInfos[0] = new ProcessStatInfo(new String[] 
-                        {"100", "proc1", "1", "100", "100", "100000", "100"});
-      procInfos[1] = new ProcessStatInfo(new String[] 
-                        {"200", "proc2", "100", "100", "100", "200000", "200"});
-      procInfos[2] = new ProcessStatInfo(new String[] 
-                        {"300", "proc3", "1", "300", "300", "300000", "300"});
-      procInfos[3] = new ProcessStatInfo(new String[] 
-                        {"400", "proc4", "100", "100", "100", "400000", "400"});
-      
-      writeStatFiles(procfsRootDir, pids, procInfos);
-      
-      // crank up the process tree class.
-      ProcfsBasedProcessTree processTree = 
-          new ProcfsBasedProcessTree("100", true, 100L, 
-                                  procfsRootDir.getAbsolutePath());
-      // build the process tree.
-      processTree.getProcessTree();
-      
-      // verify cumulative memory
-      assertEquals("Cumulative memory does not match",
-                   700000L, processTree.getCumulativeVmem());
-
-      // write one more process as child of 100.
-      String[] newPids = { "500" };
-      setupPidDirs(procfsRootDir, newPids);
-      
-      ProcessStatInfo[] newProcInfos = new ProcessStatInfo[1];
-      newProcInfos[0] = new ProcessStatInfo(new String[]
-                      {"500", "proc5", "100", "100", "100", "500000", "500"});
-      writeStatFiles(procfsRootDir, newPids, newProcInfos);
-      
-      // check memory includes the new process.
-      processTree.getProcessTree();
-      assertEquals("Cumulative vmem does not include new process",
-                   1200000L, processTree.getCumulativeVmem());
-      long cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
-                        1200L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
-      assertEquals("Cumulative rssmem does not include new process",
-                   cumuRssMem, processTree.getCumulativeRssmem());
-      
-      // however processes older than 1 iteration will retain the older value
-      assertEquals("Cumulative vmem shouldn't have included new process",
-                   700000L, processTree.getCumulativeVmem(1));
-      cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
-                   700L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
-      assertEquals("Cumulative rssmem shouldn't have included new process",
-                   cumuRssMem, processTree.getCumulativeRssmem(1));
-
-      // one more process
-      newPids = new String[]{ "600" };
-      setupPidDirs(procfsRootDir, newPids);
-      
-      newProcInfos = new ProcessStatInfo[1];
-      newProcInfos[0] = new ProcessStatInfo(new String[]
-                      {"600", "proc6", "100", "100", "100", "600000", "600"});
-      writeStatFiles(procfsRootDir, newPids, newProcInfos);
-
-      // refresh process tree
-      processTree.getProcessTree();
-      
-      // processes older than 2 iterations should be same as before.
-      assertEquals("Cumulative vmem shouldn't have included new processes",
-                   700000L, processTree.getCumulativeVmem(2));
-      cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
-                   700L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
-      assertEquals("Cumulative rssmem shouldn't have included new processes",
-                   cumuRssMem, processTree.getCumulativeRssmem(2));
-
-      // processes older than 1 iteration should not include new process,
-      // but include process 500
-      assertEquals("Cumulative vmem shouldn't have included new processes",
-                   1200000L, processTree.getCumulativeVmem(1));
-      cumuRssMem = ProcfsBasedProcessTree.PAGE_SIZE > 0 ?
-                   1200L * ProcfsBasedProcessTree.PAGE_SIZE : 0L;
-      assertEquals("Cumulative rssmem shouldn't have included new processes",
-                   cumuRssMem, processTree.getCumulativeRssmem(1));
-
-      // no processes older than 3 iterations, this should be 0
-      assertEquals("Getting non-zero vmem for processes older than 3 iterations",
-                    0L, processTree.getCumulativeVmem(3));
-      assertEquals("Getting non-zero rssmem for processes older than 3 iterations",
-                    0L, processTree.getCumulativeRssmem(3));
-    } finally {
-      FileUtil.fullyDelete(procfsRootDir);
-    }
-  }
-
-  /**
-   * Verifies ProcfsBasedProcessTree.checkPidPgrpidForMatch() in case of
-   * 'constructProcessInfo() returning null' by not writing stat file for the
-   * mock process
-   * @throws IOException if there was a problem setting up the
-   *                      fake procfs directories or files.
-   */
-  public void testDestroyProcessTree() throws IOException {
-    // test process
-    String pid = "100";
-    // create the fake procfs root directory. 
-    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
-
-    try {
-      setupProcfsRootDir(procfsRootDir);
-      
-      // crank up the process tree class.
-      ProcfsBasedProcessTree processTree = new ProcfsBasedProcessTree(
-                        pid, true, 100L, procfsRootDir.getAbsolutePath());
-
-      // Let us not create stat file for pid 100.
-      assertTrue(ProcfsBasedProcessTree.checkPidPgrpidForMatch(
-                            pid, procfsRootDir.getAbsolutePath()));
-    } finally {
-      FileUtil.fullyDelete(procfsRootDir);
-    }
-  }
-  
-  /**
-   * Test the correctness of process-tree dump.
-   * 
-   * @throws IOException
-   */
-  public void testProcessTreeDump()
-      throws IOException {
-
-    String[] pids = { "100", "200", "300", "400", "500", "600" };
-
-    File procfsRootDir = new File(TEST_ROOT_DIR, "proc");
-
-    try {
-      setupProcfsRootDir(procfsRootDir);
-      setupPidDirs(procfsRootDir, pids);
-
-      int numProcesses = pids.length;
-      // Processes 200, 300, 400 and 500 are descendants of 100. 600 is not.
-      ProcessStatInfo[] procInfos = new ProcessStatInfo[numProcesses];
-      procInfos[0] = new ProcessStatInfo(new String[] {
-          "100", "proc1", "1", "100", "100", "100000", "100", "1000", "200"});
-      procInfos[1] = new ProcessStatInfo(new String[] {
-          "200", "proc2", "100", "100", "100", "200000", "200", "2000", "400"});
-      procInfos[2] = new ProcessStatInfo(new String[] {
-          "300", "proc3", "200", "100", "100", "300000", "300", "3000", "600"});
-      procInfos[3] = new ProcessStatInfo(new String[] {
-          "400", "proc4", "200", "100", "100", "400000", "400", "4000", "800"});
-      procInfos[4] = new ProcessStatInfo(new String[] {
-          "500", "proc5", "400", "100", "100", "400000", "400", "4000", "800"});
-      procInfos[5] = new ProcessStatInfo(new String[] {
-          "600", "proc6", "1", "1", "1", "400000", "400", "4000", "800"});
-
-      String[] cmdLines = new String[numProcesses];
-      cmdLines[0] = "proc1 arg1 arg2";
-      cmdLines[1] = "proc2 arg3 arg4";
-      cmdLines[2] = "proc3 arg5 arg6";
-      cmdLines[3] = "proc4 arg7 arg8";
-      cmdLines[4] = "proc5 arg9 arg10";
-      cmdLines[5] = "proc6 arg11 arg12";
-
-      writeStatFiles(procfsRootDir, pids, procInfos);
-      writeCmdLineFiles(procfsRootDir, pids, cmdLines);
-
-      ProcfsBasedProcessTree processTree =
-          new ProcfsBasedProcessTree("100", true, 100L, procfsRootDir
-              .getAbsolutePath());
-      // build the process tree.
-      processTree.getProcessTree();
-
-      // Get the process-tree dump
-      String processTreeDump = processTree.getProcessTreeDump();
-
-      LOG.info("Process-tree dump follows: \n" + processTreeDump);
-      assertTrue("Process-tree dump doesn't start with a proper header",
-          processTreeDump.startsWith("\t|- PID PPID PGRPID SESSID CMD_NAME " +
-          "USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) " +
-          "RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n"));
-      for (int i = 0; i < 5; i++) {
-        ProcessStatInfo p = procInfos[i];
-        assertTrue(
-            "Process-tree dump doesn't contain the cmdLineDump of process "
-                + p.pid, processTreeDump.contains("\t|- " + p.pid + " "
-                + p.ppid + " " + p.pgrpId + " " + p.session + " (" + p.name
-                + ") " + p.utime + " " + p.stime + " " + p.vmem + " "
-                + p.rssmemPage + " " + cmdLines[i]));
-      }
-
-      // 600 should not be in the dump
-      ProcessStatInfo p = procInfos[5];
-      assertFalse(
-          "Process-tree dump shouldn't contain the cmdLineDump of process "
-              + p.pid, processTreeDump.contains("\t|- " + p.pid + " " + p.ppid
-              + " " + p.pgrpId + " " + p.session + " (" + p.name + ") "
-              + p.utime + " " + p.stime + " " + p.vmem + " " + cmdLines[5]));
-    } finally {
-      FileUtil.fullyDelete(procfsRootDir);
-    }
-  }
-
-  /**
-   * Create a directory to mimic the procfs file system's root.
-   * @param procfsRootDir root directory to create.
-   * @throws IOException if could not delete the procfs root directory
-   */
-  public static void setupProcfsRootDir(File procfsRootDir) {
-    // cleanup any existing process root dir.
-    if (procfsRootDir.exists()) {
-      assertTrue(FileUtil.fullyDelete(procfsRootDir));  
-    }
-
-    // create afresh
-    assertTrue(procfsRootDir.mkdirs());
-  }
-
-  /**
-   * Create PID directories under the specified procfs root directory
-   * @param procfsRootDir root directory of procfs file system
-   * @param pids the PID directories to create.
-   * @throws IOException If PID dirs could not be created
-   */
-  public static void setupPidDirs(File procfsRootDir, String[] pids) 
-                      throws IOException {
-    for (String pid : pids) {
-      File pidDir = new File(procfsRootDir, pid);
-      pidDir.mkdir();
-      if (!pidDir.exists()) {
-        throw new IOException ("couldn't make process directory under " +
-            "fake procfs");
-      } else {
-        LOG.info("created pid dir");
-      }
-    }
-  }
-  
-  /**
-   * Write stat files under the specified pid directories with data
-   * setup in the corresponding ProcessStatInfo objects
-   * @param procfsRootDir root directory of procfs file system
-   * @param pids the PID directories under which to create the stat file
-   * @param procs corresponding ProcessStatInfo objects whose data should be
-   *              written to the stat files.
-   * @throws IOException if stat files could not be written
-   */
-  public static void writeStatFiles(File procfsRootDir, String[] pids, 
-                              ProcessStatInfo[] procs) throws IOException {
-    for (int i=0; i<pids.length; i++) {
-      File statFile =
-          new File(new File(procfsRootDir, pids[i]),
-              ProcfsBasedProcessTree.PROCFS_STAT_FILE);
-      BufferedWriter bw = null;
-      try {
-        FileWriter fw = new FileWriter(statFile);
-        bw = new BufferedWriter(fw);
-        bw.write(procs[i].getStatLine());
-        LOG.info("wrote stat file for " + pids[i] + 
-                  " with contents: " + procs[i].getStatLine());
-      } finally {
-        // not handling exception - will throw an error and fail the test.
-        if (bw != null) {
-          bw.close();
-        }
-      }
-    }
-  }
-
-  private static void writeCmdLineFiles(File procfsRootDir, String[] pids,
-      String[] cmdLines)
-      throws IOException {
-    for (int i = 0; i < pids.length; i++) {
-      File statFile =
-          new File(new File(procfsRootDir, pids[i]),
-              ProcfsBasedProcessTree.PROCFS_CMDLINE_FILE);
-      BufferedWriter bw = null;
-      try {
-        bw = new BufferedWriter(new FileWriter(statFile));
-        bw.write(cmdLines[i]);
-        LOG.info("wrote command-line file for " + pids[i] + " with contents: "
-            + cmdLines[i]);
-      } finally {
-        // not handling exception - will throw an error and fail the test.
-        if (bw != null) {
-          bw.close();
-        }
-      }
-    }
-  }
-}
-- 
1.7.0.4

