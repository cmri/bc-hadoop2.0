From ac87bbec26158a5d1a8e069cda224fa054653985 Mon Sep 17 00:00:00 2001
From: Thomas Graves <tgraves@apache.org>
Date: Thu, 17 May 2012 15:06:33 +0000
Subject: [PATCH 0129/1357] MAPREDUCE-3543. Mavenize Gridmix. (tgraves)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339629 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit ab69ce95ea10fa70a01eb68c0401404f4a10ede6)
---
 .../hadoop/mapred/gridmix/AvgRecordFactory.java    |  123 ----
 .../hadoop/mapred/gridmix/ClusterSummarizer.java   |  117 ----
 .../mapred/gridmix/CompressionEmulationUtil.java   |  574 ----------------
 .../mapred/gridmix/DistributedCacheEmulator.java   |  548 ---------------
 .../hadoop/mapred/gridmix/EchoUserResolver.java    |   57 --
 .../hadoop/mapred/gridmix/ExecutionSummarizer.java |  312 ---------
 .../org/apache/hadoop/mapred/gridmix/FilePool.java |  301 --------
 .../apache/hadoop/mapred/gridmix/FileQueue.java    |  103 ---
 .../apache/hadoop/mapred/gridmix/GenerateData.java |  412 -----------
 .../mapred/gridmix/GenerateDistCacheData.java      |  259 -------
 .../org/apache/hadoop/mapred/gridmix/Gridmix.java  |  717 --------------------
 .../apache/hadoop/mapred/gridmix/GridmixJob.java   |  517 --------------
 .../mapred/gridmix/GridmixJobSubmissionPolicy.java |   89 ---
 .../apache/hadoop/mapred/gridmix/GridmixKey.java   |  301 --------
 .../hadoop/mapred/gridmix/GridmixRecord.java       |  271 --------
 .../apache/hadoop/mapred/gridmix/GridmixSplit.java |  148 ----
 .../apache/hadoop/mapred/gridmix/InputStriper.java |  139 ----
 .../mapred/gridmix/IntermediateRecordFactory.java  |  110 ---
 .../apache/hadoop/mapred/gridmix/JobCreator.java   |  135 ----
 .../apache/hadoop/mapred/gridmix/JobFactory.java   |  253 -------
 .../apache/hadoop/mapred/gridmix/JobMonitor.java   |  255 -------
 .../apache/hadoop/mapred/gridmix/JobSubmitter.java |  196 ------
 .../org/apache/hadoop/mapred/gridmix/LoadJob.java  |  549 ---------------
 .../apache/hadoop/mapred/gridmix/LoadSplit.java    |  180 -----
 .../apache/hadoop/mapred/gridmix/Progressive.java  |   25 -
 .../hadoop/mapred/gridmix/PseudoLocalFs.java       |  332 ---------
 .../hadoop/mapred/gridmix/RandomAlgorithms.java    |  209 ------
 .../mapred/gridmix/RandomTextDataGenerator.java    |  147 ----
 .../hadoop/mapred/gridmix/ReadRecordFactory.java   |   85 ---
 .../hadoop/mapred/gridmix/RecordFactory.java       |   40 --
 .../hadoop/mapred/gridmix/ReplayJobFactory.java    |  128 ----
 .../mapred/gridmix/RoundRobinUserResolver.java     |  138 ----
 .../hadoop/mapred/gridmix/SerialJobFactory.java    |  178 -----
 .../org/apache/hadoop/mapred/gridmix/SleepJob.java |  411 -----------
 .../apache/hadoop/mapred/gridmix/StatListener.java |   32 -
 .../apache/hadoop/mapred/gridmix/Statistics.java   |  330 ---------
 .../hadoop/mapred/gridmix/StressJobFactory.java    |  472 -------------
 .../mapred/gridmix/SubmitterUserResolver.java      |   59 --
 .../apache/hadoop/mapred/gridmix/Summarizer.java   |   75 --
 .../apache/hadoop/mapred/gridmix/UserResolver.java |   65 --
 .../CumulativeCpuUsageEmulatorPlugin.java          |  315 ---------
 .../resourceusage/ResourceUsageEmulatorPlugin.java |   63 --
 .../resourceusage/ResourceUsageMatcher.java        |   89 ---
 .../TotalHeapUsageEmulatorPlugin.java              |  258 -------
 .../gridmix/src/test/data/wordcount.json.gz        |  Bin 1452 -> 0 bytes
 .../hadoop/mapred/gridmix/DebugJobFactory.java     |  106 ---
 .../hadoop/mapred/gridmix/DebugJobProducer.java    |  313 ---------
 .../hadoop/mapred/gridmix/GridmixTestUtils.java    |   92 ---
 .../gridmix/TestCompressionEmulationUtils.java     |  564 ---------------
 .../apache/hadoop/mapred/gridmix/TestFilePool.java |  189 -----
 .../hadoop/mapred/gridmix/TestFileQueue.java       |  143 ----
 .../mapred/gridmix/TestGridmixMemoryEmulation.java |  453 ------------
 .../hadoop/mapred/gridmix/TestGridmixRecord.java   |  278 --------
 .../hadoop/mapred/gridmix/TestGridmixSummary.java  |  377 ----------
 .../hadoop/mapred/gridmix/TestHighRamJob.java      |  195 ------
 .../hadoop/mapred/gridmix/TestPseudoLocalFs.java   |  233 -------
 .../hadoop/mapred/gridmix/TestRandomAlgorithm.java |  134 ----
 .../gridmix/TestRandomTextDataGenerator.java       |   84 ---
 .../hadoop/mapred/gridmix/TestRecordFactory.java   |   81 ---
 .../mapred/gridmix/TestResourceUsageEmulators.java |  613 -----------------
 .../hadoop/mapred/gridmix/TestUserResolve.java     |  172 -----
 hadoop-project/pom.xml                             |    5 +
 hadoop-tools/hadoop-gridmix/pom.xml                |  131 ++++
 .../hadoop/mapred/gridmix/AvgRecordFactory.java    |  123 ++++
 .../hadoop/mapred/gridmix/ClusterSummarizer.java   |  116 ++++
 .../mapred/gridmix/CompressionEmulationUtil.java   |  574 ++++++++++++++++
 .../mapred/gridmix/DistributedCacheEmulator.java   |  548 +++++++++++++++
 .../hadoop/mapred/gridmix/EchoUserResolver.java    |   57 ++
 .../hadoop/mapred/gridmix/ExecutionSummarizer.java |  312 +++++++++
 .../org/apache/hadoop/mapred/gridmix/FilePool.java |  301 ++++++++
 .../apache/hadoop/mapred/gridmix/FileQueue.java    |  103 +++
 .../apache/hadoop/mapred/gridmix/GenerateData.java |  412 +++++++++++
 .../mapred/gridmix/GenerateDistCacheData.java      |  259 +++++++
 .../org/apache/hadoop/mapred/gridmix/Gridmix.java  |  717 ++++++++++++++++++++
 .../apache/hadoop/mapred/gridmix/GridmixJob.java   |  517 ++++++++++++++
 .../mapred/gridmix/GridmixJobSubmissionPolicy.java |   89 +++
 .../apache/hadoop/mapred/gridmix/GridmixKey.java   |  301 ++++++++
 .../hadoop/mapred/gridmix/GridmixRecord.java       |  271 ++++++++
 .../apache/hadoop/mapred/gridmix/GridmixSplit.java |  148 ++++
 .../apache/hadoop/mapred/gridmix/InputStriper.java |  139 ++++
 .../mapred/gridmix/IntermediateRecordFactory.java  |  110 +++
 .../apache/hadoop/mapred/gridmix/JobCreator.java   |  135 ++++
 .../apache/hadoop/mapred/gridmix/JobFactory.java   |  253 +++++++
 .../apache/hadoop/mapred/gridmix/JobMonitor.java   |  255 +++++++
 .../apache/hadoop/mapred/gridmix/JobSubmitter.java |  196 ++++++
 .../org/apache/hadoop/mapred/gridmix/LoadJob.java  |  549 +++++++++++++++
 .../apache/hadoop/mapred/gridmix/LoadSplit.java    |  180 +++++
 .../apache/hadoop/mapred/gridmix/Progressive.java  |   25 +
 .../hadoop/mapred/gridmix/PseudoLocalFs.java       |  332 +++++++++
 .../hadoop/mapred/gridmix/RandomAlgorithms.java    |  209 ++++++
 .../mapred/gridmix/RandomTextDataGenerator.java    |  147 ++++
 .../hadoop/mapred/gridmix/ReadRecordFactory.java   |   85 +++
 .../hadoop/mapred/gridmix/RecordFactory.java       |   40 ++
 .../hadoop/mapred/gridmix/ReplayJobFactory.java    |  128 ++++
 .../mapred/gridmix/RoundRobinUserResolver.java     |  138 ++++
 .../hadoop/mapred/gridmix/SerialJobFactory.java    |  178 +++++
 .../org/apache/hadoop/mapred/gridmix/SleepJob.java |  411 +++++++++++
 .../apache/hadoop/mapred/gridmix/StatListener.java |   32 +
 .../apache/hadoop/mapred/gridmix/Statistics.java   |  330 +++++++++
 .../hadoop/mapred/gridmix/StressJobFactory.java    |  472 +++++++++++++
 .../mapred/gridmix/SubmitterUserResolver.java      |   59 ++
 .../apache/hadoop/mapred/gridmix/Summarizer.java   |   75 ++
 .../apache/hadoop/mapred/gridmix/UserResolver.java |   65 ++
 .../CumulativeCpuUsageEmulatorPlugin.java          |  315 +++++++++
 .../resourceusage/ResourceUsageEmulatorPlugin.java |   63 ++
 .../resourceusage/ResourceUsageMatcher.java        |   89 +++
 .../TotalHeapUsageEmulatorPlugin.java              |  258 +++++++
 .../hadoop/mapred/gridmix/DebugJobFactory.java     |  106 +++
 .../hadoop/mapred/gridmix/DebugJobProducer.java    |  313 +++++++++
 .../hadoop/mapred/gridmix/GridmixTestUtils.java    |   93 +++
 .../gridmix/TestCompressionEmulationUtils.java     |  564 +++++++++++++++
 .../apache/hadoop/mapred/gridmix/TestFilePool.java |  189 +++++
 .../hadoop/mapred/gridmix/TestFileQueue.java       |  143 ++++
 .../mapred/gridmix/TestGridmixMemoryEmulation.java |  453 ++++++++++++
 .../hadoop/mapred/gridmix/TestGridmixRecord.java   |  278 ++++++++
 .../hadoop/mapred/gridmix/TestGridmixSummary.java  |  377 ++++++++++
 .../hadoop/mapred/gridmix/TestHighRamJob.java      |  195 ++++++
 .../hadoop/mapred/gridmix/TestPseudoLocalFs.java   |  233 +++++++
 .../hadoop/mapred/gridmix/TestRandomAlgorithm.java |  134 ++++
 .../gridmix/TestRandomTextDataGenerator.java       |   84 +++
 .../hadoop/mapred/gridmix/TestRecordFactory.java   |   81 +++
 .../mapred/gridmix/TestResourceUsageEmulators.java |  613 +++++++++++++++++
 .../hadoop/mapred/gridmix/TestUserResolve.java     |  172 +++++
 .../src/test/resources/data/wordcount.json.gz      |  Bin 0 -> 1452 bytes
 hadoop-tools/hadoop-tools-dist/pom.xml             |    5 +
 hadoop-tools/pom.xml                               |    1 +
 126 files changed, 14286 insertions(+), 14144 deletions(-)
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FilePool.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Progressive.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StatListener.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Statistics.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/data/wordcount.json.gz
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFilePool.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
 create mode 100644 hadoop-tools/hadoop-gridmix/pom.xml
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
 create mode 100644 hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json.gz

diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
deleted file mode 100644
index 9ba6e9a..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
+++ /dev/null
@@ -1,123 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * Given byte and record targets, emit roughly equal-sized records satisfying
- * the contract.
- */
-class AvgRecordFactory extends RecordFactory {
-
-  /**
-   * Percentage of record for key data.
-   */
-  public static final String GRIDMIX_KEY_FRC = "gridmix.key.fraction";
-  public static final String GRIDMIX_MISSING_REC_SIZE = 
-    "gridmix.missing.rec.size";
-
-
-  private final long targetBytes;
-  private final long targetRecords;
-  private final long step;
-  private final int avgrec;
-  private final int keyLen;
-  private long accBytes = 0L;
-  private long accRecords = 0L;
-  private int unspilledBytes = 0;
-  private int minSpilledBytes = 0;
-
-  /**
-   * @param targetBytes Expected byte count.
-   * @param targetRecords Expected record count.
-   * @param conf Used to resolve edge cases @see #GRIDMIX_KEY_FRC
-   */
-  public AvgRecordFactory(long targetBytes, long targetRecords,
-      Configuration conf) {
-    this(targetBytes, targetRecords, conf, 0);
-  }
-  
-  /**
-   * @param minSpilledBytes Minimum amount of data expected per record
-   */
-  public AvgRecordFactory(long targetBytes, long targetRecords,
-      Configuration conf, int minSpilledBytes) {
-    this.targetBytes = targetBytes;
-    this.targetRecords = targetRecords <= 0 && this.targetBytes >= 0
-      ? Math.max(1,
-          this.targetBytes / conf.getInt(GRIDMIX_MISSING_REC_SIZE, 64 * 1024))
-      : targetRecords;
-    final long tmp = this.targetBytes / this.targetRecords;
-    step = this.targetBytes - this.targetRecords * tmp;
-    avgrec = (int) Math.min(Integer.MAX_VALUE, tmp + 1);
-    keyLen = Math.max(1,
-        (int)(tmp * Math.min(1.0f, conf.getFloat(GRIDMIX_KEY_FRC, 0.1f))));
-    this.minSpilledBytes = minSpilledBytes;
-  }
-
-  @Override
-  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-    if (accBytes >= targetBytes) {
-      return false;
-    }
-    final int reclen = accRecords++ >= step ? avgrec - 1 : avgrec;
-    final int len = (int) Math.min(targetBytes - accBytes, reclen);
-    
-    unspilledBytes += len;
-    
-    // len != reclen?
-    if (key != null) {
-      if (unspilledBytes < minSpilledBytes && accRecords < targetRecords) {
-        key.setSize(1);
-        val.setSize(1);
-        accBytes += key.getSize() + val.getSize();
-        unspilledBytes -= (key.getSize() + val.getSize());
-      } else {
-        key.setSize(keyLen);
-        val.setSize(unspilledBytes - key.getSize());
-        accBytes += unspilledBytes;
-        unspilledBytes = 0;
-      }
-    } else {
-      if (unspilledBytes < minSpilledBytes && accRecords < targetRecords) {
-        val.setSize(1);
-        accBytes += val.getSize();
-        unspilledBytes -= val.getSize();
-      } else {
-        val.setSize(unspilledBytes);
-        accBytes += unspilledBytes;
-        unspilledBytes = 0;
-      }
-    }
-    return true;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return Math.min(1.0f, accBytes / ((float)targetBytes));
-  }
-
-  @Override
-  public void close() throws IOException {
-    // noop
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
deleted file mode 100644
index 341767c..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.lang.time.FastDateFormat;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.CommonConfigurationKeys;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.mapred.JobTracker;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-
-/**
- * Summarizes the Hadoop cluster used in this {@link Gridmix} run. 
- * Statistics that are reported are
- * <ul>
- *   <li>Total number of active trackers in the cluster</li>
- *   <li>Total number of blacklisted trackers in the cluster</li>
- *   <li>Max map task capacity of the cluster</li>
- *   <li>Max reduce task capacity of the cluster</li>
- * </ul>
- * 
- * Apart from these statistics, {@link JobTracker} and {@link FileSystem} 
- * addresses are also recorded in the summary.
- */
-class ClusterSummarizer implements StatListener<ClusterStats> {
-  static final Log LOG = LogFactory.getLog(ClusterSummarizer.class);
-  
-  private int numBlacklistedTrackers;
-  private int numActiveTrackers;
-  private int maxMapTasks;
-  private int maxReduceTasks;
-  private String jobTrackerInfo = Summarizer.NA;
-  private String namenodeInfo = Summarizer.NA;
-  
-  @Override
-  @SuppressWarnings("deprecation")
-  public void update(ClusterStats item) {
-    try {
-      numBlacklistedTrackers = item.getStatus().getBlacklistedTrackers();
-      numActiveTrackers = item.getStatus().getTaskTrackers();
-      maxMapTasks = item.getStatus().getMaxMapTasks();
-      maxReduceTasks = item.getStatus().getMaxReduceTasks();
-    } catch (Exception e) {
-      long time = System.currentTimeMillis();
-      LOG.info("Error in processing cluster status at " 
-               + FastDateFormat.getInstance().format(time));
-    }
-  }
-  
-  /**
-   * Summarizes the cluster used for this {@link Gridmix} run.
-   */
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append("Cluster Summary:-");
-    builder.append("\nJobTracker: ").append(getJobTrackerInfo());
-    builder.append("\nFileSystem: ").append(getNamenodeInfo());
-    builder.append("\nNumber of blacklisted trackers: ")
-           .append(getNumBlacklistedTrackers());
-    builder.append("\nNumber of active trackers: ")
-           .append(getNumActiveTrackers());
-    builder.append("\nMax map task capacity: ")
-           .append(getMaxMapTasks());
-    builder.append("\nMax reduce task capacity: ").append(getMaxReduceTasks());
-    builder.append("\n\n");
-    return builder.toString();
-  }
-  
-  void start(Configuration conf) {
-    jobTrackerInfo = conf.get(JTConfig.JT_IPC_ADDRESS);
-    namenodeInfo = conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY);
-  }
-  
-  // Getters
-  protected int getNumBlacklistedTrackers() {
-    return numBlacklistedTrackers;
-  }
-  
-  protected int getNumActiveTrackers() {
-    return numActiveTrackers;
-  }
-  
-  protected int getMaxMapTasks() {
-    return maxMapTasks;
-  }
-  
-  protected int getMaxReduceTasks() {
-    return maxReduceTasks;
-  }
-  
-  protected String getJobTrackerInfo() {
-    return jobTrackerInfo;
-  }
-  
-  protected String getNamenodeInfo() {
-    return namenodeInfo;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
deleted file mode 100644
index 1308869..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
+++ /dev/null
@@ -1,574 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionCodecFactory;
-import org.apache.hadoop.io.compress.CompressionInputStream;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.GenerateData.GenDataFormat;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- * This is a utility class for all the compression related modules.
- */
-class CompressionEmulationUtil {
-  static final Log LOG = LogFactory.getLog(CompressionEmulationUtil.class);
-  
-  /**
-   * Enable compression usage in GridMix runs.
-   */
-  private static final String COMPRESSION_EMULATION_ENABLE = 
-    "gridmix.compression-emulation.enable";
-  
-  /**
-   * Enable input data decompression.
-   */
-  private static final String INPUT_DECOMPRESSION_EMULATION_ENABLE = 
-    "gridmix.compression-emulation.input-decompression.enable";
-  
-  /**
-   * Configuration property for setting the compression ratio for map input 
-   * data.
-   */
-  private static final String GRIDMIX_MAP_INPUT_COMPRESSION_RATIO = 
-    "gridmix.compression-emulation.map-input.decompression-ratio";
-  
-  /**
-   * Configuration property for setting the compression ratio of map output.
-   */
-  private static final String GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO = 
-    "gridmix.compression-emulation.map-output.compression-ratio";
-  
-  /**
-   * Configuration property for setting the compression ratio of reduce output.
-   */
-  private static final String GRIDMIX_REDUCE_OUTPUT_COMPRESSION_RATIO = 
-    "gridmix.compression-emulation.reduce-output.compression-ratio";
-  
-  /**
-   * Default compression ratio.
-   */
-  static final float DEFAULT_COMPRESSION_RATIO = 0.5F;
-  
-  private static final CompressionRatioLookupTable COMPRESSION_LOOKUP_TABLE = 
-    new CompressionRatioLookupTable();
-  
-  /**
-   * This is a {@link Mapper} implementation for generating random text data.
-   * It uses {@link RandomTextDataGenerator} for generating text data and the
-   * output files are compressed.
-   */
-  public static class RandomTextDataMapper
-  extends Mapper<NullWritable, LongWritable, Text, Text> {
-    private RandomTextDataGenerator rtg;
-
-    @Override
-    protected void setup(Context context)
-        throws IOException, InterruptedException {
-      Configuration conf = context.getConfiguration();
-      int listSize = 
-        RandomTextDataGenerator.getRandomTextDataGeneratorListSize(conf);
-      int wordSize = 
-        RandomTextDataGenerator.getRandomTextDataGeneratorWordSize(conf);
-      rtg = new RandomTextDataGenerator(listSize, wordSize);
-    }
-    
-    /**
-     * Emits random words sequence of desired size. Note that the desired output
-     * size is passed as the value parameter to this map.
-     */
-    @Override
-    public void map(NullWritable key, LongWritable value, Context context)
-    throws IOException, InterruptedException {
-      //TODO Control the extra data written ..
-      //TODO Should the key\tvalue\n be considered for measuring size?
-      //     Can counters like BYTES_WRITTEN be used? What will be the value of
-      //     such counters in LocalJobRunner?
-      for (long bytes = value.get(); bytes > 0;) {
-        String randomKey = rtg.getRandomWord();
-        String randomValue = rtg.getRandomWord();
-        context.write(new Text(randomKey), new Text(randomValue));
-        bytes -= (randomValue.getBytes().length + randomKey.getBytes().length);
-      }
-    }
-  }
-  
-  /**
-   * Configure the {@link Job} for enabling compression emulation.
-   */
-  static void configure(final Job job) throws IOException, InterruptedException,
-                                              ClassNotFoundException {
-    // set the random text mapper
-    job.setMapperClass(RandomTextDataMapper.class);
-    job.setNumReduceTasks(0);
-    job.setMapOutputKeyClass(Text.class);
-    job.setMapOutputValueClass(Text.class);
-    job.setInputFormatClass(GenDataFormat.class);
-    job.setJarByClass(GenerateData.class);
-
-    // set the output compression true
-    FileOutputFormat.setCompressOutput(job, true);
-    try {
-      FileInputFormat.addInputPath(job, new Path("ignored"));
-    } catch (IOException e) {
-      LOG.error("Error while adding input path ", e);
-    }
-  }
-
-  /**
-   * This is the lookup table for mapping compression ratio to the size of the 
-   * word in the {@link RandomTextDataGenerator}'s dictionary. 
-   * 
-   * Note that this table is computed (empirically) using a dictionary of 
-   * default length i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
-   */
-  private static class CompressionRatioLookupTable {
-    private static Map<Float, Integer> map = new HashMap<Float, Integer>(60);
-    private static final float MIN_RATIO = 0.07F;
-    private static final float MAX_RATIO = 0.68F;
-    
-    // add the empirically obtained data points in the lookup table
-    CompressionRatioLookupTable() {
-      map.put(.07F,30);
-      map.put(.08F,25);
-      map.put(.09F,60);
-      map.put(.10F,20);
-      map.put(.11F,70);
-      map.put(.12F,15);
-      map.put(.13F,80);
-      map.put(.14F,85);
-      map.put(.15F,90);
-      map.put(.16F,95);
-      map.put(.17F,100);
-      map.put(.18F,105);
-      map.put(.19F,110);
-      map.put(.20F,115);
-      map.put(.21F,120);
-      map.put(.22F,125);
-      map.put(.23F,130);
-      map.put(.24F,140);
-      map.put(.25F,145);
-      map.put(.26F,150);
-      map.put(.27F,155);
-      map.put(.28F,160);
-      map.put(.29F,170);
-      map.put(.30F,175);
-      map.put(.31F,180);
-      map.put(.32F,190);
-      map.put(.33F,195);
-      map.put(.34F,205);
-      map.put(.35F,215);
-      map.put(.36F,225);
-      map.put(.37F,230);
-      map.put(.38F,240);
-      map.put(.39F,250);
-      map.put(.40F,260);
-      map.put(.41F,270);
-      map.put(.42F,280);
-      map.put(.43F,295);
-      map.put(.44F,310);
-      map.put(.45F,325);
-      map.put(.46F,335);
-      map.put(.47F,355);
-      map.put(.48F,375);
-      map.put(.49F,395);
-      map.put(.50F,420);
-      map.put(.51F,440);
-      map.put(.52F,465);
-      map.put(.53F,500);
-      map.put(.54F,525);
-      map.put(.55F,550);
-      map.put(.56F,600);
-      map.put(.57F,640);
-      map.put(.58F,680);
-      map.put(.59F,734);
-      map.put(.60F,813);
-      map.put(.61F,905);
-      map.put(.62F,1000);
-      map.put(.63F,1055);
-      map.put(.64F,1160);
-      map.put(.65F,1355);
-      map.put(.66F,1510);
-      map.put(.67F,1805);
-      map.put(.68F,2170);
-    }
-    
-    /**
-     * Returns the size of the word in {@link RandomTextDataGenerator}'s 
-     * dictionary that can generate text with the desired compression ratio.
-     * 
-     * @throws RuntimeException If ratio is less than {@value #MIN_RATIO} or 
-     *                          greater than {@value #MAX_RATIO}.
-     */
-    int getWordSizeForRatio(float ratio) {
-      ratio = standardizeCompressionRatio(ratio);
-      if (ratio >= MIN_RATIO && ratio <= MAX_RATIO) {
-        return map.get(ratio);
-      } else {
-        throw new RuntimeException("Compression ratio should be in the range [" 
-          + MIN_RATIO + "," + MAX_RATIO + "]. Configured compression ratio is " 
-          + ratio + ".");
-      }
-    }
-  }
-  
-  /**
-   * Setup the data generator's configuration to generate compressible random 
-   * text data with the desired compression ratio.
-   * Note that the compression ratio, if configured, will set the 
-   * {@link RandomTextDataGenerator}'s list-size and word-size based on 
-   * empirical values using the compression ratio set in the configuration. 
-   * 
-   * Hence to achieve the desired compression ratio, 
-   * {@link RandomTextDataGenerator}'s list-size will be set to the default 
-   * value i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
-   */
-  static void setupDataGeneratorConfig(Configuration conf) {
-    boolean compress = isCompressionEmulationEnabled(conf);
-    if (compress) {
-      float ratio = getMapInputCompressionEmulationRatio(conf);
-      LOG.info("GridMix is configured to generate compressed input data with "
-               + " a compression ratio of " + ratio);
-      int wordSize = COMPRESSION_LOOKUP_TABLE.getWordSizeForRatio(ratio);
-      RandomTextDataGenerator.setRandomTextDataGeneratorWordSize(conf, 
-                                                                 wordSize);
-
-      // since the compression ratios are computed using the default value of 
-      // list size
-      RandomTextDataGenerator.setRandomTextDataGeneratorListSize(conf, 
-          RandomTextDataGenerator.DEFAULT_LIST_SIZE);
-    }
-  }
-  
-  /**
-   * Returns a {@link RandomTextDataGenerator} that generates random 
-   * compressible text with the desired compression ratio.
-   */
-  static RandomTextDataGenerator getRandomTextDataGenerator(float ratio, 
-                                                            long seed) {
-    int wordSize = COMPRESSION_LOOKUP_TABLE.getWordSizeForRatio(ratio);
-    RandomTextDataGenerator rtg = 
-      new RandomTextDataGenerator(RandomTextDataGenerator.DEFAULT_LIST_SIZE, 
-            seed, wordSize);
-    return rtg;
-  }
-  
-  /** Publishes compression related data statistics. Following statistics are
-   * published
-   * <ul>
-   *   <li>Total compressed input data size</li>
-   *   <li>Number of compressed input data files</li>
-   *   <li>Compression Ratio</li>
-   *   <li>Text data dictionary size</li>
-   *   <li>Random text word size</li>
-   * </ul>
-   */
-  static DataStatistics publishCompressedDataStatistics(Path inputDir, 
-                          Configuration conf, long uncompressedDataSize) 
-  throws IOException {
-    FileSystem fs = inputDir.getFileSystem(conf);
-    CompressionCodecFactory compressionCodecs = 
-      new CompressionCodecFactory(conf);
-
-    // iterate over compressed files and sum up the compressed file sizes
-    long compressedDataSize = 0;
-    int numCompressedFiles = 0;
-    // obtain input data file statuses
-    FileStatus[] outFileStatuses = 
-      fs.listStatus(inputDir, new Utils.OutputFileUtils.OutputFilesFilter());
-    for (FileStatus status : outFileStatuses) {
-      // check if the input file is compressed
-      if (compressionCodecs != null) {
-        CompressionCodec codec = compressionCodecs.getCodec(status.getPath());
-        if (codec != null) {
-          ++numCompressedFiles;
-          compressedDataSize += status.getLen();
-        }
-      }
-    }
-
-    LOG.info("Gridmix is configured to use compressed input data.");
-    // publish the input data size
-    LOG.info("Total size of compressed input data : " 
-             + StringUtils.humanReadableInt(compressedDataSize));
-    LOG.info("Total number of compressed input data files : " 
-             + numCompressedFiles);
-
-    if (numCompressedFiles == 0) {
-      throw new RuntimeException("No compressed file found in the input" 
-          + " directory : " + inputDir.toString() + ". To enable compression"
-          + " emulation, run Gridmix either with "
-          + " an input directory containing compressed input file(s) or" 
-          + " use the -generate option to (re)generate it. If compression"
-          + " emulation is not desired, disable it by setting '" 
-          + COMPRESSION_EMULATION_ENABLE + "' to 'false'.");
-    }
-    
-    // publish compression ratio only if its generated in this gridmix run
-    if (uncompressedDataSize > 0) {
-      // compute the compression ratio
-      double ratio = ((double)compressedDataSize) / uncompressedDataSize;
-
-      // publish the compression ratio
-      LOG.info("Input Data Compression Ratio : " + ratio);
-    }
-    
-    return new DataStatistics(compressedDataSize, numCompressedFiles, true);
-  }
-  
-  /**
-   * Enables/Disables compression emulation.
-   * @param conf Target configuration where the parameter 
-   * {@value #COMPRESSION_EMULATION_ENABLE} will be set. 
-   * @param val The value to be set.
-   */
-  static void setCompressionEmulationEnabled(Configuration conf, boolean val) {
-    conf.setBoolean(COMPRESSION_EMULATION_ENABLE, val);
-  }
-  
-  /**
-   * Checks if compression emulation is enabled or not. Default is {@code true}.
-   */
-  static boolean isCompressionEmulationEnabled(Configuration conf) {
-    return conf.getBoolean(COMPRESSION_EMULATION_ENABLE, true);
-  }
-  
-  /**
-   * Enables/Disables input decompression emulation.
-   * @param conf Target configuration where the parameter 
-   * {@value #INPUT_DECOMPRESSION_EMULATION_ENABLE} will be set. 
-   * @param val The value to be set.
-   */
-  static void setInputCompressionEmulationEnabled(Configuration conf, 
-                                                  boolean val) {
-    conf.setBoolean(INPUT_DECOMPRESSION_EMULATION_ENABLE, val);
-  }
-  
-  /**
-   * Check if input decompression emulation is enabled or not. 
-   * Default is {@code false}.
-   */
-  static boolean isInputCompressionEmulationEnabled(Configuration conf) {
-    return conf.getBoolean(INPUT_DECOMPRESSION_EMULATION_ENABLE, false);
-  }
-  
-  /**
-   * Set the map input data compression ratio in the given conf.
-   */
-  static void setMapInputCompressionEmulationRatio(Configuration conf, 
-                                                   float ratio) {
-    conf.setFloat(GRIDMIX_MAP_INPUT_COMPRESSION_RATIO, ratio);
-  }
-  
-  /**
-   * Get the map input data compression ratio using the given configuration.
-   * If the compression ratio is not set in the configuration then use the 
-   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
-   */
-  static float getMapInputCompressionEmulationRatio(Configuration conf) {
-    return conf.getFloat(GRIDMIX_MAP_INPUT_COMPRESSION_RATIO, 
-                         DEFAULT_COMPRESSION_RATIO);
-  }
-  
-  /**
-   * Set the map output data compression ratio in the given configuration.
-   */
-  static void setMapOutputCompressionEmulationRatio(Configuration conf, 
-                                                    float ratio) {
-    conf.setFloat(GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO, ratio);
-  }
-  
-  /**
-   * Get the map output data compression ratio using the given configuration.
-   * If the compression ratio is not set in the configuration then use the 
-   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
-   */
-  static float getMapOutputCompressionEmulationRatio(Configuration conf) {
-    return conf.getFloat(GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO, 
-                         DEFAULT_COMPRESSION_RATIO);
-  }
-  
-  /**
-   * Set the reduce output data compression ratio in the given configuration.
-   */
-  static void setReduceOutputCompressionEmulationRatio(Configuration conf, 
-                                                       float ratio) {
-    conf.setFloat(GRIDMIX_REDUCE_OUTPUT_COMPRESSION_RATIO, ratio);
-  }
-  
-  /**
-   * Get the reduce output data compression ratio using the given configuration.
-   * If the compression ratio is not set in the configuration then use the 
-   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
-   */
-  static float getReduceOutputCompressionEmulationRatio(Configuration conf) {
-    return conf.getFloat(GRIDMIX_REDUCE_OUTPUT_COMPRESSION_RATIO, 
-                         DEFAULT_COMPRESSION_RATIO);
-  }
-  
-  /**
-   * Standardize the compression ratio i.e round off the compression ratio to
-   * only 2 significant digits.
-   */
-  static float standardizeCompressionRatio(float ratio) {
-    // round off to 2 significant digits
-    int significant = (int)Math.round(ratio * 100);
-    return ((float)significant)/100;
-  }
-  
-  /**
-   * Returns a {@link InputStream} for a file that might be compressed.
-   */
-  static InputStream getPossiblyDecompressedInputStream(Path file, 
-                                                        Configuration conf,
-                                                        long offset)
-  throws IOException {
-    FileSystem fs = file.getFileSystem(conf);
-    if (isCompressionEmulationEnabled(conf)
-        && isInputCompressionEmulationEnabled(conf)) {
-      CompressionCodecFactory compressionCodecs = 
-        new CompressionCodecFactory(conf);
-      CompressionCodec codec = compressionCodecs.getCodec(file);
-      if (codec != null) {
-        Decompressor decompressor = CodecPool.getDecompressor(codec);
-        if (decompressor != null) {
-          CompressionInputStream in = 
-            codec.createInputStream(fs.open(file), decompressor);
-          //TODO Seek doesnt work with compressed input stream. 
-          //     Use SplittableCompressionCodec?
-          return (InputStream)in;
-        }
-      }
-    }
-    FSDataInputStream in = fs.open(file);
-    in.seek(offset);
-    return (InputStream)in;
-  }
-  
-  /**
-   * Returns a {@link OutputStream} for a file that might need 
-   * compression.
-   */
-  static OutputStream getPossiblyCompressedOutputStream(Path file, 
-                                                        Configuration conf)
-  throws IOException {
-    FileSystem fs = file.getFileSystem(conf);
-    JobConf jConf = new JobConf(conf);
-    if (org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput(jConf)) {
-      // get the codec class
-      Class<? extends CompressionCodec> codecClass =
-        org.apache.hadoop.mapred.FileOutputFormat
-                                .getOutputCompressorClass(jConf, 
-                                                          GzipCodec.class);
-      // get the codec implementation
-      CompressionCodec codec = ReflectionUtils.newInstance(codecClass, conf);
-
-      // add the appropriate extension
-      file = file.suffix(codec.getDefaultExtension());
-
-      if (isCompressionEmulationEnabled(conf)) {
-        FSDataOutputStream fileOut = fs.create(file, false);
-        return new DataOutputStream(codec.createOutputStream(fileOut));
-      }
-    }
-    return fs.create(file, false);
-  }
-  
-  /**
-   * Extracts compression/decompression related configuration parameters from 
-   * the source configuration to the target configuration.
-   */
-  static void configureCompressionEmulation(Configuration source, 
-                                            Configuration target) {
-    // enable output compression
-    target.setBoolean(FileOutputFormat.COMPRESS, 
-        source.getBoolean(FileOutputFormat.COMPRESS, false));
-
-    // set the job output compression codec
-    String jobOutputCompressionCodec = 
-      source.get(FileOutputFormat.COMPRESS_CODEC);
-    if (jobOutputCompressionCodec != null) {
-      target.set(FileOutputFormat.COMPRESS_CODEC, jobOutputCompressionCodec);
-    }
-
-    // set the job output compression type
-    String jobOutputCompressionType = 
-      source.get(FileOutputFormat.COMPRESS_TYPE);
-    if (jobOutputCompressionType != null) {
-      target.set(FileOutputFormat.COMPRESS_TYPE, jobOutputCompressionType);
-    }
-
-    // enable map output compression
-    target.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,
-        source.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));
-
-    // set the map output compression codecs
-    String mapOutputCompressionCodec = 
-      source.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC);
-    if (mapOutputCompressionCodec != null) {
-      target.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, 
-                 mapOutputCompressionCodec);
-    }
-
-    // enable input decompression
-    //TODO replace with mapInputBytes and hdfsBytesRead
-    Path[] inputs = 
-      org.apache.hadoop.mapred.FileInputFormat
-         .getInputPaths(new JobConf(source));
-    boolean needsCompressedInput = false;
-    CompressionCodecFactory compressionCodecs = 
-      new CompressionCodecFactory(source);
-    for (Path input : inputs) {
-      CompressionCodec codec = compressionCodecs.getCodec(input);
-      if (codec != null) {
-        needsCompressedInput = true;
-      }
-    }
-    setInputCompressionEmulationEnabled(target, needsCompressedInput);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
deleted file mode 100644
index 8b80d42..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
+++ /dev/null
@@ -1,548 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsAction;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.MD5Hash;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants;
-
-import java.io.IOException;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Emulation of Distributed Cache Usage in gridmix.
- * <br> Emulation of Distributed Cache Load in gridmix will put load on
- * TaskTrackers and affects execution time of tasks because of localization of
- * distributed cache files by TaskTrackers.
- * <br> Gridmix creates distributed cache files for simulated jobs by launching
- * a MapReduce job {@link GenerateDistCacheData} in advance i.e. before
- * launching simulated jobs.
- * <br> The distributed cache file paths used in the original cluster are mapped
- * to unique file names in the simulated cluster.
- * <br> All HDFS-based distributed cache files generated by gridmix are
- * public distributed cache files. But Gridmix makes sure that load incurred due
- * to localization of private distributed cache files on the original cluster
- * is also faithfully simulated. Gridmix emulates the load due to private
- * distributed cache files by mapping private distributed cache files of
- * different users in the original cluster to different public distributed cache
- * files in the simulated cluster.
- *
- * <br> The configuration properties like
- * {@link MRJobConfig#CACHE_FILES}, {@link MRJobConfig#CACHE_FILE_VISIBILITIES},
- * {@link MRJobConfig#CACHE_FILES_SIZES} and
- * {@link MRJobConfig#CACHE_FILE_TIMESTAMPS} obtained from trace are used to
- *  decide
- * <li> file size of each distributed cache file to be generated
- * <li> whether a distributed cache file is already seen in this trace file
- * <li> whether a distributed cache file was considered public or private.
- * <br>
- * <br> Gridmix configures these generated files as distributed cache files for
- * the simulated jobs.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-class DistributedCacheEmulator {
-  private static final Log LOG =
-      LogFactory.getLog(DistributedCacheEmulator.class);
-
-  static final long AVG_BYTES_PER_MAP = 128 * 1024 * 1024L;// 128MB
-
-  // If at least 1 distributed cache file is missing in the expected
-  // distributed cache dir, Gridmix cannot proceed with emulation of
-  // distributed cache load.
-  int MISSING_DIST_CACHE_FILES_ERROR = 1;
-
-  private Path distCachePath;
-
-  /**
-   * Map between simulated cluster's distributed cache file paths and their
-   * file sizes. Unique distributed cache files are entered into this map.
-   * 2 distributed cache files are considered same if and only if their
-   * file paths, visibilities and timestamps are same.
-   */
-  private Map<String, Long> distCacheFiles = new HashMap<String, Long>();
-
-  /**
-   * Configuration property for whether gridmix should emulate
-   * distributed cache usage or not. Default value is true.
-   */
-  static final String GRIDMIX_EMULATE_DISTRIBUTEDCACHE =
-      "gridmix.distributed-cache-emulation.enable";
-
-  // Whether to emulate distributed cache usage or not
-  boolean emulateDistributedCache = true;
-
-  // Whether to generate distributed cache data or not
-  boolean generateDistCacheData = false;
-
-  Configuration conf; // gridmix configuration
-
-  // Pseudo local file system where local FS based distributed cache files are
-  // created by gridmix.
-  FileSystem pseudoLocalFs = null;
-
-  {
-    // Need to handle deprecation of these MapReduce-internal configuration
-    // properties as MapReduce doesn't handle their deprecation.
-    Configuration.addDeprecation("mapred.cache.files.filesizes",
-        new String[] {MRJobConfig.CACHE_FILES_SIZES});
-    Configuration.addDeprecation("mapred.cache.files.visibilities",
-        new String[] {MRJobConfig.CACHE_FILE_VISIBILITIES});
-  }
-
-  /**
-   * @param conf gridmix configuration
-   * @param ioPath &lt;ioPath&gt;/distributedCache/ is the gridmix Distributed
-   *               Cache directory
-   */
-  public DistributedCacheEmulator(Configuration conf, Path ioPath) {
-    this.conf = conf;
-    distCachePath = new Path(ioPath, "distributedCache");
-    this.conf.setClass("fs.pseudo.impl", PseudoLocalFs.class, FileSystem.class);
-  }
-
-  /**
-   * This is to be called before any other method of DistributedCacheEmulator.
-   * <br> Checks if emulation of distributed cache load is needed and is feasible.
-   *  Sets the flags generateDistCacheData and emulateDistributedCache to the
-   *  appropriate values.
-   * <br> Gridmix does not emulate distributed cache load if
-   * <ol><li> the specific gridmix job type doesn't need emulation of
-   * distributed cache load OR
-   * <li> the trace is coming from a stream instead of file OR
-   * <li> the distributed cache dir where distributed cache data is to be
-   * generated by gridmix is on local file system OR
-   * <li> execute permission is not there for any of the ascendant directories
-   * of &lt;ioPath&gt; till root. This is because for emulation of distributed
-   * cache load, distributed cache files created under
-   * &lt;ioPath/distributedCache/public/&gt; should be considered by hadoop
-   * as public distributed cache files.
-   * <li> creation of pseudo local file system fails.</ol>
-   * <br> For (2), (3), (4) and (5), generation of distributed cache data
-   * is also disabled.
-   * 
-   * @param traceIn trace file path. If this is '-', then trace comes from the
-   *                stream stdin.
-   * @param jobCreator job creator of gridmix jobs of a specific type
-   * @param generate  true if -generate option was specified
-   * @throws IOException
-   */
-  void init(String traceIn, JobCreator jobCreator, boolean generate)
-      throws IOException {
-    emulateDistributedCache = jobCreator.canEmulateDistCacheLoad()
-        && conf.getBoolean(GRIDMIX_EMULATE_DISTRIBUTEDCACHE, true);
-    generateDistCacheData = generate;
-
-    if (generateDistCacheData || emulateDistributedCache) {
-      if ("-".equals(traceIn)) {// trace is from stdin
-        LOG.warn("Gridmix will not emulate Distributed Cache load because "
-            + "the input trace source is a stream instead of file.");
-        emulateDistributedCache = generateDistCacheData = false;
-      } else if (FileSystem.getLocal(conf).getUri().getScheme().equals(
-          distCachePath.toUri().getScheme())) {// local FS
-        LOG.warn("Gridmix will not emulate Distributed Cache load because "
-            + "<iopath> provided is on local file system.");
-        emulateDistributedCache = generateDistCacheData = false;
-      } else {
-        // Check if execute permission is there for all the ascendant
-        // directories of distCachePath till root.
-        FileSystem fs = FileSystem.get(conf);
-        Path cur = distCachePath.getParent();
-        while (cur != null) {
-          if (cur.toString().length() > 0) {
-            FsPermission perm = fs.getFileStatus(cur).getPermission();
-            if (!perm.getOtherAction().and(FsAction.EXECUTE).equals(
-                FsAction.EXECUTE)) {
-              LOG.warn("Gridmix will not emulate Distributed Cache load "
-                  + "because the ascendant directory (of distributed cache "
-                  + "directory) " + cur + " doesn't have execute permission "
-                  + "for others.");
-              emulateDistributedCache = generateDistCacheData = false;
-              break;
-            }
-          }
-          cur = cur.getParent();
-        }
-      }
-    }
-
-    // Check if pseudo local file system can be created
-    try {
-      pseudoLocalFs = FileSystem.get(new URI("pseudo:///"), conf);
-    } catch (URISyntaxException e) {
-      LOG.warn("Gridmix will not emulate Distributed Cache load because "
-          + "creation of pseudo local file system failed.");
-      e.printStackTrace();
-      emulateDistributedCache = generateDistCacheData = false;
-      return;
-    }
-  }
-
-  /**
-   * @return true if gridmix should emulate distributed cache load
-   */
-  boolean shouldEmulateDistCacheLoad() {
-    return emulateDistributedCache;
-  }
-
-  /**
-   * @return true if gridmix should generate distributed cache data
-   */
-  boolean shouldGenerateDistCacheData() {
-    return generateDistCacheData;
-  }
-
-  /**
-   * @return the distributed cache directory path
-   */
-  Path getDistributedCacheDir() {
-    return distCachePath;
-  }
-
-  /**
-   * Create distributed cache directories.
-   * Also create a file that contains the list of distributed cache files
-   * that will be used as distributed cache files for all the simulated jobs.
-   * @param jsp job story producer for the trace
-   * @return exit code
-   * @throws IOException
-   */
-  int setupGenerateDistCacheData(JobStoryProducer jsp)
-      throws IOException {
-
-    createDistCacheDirectory();
-    return buildDistCacheFilesList(jsp);
-  }
-
-  /**
-   * Create distributed cache directory where distributed cache files will be
-   * created by the MapReduce job {@link GenerateDistCacheData#JOB_NAME}.
-   * @throws IOException
-   */
-  private void createDistCacheDirectory() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-    FileSystem.mkdirs(fs, distCachePath, new FsPermission((short) 0777));
-  }
-
-  /**
-   * Create the list of unique distributed cache files needed for all the
-   * simulated jobs and write the list to a special file.
-   * @param jsp job story producer for the trace
-   * @return exit code
-   * @throws IOException
-   */
-  private int buildDistCacheFilesList(JobStoryProducer jsp) throws IOException {
-    // Read all the jobs from the trace file and build the list of unique
-    // distributed cache files.
-    JobStory jobStory;
-    while ((jobStory = jsp.getNextJob()) != null) {
-      if (jobStory.getOutcome() == Pre21JobHistoryConstants.Values.SUCCESS && 
-         jobStory.getSubmissionTime() >= 0) {
-        updateHDFSDistCacheFilesList(jobStory);
-      }
-    }
-    jsp.close();
-
-    return writeDistCacheFilesList();
-  }
-
-  /**
-   * For the job to be simulated, identify the needed distributed cache files by
-   * mapping original cluster's distributed cache file paths to the simulated cluster's
-   * paths and add these paths in the map {@code distCacheFiles}.
-   *<br>
-   * JobStory should contain distributed cache related properties like
-   * <li> {@link MRJobConfig#CACHE_FILES}
-   * <li> {@link MRJobConfig#CACHE_FILE_VISIBILITIES}
-   * <li> {@link MRJobConfig#CACHE_FILES_SIZES}
-   * <li> {@link MRJobConfig#CACHE_FILE_TIMESTAMPS}
-   * <li> {@link MRJobConfig#CLASSPATH_FILES}
-   *
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES}
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES_VISIBILITIES}
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES_SIZES}
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES_TIMESTAMPS}
-   * <li> {@link MRJobConfig#CLASSPATH_ARCHIVES}
-   *
-   * <li> {@link MRJobConfig#CACHE_SYMLINK}
-   *
-   * @param jobdesc JobStory of original job obtained from trace
-   * @throws IOException
-   */
-  void updateHDFSDistCacheFilesList(JobStory jobdesc) throws IOException {
-
-    // Map original job's distributed cache file paths to simulated cluster's
-    // paths, to be used by this simulated job.
-    JobConf jobConf = jobdesc.getJobConf();
-
-    String[] files = jobConf.getStrings(MRJobConfig.CACHE_FILES);
-    if (files != null) {
-
-      String[] fileSizes = jobConf.getStrings(MRJobConfig.CACHE_FILES_SIZES);
-      String[] visibilities =
-        jobConf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES);
-      String[] timeStamps =
-        jobConf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS);
-
-      FileSystem fs = FileSystem.get(conf);
-      String user = jobConf.getUser();
-      for (int i = 0; i < files.length; i++) {
-        // Check if visibilities are available because older hadoop versions
-        // didn't have public, private Distributed Caches separately.
-        boolean visibility =
-            (visibilities == null) ? true : Boolean.valueOf(visibilities[i]);
-        if (isLocalDistCacheFile(files[i], user, visibility)) {
-          // local FS based distributed cache file.
-          // Create this file on the pseudo local FS on the fly (i.e. when the
-          // simulated job is submitted).
-          continue;
-        }
-        // distributed cache file on hdfs
-        String mappedPath = mapDistCacheFilePath(files[i], timeStamps[i],
-                                                 visibility, user);
-
-        // No need to add a distributed cache file path to the list if
-        // (1) the mapped path is already there in the list OR
-        // (2) the file with the mapped path already exists.
-        // In any of the above 2 cases, file paths, timestamps, file sizes and
-        // visibilities match. File sizes should match if file paths and
-        // timestamps match because single file path with single timestamp
-        // should correspond to a single file size.
-        if (distCacheFiles.containsKey(mappedPath) ||
-            fs.exists(new Path(mappedPath))) {
-          continue;
-        }
-        distCacheFiles.put(mappedPath, Long.valueOf(fileSizes[i]));
-      }
-    }
-  }
-
-  /**
-   * Check if the file path provided was constructed by MapReduce for a
-   * distributed cache file on local file system.
-   * @param filePath path of the distributed cache file
-   * @param user job submitter of the job for which &lt;filePath&gt; is a
-   *             distributed cache file
-   * @param visibility <code>true</code> for public distributed cache file
-   * @return true if the path provided is of a local file system based
-   *              distributed cache file
-   */
-  static boolean isLocalDistCacheFile(String filePath, String user,
-                                       boolean visibility) {
-    return (!visibility && filePath.contains(user + "/.staging"));
-  }
-
-  /**
-   * Map the HDFS based distributed cache file path from original cluster to
-   * a unique file name on the simulated cluster.
-   * <br> Unique  distributed file names on simulated cluster are generated
-   * using original cluster's <li>file path, <li>timestamp and <li> the
-   * job-submitter for private distributed cache file.
-   * <br> This implies that if on original cluster, a single HDFS file
-   * considered as two private distributed cache files for two jobs of
-   * different users, then the corresponding simulated jobs will have two
-   * different files of the same size in public distributed cache, one for each
-   * user. Both these simulated jobs will not share these distributed cache
-   * files, thus leading to the same load as seen in the original cluster.
-   * @param file distributed cache file path
-   * @param timeStamp time stamp of dist cachce file
-   * @param isPublic true if this distributed cache file is a public
-   *                 distributed cache file
-   * @param user job submitter on original cluster
-   * @return the mapped path on simulated cluster
-   */
-  private String mapDistCacheFilePath(String file, String timeStamp,
-      boolean isPublic, String user) {
-    String id = file + timeStamp;
-    if (!isPublic) {
-      // consider job-submitter for private distributed cache file
-      id = id.concat(user);
-    }
-    return new Path(distCachePath, MD5Hash.digest(id).toString()).toUri()
-               .getPath();
-  }
-
-  /**
-   * Write the list of distributed cache files in the decreasing order of
-   * file sizes into the sequence file. This file will be input to the job
-   * {@link GenerateDistCacheData}.
-   * Also validates if -generate option is missing and distributed cache files
-   * are missing.
-   * @return exit code
-   * @throws IOException
-   */
-  private int writeDistCacheFilesList()
-      throws IOException {
-    // Sort the distributed cache files in the decreasing order of file sizes.
-    List dcFiles = new ArrayList(distCacheFiles.entrySet());
-    Collections.sort(dcFiles, new Comparator() {
-      public int compare(Object dc1, Object dc2) {
-        return ((Comparable) ((Map.Entry) (dc2)).getValue())
-            .compareTo(((Map.Entry) (dc1)).getValue());
-      }
-    });
-
-    // write the sorted distributed cache files to the sequence file
-    FileSystem fs = FileSystem.get(conf);
-    Path distCacheFilesList = new Path(distCachePath, "_distCacheFiles.txt");
-    conf.set(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST,
-        distCacheFilesList.toString());
-    SequenceFile.Writer src_writer = SequenceFile.createWriter(fs, conf,
-        distCacheFilesList, LongWritable.class, BytesWritable.class,
-        SequenceFile.CompressionType.NONE);
-
-    // Total number of unique distributed cache files
-    int fileCount = dcFiles.size();
-    long byteCount = 0;// Total size of all distributed cache files
-    long bytesSync = 0;// Bytes after previous sync;used to add sync marker
-
-    for (Iterator it = dcFiles.iterator(); it.hasNext();) {
-      Map.Entry entry = (Map.Entry)it.next();
-      LongWritable fileSize =
-          new LongWritable(Long.valueOf(entry.getValue().toString()));
-      BytesWritable filePath =
-          new BytesWritable(entry.getKey().toString().getBytes());
-
-      byteCount += fileSize.get();
-      bytesSync += fileSize.get();
-      if (bytesSync > AVG_BYTES_PER_MAP) {
-        src_writer.sync();
-        bytesSync = fileSize.get();
-      }
-      src_writer.append(fileSize, filePath);
-    }
-    if (src_writer != null) {
-      src_writer.close();
-    }
-    // Set delete on exit for 'dist cache files list' as it is not needed later.
-    fs.deleteOnExit(distCacheFilesList);
-
-    conf.setInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, fileCount);
-    conf.setLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, byteCount);
-    LOG.info("Number of HDFS based distributed cache files to be generated is "
-        + fileCount + ". Total size of HDFS based distributed cache files "
-        + "to be generated is " + byteCount);
-
-    if (!shouldGenerateDistCacheData() && fileCount > 0) {
-      LOG.error("Missing " + fileCount + " distributed cache files under the "
-          + " directory\n" + distCachePath + "\nthat are needed for gridmix"
-          + " to emulate distributed cache load. Either use -generate\noption"
-          + " to generate distributed cache data along with input data OR "
-          + "disable\ndistributed cache emulation by configuring '"
-          + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE
-          + "' to false.");
-      return MISSING_DIST_CACHE_FILES_ERROR;
-    }
-    return 0;
-  }
-
-  /**
-   * If gridmix needs to emulate distributed cache load, then configure
-   * distributed cache files of a simulated job by mapping the original
-   * cluster's distributed cache file paths to the simulated cluster's paths and
-   * setting these mapped paths in the job configuration of the simulated job.
-   * <br>
-   * Configure local FS based distributed cache files through the property
-   * "tmpfiles" and hdfs based distributed cache files through the property
-   * {@link MRJobConfig#CACHE_FILES}.
-   * @param conf configuration for the simulated job to be run
-   * @param jobConf job configuration of original cluster's job, obtained from
-   *                trace
-   * @throws IOException
-   */
-  void configureDistCacheFiles(Configuration conf, JobConf jobConf)
-      throws IOException {
-    if (shouldEmulateDistCacheLoad()) {
-
-      String[] files = jobConf.getStrings(MRJobConfig.CACHE_FILES);
-      if (files != null) {
-        // hdfs based distributed cache files to be configured for simulated job
-        List<String> cacheFiles = new ArrayList<String>();
-        // local FS based distributed cache files to be configured for
-        // simulated job
-        List<String> localCacheFiles = new ArrayList<String>();
-
-        String[] visibilities =
-          jobConf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES);
-        String[] timeStamps =
-          jobConf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS);
-        String[] fileSizes = jobConf.getStrings(MRJobConfig.CACHE_FILES_SIZES);
-
-        String user = jobConf.getUser();
-        for (int i = 0; i < files.length; i++) {
-          // Check if visibilities are available because older hadoop versions
-          // didn't have public, private Distributed Caches separately.
-          boolean visibility =
-            (visibilities == null) ? true : Boolean.valueOf(visibilities[i]);
-          if (isLocalDistCacheFile(files[i], user, visibility)) {
-            // local FS based distributed cache file.
-            // Create this file on the pseudo local FS.
-            String fileId = MD5Hash.digest(files[i] + timeStamps[i]).toString();
-            long fileSize = Long.valueOf(fileSizes[i]);
-            Path mappedLocalFilePath =
-                PseudoLocalFs.generateFilePath(fileId, fileSize)
-                    .makeQualified(pseudoLocalFs.getUri(),
-                                   pseudoLocalFs.getWorkingDirectory());
-            pseudoLocalFs.create(mappedLocalFilePath);
-            localCacheFiles.add(mappedLocalFilePath.toUri().toString());
-          } else {
-            // hdfs based distributed cache file.
-            // Get the mapped HDFS path on simulated cluster
-            String mappedPath = mapDistCacheFilePath(files[i], timeStamps[i],
-                                                     visibility, user);
-            cacheFiles.add(mappedPath);
-          }
-        }
-        if (cacheFiles.size() > 0) {
-          // configure hdfs based distributed cache files for simulated job
-          conf.setStrings(MRJobConfig.CACHE_FILES,
-                          cacheFiles.toArray(new String[cacheFiles.size()]));
-        }
-        if (localCacheFiles.size() > 0) {
-          // configure local FS based distributed cache files for simulated job
-          conf.setStrings("tmpfiles", localCacheFiles.toArray(
-                                        new String[localCacheFiles.size()]));
-        }
-      }
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
deleted file mode 100644
index 2fcb39d..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Echos the UGI offered.
- */
-public class EchoUserResolver implements UserResolver {
-  public static final Log LOG = LogFactory.getLog(Gridmix.class);
-
-  public EchoUserResolver() {
-    LOG.info(" Current user resolver is EchoUserResolver ");
-  }
-
-  public synchronized boolean setTargetUsers(URI userdesc, Configuration conf)
-  throws IOException {
-    return false;
-  }
-
-  public synchronized UserGroupInformation getTargetUgi(
-    UserGroupInformation ugi) {
-    return ugi;
-  }
-
-  /**
-   * {@inheritDoc}
-   * <br><br>
-   * Since {@link EchoUserResolver} simply returns the user's name passed as
-   * the argument, it doesn't need a target list of users.
-   */
-  public boolean needsTargetUsersList() {
-    return false;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
deleted file mode 100644
index fc362c5..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
+++ /dev/null
@@ -1,312 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.commons.lang.time.FastDateFormat;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.MD5Hash;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- * Summarizes a {@link Gridmix} run. Statistics that are reported are
- * <ul>
- *   <li>Total number of jobs in the input trace</li>
- *   <li>Trace signature</li>
- *   <li>Total number of jobs processed from the input trace</li>
- *   <li>Total number of jobs submitted</li>
- *   <li>Total number of successful and failed jobs</li>
- *   <li>Total number of map/reduce tasks launched</li>
- *   <li>Gridmix start & end time</li>
- *   <li>Total time for the Gridmix run (data-generation and simulation)</li>
- *   <li>Gridmix Configuration (i.e job-type, submission-type, resolver)</li>
- * </ul>
- */
-class ExecutionSummarizer implements StatListener<JobStats> {
-  static final Log LOG = LogFactory.getLog(ExecutionSummarizer.class);
-  private static final FastDateFormat UTIL = FastDateFormat.getInstance();
-  
-  private int numJobsInInputTrace;
-  private int totalSuccessfulJobs;
-  private int totalFailedJobs;
-  private int totalMapTasksLaunched;
-  private int totalReduceTasksLaunched;
-  private long totalSimulationTime;
-  private long totalRuntime;
-  private final String commandLineArgs;
-  private long startTime;
-  private long endTime;
-  private long simulationStartTime;
-  private String inputTraceLocation;
-  private String inputTraceSignature;
-  private String jobSubmissionPolicy;
-  private String resolver;
-  private DataStatistics dataStats;
-  private String expectedDataSize;
-  
-  /**
-   * Basic constructor initialized with the runtime arguments. 
-   */
-  ExecutionSummarizer(String[] args) {
-    startTime = System.currentTimeMillis();
-    // flatten the args string and store it
-    commandLineArgs = 
-      org.apache.commons.lang.StringUtils.join(args, ' '); 
-  }
-  
-  /**
-   * Default constructor. 
-   */
-  ExecutionSummarizer() {
-    startTime = System.currentTimeMillis();
-    commandLineArgs = Summarizer.NA; 
-  }
-  
-  void start(Configuration conf) {
-    simulationStartTime = System.currentTimeMillis();
-  }
-  
-  private void processJobState(JobStats stats) throws Exception {
-    Job job = stats.getJob();
-    if (job.isSuccessful()) {
-      ++totalSuccessfulJobs;
-    } else {
-      ++totalFailedJobs;
-    }
-  }
-  
-  private void processJobTasks(JobStats stats) throws Exception {
-    totalMapTasksLaunched += stats.getNoOfMaps();
-    Job job = stats.getJob();
-    totalReduceTasksLaunched += job.getNumReduceTasks();
-  }
-  
-  private void process(JobStats stats) {
-    try {
-      // process the job run state
-      processJobState(stats);
-      
-      // process the tasks information
-      processJobTasks(stats);
-    } catch (Exception e) {
-      LOG.info("Error in processing job " + stats.getJob().getJobID() + ".");
-    }
-  }
-  
-  @Override
-  public void update(JobStats item) {
-    // process only if the simulation has started
-    if (simulationStartTime > 0) {
-      process(item);
-      totalSimulationTime = 
-        System.currentTimeMillis() - getSimulationStartTime();
-    }
-  }
-  
-  // Generates a signature for the trace file based on
-  //   - filename
-  //   - modification time
-  //   - file length
-  //   - owner
-  protected static String getTraceSignature(String input) throws IOException {
-    Path inputPath = new Path(input);
-    FileSystem fs = inputPath.getFileSystem(new Configuration());
-    FileStatus status = fs.getFileStatus(inputPath);
-    Path qPath = fs.makeQualified(status.getPath());
-    String traceID = status.getModificationTime() + qPath.toString()
-                     + status.getOwner() + status.getLen();
-    return MD5Hash.digest(traceID).toString();
-  }
-  
-  @SuppressWarnings("unchecked")
-  void finalize(JobFactory factory, String inputPath, long dataSize, 
-                UserResolver userResolver, DataStatistics stats,
-                Configuration conf) 
-  throws IOException {
-    numJobsInInputTrace = factory.numJobsInTrace;
-    endTime = System.currentTimeMillis();
-     if ("-".equals(inputPath)) {
-      inputTraceLocation = Summarizer.NA;
-      inputTraceSignature = Summarizer.NA;
-    } else {
-      Path inputTracePath = new Path(inputPath);
-      FileSystem fs = inputTracePath.getFileSystem(conf);
-      inputTraceLocation = fs.makeQualified(inputTracePath).toString();
-      inputTraceSignature = getTraceSignature(inputPath);
-    }
-    jobSubmissionPolicy = Gridmix.getJobSubmissionPolicy(conf).name();
-    resolver = userResolver.getClass().getName();
-    if (dataSize > 0) {
-      expectedDataSize = StringUtils.humanReadableInt(dataSize);
-    } else {
-      expectedDataSize = Summarizer.NA;
-    }
-    dataStats = stats;
-    totalRuntime = System.currentTimeMillis() - getStartTime();
-  }
-  
-  /**
-   * Summarizes the current {@link Gridmix} run.
-   */
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append("Execution Summary:-");
-    builder.append("\nInput trace: ").append(getInputTraceLocation());
-    builder.append("\nInput trace signature: ")
-           .append(getInputTraceSignature());
-    builder.append("\nTotal number of jobs in trace: ")
-           .append(getNumJobsInTrace());
-    builder.append("\nExpected input data size: ")
-           .append(getExpectedDataSize());
-    builder.append("\nInput data statistics: ")
-           .append(getInputDataStatistics());
-    builder.append("\nTotal number of jobs processed: ")
-           .append(getNumSubmittedJobs());
-    builder.append("\nTotal number of successful jobs: ")
-           .append(getNumSuccessfulJobs());
-    builder.append("\nTotal number of failed jobs: ")
-           .append(getNumFailedJobs());
-    builder.append("\nTotal number of map tasks launched: ")
-           .append(getNumMapTasksLaunched());
-    builder.append("\nTotal number of reduce task launched: ")
-           .append(getNumReduceTasksLaunched());
-    builder.append("\nGridmix start time: ")
-           .append(UTIL.format(getStartTime()));
-    builder.append("\nGridmix end time: ").append(UTIL.format(getEndTime()));
-    builder.append("\nGridmix simulation start time: ")
-           .append(UTIL.format(getStartTime()));
-    builder.append("\nGridmix runtime: ")
-           .append(StringUtils.formatTime(getRuntime()));
-    builder.append("\nTime spent in initialization (data-gen etc): ")
-           .append(StringUtils.formatTime(getInitTime()));
-    builder.append("\nTime spent in simulation: ")
-           .append(StringUtils.formatTime(getSimulationTime()));
-    builder.append("\nGridmix configuration parameters: ")
-           .append(getCommandLineArgsString());
-    builder.append("\nGridmix job submission policy: ")
-           .append(getJobSubmissionPolicy());
-    builder.append("\nGridmix resolver: ").append(getUserResolver());
-    builder.append("\n\n");
-    return builder.toString();
-  }
-  
-  // Gets the stringified version of DataStatistics
-  static String stringifyDataStatistics(DataStatistics stats) {
-    if (stats != null) {
-      StringBuffer buffer = new StringBuffer();
-      String compressionStatus = stats.isDataCompressed() 
-                                 ? "Compressed" 
-                                 : "Uncompressed";
-      buffer.append(compressionStatus).append(" input data size: ");
-      buffer.append(StringUtils.humanReadableInt(stats.getDataSize()));
-      buffer.append(", ");
-      buffer.append("Number of files: ").append(stats.getNumFiles());
-
-      return buffer.toString();
-    } else {
-      return Summarizer.NA;
-    }
-  }
-  
-  // Getters
-  protected String getExpectedDataSize() {
-    return expectedDataSize;
-  }
-  
-  protected String getUserResolver() {
-    return resolver;
-  }
-  
-  protected String getInputDataStatistics() {
-    return stringifyDataStatistics(dataStats);
-  }
-  
-  protected String getInputTraceSignature() {
-    return inputTraceSignature;
-  }
-  
-  protected String getInputTraceLocation() {
-    return inputTraceLocation;
-  }
-  
-  protected int getNumJobsInTrace() {
-    return numJobsInInputTrace;
-  }
-  
-  protected int getNumSuccessfulJobs() {
-    return totalSuccessfulJobs;
-  }
-  
-  protected int getNumFailedJobs() {
-    return totalFailedJobs;
-  }
-  
-  protected int getNumSubmittedJobs() {
-    return totalSuccessfulJobs + totalFailedJobs;
-  }
-  
-  protected int getNumMapTasksLaunched() {
-    return totalMapTasksLaunched;
-  }
-  
-  protected int getNumReduceTasksLaunched() {
-    return totalReduceTasksLaunched;
-  }
-  
-  protected long getStartTime() {
-    return startTime;
-  }
-  
-  protected long getEndTime() {
-    return endTime;
-  }
-  
-  protected long getInitTime() {
-    return simulationStartTime - startTime;
-  }
-  
-  protected long getSimulationStartTime() {
-    return simulationStartTime;
-  }
-  
-  protected long getSimulationTime() {
-    return totalSimulationTime;
-  }
-  
-  protected long getRuntime() {
-    return totalRuntime;
-  }
-  
-  protected String getCommandLineArgsString() {
-    return commandLineArgs;
-  }
-  
-  protected String getJobSubmissionPolicy() {
-    return jobSubmissionPolicy;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FilePool.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FilePool.java
deleted file mode 100644
index ba83bd9..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FilePool.java
+++ /dev/null
@@ -1,301 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Random;
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapred.gridmix.RandomAlgorithms.Selector;
-
-/**
- * Class for caching a pool of input data to be used by synthetic jobs for
- * simulating read traffic.
- */
-class FilePool {
-
-  public static final Log LOG = LogFactory.getLog(FilePool.class);
-
-  /**
-   * The minimum file size added to the pool. Default 128MiB.
-   */
-  public static final String GRIDMIX_MIN_FILE = "gridmix.min.file.size";
-
-  /**
-   * The maximum size for files added to the pool. Defualts to 100TiB.
-   */
-  public static final String GRIDMIX_MAX_TOTAL = "gridmix.max.total.scan";
-
-  private Node root;
-  private final Path path;
-  private final FileSystem fs;
-  private final Configuration conf;
-  private final ReadWriteLock updateLock;
-
-  /**
-   * Initialize a filepool under the path provided, but do not populate the
-   * cache.
-   */
-  public FilePool(Configuration conf, Path input) throws IOException {
-    root = null;
-    this.conf = conf;
-    this.path = input;
-    this.fs = path.getFileSystem(conf);
-    updateLock = new ReentrantReadWriteLock();
-  }
-
-  /**
-   * Gather a collection of files at least as large as minSize.
-   * @return The total size of files returned.
-   */
-  public long getInputFiles(long minSize, Collection<FileStatus> files)
-      throws IOException {
-    updateLock.readLock().lock();
-    try {
-      return root.selectFiles(minSize, files);
-    } finally {
-      updateLock.readLock().unlock();
-    }
-  }
-
-  /**
-   * (Re)generate cache of input FileStatus objects.
-   */
-  public void refresh() throws IOException {
-    updateLock.writeLock().lock();
-    try {
-      root = new InnerDesc(fs, fs.getFileStatus(path),
-        new MinFileFilter(conf.getLong(GRIDMIX_MIN_FILE, 128 * 1024 * 1024),
-                          conf.getLong(GRIDMIX_MAX_TOTAL, 100L * (1L << 40))));
-      if (0 == root.getSize()) {
-        throw new IOException("Found no satisfactory file in " + path);
-      }
-    } finally {
-      updateLock.writeLock().unlock();
-    }
-  }
-
-  /**
-   * Get a set of locations for the given file.
-   */
-  public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
-      throws IOException {
-    // TODO cache
-    return fs.getFileBlockLocations(stat, start, len);
-  }
-
-  static abstract class Node {
-
-    protected final static Random rand = new Random();
-
-    /**
-     * Total size of files and directories under the current node.
-     */
-    abstract long getSize();
-
-    /**
-     * Return a set of files whose cumulative size is at least
-     * <tt>targetSize</tt>.
-     * TODO Clearly size is not the only criterion, e.g. refresh from
-     * generated data without including running task output, tolerance
-     * for permission issues, etc.
-     */
-    abstract long selectFiles(long targetSize, Collection<FileStatus> files)
-        throws IOException;
-  }
-
-  /**
-   * Files in current directory of this Node.
-   */
-  static class LeafDesc extends Node {
-    final long size;
-    final ArrayList<FileStatus> curdir;
-
-    LeafDesc(ArrayList<FileStatus> curdir, long size) {
-      this.size = size;
-      this.curdir = curdir;
-    }
-
-    @Override
-    public long getSize() {
-      return size;
-    }
-
-    @Override
-    public long selectFiles(long targetSize, Collection<FileStatus> files)
-        throws IOException {
-      if (targetSize >= getSize()) {
-        files.addAll(curdir);
-        return getSize();
-      }
-
-      Selector selector = new Selector(curdir.size(), (double) targetSize
-          / getSize(), rand);
-      
-      ArrayList<Integer> selected = new ArrayList<Integer>();
-      long ret = 0L;
-      do {
-        int index = selector.next();
-        selected.add(index);
-        ret += curdir.get(index).getLen();
-      } while (ret < targetSize);
-
-      for (Integer i : selected) {
-        files.add(curdir.get(i));
-      }
-
-      return ret;
-    }
-  }
-
-  /**
-   * A subdirectory of the current Node.
-   */
-  static class InnerDesc extends Node {
-    final long size;
-    final double[] dist;
-    final Node[] subdir;
-
-    private static final Comparator<Node> nodeComparator =
-      new Comparator<Node>() {
-          public int compare(Node n1, Node n2) {
-            return n1.getSize() < n2.getSize() ? -1
-                 : n1.getSize() > n2.getSize() ? 1 : 0;
-          }
-    };
-
-    InnerDesc(final FileSystem fs, FileStatus thisDir, MinFileFilter filter)
-        throws IOException {
-      long fileSum = 0L;
-      final ArrayList<FileStatus> curFiles = new ArrayList<FileStatus>();
-      final ArrayList<FileStatus> curDirs = new ArrayList<FileStatus>();
-      for (FileStatus stat : fs.listStatus(thisDir.getPath())) {
-        if (stat.isDirectory()) {
-          curDirs.add(stat);
-        } else if (filter.accept(stat)) {
-          curFiles.add(stat);
-          fileSum += stat.getLen();
-        }
-      }
-      ArrayList<Node> subdirList = new ArrayList<Node>();
-      if (!curFiles.isEmpty()) {
-        subdirList.add(new LeafDesc(curFiles, fileSum));
-      }
-      for (Iterator<FileStatus> i = curDirs.iterator();
-          !filter.done() && i.hasNext();) {
-        // add subdirectories
-        final Node d = new InnerDesc(fs, i.next(), filter);
-        final long dSize = d.getSize();
-        if (dSize > 0) {
-          fileSum += dSize;
-          subdirList.add(d);
-        }
-      }
-      size = fileSum;
-      LOG.debug(size + " bytes in " + thisDir.getPath());
-      subdir = subdirList.toArray(new Node[subdirList.size()]);
-      Arrays.sort(subdir, nodeComparator);
-      dist = new double[subdir.length];
-      for (int i = dist.length - 1; i > 0; --i) {
-        fileSum -= subdir[i].getSize();
-        dist[i] = fileSum / (1.0 * size);
-      }
-    }
-
-    @Override
-    public long getSize() {
-      return size;
-    }
-
-    @Override
-    public long selectFiles(long targetSize, Collection<FileStatus> files)
-        throws IOException {
-      long ret = 0L;
-      if (targetSize >= getSize()) {
-        // request larger than all subdirs; add everything
-        for (Node n : subdir) {
-          long added = n.selectFiles(targetSize, files);
-          ret += added;
-          targetSize -= added;
-        }
-        return ret;
-      }
-
-      // can satisfy request in proper subset of contents
-      // select random set, weighted by size
-      final HashSet<Node> sub = new HashSet<Node>();
-      do {
-        assert sub.size() < subdir.length;
-        final double r = rand.nextDouble();
-        int pos = Math.abs(Arrays.binarySearch(dist, r) + 1) - 1;
-        while (sub.contains(subdir[pos])) {
-          pos = (pos + 1) % subdir.length;
-        }
-        long added = subdir[pos].selectFiles(targetSize, files);
-        ret += added;
-        targetSize -= added;
-        sub.add(subdir[pos]);
-      } while (targetSize > 0);
-      return ret;
-    }
-  }
-
-  /**
-   * Filter enforcing the minFile/maxTotal parameters of the scan.
-   */
-  private static class MinFileFilter {
-
-    private long totalScan;
-    private final long minFileSize;
-
-    public MinFileFilter(long minFileSize, long totalScan) {
-      this.minFileSize = minFileSize;
-      this.totalScan = totalScan;
-    }
-    public boolean done() {
-      return totalScan <= 0;
-    }
-    public boolean accept(FileStatus stat) {
-      final boolean done = done();
-      if (!done && stat.getLen() >= minFileSize) {
-        totalScan -= stat.getLen();
-        return true;
-      }
-      return false;
-    }
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FileQueue.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
deleted file mode 100644
index 2e4222c..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-/**
- * Given a {@link org.apache.hadoop.mapreduce.lib.input.CombineFileSplit},
- * circularly read through each input source.
- */
-class FileQueue extends InputStream {
-
-  private int idx = -1;
-  private long curlen = -1L;
-  private InputStream input;
-  private final byte[] z = new byte[1];
-  private final Path[] paths;
-  private final long[] lengths;
-  private final long[] startoffset;
-  private final Configuration conf;
-
-  /**
-   * @param split Description of input sources.
-   * @param conf Used to resolve FileSystem instances.
-   */
-  public FileQueue(CombineFileSplit split, Configuration conf)
-      throws IOException {
-    this.conf = conf;
-    paths = split.getPaths();
-    startoffset = split.getStartOffsets();
-    lengths = split.getLengths();
-    nextSource();
-  }
-
-  protected void nextSource() throws IOException {
-    if (0 == paths.length) {
-      return;
-    }
-    if (input != null) {
-      input.close();
-    }
-    idx = (idx + 1) % paths.length;
-    curlen = lengths[idx];
-    final Path file = paths[idx];
-    input = 
-      CompressionEmulationUtil.getPossiblyDecompressedInputStream(file, 
-                                 conf, startoffset[idx]);
-  }
-
-  @Override
-  public int read() throws IOException {
-    final int tmp = read(z);
-    return tmp == -1 ? -1 : (0xFF & z[0]);
-  }
-
-  @Override
-  public int read(byte[] b) throws IOException {
-    return read(b, 0, b.length);
-  }
-
-  @Override
-  public int read(byte[] b, int off, int len) throws IOException {
-    int kvread = 0;
-    while (kvread < len) {
-      if (curlen <= 0) {
-        nextSource();
-        continue;
-      }
-      final int srcRead = (int) Math.min(len - kvread, curlen);
-      IOUtils.readFully(input, b, kvread, srcRead);
-      curlen -= srcRead;
-      kvread += srcRead;
-    }
-    return kvread;
-  }
-
-  @Override
-  public void close() throws IOException {
-    input.close();
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
deleted file mode 100644
index 41e937f..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
+++ /dev/null
@@ -1,412 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.OutputStream;
-import java.security.PrivilegedExceptionAction;
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocatedFileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.fs.RemoteIterator;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.StringUtils;
-
-// TODO can replace with form of GridmixJob
-class GenerateData extends GridmixJob {
-
-  /**
-   * Total bytes to write.
-   */
-  public static final String GRIDMIX_GEN_BYTES = "gridmix.gen.bytes";
-
-  /**
-   * Maximum size per file written.
-   */
-  public static final String GRIDMIX_GEN_CHUNK = "gridmix.gen.bytes.per.file";
-
-  /**
-   * Size of writes to output file.
-   */
-  public static final String GRIDMIX_VAL_BYTES = "gendata.val.bytes";
-
-  /**
-   * Status reporting interval, in megabytes.
-   */
-  public static final String GRIDMIX_GEN_INTERVAL = "gendata.interval.mb";
-
-  /**
-   * Blocksize of generated data.
-   */
-  public static final String GRIDMIX_GEN_BLOCKSIZE = "gridmix.gen.blocksize";
-
-  /**
-   * Replication of generated data.
-   */
-  public static final String GRIDMIX_GEN_REPLICATION = "gridmix.gen.replicas";
-  static final String JOB_NAME = "GRIDMIX_GENERATE_INPUT_DATA";
-
-  public GenerateData(Configuration conf, Path outdir, long genbytes)
-      throws IOException {
-    super(conf, 0L, JOB_NAME);
-    job.getConfiguration().setLong(GRIDMIX_GEN_BYTES, genbytes);
-    FileOutputFormat.setOutputPath(job, outdir);
-  }
-
-  /**
-   * Represents the input data characteristics.
-   */
-  static class DataStatistics {
-    private long dataSize;
-    private long numFiles;
-    private boolean isDataCompressed;
-    
-    DataStatistics(long dataSize, long numFiles, boolean isCompressed) {
-      this.dataSize = dataSize;
-      this.numFiles = numFiles;
-      this.isDataCompressed = isCompressed;
-    }
-    
-    long getDataSize() {
-      return dataSize;
-    }
-    
-    long getNumFiles() {
-      return numFiles;
-    }
-    
-    boolean isDataCompressed() {
-      return isDataCompressed;
-    }
-  }
-  
-  /**
-   * Publish the data statistics.
-   */
-  static DataStatistics publishDataStatistics(Path inputDir, long genBytes, 
-                                              Configuration conf) 
-  throws IOException {
-    if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)) {
-      return CompressionEmulationUtil.publishCompressedDataStatistics(inputDir, 
-                                        conf, genBytes);
-    } else {
-      return publishPlainDataStatistics(conf, inputDir);
-    }
-  }
-  
-  static DataStatistics publishPlainDataStatistics(Configuration conf, 
-                                                   Path inputDir) 
-  throws IOException {
-    FileSystem fs = inputDir.getFileSystem(conf);
-
-    // obtain input data file statuses
-    long dataSize = 0;
-    long fileCount = 0;
-    RemoteIterator<LocatedFileStatus> iter = fs.listFiles(inputDir, true);
-    PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();
-    while (iter.hasNext()) {
-      LocatedFileStatus lStatus = iter.next();
-      if (filter.accept(lStatus.getPath())) {
-        dataSize += lStatus.getLen();
-        ++fileCount;
-      }
-    }
-
-    // publish the plain data statistics
-    LOG.info("Total size of input data : " 
-             + StringUtils.humanReadableInt(dataSize));
-    LOG.info("Total number of input data files : " + fileCount);
-    
-    return new DataStatistics(dataSize, fileCount, false);
-  }
-  
-  @Override
-  public Job call() throws IOException, InterruptedException,
-                           ClassNotFoundException {
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-    ugi.doAs( new PrivilegedExceptionAction <Job>() {
-       public Job run() throws IOException, ClassNotFoundException,
-                               InterruptedException {
-         // check if compression emulation is enabled
-         if (CompressionEmulationUtil
-             .isCompressionEmulationEnabled(job.getConfiguration())) {
-           CompressionEmulationUtil.configure(job);
-         } else {
-           configureRandomBytesDataGenerator();
-         }
-         job.submit();
-         return job;
-       }
-       
-       private void configureRandomBytesDataGenerator() {
-        job.setMapperClass(GenDataMapper.class);
-        job.setNumReduceTasks(0);
-        job.setMapOutputKeyClass(NullWritable.class);
-        job.setMapOutputValueClass(BytesWritable.class);
-        job.setInputFormatClass(GenDataFormat.class);
-        job.setOutputFormatClass(RawBytesOutputFormat.class);
-        job.setJarByClass(GenerateData.class);
-        try {
-          FileInputFormat.addInputPath(job, new Path("ignored"));
-        } catch (IOException e) {
-          LOG.error("Error while adding input path ", e);
-        }
-      }
-    });
-    return job;
-  }
-  
-  @Override
-  protected boolean canEmulateCompression() {
-    return false;
-  }
-
-  public static class GenDataMapper
-      extends Mapper<NullWritable,LongWritable,NullWritable,BytesWritable> {
-
-    private BytesWritable val;
-    private final Random r = new Random();
-
-    @Override
-    protected void setup(Context context)
-        throws IOException, InterruptedException {
-      val = new BytesWritable(new byte[
-          context.getConfiguration().getInt(GRIDMIX_VAL_BYTES, 1024 * 1024)]);
-    }
-
-    @Override
-    public void map(NullWritable key, LongWritable value, Context context)
-        throws IOException, InterruptedException {
-      for (long bytes = value.get(); bytes > 0; bytes -= val.getLength()) {
-        r.nextBytes(val.getBytes());
-        val.setSize((int)Math.min(val.getLength(), bytes));
-        context.write(key, val);
-      }
-    }
-
-  }
-
-  static class GenDataFormat extends InputFormat<NullWritable,LongWritable> {
-
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      final JobClient client =
-        new JobClient(new JobConf(jobCtxt.getConfiguration()));
-      ClusterStatus stat = client.getClusterStatus(true);
-      final long toGen =
-        jobCtxt.getConfiguration().getLong(GRIDMIX_GEN_BYTES, -1);
-      if (toGen < 0) {
-        throw new IOException("Invalid/missing generation bytes: " + toGen);
-      }
-      final int nTrackers = stat.getTaskTrackers();
-      final long bytesPerTracker = toGen / nTrackers;
-      final ArrayList<InputSplit> splits = new ArrayList<InputSplit>(nTrackers);
-      final Pattern trackerPattern = Pattern.compile("tracker_([^:]*):.*");
-      final Matcher m = trackerPattern.matcher("");
-      for (String tracker : stat.getActiveTrackerNames()) {
-        m.reset(tracker);
-        if (!m.find()) {
-          System.err.println("Skipping node: " + tracker);
-          continue;
-        }
-        final String name = m.group(1);
-        splits.add(new GenSplit(bytesPerTracker, new String[] { name }));
-      }
-      return splits;
-    }
-
-    @Override
-    public RecordReader<NullWritable,LongWritable> createRecordReader(
-        InputSplit split, final TaskAttemptContext taskContext)
-        throws IOException {
-      return new RecordReader<NullWritable,LongWritable>() {
-        long written = 0L;
-        long write = 0L;
-        long RINTERVAL;
-        long toWrite;
-        final NullWritable key = NullWritable.get();
-        final LongWritable val = new LongWritable();
-
-        @Override
-        public void initialize(InputSplit split, TaskAttemptContext ctxt)
-            throws IOException, InterruptedException {
-          toWrite = split.getLength();
-          RINTERVAL = ctxt.getConfiguration().getInt(
-              GRIDMIX_GEN_INTERVAL, 10) << 20;
-        }
-        @Override
-        public boolean nextKeyValue() throws IOException {
-          written += write;
-          write = Math.min(toWrite - written, RINTERVAL);
-          val.set(write);
-          return written < toWrite;
-        }
-        @Override
-        public float getProgress() throws IOException {
-          return written / ((float)toWrite);
-        }
-        @Override
-        public NullWritable getCurrentKey() { return key; }
-        @Override
-        public LongWritable getCurrentValue() { return val; }
-        @Override
-        public void close() throws IOException {
-          taskContext.setStatus("Wrote " + toWrite);
-        }
-      };
-    }
-  }
-
-  static class GenSplit extends InputSplit implements Writable {
-    private long bytes;
-    private int nLoc;
-    private String[] locations;
-
-    public GenSplit() { }
-    public GenSplit(long bytes, String[] locations) {
-      this(bytes, locations.length, locations);
-    }
-    public GenSplit(long bytes, int nLoc, String[] locations) {
-      this.bytes = bytes;
-      this.nLoc = nLoc;
-      this.locations = Arrays.copyOf(locations, nLoc);
-    }
-    @Override
-    public long getLength() {
-      return bytes;
-    }
-    @Override
-    public String[] getLocations() {
-      return locations;
-    }
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      bytes = in.readLong();
-      nLoc = in.readInt();
-      if (null == locations || locations.length < nLoc) {
-        locations = new String[nLoc];
-      }
-      for (int i = 0; i < nLoc; ++i) {
-        locations[i] = Text.readString(in);
-      }
-    }
-    @Override
-    public void write(DataOutput out) throws IOException {
-      out.writeLong(bytes);
-      out.writeInt(nLoc);
-      for (int i = 0; i < nLoc; ++i) {
-        Text.writeString(out, locations[i]);
-      }
-    }
-  }
-
-  static class RawBytesOutputFormat
-      extends FileOutputFormat<NullWritable,BytesWritable> {
-
-    @Override
-    public RecordWriter<NullWritable,BytesWritable> getRecordWriter(
-        TaskAttemptContext job) throws IOException {
-
-      return new ChunkWriter(getDefaultWorkFile(job, ""),
-          job.getConfiguration());
-    }
-
-    static class ChunkWriter extends RecordWriter<NullWritable,BytesWritable> {
-      private final Path outDir;
-      private final FileSystem fs;
-      private final int blocksize;
-      private final short replicas;
-      private final FsPermission genPerms = new FsPermission((short) 0777);
-      private final long maxFileBytes;
-
-      private long accFileBytes = 0L;
-      private long fileIdx = -1L;
-      private OutputStream fileOut = null;
-
-      public ChunkWriter(Path outDir, Configuration conf) throws IOException {
-        this.outDir = outDir;
-        fs = outDir.getFileSystem(conf);
-        blocksize = conf.getInt(GRIDMIX_GEN_BLOCKSIZE, 1 << 28);
-        replicas = (short) conf.getInt(GRIDMIX_GEN_REPLICATION, 3);
-        maxFileBytes = conf.getLong(GRIDMIX_GEN_CHUNK, 1L << 30);
-        nextDestination();
-      }
-      private void nextDestination() throws IOException {
-        if (fileOut != null) {
-          fileOut.close();
-        }
-        fileOut = fs.create(new Path(outDir, "segment-" + (++fileIdx)),
-                            genPerms, false, 64 * 1024, replicas, 
-                            blocksize, null);
-        accFileBytes = 0L;
-      }
-      @Override
-      public void write(NullWritable key, BytesWritable value)
-          throws IOException {
-        int written = 0;
-        final int total = value.getLength();
-        while (written < total) {
-          if (accFileBytes >= maxFileBytes) {
-            nextDestination();
-          }
-          final int write = (int)
-            Math.min(total - written, maxFileBytes - accFileBytes);
-          fileOut.write(value.getBytes(), written, write);
-          written += write;
-          accFileBytes += write;
-        }
-      }
-      @Override
-      public void close(TaskAttemptContext ctxt) throws IOException {
-        fileOut.close();
-      }
-    }
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
deleted file mode 100644
index c90e17c..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
+++ /dev/null
@@ -1,259 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * GridmixJob that generates distributed cache files.
- * {@link GenerateDistCacheData} expects a list of distributed cache files to be
- * generated as input. This list is expected to be stored as a sequence file
- * and the filename is expected to be configured using
- * {@code gridmix.distcache.file.list}.
- * This input file contains the list of distributed cache files and their sizes.
- * For each record (i.e. file size and file path) in this input file,
- * a file with the specific file size at the specific path is created.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-class GenerateDistCacheData extends GridmixJob {
-
-  /**
-   * Number of distributed cache files to be created by gridmix
-   */
-  static final String GRIDMIX_DISTCACHE_FILE_COUNT =
-      "gridmix.distcache.file.count";
-  /**
-   * Total number of bytes to be written to the distributed cache files by
-   * gridmix. i.e. Sum of sizes of all unique distributed cache files to be
-   * created by gridmix.
-   */
-  static final String GRIDMIX_DISTCACHE_BYTE_COUNT =
-      "gridmix.distcache.byte.count";
-  /**
-   * The special file created(and used) by gridmix, that contains the list of
-   * unique distributed cache files that are to be created and their sizes.
-   */
-  static final String GRIDMIX_DISTCACHE_FILE_LIST =
-      "gridmix.distcache.file.list";
-  static final String JOB_NAME = "GRIDMIX_GENERATE_DISTCACHE_DATA";
-
-  public GenerateDistCacheData(Configuration conf) throws IOException {
-    super(conf, 0L, JOB_NAME);
-  }
-
-  @Override
-  public Job call() throws IOException, InterruptedException,
-                           ClassNotFoundException {
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-    ugi.doAs( new PrivilegedExceptionAction <Job>() {
-       public Job run() throws IOException, ClassNotFoundException,
-                               InterruptedException {
-        job.setMapperClass(GenDCDataMapper.class);
-        job.setNumReduceTasks(0);
-        job.setMapOutputKeyClass(NullWritable.class);
-        job.setMapOutputValueClass(BytesWritable.class);
-        job.setInputFormatClass(GenDCDataFormat.class);
-        job.setOutputFormatClass(NullOutputFormat.class);
-        job.setJarByClass(GenerateDistCacheData.class);
-        try {
-          FileInputFormat.addInputPath(job, new Path("ignored"));
-        } catch (IOException e) {
-          LOG.error("Error while adding input path ", e);
-        }
-        job.submit();
-        return job;
-      }
-    });
-    return job;
-  }
-
-  @Override
-  protected boolean canEmulateCompression() {
-    return false;
-  }
-
-  public static class GenDCDataMapper
-      extends Mapper<LongWritable, BytesWritable, NullWritable, BytesWritable> {
-
-    private BytesWritable val;
-    private final Random r = new Random();
-    private FileSystem fs;
-
-    @Override
-    protected void setup(Context context)
-        throws IOException, InterruptedException {
-      val = new BytesWritable(new byte[context.getConfiguration().getInt(
-              GenerateData.GRIDMIX_VAL_BYTES, 1024 * 1024)]);
-      fs = FileSystem.get(context.getConfiguration());
-    }
-
-    // Create one distributed cache file with the needed file size.
-    // key is distributed cache file size and
-    // value is distributed cache file path.
-    @Override
-    public void map(LongWritable key, BytesWritable value, Context context)
-        throws IOException, InterruptedException {
-
-      String fileName = new String(value.getBytes(), 0, value.getLength());
-      Path path = new Path(fileName);
-
-      /**
-       * Create distributed cache file with the permissions 0755.
-       * Since the private distributed cache directory doesn't have execute
-       * permission for others, it is OK to set read permission for others for
-       * the files under that directory and still they will become 'private'
-       * distributed cache files on the simulated cluster.
-       */
-      FSDataOutputStream dos =
-          FileSystem.create(fs, path, new FsPermission((short)0755));
-
-      for (long bytes = key.get(); bytes > 0; bytes -= val.getLength()) {
-        r.nextBytes(val.getBytes());
-        val.setSize((int)Math.min(val.getLength(), bytes));
-        dos.write(val.getBytes(), 0, val.getLength());// Write to distCache file
-      }
-      dos.close();
-    }
-  }
-
-  /**
-   * InputFormat for GenerateDistCacheData.
-   * Input to GenerateDistCacheData is the special file(in SequenceFile format)
-   * that contains the list of distributed cache files to be generated along
-   * with their file sizes.
-   */
-  static class GenDCDataFormat
-      extends InputFormat<LongWritable, BytesWritable> {
-
-    // Split the special file that contains the list of distributed cache file
-    // paths and their file sizes such that each split corresponds to
-    // approximately same amount of distributed cache data to be generated.
-    // Consider numTaskTrackers * numMapSlotsPerTracker as the number of maps
-    // for this job, if there is lot of data to be generated.
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      final JobConf jobConf = new JobConf(jobCtxt.getConfiguration());
-      final JobClient client = new JobClient(jobConf);
-      ClusterStatus stat = client.getClusterStatus(true);
-      int numTrackers = stat.getTaskTrackers();
-      final int fileCount = jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);
-
-      // Total size of distributed cache files to be generated
-      final long totalSize = jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);
-      // Get the path of the special file
-      String distCacheFileList = jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);
-      if (fileCount < 0 || totalSize < 0 || distCacheFileList == null) {
-        throw new RuntimeException("Invalid metadata: #files (" + fileCount
-            + "), total_size (" + totalSize + "), filelisturi ("
-            + distCacheFileList + ")");
-      }
-
-      Path sequenceFile = new Path(distCacheFileList);
-      FileSystem fs = sequenceFile.getFileSystem(jobConf);
-      FileStatus srcst = fs.getFileStatus(sequenceFile);
-      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.
-      int numMapSlotsPerTracker = jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);
-      int numSplits = numTrackers * numMapSlotsPerTracker;
-
-      List<InputSplit> splits = new ArrayList<InputSplit>(numSplits);
-      LongWritable key = new LongWritable();
-      BytesWritable value = new BytesWritable();
-
-      // Average size of data to be generated by each map task
-      final long targetSize = Math.max(totalSize / numSplits,
-                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);
-      long splitStartPosition = 0L;
-      long splitEndPosition = 0L;
-      long acc = 0L;
-      long bytesRemaining = srcst.getLen();
-      SequenceFile.Reader reader = null;
-      try {
-        reader = new SequenceFile.Reader(fs, sequenceFile, jobConf);
-        while (reader.next(key, value)) {
-
-          // If adding this file would put this split past the target size,
-          // cut the last split and put this file in the next split.
-          if (acc + key.get() > targetSize && acc != 0) {
-            long splitSize = splitEndPosition - splitStartPosition;
-            splits.add(new FileSplit(
-                sequenceFile, splitStartPosition, splitSize, (String[])null));
-            bytesRemaining -= splitSize;
-            splitStartPosition = splitEndPosition;
-            acc = 0L;
-          }
-          acc += key.get();
-          splitEndPosition = reader.getPosition();
-        }
-      } finally {
-        if (reader != null) {
-          reader.close();
-        }
-      }
-      if (bytesRemaining != 0) {
-        splits.add(new FileSplit(
-            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));
-      }
-
-      return splits;
-    }
-
-    /**
-     * Returns a reader for this split of the distributed cache file list.
-     */
-    @Override
-    public RecordReader<LongWritable, BytesWritable> createRecordReader(
-        InputSplit split, final TaskAttemptContext taskContext)
-        throws IOException, InterruptedException {
-      return new SequenceFileRecordReader<LongWritable, BytesWritable>();
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
deleted file mode 100644
index d8f3854..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
+++ /dev/null
@@ -1,717 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.PrintStream;
-import java.net.URI;
-import java.security.PrivilegedExceptionAction;
-import java.util.List;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FsShell;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.ZombieJobProducer;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Driver class for the Gridmix3 benchmark. Gridmix accepts a timestamped
- * stream (trace) of job/task descriptions. For each job in the trace, the
- * client will submit a corresponding, synthetic job to the target cluster at
- * the rate in the original trace. The intent is to provide a benchmark that
- * can be configured and extended to closely match the measured resource
- * profile of actual, production loads.
- */
-public class Gridmix extends Configured implements Tool {
-
-  public static final Log LOG = LogFactory.getLog(Gridmix.class);
-
-  /**
-   * Output (scratch) directory for submitted jobs. Relative paths are
-   * resolved against the path provided as input and absolute paths remain
-   * independent of it. The default is &quot;gridmix&quot;.
-   */
-  public static final String GRIDMIX_OUT_DIR = "gridmix.output.directory";
-
-  /**
-   * Number of submitting threads at the client and upper bound for
-   * in-memory split data. Submitting threads precompute InputSplits for
-   * submitted jobs. This limits the number of splits held in memory waiting
-   * for submission and also permits parallel computation of split data.
-   */
-  public static final String GRIDMIX_SUB_THR = "gridmix.client.submit.threads";
-
-  /**
-   * The depth of the queue of job descriptions. Before splits are computed,
-   * a queue of pending descriptions is stored in memoory. This parameter
-   * limits the depth of that queue.
-   */
-  public static final String GRIDMIX_QUE_DEP =
-    "gridmix.client.pending.queue.depth";
-
-  /**
-   * Multiplier to accelerate or decelerate job submission. As a crude means of
-   * sizing a job trace to a cluster, the time separating two jobs is
-   * multiplied by this factor.
-   */
-  public static final String GRIDMIX_SUB_MUL = "gridmix.submit.multiplier";
-
-  /**
-   * Class used to resolve users in the trace to the list of target users
-   * on the cluster.
-   */
-  public static final String GRIDMIX_USR_RSV = "gridmix.user.resolve.class";
-
-  /**
-   * Configuration property set in simulated job's configuration whose value is
-   * set to the corresponding original job's name. This is not configurable by
-   * gridmix user.
-   */
-  public static final String ORIGINAL_JOB_NAME =
-      "gridmix.job.original-job-name";
-  /**
-   * Configuration property set in simulated job's configuration whose value is
-   * set to the corresponding original job's id. This is not configurable by
-   * gridmix user.
-   */
-  public static final String ORIGINAL_JOB_ID = "gridmix.job.original-job-id";
-
-  private DistributedCacheEmulator distCacheEmulator;
-
-  // Submit data structures
-  private JobFactory factory;
-  private JobSubmitter submitter;
-  private JobMonitor monitor;
-  private Statistics statistics;
-  private Summarizer summarizer;
-
-  // Shutdown hook
-  private final Shutdown sdh = new Shutdown();
-
-  Gridmix(String[] args) {
-    summarizer = new Summarizer(args);
-  }
-  
-  public Gridmix() {
-    summarizer = new Summarizer();
-  }
-  
-  // Get the input data directory for Gridmix. Input directory is 
-  // <io-path>/input
-  static Path getGridmixInputDataPath(Path ioPath) {
-    return new Path(ioPath, "input");
-  }
-  
-  /**
-   * Write random bytes at the path &lt;inputDir&gt;.
-   * @see org.apache.hadoop.mapred.gridmix.GenerateData
-   */
-  protected void writeInputData(long genbytes, Path inputDir)
-      throws IOException, InterruptedException {
-    final Configuration conf = getConf();
-    
-    // configure the compression ratio if needed
-    CompressionEmulationUtil.setupDataGeneratorConfig(conf);
-    
-    final GenerateData genData = new GenerateData(conf, inputDir, genbytes);
-    LOG.info("Generating " + StringUtils.humanReadableInt(genbytes) +
-        " of test data...");
-    launchGridmixJob(genData);
-    
-    FsShell shell = new FsShell(conf);
-    try {
-      LOG.info("Changing the permissions for inputPath " + inputDir.toString());
-      shell.run(new String[] {"-chmod","-R","777", inputDir.toString()});
-    } catch (Exception e) {
-      LOG.error("Couldnt change the file permissions " , e);
-      throw new IOException(e);
-    }
-    
-    LOG.info("Input data generation successful.");
-  }
-
-  /**
-   * Write random bytes in the distributed cache files that will be used by all
-   * simulated jobs of current gridmix run, if files are to be generated.
-   * Do this as part of the MapReduce job {@link GenerateDistCacheData#JOB_NAME}
-   * @see org.apache.hadoop.mapred.gridmix.GenerateDistCacheData
-   */
-  protected void writeDistCacheData(Configuration conf)
-      throws IOException, InterruptedException {
-    int fileCount =
-        conf.getInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, -1);
-    if (fileCount > 0) {// generate distributed cache files
-      final GridmixJob genDistCacheData = new GenerateDistCacheData(conf);
-      LOG.info("Generating distributed cache data of size " + conf.getLong(
-          GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1));
-      launchGridmixJob(genDistCacheData);
-    }
-  }
-
-  // Launch Input/DistCache Data Generation job and wait for completion
-  void launchGridmixJob(GridmixJob job)
-      throws IOException, InterruptedException {
-    submitter.add(job);
-
-    // TODO add listeners, use for job dependencies
-    TimeUnit.SECONDS.sleep(10);
-    try {
-      job.getJob().waitForCompletion(false);
-    } catch (ClassNotFoundException e) {
-      throw new IOException("Internal error", e);
-    }
-    if (!job.getJob().isSuccessful()) {
-      throw new IOException(job.getJob().getJobName() + " job failed!");
-    }
-  }
-
-  /**
-   * Create an appropriate {@code JobStoryProducer} object for the
-   * given trace.
-   * 
-   * @param traceIn the path to the trace file. The special path
-   * &quot;-&quot; denotes the standard input stream.
-   *
-   * @param conf the configuration to be used.
-   *
-   * @throws IOException if there was an error.
-   */
-  protected JobStoryProducer createJobStoryProducer(String traceIn,
-      Configuration conf) throws IOException {
-    if ("-".equals(traceIn)) {
-      return new ZombieJobProducer(System.in, null);
-    }
-    return new ZombieJobProducer(new Path(traceIn), null, conf);
-  }
-
-  // get the gridmix job submission policy
-  protected static GridmixJobSubmissionPolicy getJobSubmissionPolicy(
-                                                Configuration conf) {
-    return GridmixJobSubmissionPolicy.getPolicy(conf, 
-                                        GridmixJobSubmissionPolicy.STRESS);
-  }
-  
-  /**
-   * Create each component in the pipeline and start it.
-   * @param conf Configuration data, no keys specific to this context
-   * @param traceIn Either a Path to the trace data or &quot;-&quot; for
-   *                stdin
-   * @param ioPath &lt;ioPath&gt;/input/ is the dir from which input data is
-   *               read and &lt;ioPath&gt;/distributedCache/ is the gridmix
-   *               distributed cache directory.
-   * @param scratchDir Path into which job output is written
-   * @param startFlag Semaphore for starting job trace pipeline
-   */
-  private void startThreads(Configuration conf, String traceIn, Path ioPath,
-      Path scratchDir, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-    try {
-      Path inputDir = getGridmixInputDataPath(ioPath);
-      GridmixJobSubmissionPolicy policy = getJobSubmissionPolicy(conf);
-      LOG.info(" Submission policy is " + policy.name());
-      statistics = new Statistics(conf, policy.getPollingInterval(), startFlag);
-      monitor = createJobMonitor(statistics);
-      int noOfSubmitterThreads = 
-        (policy == GridmixJobSubmissionPolicy.SERIAL) 
-        ? 1
-        : Runtime.getRuntime().availableProcessors() + 1;
-
-      int numThreads = conf.getInt(GRIDMIX_SUB_THR, noOfSubmitterThreads);
-      int queueDep = conf.getInt(GRIDMIX_QUE_DEP, 5);
-      submitter = createJobSubmitter(monitor, numThreads, queueDep,
-                                     new FilePool(conf, inputDir), userResolver, 
-                                     statistics);
-      distCacheEmulator = new DistributedCacheEmulator(conf, ioPath);
-
-      factory = createJobFactory(submitter, traceIn, scratchDir, conf, 
-                                 startFlag, userResolver);
-      factory.jobCreator.setDistCacheEmulator(distCacheEmulator);
-
-      if (policy == GridmixJobSubmissionPolicy.SERIAL) {
-        statistics.addJobStatsListeners(factory);
-      } else {
-        statistics.addClusterStatsObservers(factory);
-      }
-
-      // add the gridmix run summarizer to the statistics
-      statistics.addJobStatsListeners(summarizer.getExecutionSummarizer());
-      statistics.addClusterStatsObservers(summarizer.getClusterSummarizer());
-      
-      monitor.start();
-      submitter.start();
-    }catch(Exception e) {
-      LOG.error(" Exception at start " ,e);
-      throw new IOException(e);
-    }
-   }
-
-  protected JobMonitor createJobMonitor(Statistics stats) throws IOException {
-    return new JobMonitor(stats);
-  }
-
-  protected JobSubmitter createJobSubmitter(JobMonitor monitor, int threads,
-      int queueDepth, FilePool pool, UserResolver resolver, 
-      Statistics statistics) throws IOException {
-    return new JobSubmitter(monitor, threads, queueDepth, pool, statistics);
-  }
-
-  protected JobFactory createJobFactory(JobSubmitter submitter, String traceIn,
-      Path scratchDir, Configuration conf, CountDownLatch startFlag, 
-      UserResolver resolver)
-      throws IOException {
-     return GridmixJobSubmissionPolicy.getPolicy(
-       conf, GridmixJobSubmissionPolicy.STRESS).createJobFactory(
-       submitter, createJobStoryProducer(traceIn, conf), scratchDir, conf,
-       startFlag, resolver);
-  }
-
-  private static UserResolver userResolver;
-
-  public UserResolver getCurrentUserResolver() {
-    return userResolver;
-  }
-  
-  public int run(final String[] argv) throws IOException, InterruptedException {
-    int val = -1;
-    final Configuration conf = getConf();
-    UserGroupInformation.setConfiguration(conf);
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-
-    val = ugi.doAs(new PrivilegedExceptionAction<Integer>() {
-      public Integer run() throws Exception {
-        return runJob(conf, argv);
-      }
-    });
-    
-    // print the gridmix summary if the run was successful
-    if (val == 0) {
-        // print the run summary
-        System.out.print("\n\n");
-        System.out.println(summarizer.toString());
-    }
-    
-    return val; 
-  }
-
-  private int runJob(Configuration conf, String[] argv)
-    throws IOException, InterruptedException {
-    if (argv.length < 2) {
-      printUsage(System.err);
-      return 1;
-    }
-    
-    // Should gridmix generate distributed cache data ?
-    boolean generate = false;
-    long genbytes = -1L;
-    String traceIn = null;
-    Path ioPath = null;
-    URI userRsrc = null;
-    userResolver = ReflectionUtils.newInstance(
-                     conf.getClass(GRIDMIX_USR_RSV, 
-                       SubmitterUserResolver.class,
-                       UserResolver.class), 
-                     conf);
-    try {
-      for (int i = 0; i < argv.length - 2; ++i) {
-        if ("-generate".equals(argv[i])) {
-          genbytes = StringUtils.TraditionalBinaryPrefix.string2long(argv[++i]);
-          generate = true;
-        } else if ("-users".equals(argv[i])) {
-          userRsrc = new URI(argv[++i]);
-        } else {
-          printUsage(System.err);
-          return 1;
-        }
-      }
-
-      if (userResolver.needsTargetUsersList()) {
-        if (userRsrc != null) {
-          if (!userResolver.setTargetUsers(userRsrc, conf)) {
-            LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
-          }
-        } else {
-          System.err.println("\n\n" + userResolver.getClass()
-              + " needs target user list. Use -users option." + "\n\n");
-          printUsage(System.err);
-          return 1;
-        }
-      } else if (userRsrc != null) {
-        LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
-      }
-
-      ioPath = new Path(argv[argv.length - 2]);
-      traceIn = argv[argv.length - 1];
-    } catch (Exception e) {
-      e.printStackTrace();
-      printUsage(System.err);
-      return 1;
-    }
-    return start(conf, traceIn, ioPath, genbytes, userResolver, generate);
-  }
-
-  /**
-   * 
-   * @param conf gridmix configuration
-   * @param traceIn trace file path(if it is '-', then trace comes from the
-   *                stream stdin)
-   * @param ioPath Working directory for gridmix. GenerateData job
-   *               will generate data in the directory &lt;ioPath&gt;/input/ and
-   *               distributed cache data is generated in the directory
-   *               &lt;ioPath&gt;/distributedCache/, if -generate option is
-   *               specified.
-   * @param genbytes size of input data to be generated under the directory
-   *                 &lt;ioPath&gt;/input/
-   * @param userResolver gridmix user resolver
-   * @param generate true if -generate option was specified
-   * @return exit code
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  int start(Configuration conf, String traceIn, Path ioPath, long genbytes,
-      UserResolver userResolver, boolean generate)
-      throws IOException, InterruptedException {
-    DataStatistics stats = null;
-    InputStream trace = null;
-    ioPath = ioPath.makeQualified(ioPath.getFileSystem(conf));
-
-    try {
-      Path scratchDir = new Path(ioPath, conf.get(GRIDMIX_OUT_DIR, "gridmix"));
-
-      // add shutdown hook for SIGINT, etc.
-      Runtime.getRuntime().addShutdownHook(sdh);
-      CountDownLatch startFlag = new CountDownLatch(1);
-      try {
-        // Create, start job submission threads
-        startThreads(conf, traceIn, ioPath, scratchDir, startFlag,
-                     userResolver);
-        
-        Path inputDir = getGridmixInputDataPath(ioPath);
-        
-        // Write input data if specified
-        if (genbytes > 0) {
-          writeInputData(genbytes, inputDir);
-        }
-        
-        // publish the data statistics
-        stats = GenerateData.publishDataStatistics(inputDir, genbytes, conf);
-        
-        // scan input dir contents
-        submitter.refreshFilePool();
-
-        // set up the needed things for emulation of various loads
-        int exitCode = setupEmulation(conf, traceIn, scratchDir, ioPath,
-                                      generate);
-        if (exitCode != 0) {
-          return exitCode;
-        }
-
-        // start the summarizer
-        summarizer.start(conf);
-        
-        factory.start();
-        statistics.start();
-      } catch (Throwable e) {
-        LOG.error("Startup failed", e);
-        if (factory != null) factory.abort(); // abort pipeline
-      } finally {
-        // signal for factory to start; sets start time
-        startFlag.countDown();
-      }
-      if (factory != null) {
-        // wait for input exhaustion
-        factory.join(Long.MAX_VALUE);
-        final Throwable badTraceException = factory.error();
-        if (null != badTraceException) {
-          LOG.error("Error in trace", badTraceException);
-          throw new IOException("Error in trace", badTraceException);
-        }
-        // wait for pending tasks to be submitted
-        submitter.shutdown();
-        submitter.join(Long.MAX_VALUE);
-        // wait for running tasks to complete
-        monitor.shutdown();
-        monitor.join(Long.MAX_VALUE);
-
-        statistics.shutdown();
-        statistics.join(Long.MAX_VALUE);
-
-      }
-    } finally {
-      if (factory != null) {
-        summarizer.finalize(factory, traceIn, genbytes, userResolver, stats, 
-                            conf);
-      }
-      IOUtils.cleanup(LOG, trace);
-    }
-    return 0;
-  }
-
-  /**
-   * Create gridmix output directory. Setup things for emulation of
-   * various loads, if needed.
-   * @param conf gridmix configuration
-   * @param traceIn trace file path(if it is '-', then trace comes from the
-   *                stream stdin)
-   * @param scratchDir gridmix output directory
-   * @param ioPath Working directory for gridmix.
-   * @param generate true if -generate option was specified
-   * @return exit code
-   * @throws IOException
-   * @throws InterruptedException 
-   */
-  private int setupEmulation(Configuration conf, String traceIn,
-      Path scratchDir, Path ioPath, boolean generate)
-      throws IOException, InterruptedException {
-    // create scratch directory(output directory of gridmix)
-    final FileSystem scratchFs = scratchDir.getFileSystem(conf);
-    FileSystem.mkdirs(scratchFs, scratchDir, new FsPermission((short) 0777));
-
-    // Setup things needed for emulation of distributed cache load
-    return setupDistCacheEmulation(conf, traceIn, ioPath, generate);
-    // Setup emulation of other loads like CPU load, Memory load
-  }
-
-  /**
-   * Setup gridmix for emulation of distributed cache load. This includes
-   * generation of distributed cache files, if needed.
-   * @param conf gridmix configuration
-   * @param traceIn trace file path(if it is '-', then trace comes from the
-   *                stream stdin)
-   * @param ioPath &lt;ioPath&gt;/input/ is the dir where input data (a) exists
-   *               or (b) is generated. &lt;ioPath&gt;/distributedCache/ is the
-   *               folder where distributed cache data (a) exists or (b) is to be
-   *               generated by gridmix.
-   * @param generate true if -generate option was specified
-   * @return exit code
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  private int setupDistCacheEmulation(Configuration conf, String traceIn,
-      Path ioPath, boolean generate) throws IOException, InterruptedException {
-    distCacheEmulator.init(traceIn, factory.jobCreator, generate);
-    int exitCode = 0;
-    if (distCacheEmulator.shouldGenerateDistCacheData() ||
-        distCacheEmulator.shouldEmulateDistCacheLoad()) {
-
-      JobStoryProducer jsp = createJobStoryProducer(traceIn, conf);
-      exitCode = distCacheEmulator.setupGenerateDistCacheData(jsp);
-      if (exitCode == 0) {
-        // If there are files to be generated, run a MapReduce job to generate
-        // these distributed cache files of all the simulated jobs of this trace.
-        writeDistCacheData(conf);
-      }
-    }
-    return exitCode;
-  }
-
-  /**
-   * Handles orderly shutdown by requesting that each component in the
-   * pipeline abort its progress, waiting for each to exit and killing
-   * any jobs still running on the cluster.
-   */
-  class Shutdown extends Thread {
-
-    static final long FAC_SLEEP = 1000;
-    static final long SUB_SLEEP = 4000;
-    static final long MON_SLEEP = 15000;
-
-    private void killComponent(Component<?> component, long maxwait) {
-      if (component == null) {
-        return;
-      }
-      component.abort();
-      try {
-        component.join(maxwait);
-      } catch (InterruptedException e) {
-        LOG.warn("Interrupted waiting for " + component);
-      }
-
-    }
-
-    @Override
-    public void run() {
-      LOG.info("Exiting...");
-      try {
-        killComponent(factory, FAC_SLEEP);   // read no more tasks
-        killComponent(submitter, SUB_SLEEP); // submit no more tasks
-        killComponent(monitor, MON_SLEEP);   // process remaining jobs here
-        killComponent(statistics,MON_SLEEP);
-      } finally {
-        if (monitor == null) {
-          return;
-        }
-        List<Job> remainingJobs = monitor.getRemainingJobs();
-        if (remainingJobs.isEmpty()) {
-          return;
-        }
-        LOG.info("Killing running jobs...");
-        for (Job job : remainingJobs) {
-          try {
-            if (!job.isComplete()) {
-              job.killJob();
-              LOG.info("Killed " + job.getJobName() + " (" + job.getJobID() + ")");
-            } else {
-              if (job.isSuccessful()) {
-                monitor.onSuccess(job);
-              } else {
-                monitor.onFailure(job);
-              }
-            }
-          } catch (IOException e) {
-            LOG.warn("Failure killing " + job.getJobName(), e);
-          } catch (Exception e) {
-            LOG.error("Unexcpected exception", e);
-          }
-        }
-        LOG.info("Done.");
-      }
-    }
-
-  }
-
-  public static void main(String[] argv) throws Exception {
-    int res = -1;
-    try {
-      res = ToolRunner.run(new Configuration(), new Gridmix(argv), argv);
-    } finally {
-      System.exit(res);
-    }
-  }
-
-  private String getEnumValues(Enum<?>[] e) {
-    StringBuilder sb = new StringBuilder();
-    String sep = "";
-    for (Enum<?> v : e) {
-      sb.append(sep);
-      sb.append(v.name());
-      sep = "|";
-    }
-    return sb.toString();
-  }
-  
-  private String getJobTypes() {
-    return getEnumValues(JobCreator.values());
-  }
-  
-  private String getSubmissionPolicies() {
-    return getEnumValues(GridmixJobSubmissionPolicy.values());
-  }
-  
-  protected void printUsage(PrintStream out) {
-    ToolRunner.printGenericCommandUsage(out);
-    out.println("Usage: gridmix [-generate <MiB>] [-users URI] [-Dname=value ...] <iopath> <trace>");
-    out.println("  e.g. gridmix -generate 100m foo -");
-    out.println("Options:");
-    out.println("   -generate <MiB> : Generate input data of size MiB under "
-        + "<iopath>/input/ and generate\n\t\t     distributed cache data under "
-        + "<iopath>/distributedCache/.");
-    out.println("   -users <usersResourceURI> : URI that contains the users list.");
-    out.println("Configuration parameters:");
-    out.println("   General parameters:");
-    out.printf("       %-48s : Output directory\n", GRIDMIX_OUT_DIR);
-    out.printf("       %-48s : Submitting threads\n", GRIDMIX_SUB_THR);
-    out.printf("       %-48s : Queued job desc\n", GRIDMIX_QUE_DEP);
-    out.printf("       %-48s : User resolution class\n", GRIDMIX_USR_RSV);
-    out.printf("       %-48s : Job types (%s)\n", JobCreator.GRIDMIX_JOB_TYPE, getJobTypes());
-    out.println("   Parameters related to job submission:");    
-    out.printf("       %-48s : Default queue\n",
-        GridmixJob.GRIDMIX_DEFAULT_QUEUE);
-    out.printf("       %-48s : Enable/disable using queues in trace\n",
-        GridmixJob.GRIDMIX_USE_QUEUE_IN_TRACE);
-    out.printf("       %-48s : Job submission policy (%s)\n",
-        GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, getSubmissionPolicies());
-    out.println("   Parameters specific for LOADJOB:");
-    out.printf("       %-48s : Key fraction of rec\n",
-        AvgRecordFactory.GRIDMIX_KEY_FRC);
-    out.println("   Parameters specific for SLEEPJOB:");
-    out.printf("       %-48s : Whether to ignore reduce tasks\n",
-        SleepJob.SLEEPJOB_MAPTASK_ONLY);
-    out.printf("       %-48s : Number of fake locations for map tasks\n",
-        JobCreator.SLEEPJOB_RANDOM_LOCATIONS);
-    out.printf("       %-48s : Maximum map task runtime in mili-sec\n",
-        SleepJob.GRIDMIX_SLEEP_MAX_MAP_TIME);
-    out.printf("       %-48s : Maximum reduce task runtime in mili-sec (merge+reduce)\n",
-        SleepJob.GRIDMIX_SLEEP_MAX_REDUCE_TIME);
-    out.println("   Parameters specific for STRESS submission throttling policy:");
-    out.printf("       %-48s : jobs vs task-tracker ratio\n",
-        StressJobFactory.CONF_MAX_JOB_TRACKER_RATIO);
-    out.printf("       %-48s : maps vs map-slot ratio\n",
-        StressJobFactory.CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO);
-    out.printf("       %-48s : reduces vs reduce-slot ratio\n",
-        StressJobFactory.CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO);
-    out.printf("       %-48s : map-slot share per job\n",
-        StressJobFactory.CONF_MAX_MAPSLOT_SHARE_PER_JOB);
-    out.printf("       %-48s : reduce-slot share per job\n",
-        StressJobFactory.CONF_MAX_REDUCESLOT_SHARE_PER_JOB);
-   }
-
-  /**
-   * Components in the pipeline must support the following operations for
-   * orderly startup and shutdown.
-   */
-  interface Component<T> {
-
-    /**
-     * Accept an item into this component from an upstream component. If
-     * shutdown or abort have been called, this may fail, depending on the
-     * semantics for the component.
-     */
-    void add(T item) throws InterruptedException;
-
-    /**
-     * Attempt to start the service.
-     */
-    void start();
-
-    /**
-     * Wait until the service completes. It is assumed that either a
-     * {@link #shutdown} or {@link #abort} has been requested.
-     */
-    void join(long millis) throws InterruptedException;
-
-    /**
-     * Shut down gracefully, finishing all pending work. Reject new requests.
-     */
-    void shutdown();
-
-    /**
-     * Shut down immediately, aborting any work in progress and discarding
-     * all pending work. It is legal to store pending work for another
-     * thread to process.
-     */
-    void abort();
-  }
-
-}
-
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
deleted file mode 100644
index 77ec697..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
+++ /dev/null
@@ -1,517 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Formatter;
-import java.util.List;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.Delayed;
-import java.util.concurrent.TimeUnit;
-import java.security.PrivilegedExceptionAction;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.Partitioner;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import static org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser.extractMaxHeapOpts;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Synthetic job generated from a trace description.
- */
-abstract class GridmixJob implements Callable<Job>, Delayed {
-
-  // Gridmix job name format is GRIDMIX<6 digit sequence number>
-  public static final String JOB_NAME_PREFIX = "GRIDMIX";
-  public static final Log LOG = LogFactory.getLog(GridmixJob.class);
-
-  private static final ThreadLocal<Formatter> nameFormat =
-    new ThreadLocal<Formatter>() {
-      @Override
-      protected Formatter initialValue() {
-        final StringBuilder sb =
-            new StringBuilder(JOB_NAME_PREFIX.length() + 6);
-        sb.append(JOB_NAME_PREFIX);
-        return new Formatter(sb);
-      }
-    };
-
-  protected final int seq;
-  protected final Path outdir;
-  protected final Job job;
-  protected final JobStory jobdesc;
-  protected final UserGroupInformation ugi;
-  protected final long submissionTimeNanos;
-  private static final ConcurrentHashMap<Integer,List<InputSplit>> descCache =
-     new ConcurrentHashMap<Integer,List<InputSplit>>();
-  protected static final String GRIDMIX_JOB_SEQ = "gridmix.job.seq";
-  protected static final String GRIDMIX_USE_QUEUE_IN_TRACE = 
-      "gridmix.job-submission.use-queue-in-trace";
-  protected static final String GRIDMIX_DEFAULT_QUEUE = 
-      "gridmix.job-submission.default-queue";
-  // configuration key to enable/disable High-Ram feature emulation
-  static final String GRIDMIX_HIGHRAM_EMULATION_ENABLE = 
-    "gridmix.highram-emulation.enable";
-  // configuration key to enable/disable task jvm options
-  static final String GRIDMIX_TASK_JVM_OPTIONS_ENABLE = 
-    "gridmix.task.jvm-options.enable";
-
-  private static void setJobQueue(Job job, String queue) {
-    if (queue != null) {
-      job.getConfiguration().set(MRJobConfig.QUEUE_NAME, queue);
-    }
-  }
-  
-  public GridmixJob(final Configuration conf, long submissionMillis,
-      final JobStory jobdesc, Path outRoot, UserGroupInformation ugi, 
-      final int seq) throws IOException {
-    this.ugi = ugi;
-    this.jobdesc = jobdesc;
-    this.seq = seq;
-
-    ((StringBuilder)nameFormat.get().out()).setLength(JOB_NAME_PREFIX.length());
-    try {
-      job = this.ugi.doAs(new PrivilegedExceptionAction<Job>() {
-        public Job run() throws IOException {
-
-          String jobId = null == jobdesc.getJobID() 
-                         ? "<unknown>" 
-                         : jobdesc.getJobID().toString();
-          Job ret = new Job(conf,
-                            nameFormat.get().format("%06d", seq).toString());
-          ret.getConfiguration().setInt(GRIDMIX_JOB_SEQ, seq);
-
-          ret.getConfiguration().set(Gridmix.ORIGINAL_JOB_ID, jobId);
-          ret.getConfiguration().set(Gridmix.ORIGINAL_JOB_NAME,
-                                     jobdesc.getName());
-          if (conf.getBoolean(GRIDMIX_USE_QUEUE_IN_TRACE, false)) {
-            setJobQueue(ret, jobdesc.getQueueName());
-          } else {
-            setJobQueue(ret, conf.get(GRIDMIX_DEFAULT_QUEUE));
-          }
-
-          // check if the job can emulate compression
-          if (canEmulateCompression()) {
-            // set the compression related configs if compression emulation is
-            // enabled
-            if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)) {
-              CompressionEmulationUtil.configureCompressionEmulation(
-                  jobdesc.getJobConf(), ret.getConfiguration());
-            }
-          }
-          
-          // configure high ram properties if enabled
-          if (conf.getBoolean(GRIDMIX_HIGHRAM_EMULATION_ENABLE, true)) {
-            configureHighRamProperties(jobdesc.getJobConf(), 
-                                       ret.getConfiguration());
-          }
-          
-          // configure task jvm options if enabled
-          // this knob can be turned off if there is a mismatch between the
-          // target (simulation) cluster and the original cluster. Such a 
-          // mismatch can result in job failures (due to memory issues) on the 
-          // target (simulated) cluster.
-          //
-          // TODO If configured, scale the original task's JVM (heap related)
-          //      options to suit the target (simulation) cluster
-          if (conf.getBoolean(GRIDMIX_TASK_JVM_OPTIONS_ENABLE, true)) {
-            configureTaskJVMOptions(jobdesc.getJobConf(), 
-                                    ret.getConfiguration());
-          }
-          
-          return ret;
-        }
-      });
-    } catch (InterruptedException e) {
-      throw new IOException(e);
-    }
-
-    submissionTimeNanos = TimeUnit.NANOSECONDS.convert(
-        submissionMillis, TimeUnit.MILLISECONDS);
-    outdir = new Path(outRoot, "" + seq);
-  }
-  
-  @SuppressWarnings("deprecation")
-  protected static void configureTaskJVMOptions(Configuration originalJobConf,
-                                                Configuration simulatedJobConf){
-    // Get the heap related java opts used for the original job and set the 
-    // same for the simulated job.
-    //    set task task heap options
-    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
-                                   JobConf.MAPRED_TASK_JAVA_OPTS);
-    //  set map task heap options
-    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
-                                   MRJobConfig.MAP_JAVA_OPTS);
-
-    //  set reduce task heap options
-    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
-                                   MRJobConfig.REDUCE_JAVA_OPTS);
-  }
-  
-  // Configures the task's max heap options using the specified key
-  private static void configureTaskJVMMaxHeapOptions(Configuration srcConf, 
-                                                     Configuration destConf,
-                                                     String key) {
-    String srcHeapOpts = srcConf.get(key);
-    if (srcHeapOpts != null) {
-      List<String> srcMaxOptsList = new ArrayList<String>();
-      // extract the max heap options and ignore the rest
-      extractMaxHeapOpts(srcHeapOpts, srcMaxOptsList, 
-                         new ArrayList<String>());
-      if (srcMaxOptsList.size() > 0) {
-        List<String> destOtherOptsList = new ArrayList<String>();
-        // extract the other heap options and ignore the max options in the 
-        // destination configuration
-        String destHeapOpts = destConf.get(key);
-        if (destHeapOpts != null) {
-          extractMaxHeapOpts(destHeapOpts, new ArrayList<String>(), 
-                             destOtherOptsList);
-        }
-        
-        // the source configuration might have some task level max heap opts set
-        // remove these opts from the destination configuration and replace
-        // with the options set in the original configuration
-        StringBuilder newHeapOpts = new StringBuilder();
-        
-        for (String otherOpt : destOtherOptsList) {
-          newHeapOpts.append(otherOpt).append(" ");
-        }
-        
-        for (String opts : srcMaxOptsList) {
-          newHeapOpts.append(opts).append(" ");
-        }
-        
-        // set the final heap opts 
-        destConf.set(key, newHeapOpts.toString().trim());
-      }
-    }
-  }
-
-  // Scales the desired job-level configuration parameter. This API makes sure 
-  // that the ratio of the job level configuration parameter to the cluster 
-  // level configuration parameter is maintained in the simulated run. Hence 
-  // the values are scaled from the original cluster's configuration to the 
-  // simulated cluster's configuration for higher emulation accuracy.
-  // This kind of scaling is useful for memory parameters.
-  private static void scaleConfigParameter(Configuration sourceConf, 
-                        Configuration destConf, String clusterValueKey, 
-                        String jobValueKey, long defaultValue) {
-    long simulatedClusterDefaultValue = 
-           destConf.getLong(clusterValueKey, defaultValue);
-    
-    long originalClusterDefaultValue = 
-           sourceConf.getLong(clusterValueKey, defaultValue);
-    
-    long originalJobValue = 
-           sourceConf.getLong(jobValueKey, defaultValue);
-    
-    double scaleFactor = (double)originalJobValue/originalClusterDefaultValue;
-    
-    long simulatedJobValue = (long)(scaleFactor * simulatedClusterDefaultValue);
-    
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("For the job configuration parameter '" + jobValueKey 
-                + "' and the cluster configuration parameter '" 
-                + clusterValueKey + "', the original job's configuration value"
-                + " is scaled from '" + originalJobValue + "' to '" 
-                + simulatedJobValue + "' using the default (unit) value of "
-                + "'" + originalClusterDefaultValue + "' for the original "
-                + " cluster and '" + simulatedClusterDefaultValue + "' for the"
-                + " simulated cluster.");
-    }
-    
-    destConf.setLong(jobValueKey, simulatedJobValue);
-  }
-  
-  // Checks if the scaling of original job's memory parameter value is 
-  // valid
-  @SuppressWarnings("deprecation")
-  private static boolean checkMemoryUpperLimits(String jobKey, String limitKey,  
-                                                Configuration conf, 
-                                                boolean convertLimitToMB) {
-    if (conf.get(limitKey) != null) {
-      long limit = conf.getLong(limitKey, JobConf.DISABLED_MEMORY_LIMIT);
-      // scale only if the max memory limit is set.
-      if (limit >= 0) {
-        if (convertLimitToMB) {
-          limit /= (1024 * 1024); //Converting to MB
-        }
-        
-        long scaledConfigValue = 
-               conf.getLong(jobKey, JobConf.DISABLED_MEMORY_LIMIT);
-        
-        // check now
-        if (scaledConfigValue > limit) {
-          throw new RuntimeException("Simulated job's configuration" 
-              + " parameter '" + jobKey + "' got scaled to a value '" 
-              + scaledConfigValue + "' which exceeds the upper limit of '" 
-              + limit + "' defined for the simulated cluster by the key '" 
-              + limitKey + "'. To disable High-Ram feature emulation, set '" 
-              + GRIDMIX_HIGHRAM_EMULATION_ENABLE + "' to 'false'.");
-        }
-        return true;
-      }
-    }
-    return false;
-  }
-  
-  // Check if the parameter scaling does not exceed the cluster limits.
-  @SuppressWarnings("deprecation")
-  private static void validateTaskMemoryLimits(Configuration conf, 
-                        String jobKey, String clusterMaxKey) {
-    if (!checkMemoryUpperLimits(jobKey, 
-        JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, conf, true)) {
-      checkMemoryUpperLimits(jobKey, clusterMaxKey, conf, false);
-    }
-  }
-
-  /**
-   * Sets the high ram job properties in the simulated job's configuration.
-   */
-  @SuppressWarnings("deprecation")
-  static void configureHighRamProperties(Configuration sourceConf, 
-                                         Configuration destConf) {
-    // set the memory per map task
-    scaleConfigParameter(sourceConf, destConf, 
-                         MRConfig.MAPMEMORY_MB, MRJobConfig.MAP_MEMORY_MB, 
-                         JobConf.DISABLED_MEMORY_LIMIT);
-    
-    // validate and fail early
-    validateTaskMemoryLimits(destConf, MRJobConfig.MAP_MEMORY_MB, 
-                             JTConfig.JT_MAX_MAPMEMORY_MB);
-    
-    // set the memory per reduce task
-    scaleConfigParameter(sourceConf, destConf, 
-                         MRConfig.REDUCEMEMORY_MB, MRJobConfig.REDUCE_MEMORY_MB,
-                         JobConf.DISABLED_MEMORY_LIMIT);
-    // validate and fail early
-    validateTaskMemoryLimits(destConf, MRJobConfig.REDUCE_MEMORY_MB, 
-                             JTConfig.JT_MAX_REDUCEMEMORY_MB);
-  }
-  
-  /**
-   * Indicates whether this {@link GridmixJob} supports compression emulation.
-   */
-  protected abstract boolean canEmulateCompression();
-  
-  protected GridmixJob(final Configuration conf, long submissionMillis, 
-                       final String name) throws IOException {
-    submissionTimeNanos = TimeUnit.NANOSECONDS.convert(
-        submissionMillis, TimeUnit.MILLISECONDS);
-    jobdesc = null;
-    outdir = null;
-    seq = -1;
-    ugi = UserGroupInformation.getCurrentUser();
-
-    try {
-      job = this.ugi.doAs(new PrivilegedExceptionAction<Job>() {
-        public Job run() throws IOException {
-          Job ret = new Job(conf, name);
-          ret.getConfiguration().setInt(GRIDMIX_JOB_SEQ, seq);
-          setJobQueue(ret, conf.get(GRIDMIX_DEFAULT_QUEUE));
-          return ret;
-        }
-      });
-    } catch (InterruptedException e) {
-      throw new IOException(e);
-    }
-  }
-
-  public UserGroupInformation getUgi() {
-    return ugi;
-  }
-
-  public String toString() {
-    return job.getJobName();
-  }
-
-  public long getDelay(TimeUnit unit) {
-    return unit.convert(submissionTimeNanos - System.nanoTime(),
-        TimeUnit.NANOSECONDS);
-  }
-
-  @Override
-  public int compareTo(Delayed other) {
-    if (this == other) {
-      return 0;
-    }
-    if (other instanceof GridmixJob) {
-      final long otherNanos = ((GridmixJob)other).submissionTimeNanos;
-      if (otherNanos < submissionTimeNanos) {
-        return 1;
-      }
-      if (otherNanos > submissionTimeNanos) {
-        return -1;
-      }
-      return id() - ((GridmixJob)other).id();
-    }
-    final long diff =
-      getDelay(TimeUnit.NANOSECONDS) - other.getDelay(TimeUnit.NANOSECONDS);
-    return 0 == diff ? 0 : (diff > 0 ? 1 : -1);
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (this == other) {
-      return true;
-    }
-    // not possible unless job is cloned; all jobs should be unique
-    return other instanceof GridmixJob && id() == ((GridmixJob)other).id();
-  }
-
-  @Override
-  public int hashCode() {
-    return id();
-  }
-
-  int id() {
-    return seq;
-  }
-
-  Job getJob() {
-    return job;
-  }
-
-  JobStory getJobDesc() {
-    return jobdesc;
-  }
-
-  static void pushDescription(int seq, List<InputSplit> splits) {
-    if (null != descCache.putIfAbsent(seq, splits)) {
-      throw new IllegalArgumentException("Description exists for id " + seq);
-    }
-  }
-
-  static List<InputSplit> pullDescription(JobContext jobCtxt) {
-    return pullDescription(GridmixJob.getJobSeqId(jobCtxt));
-  }
-  
-  static List<InputSplit> pullDescription(int seq) {
-    return descCache.remove(seq);
-  }
-
-  static void clearAll() {
-    descCache.clear();
-  }
-
-  void buildSplits(FilePool inputDir) throws IOException {
-
-  }
-  static int getJobSeqId(JobContext job) {
-    return job.getConfiguration().getInt(GRIDMIX_JOB_SEQ,-1);
-  }
-
-  public static class DraftPartitioner<V> extends Partitioner<GridmixKey,V> {
-    public int getPartition(GridmixKey key, V value, int numReduceTasks) {
-      return key.getPartition();
-    }
-  }
-
-  public static class SpecGroupingComparator
-      implements RawComparator<GridmixKey> {
-    private final DataInputBuffer di = new DataInputBuffer();
-    private final byte[] reset = di.getData();
-    @Override
-    public int compare(GridmixKey g1, GridmixKey g2) {
-      final byte t1 = g1.getType();
-      final byte t2 = g2.getType();
-      if (t1 == GridmixKey.REDUCE_SPEC ||
-          t2 == GridmixKey.REDUCE_SPEC) {
-        return t1 - t2;
-      }
-      assert t1 == GridmixKey.DATA;
-      assert t2 == GridmixKey.DATA;
-      return g1.compareTo(g2);
-    }
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      try {
-        final int ret;
-        di.reset(b1, s1, l1);
-        final int x1 = WritableUtils.readVInt(di);
-        di.reset(b2, s2, l2);
-        final int x2 = WritableUtils.readVInt(di);
-        final int t1 = b1[s1 + x1];
-        final int t2 = b2[s2 + x2];
-        if (t1 == GridmixKey.REDUCE_SPEC ||
-            t2 == GridmixKey.REDUCE_SPEC) {
-          ret = t1 - t2;
-        } else {
-          assert t1 == GridmixKey.DATA;
-          assert t2 == GridmixKey.DATA;
-          ret =
-            WritableComparator.compareBytes(b1, s1, x1, b2, s2, x2);
-        }
-        di.reset(reset, 0, 0);
-        return ret;
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  static class RawBytesOutputFormat<K>
-      extends FileOutputFormat<K,GridmixRecord> {
-
-    @Override
-    public RecordWriter<K,GridmixRecord> getRecordWriter(
-        TaskAttemptContext job) throws IOException {
-
-      Path file = getDefaultWorkFile(job, "");
-      final DataOutputStream fileOut;
-
-      fileOut = 
-        new DataOutputStream(CompressionEmulationUtil
-            .getPossiblyCompressedOutputStream(file, job.getConfiguration()));
-
-      return new RecordWriter<K,GridmixRecord>() {
-        @Override
-        public void write(K ignored, GridmixRecord value)
-            throws IOException {
-          // Let the Gridmix record fill itself.
-          value.write(fileOut);
-        }
-        @Override
-        public void close(TaskAttemptContext ctxt) throws IOException {
-          fileOut.close();
-        }
-      };
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
deleted file mode 100644
index 83eb947..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-
-import java.util.concurrent.CountDownLatch;
-import java.io.IOException;
-
-enum GridmixJobSubmissionPolicy {
-
-  REPLAY("REPLAY", 320000) {
-    @Override
-    public JobFactory<ClusterStats> createJobFactory(
-      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-      return new ReplayJobFactory(
-        submitter, producer, scratchDir, conf, startFlag, userResolver);
-    }
-  },
-
-  STRESS("STRESS", 5000) {
-    @Override
-    public JobFactory<ClusterStats> createJobFactory(
-      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-      return new StressJobFactory(
-        submitter, producer, scratchDir, conf, startFlag, userResolver);
-    }
-  },
-
-  SERIAL("SERIAL", 0) {
-    @Override
-    public JobFactory<JobStats> createJobFactory(
-      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-      return new SerialJobFactory(
-        submitter, producer, scratchDir, conf, startFlag, userResolver);
-    }
-  };
-
-  public static final String JOB_SUBMISSION_POLICY =
-    "gridmix.job-submission.policy";
-
-  private final String name;
-  private final int pollingInterval;
-
-  GridmixJobSubmissionPolicy(String name, int pollingInterval) {
-    this.name = name;
-    this.pollingInterval = pollingInterval;
-  }
-
-  public abstract JobFactory createJobFactory(
-    JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-    Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-    throws IOException;
-
-  public int getPollingInterval() {
-    return pollingInterval;
-  }
-
-  public static GridmixJobSubmissionPolicy getPolicy(
-    Configuration conf, GridmixJobSubmissionPolicy defaultPolicy) {
-    String policy = conf.get(JOB_SUBMISSION_POLICY, defaultPolicy.name());
-    return valueOf(policy.toUpperCase());
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
deleted file mode 100644
index e03e1b9..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
+++ /dev/null
@@ -1,301 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-class GridmixKey extends GridmixRecord {
-  static final byte REDUCE_SPEC = 0;
-  static final byte DATA = 1;
-
-  static final int META_BYTES = 1;
-
-  private byte type;
-  private int partition; // NOT serialized
-  private Spec spec = new Spec();
-
-  GridmixKey() {
-    this(DATA, 1, 0L);
-  }
-  GridmixKey(byte type, int size, long seed) {
-    super(size, seed);
-    this.type = type;
-    // setting type may change pcnt random bytes
-    setSize(size);
-  }
-
-  @Override
-  public int getSize() {
-    switch (type) {
-      case REDUCE_SPEC:
-        return super.getSize() + spec.getSize() + META_BYTES;
-      case DATA:
-        return super.getSize() + META_BYTES;
-      default:
-        throw new IllegalStateException("Invalid type: " + type);
-    }
-  }
-
-  @Override
-  public void setSize(int size) {
-    switch (type) {
-      case REDUCE_SPEC:
-        super.setSize(size - (META_BYTES + spec.getSize()));
-        break;
-      case DATA:
-        super.setSize(size - META_BYTES);
-        break;
-      default:
-        throw new IllegalStateException("Invalid type: " + type);
-    }
-  }
-
-  /**
-   * Partition is not serialized.
-   */
-  public int getPartition() {
-    return partition;
-  }
-  public void setPartition(int partition) {
-    this.partition = partition;
-  }
-
-  public long getReduceInputRecords() {
-    assert REDUCE_SPEC == getType();
-    return spec.rec_in;
-  }
-  public void setReduceInputRecords(long rec_in) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    spec.rec_in = rec_in;
-    setSize(origSize);
-  }
-
-  public long getReduceOutputRecords() {
-    assert REDUCE_SPEC == getType();
-    return spec.rec_out;
-  }
-  public void setReduceOutputRecords(long rec_out) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    spec.rec_out = rec_out;
-    setSize(origSize);
-  }
-
-  public long getReduceOutputBytes() {
-    assert REDUCE_SPEC == getType();
-    return spec.bytes_out;
-  };
-  public void setReduceOutputBytes(long b_out) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    spec.bytes_out = b_out;
-    setSize(origSize);
-  }
-
-  /**
-   * Get the {@link ResourceUsageMetrics} stored in the key.
-   */
-  public ResourceUsageMetrics getReduceResourceUsageMetrics() {
-    assert REDUCE_SPEC == getType();
-    return spec.metrics;
-  }
-  
-  /**
-   * Store the {@link ResourceUsageMetrics} in the key.
-   */
-  public void setReduceResourceUsageMetrics(ResourceUsageMetrics metrics) {
-    assert REDUCE_SPEC == getType();
-    spec.setResourceUsageSpecification(metrics);
-  }
-  
-  public byte getType() {
-    return type;
-  }
-  public void setType(byte type) throws IOException {
-    final int origSize = getSize();
-    switch (type) {
-      case REDUCE_SPEC:
-      case DATA:
-        this.type = type;
-        break;
-      default:
-        throw new IOException("Invalid type: " + type);
-    }
-    setSize(origSize);
-  }
-
-  public void setSpec(Spec spec) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    this.spec.set(spec);
-    setSize(origSize);
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    setType(in.readByte());
-    if (REDUCE_SPEC == getType()) {
-      spec.readFields(in);
-    }
-  }
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    final byte t = getType();
-    out.writeByte(t);
-    if (REDUCE_SPEC == t) {
-      spec.write(out);
-    }
-  }
-  int fixedBytes() {
-    return super.fixedBytes() +
-      (REDUCE_SPEC == getType() ? spec.getSize() : 0) + META_BYTES;
-  }
-  @Override
-  public int compareTo(GridmixRecord other) {
-    final GridmixKey o = (GridmixKey) other;
-    final byte t1 = getType();
-    final byte t2 = o.getType();
-    if (t1 != t2) {
-      return t1 - t2;
-    }
-    return super.compareTo(other);
-  }
-
-  /**
-   * Note that while the spec is not explicitly included, changing the spec
-   * may change its size, which will affect equality.
-   */
-  @Override
-  public boolean equals(Object other) {
-    if (this == other) {
-      return true;
-    }
-    if (other != null && other.getClass() == getClass()) {
-      final GridmixKey o = ((GridmixKey)other);
-      return getType() == o.getType() && super.equals(o);
-    }
-    return false;
-  }
-
-  @Override
-  public int hashCode() {
-    return super.hashCode() ^ getType();
-  }
-
-  public static class Spec implements Writable {
-    long rec_in;
-    long rec_out;
-    long bytes_out;
-    private ResourceUsageMetrics metrics = null;
-    private int sizeOfResourceUsageMetrics = 0;
-    public Spec() { }
-
-    public void set(Spec other) {
-      rec_in = other.rec_in;
-      bytes_out = other.bytes_out;
-      rec_out = other.rec_out;
-      setResourceUsageSpecification(other.metrics);
-    }
-
-    /**
-     * Sets the {@link ResourceUsageMetrics} for this {@link Spec}.
-     */
-    public void setResourceUsageSpecification(ResourceUsageMetrics metrics) {
-      this.metrics = metrics;
-      if (metrics != null) {
-        this.sizeOfResourceUsageMetrics = metrics.size();
-      } else {
-        this.sizeOfResourceUsageMetrics = 0;
-      }
-    }
-    
-    public int getSize() {
-      return WritableUtils.getVIntSize(rec_in) +
-             WritableUtils.getVIntSize(rec_out) +
-             WritableUtils.getVIntSize(bytes_out) +
-             WritableUtils.getVIntSize(sizeOfResourceUsageMetrics) +
-             sizeOfResourceUsageMetrics;
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      rec_in = WritableUtils.readVLong(in);
-      rec_out = WritableUtils.readVLong(in);
-      bytes_out = WritableUtils.readVLong(in);
-      sizeOfResourceUsageMetrics =  WritableUtils.readVInt(in);
-      if (sizeOfResourceUsageMetrics > 0) {
-        metrics = new ResourceUsageMetrics();
-        metrics.readFields(in);
-      }
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      WritableUtils.writeVLong(out, rec_in);
-      WritableUtils.writeVLong(out, rec_out);
-      WritableUtils.writeVLong(out, bytes_out);
-      WritableUtils.writeVInt(out, sizeOfResourceUsageMetrics);
-      if (sizeOfResourceUsageMetrics > 0) {
-        metrics.write(out);
-      }
-    }
-  }
-
-  public static class Comparator extends GridmixRecord.Comparator {
-
-    private final DataInputBuffer di = new DataInputBuffer();
-    private final byte[] reset = di.getData();
-
-    public Comparator() {
-      super(GridmixKey.class);
-    }
-
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      try {
-        di.reset(b1, s1, l1);
-        final int x1 = WritableUtils.readVInt(di);
-        di.reset(b2, s2, l2);
-        final int x2 = WritableUtils.readVInt(di);
-        final int ret = (b1[s1 + x1] != b2[s2 + x2])
-          ? b1[s1 + x1] - b2[s2 + x2]
-          : super.compare(b1, s1, x1, b2, s2, x2);
-        di.reset(reset, 0, 0);
-        return ret;
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    static {
-      WritableComparator.define(GridmixKey.class, new Comparator());
-    }
-  }
-}
-
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
deleted file mode 100644
index 481799f..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
+++ /dev/null
@@ -1,271 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-
-class GridmixRecord implements WritableComparable<GridmixRecord> {
-
-  private static final int FIXED_BYTES = 1;
-  private int size = -1;
-  private long seed;
-  private final DataInputBuffer dib =
-    new DataInputBuffer();
-  private final DataOutputBuffer dob =
-    new DataOutputBuffer(Long.SIZE / Byte.SIZE);
-  private byte[] literal = dob.getData();
-  private boolean compressible = false;
-  private float compressionRatio = 
-    CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;
-  private RandomTextDataGenerator rtg = null;
-
-  GridmixRecord() {
-    this(1, 0L);
-  }
-
-  GridmixRecord(int size, long seed) {
-    this.seed = seed;
-    setSizeInternal(size);
-  }
-
-  public int getSize() {
-    return size;
-  }
-
-  public void setSize(int size) {
-    setSizeInternal(size);
-  }
-
-  void setCompressibility(boolean compressible, float ratio) {
-    this.compressible = compressible;
-    this.compressionRatio = ratio;
-    // Initialize the RandomTextDataGenerator once for every GridMix record
-    // Note that RandomTextDataGenerator is needed only when the GridMix record
-    // is configured to generate compressible text data.
-    if (compressible) {
-      rtg = 
-        CompressionEmulationUtil.getRandomTextDataGenerator(ratio, 
-                                   RandomTextDataGenerator.DEFAULT_SEED);
-    }
-  }
-  
-  private void setSizeInternal(int size) {
-    this.size = Math.max(1, size);
-    try {
-      seed = maskSeed(seed, this.size);
-      dob.reset();
-      dob.writeLong(seed);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  public final void setSeed(long seed) {
-    this.seed = seed;
-  }
-
-  /** Marsaglia, 2003. */
-  long nextRand(long x) {
-    x ^= (x << 13);
-    x ^= (x >>> 7);
-    return (x ^= (x << 17));
-  }
-
-  /**
-   * Generate random text data that can be compressed. If the record is marked
-   * compressible (via {@link FileOutputFormat#COMPRESS}), only then the 
-   * random data will be text data else 
-   * {@link GridmixRecord#writeRandom(DataOutput, int)} will be invoked.
-   */
-  private void writeRandomText(DataOutput out, final int size) 
-  throws IOException {
-    long tmp = seed;
-    out.writeLong(tmp);
-    int i = size - (Long.SIZE / Byte.SIZE);
-    //TODO Should we use long for size. What if the data is more than 4G?
-    
-    String randomWord = rtg.getRandomWord();
-    byte[] bytes = randomWord.getBytes("UTF-8");
-    long randomWordSize = bytes.length;
-    while (i >= randomWordSize) {
-      out.write(bytes);
-      i -= randomWordSize;
-      
-      // get the next random word
-      randomWord = rtg.getRandomWord();
-      bytes = randomWord.getBytes("UTF-8");
-      // determine the random word size
-      randomWordSize = bytes.length;
-    }
-    
-    // pad the remaining bytes
-    if (i > 0) {
-      out.write(bytes, 0, i);
-    }
-  }
-  
-  public void writeRandom(DataOutput out, final int size) throws IOException {
-    long tmp = seed;
-    out.writeLong(tmp);
-    int i = size - (Long.SIZE / Byte.SIZE);
-    while (i > Long.SIZE / Byte.SIZE - 1) {
-      tmp = nextRand(tmp);
-      out.writeLong(tmp);
-      i -= Long.SIZE / Byte.SIZE;
-    }
-    for (tmp = nextRand(tmp); i > 0; --i) {
-      out.writeByte((int)(tmp & 0xFF));
-      tmp >>>= Byte.SIZE;
-    }
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    size = WritableUtils.readVInt(in);
-    int payload = size - WritableUtils.getVIntSize(size);
-    if (payload > Long.SIZE / Byte.SIZE) {
-      seed = in.readLong();
-      payload -= Long.SIZE / Byte.SIZE;
-    } else {
-      Arrays.fill(literal, (byte)0);
-      in.readFully(literal, 0, payload);
-      dib.reset(literal, 0, literal.length);
-      seed = dib.readLong();
-      payload = 0;
-    }
-    final int vBytes = in.skipBytes(payload);
-    if (vBytes != payload) {
-      throw new EOFException("Expected " + payload + ", read " + vBytes);
-    }
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    // data bytes including vint encoding
-    WritableUtils.writeVInt(out, size);
-    final int payload = size - WritableUtils.getVIntSize(size);
-    if (payload > Long.SIZE / Byte.SIZE) {
-      if (compressible) {
-        writeRandomText(out, payload);
-      } else {
-        writeRandom(out, payload);
-      }
-    } else if (payload > 0) {
-      //TODO What is compressible is turned on? LOG is a bad idea!
-      out.write(literal, 0, payload);
-    }
-  }
-
-  @Override
-  public int compareTo(GridmixRecord other) {
-    return compareSeed(other.seed,
-        Math.max(0, other.getSize() - other.fixedBytes()));
-  }
-
-  int fixedBytes() {
-    // min vint size
-    return FIXED_BYTES;
-  }
-
-  private static long maskSeed(long sd, int sz) {
-    // Don't use fixedBytes here; subclasses will set intended random len
-    if (sz <= FIXED_BYTES) {
-      sd = 0L;
-    } else if (sz < Long.SIZE / Byte.SIZE + FIXED_BYTES) {
-      final int tmp = sz - FIXED_BYTES;
-      final long mask = (1L << (Byte.SIZE * tmp)) - 1;
-      sd &= mask << (Byte.SIZE * (Long.SIZE / Byte.SIZE - tmp));
-    }
-    return sd;
-  }
-
-  int compareSeed(long jSeed, int jSize) {
-    final int iSize = Math.max(0, getSize() - fixedBytes());
-    final int seedLen = Math.min(iSize, jSize) + FIXED_BYTES;
-    jSeed = maskSeed(jSeed, seedLen);
-    long iSeed = maskSeed(seed, seedLen);
-    final int cmplen = Math.min(iSize, jSize);
-    for (int i = 0; i < cmplen; i += Byte.SIZE) {
-      final int k = cmplen - i;
-      for (long j = Long.SIZE - Byte.SIZE;
-          j >= Math.max(0, Long.SIZE / Byte.SIZE - k) * Byte.SIZE;
-          j -= Byte.SIZE) {
-        final int xi = (int)((iSeed >>> j) & 0xFFL);
-        final int xj = (int)((jSeed >>> j) & 0xFFL);
-        if (xi != xj) {
-          return xi - xj;
-        }
-      }
-      iSeed = nextRand(iSeed);
-      jSeed = nextRand(jSeed);
-    }
-    return iSize - jSize;
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (this == other) {
-      return true;
-    }
-    if (other != null && other.getClass() == getClass()) {
-      final GridmixRecord o = ((GridmixRecord)other);
-      return getSize() == o.getSize() && seed == o.seed;
-    }
-    return false;
-  }
-
-  @Override
-  public int hashCode() {
-    return (int)(seed * getSize());
-  }
-
-  public static class Comparator extends WritableComparator {
-
-    public Comparator() {
-      super(GridmixRecord.class);
-    }
-
-    public Comparator(Class<? extends WritableComparable<?>> sub) {
-      super(sub);
-    }
-
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      int n1 = WritableUtils.decodeVIntSize(b1[s1]);
-      int n2 = WritableUtils.decodeVIntSize(b2[s2]);
-      n1 -= WritableUtils.getVIntSize(n1);
-      n2 -= WritableUtils.getVIntSize(n2);
-      return compareBytes(b1, s1+n1, l1-n1, b2, s2+n2, l2-n2);
-    }
-
-    static {
-      WritableComparator.define(GridmixRecord.class, new Comparator());
-    }
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
deleted file mode 100644
index b611c9d..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-class GridmixSplit extends CombineFileSplit {
-  private int id;
-  private int nSpec;
-  private int maps;
-  private int reduces;
-  private long inputRecords;
-  private long outputBytes;
-  private long outputRecords;
-  private long maxMemory;
-  private double[] reduceBytes = new double[0];
-  private double[] reduceRecords = new double[0];
-
-  // Spec for reduces id mod this
-  private long[] reduceOutputBytes = new long[0];
-  private long[] reduceOutputRecords = new long[0];
-
-  GridmixSplit() {
-    super();
-  }
-
-  public GridmixSplit(CombineFileSplit cfsplit, int maps, int id,
-      long inputBytes, long inputRecords, long outputBytes,
-      long outputRecords, double[] reduceBytes, double[] reduceRecords,
-      long[] reduceOutputBytes, long[] reduceOutputRecords)
-      throws IOException {
-    super(cfsplit);
-    this.id = id;
-    this.maps = maps;
-    reduces = reduceBytes.length;
-    this.inputRecords = inputRecords;
-    this.outputBytes = outputBytes;
-    this.outputRecords = outputRecords;
-    this.reduceBytes = reduceBytes;
-    this.reduceRecords = reduceRecords;
-    nSpec = reduceOutputBytes.length;
-    this.reduceOutputBytes = reduceOutputBytes;
-    this.reduceOutputRecords = reduceOutputRecords;
-  }
-  public int getId() {
-    return id;
-  }
-  public int getMapCount() {
-    return maps;
-  }
-  public long getInputRecords() {
-    return inputRecords;
-  }
-  public long[] getOutputBytes() {
-    if (0 == reduces) {
-      return new long[] { outputBytes };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputBytes * reduceBytes[i]);
-    }
-    return ret;
-  }
-  public long[] getOutputRecords() {
-    if (0 == reduces) {
-      return new long[] { outputRecords };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputRecords * reduceRecords[i]);
-    }
-    return ret;
-  }
-  public long getReduceBytes(int i) {
-    return reduceOutputBytes[i];
-  }
-  public long getReduceRecords(int i) {
-    return reduceOutputRecords[i];
-  }
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    WritableUtils.writeVInt(out, id);
-    WritableUtils.writeVInt(out, maps);
-    WritableUtils.writeVLong(out, inputRecords);
-    WritableUtils.writeVLong(out, outputBytes);
-    WritableUtils.writeVLong(out, outputRecords);
-    WritableUtils.writeVLong(out, maxMemory);
-    WritableUtils.writeVInt(out, reduces);
-    for (int i = 0; i < reduces; ++i) {
-      out.writeDouble(reduceBytes[i]);
-      out.writeDouble(reduceRecords[i]);
-    }
-    WritableUtils.writeVInt(out, nSpec);
-    for (int i = 0; i < nSpec; ++i) {
-      WritableUtils.writeVLong(out, reduceOutputBytes[i]);
-      WritableUtils.writeVLong(out, reduceOutputRecords[i]);
-    }
-  }
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    id = WritableUtils.readVInt(in);
-    maps = WritableUtils.readVInt(in);
-    inputRecords = WritableUtils.readVLong(in);
-    outputBytes = WritableUtils.readVLong(in);
-    outputRecords = WritableUtils.readVLong(in);
-    maxMemory = WritableUtils.readVLong(in);
-    reduces = WritableUtils.readVInt(in);
-    if (reduceBytes.length < reduces) {
-      reduceBytes = new double[reduces];
-      reduceRecords = new double[reduces];
-    }
-    for (int i = 0; i < reduces; ++i) {
-      reduceBytes[i] = in.readDouble();
-      reduceRecords[i] = in.readDouble();
-    }
-    nSpec = WritableUtils.readVInt(in);
-    if (reduceOutputBytes.length < nSpec) {
-      reduceOutputBytes = new long[nSpec];
-      reduceOutputRecords = new long[nSpec];
-    }
-    for (int i = 0; i < nSpec; ++i) {
-      reduceOutputBytes[i] = WritableUtils.readVLong(in);
-      reduceOutputRecords[i] = WritableUtils.readVLong(in);
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
deleted file mode 100644
index a9d404d..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
+++ /dev/null
@@ -1,139 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionCodecFactory;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Given a {@link #FilePool}, obtain a set of files capable of satisfying
- * a full set of splits, then iterate over each source to fill the request.
- */
-class InputStriper {
-  public static final Log LOG = LogFactory.getLog(InputStriper.class);
-  int idx;
-  long currentStart;
-  FileStatus current;
-  final List<FileStatus> files = new ArrayList<FileStatus>();
-  final Configuration conf = new Configuration();
-
-  /**
-   * @param inputDir Pool from which files are requested.
-   * @param mapBytes Sum of all expected split requests.
-   */
-  InputStriper(FilePool inputDir, long mapBytes)
-      throws IOException {
-    final long inputBytes = inputDir.getInputFiles(mapBytes, files);
-    if (mapBytes > inputBytes) {
-      LOG.warn("Using " + inputBytes + "/" + mapBytes + " bytes");
-    }
-    if (files.isEmpty() && mapBytes > 0) {
-      throw new IOException("Failed to satisfy request for " + mapBytes);
-    }
-    current = files.isEmpty() ? null : files.get(0);
-  }
-
-  /**
-   * @param inputDir Pool used to resolve block locations.
-   * @param bytes Target byte count
-   * @param nLocs Number of block locations per split.
-   * @return A set of files satisfying the byte count, with locations weighted
-   *         to the dominating proportion of input bytes.
-   */
-  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)
-      throws IOException {
-    final ArrayList<Path> paths = new ArrayList<Path>();
-    final ArrayList<Long> start = new ArrayList<Long>();
-    final ArrayList<Long> length = new ArrayList<Long>();
-    final HashMap<String,Double> sb = new HashMap<String,Double>();
-    do {
-      paths.add(current.getPath());
-      start.add(currentStart);
-      final long fromFile = Math.min(bytes, current.getLen() - currentStart);
-      length.add(fromFile);
-      for (BlockLocation loc :
-          inputDir.locationsFor(current, currentStart, fromFile)) {
-        final double tedium = loc.getLength() / (1.0 * bytes);
-        for (String l : loc.getHosts()) {
-          Double j = sb.get(l);
-          if (null == j) {
-            sb.put(l, tedium);
-          } else {
-            sb.put(l, j.doubleValue() + tedium);
-          }
-        }
-      }
-      currentStart += fromFile;
-      bytes -= fromFile;
-      // Switch to a new file if
-      //  - the current file is uncompressed and completely used
-      //  - the current file is compressed
-      
-      CompressionCodecFactory compressionCodecs = 
-        new CompressionCodecFactory(conf);
-      CompressionCodec codec = compressionCodecs.getCodec(current.getPath());
-      if (current.getLen() - currentStart == 0
-          || codec != null) {
-        current = files.get(++idx % files.size());
-        currentStart = 0;
-      }
-    } while (bytes > 0);
-    final ArrayList<Entry<String,Double>> sort =
-      new ArrayList<Entry<String,Double>>(sb.entrySet());
-    Collections.sort(sort, hostRank);
-    final String[] hosts = new String[Math.min(nLocs, sort.size())];
-    for (int i = 0; i < nLocs && i < sort.size(); ++i) {
-      hosts[i] = sort.get(i).getKey();
-    }
-    return new CombineFileSplit(paths.toArray(new Path[0]),
-        toLongArray(start), toLongArray(length), hosts);
-  }
-
-  private long[] toLongArray(final ArrayList<Long> sigh) {
-    final long[] ret = new long[sigh.size()];
-    for (int i = 0; i < ret.length; ++i) {
-      ret[i] = sigh.get(i);
-    }
-    return ret;
-  }
-
-  static final Comparator<Entry<String,Double>> hostRank =
-    new Comparator<Entry<String,Double>>() {
-      public int compare(Entry<String,Double> a, Entry<String,Double> b) {
-          final double va = a.getValue();
-          final double vb = b.getValue();
-          return va > vb ? -1 : va < vb ? 1 : 0;
-        }
-    };
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
deleted file mode 100644
index a6fc6c6..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * Factory passing reduce specification as its last record.
- */
-class IntermediateRecordFactory extends RecordFactory {
-
-  private final GridmixKey.Spec spec;
-  private final RecordFactory factory;
-  private final int partition;
-  private final long targetRecords;
-  private boolean done = false;
-  private long accRecords = 0L;
-
-  /**
-   * @param targetBytes Expected byte count.
-   * @param targetRecords Expected record count; will emit spec records after
-   *                      this boundary is passed.
-   * @param partition Reduce to which records are emitted.
-   * @param spec Specification to emit.
-   * @param conf Unused.
-   */
-  public IntermediateRecordFactory(long targetBytes, long targetRecords,
-      int partition, GridmixKey.Spec spec, Configuration conf) {
-    this(new AvgRecordFactory(targetBytes, targetRecords, conf), partition,
-        targetRecords, spec, conf);
-  }
-
-  /**
-   * @param factory Factory from which byte/record counts are obtained.
-   * @param partition Reduce to which records are emitted.
-   * @param targetRecords Expected record count; will emit spec records after
-   *                      this boundary is passed.
-   * @param spec Specification to emit.
-   * @param conf Unused.
-   */
-  public IntermediateRecordFactory(RecordFactory factory, int partition,
-      long targetRecords, GridmixKey.Spec spec, Configuration conf) {
-    this.spec = spec;
-    this.factory = factory;
-    this.partition = partition;
-    this.targetRecords = targetRecords;
-  }
-
-  @Override
-  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-    assert key != null;
-    final boolean rslt = factory.next(key, val);
-    ++accRecords;
-    if (rslt) {
-      if (accRecords < targetRecords) {
-        key.setType(GridmixKey.DATA);
-      } else {
-        final int orig = key.getSize();
-        key.setType(GridmixKey.REDUCE_SPEC);
-        spec.rec_in = accRecords;
-        key.setSpec(spec);
-        val.setSize(val.getSize() - (key.getSize() - orig));
-        // reset counters
-        accRecords = 0L;
-        spec.bytes_out = 0L;
-        spec.rec_out = 0L;
-        done = true;
-      }
-    } else if (!done) {
-      // ensure spec emitted
-      key.setType(GridmixKey.REDUCE_SPEC);
-      key.setPartition(partition);
-      key.setSize(0);
-      val.setSize(0);
-      spec.rec_in = 0L;
-      key.setSpec(spec);
-      done = true;
-      return true;
-    }
-    key.setPartition(partition);
-    return rslt;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return factory.getProgress();
-  }
-
-  @Override
-  public void close() throws IOException {
-    factory.close();
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobCreator.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
deleted file mode 100644
index f31e854..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public enum JobCreator {
-
-  LOADJOB {
-    @Override
-    public GridmixJob createGridmixJob(
-      Configuration gridmixConf, long submissionMillis, JobStory jobdesc,
-      Path outRoot, UserGroupInformation ugi, int seq) throws IOException {
-
-      // Build configuration for this simulated job
-      Configuration conf = new Configuration(gridmixConf);
-      dce.configureDistCacheFiles(conf, jobdesc.getJobConf());
-      return new LoadJob(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
-    }
-
-    @Override
-    public boolean canEmulateDistCacheLoad() {
-      return true;
-    }
-  },
-
-  SLEEPJOB {
-    private String[] hosts;
-      
-    @Override
-    public GridmixJob createGridmixJob(
-      Configuration conf, long submissionMillis, JobStory jobdesc, Path outRoot,
-      UserGroupInformation ugi, int seq) throws IOException {
-      int numLocations = conf.getInt(SLEEPJOB_RANDOM_LOCATIONS, 0);
-      if (numLocations < 0) numLocations=0;
-      if ((numLocations > 0) && (hosts == null)) {
-        final JobClient client = new JobClient(new JobConf(conf));
-        ClusterStatus stat = client.getClusterStatus(true);
-        final int nTrackers = stat.getTaskTrackers();
-        final ArrayList<String> hostList = new ArrayList<String>(nTrackers);
-        final Pattern trackerPattern = Pattern.compile("tracker_([^:]*):.*");
-        final Matcher m = trackerPattern.matcher("");
-        for (String tracker : stat.getActiveTrackerNames()) {
-          m.reset(tracker);
-          if (!m.find()) {
-            continue;
-          }
-          final String name = m.group(1);
-          hostList.add(name);
-        }
-        hosts = hostList.toArray(new String[hostList.size()]);
-      }
-      return new SleepJob(conf, submissionMillis, jobdesc, outRoot, ugi, seq,
-          numLocations, hosts);
-    }
-
-    @Override
-    public boolean canEmulateDistCacheLoad() {
-      return false;
-    }
-  };
-
-  public static final String GRIDMIX_JOB_TYPE = "gridmix.job.type";
-  public static final String SLEEPJOB_RANDOM_LOCATIONS = 
-    "gridmix.sleep.fake-locations";
-
-  /**
-   * Create Gridmix simulated job.
-   * @param conf configuration of simulated job
-   * @param submissionMillis At what time submission of this simulated job be
-   *                         done
-   * @param jobdesc JobStory obtained from trace
-   * @param outRoot gridmix output directory
-   * @param ugi UGI of job submitter of this simulated job
-   * @param seq job sequence number
-   * @return the created simulated job
-   * @throws IOException
-   */
-  public abstract GridmixJob createGridmixJob(
-    final Configuration conf, long submissionMillis, final JobStory jobdesc,
-    Path outRoot, UserGroupInformation ugi, final int seq) throws IOException;
-
-  public static JobCreator getPolicy(
-    Configuration conf, JobCreator defaultPolicy) {
-    return conf.getEnum(GRIDMIX_JOB_TYPE, defaultPolicy);
-  }
-
-  /**
-   * @return true if gridmix simulated jobs of this job type can emulate
-   *         distributed cache load
-   */
-  abstract boolean canEmulateDistCacheLoad();
-
-  DistributedCacheEmulator dce;
-  /**
-   * This method is to be called before calling any other method in JobCreator
-   * except canEmulateDistCacheLoad(), especially if canEmulateDistCacheLoad()
-   * returns true for that job type.
-   * @param e Distributed Cache Emulator
-   */
-  void setDistCacheEmulator(DistributedCacheEmulator e) {
-    this.dce = e;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
deleted file mode 100644
index b4737cf..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
+++ /dev/null
@@ -1,253 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants.Values;
-import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-import org.apache.hadoop.tools.rumen.ZombieJobProducer;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.locks.ReentrantLock;
-import java.util.concurrent.atomic.AtomicInteger;
-
-
-/**
- * Component reading job traces generated by Rumen. Each job in the trace is
- * assigned a sequence number and given a submission time relative to the
- * job that preceded it. Jobs are enqueued in the JobSubmitter provided at
- * construction.
- * @see org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer
- */
-abstract class JobFactory<T> implements Gridmix.Component<Void>,StatListener<T> {
-
-  public static final Log LOG = LogFactory.getLog(JobFactory.class);
-
-  protected final Path scratch;
-  protected final float rateFactor;
-  protected final Configuration conf;
-  protected final Thread rThread;
-  protected final AtomicInteger sequence;
-  protected final JobSubmitter submitter;
-  protected final CountDownLatch startFlag;
-  protected final UserResolver userResolver;
-  protected final JobCreator jobCreator;
-  protected volatile IOException error = null;
-  protected final JobStoryProducer jobProducer;
-  protected final ReentrantLock lock = new ReentrantLock(true);
-  protected int numJobsInTrace = 0;
-
-  /**
-   * Creating a new instance does not start the thread.
-   * @param submitter Component to which deserialized jobs are passed
-   * @param jobTrace Stream of job traces with which to construct a
-   *                 {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch Directory into which to write output from simulated jobs
-   * @param conf Config passed to all jobs to be submitted
-   * @param startFlag Latch released from main to start pipeline
-   * @throws java.io.IOException
-   */
-  public JobFactory(JobSubmitter submitter, InputStream jobTrace,
-      Path scratch, Configuration conf, CountDownLatch startFlag,
-      UserResolver userResolver) throws IOException {
-    this(submitter, new ZombieJobProducer(jobTrace, null), scratch, conf,
-        startFlag, userResolver);
-  }
-
-  /**
-   * Constructor permitting JobStoryProducer to be mocked.
-   * @param submitter Component to which deserialized jobs are passed
-   * @param jobProducer Producer generating JobStory objects.
-   * @param scratch Directory into which to write output from simulated jobs
-   * @param conf Config passed to all jobs to be submitted
-   * @param startFlag Latch released from main to start pipeline
-   */
-  protected JobFactory(JobSubmitter submitter, JobStoryProducer jobProducer,
-      Path scratch, Configuration conf, CountDownLatch startFlag,
-      UserResolver userResolver) {
-    sequence = new AtomicInteger(0);
-    this.scratch = scratch;
-    this.rateFactor = conf.getFloat(Gridmix.GRIDMIX_SUB_MUL, 1.0f);
-    this.jobProducer = jobProducer;
-    this.conf = new Configuration(conf);
-    this.submitter = submitter;
-    this.startFlag = startFlag;
-    this.rThread = createReaderThread();
-    if(LOG.isDebugEnabled()) {
-      LOG.debug(" The submission thread name is " + rThread.getName());
-    }
-    this.userResolver = userResolver;
-    this.jobCreator = JobCreator.getPolicy(conf, JobCreator.LOADJOB);
-  }
-
-  static class MinTaskInfo extends TaskInfo {
-    public MinTaskInfo(TaskInfo info) {
-      super(info.getInputBytes(), info.getInputRecords(),
-            info.getOutputBytes(), info.getOutputRecords(),
-            info.getTaskMemory(), info.getResourceUsageMetrics());
-    }
-    public long getInputBytes() {
-      return Math.max(0, super.getInputBytes());
-    }
-    public int getInputRecords() {
-      return Math.max(0, super.getInputRecords());
-    }
-    public long getOutputBytes() {
-      return Math.max(0, super.getOutputBytes());
-    }
-    public int getOutputRecords() {
-      return Math.max(0, super.getOutputRecords());
-    }
-    public long getTaskMemory() {
-      return Math.max(0, super.getTaskMemory());
-    }
-  }
-
-  protected static class FilterJobStory implements JobStory {
-
-    protected final JobStory job;
-
-    public FilterJobStory(JobStory job) {
-      this.job = job;
-    }
-    public JobConf getJobConf() { return job.getJobConf(); }
-    public String getName() { return job.getName(); }
-    public JobID getJobID() { return job.getJobID(); }
-    public String getUser() { return job.getUser(); }
-    public long getSubmissionTime() { return job.getSubmissionTime(); }
-    public InputSplit[] getInputSplits() { return job.getInputSplits(); }
-    public int getNumberMaps() { return job.getNumberMaps(); }
-    public int getNumberReduces() { return job.getNumberReduces(); }
-    public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
-      return job.getTaskInfo(taskType, taskNumber);
-    }
-    public TaskAttemptInfo getTaskAttemptInfo(TaskType taskType, int taskNumber,
-        int taskAttemptNumber) {
-      return job.getTaskAttemptInfo(taskType, taskNumber, taskAttemptNumber);
-    }
-    public TaskAttemptInfo getMapTaskAttemptInfoAdjusted(
-        int taskNumber, int taskAttemptNumber, int locality) {
-      return job.getMapTaskAttemptInfoAdjusted(
-          taskNumber, taskAttemptNumber, locality);
-    }
-    public Values getOutcome() {
-      return job.getOutcome();
-    }
-    public String getQueueName() {
-      return job.getQueueName();
-    }
-  }
-
-  protected abstract Thread createReaderThread() ; 
-
-  // gets the next job from the trace and does some bookkeeping for the same
-  private JobStory getNextJobFromTrace() throws IOException {
-    JobStory story = jobProducer.getNextJob();
-    if (story != null) {
-      ++numJobsInTrace;
-    }
-    return story;
-  }
-  
-  protected JobStory getNextJobFiltered() throws IOException {
-    JobStory job = getNextJobFromTrace();
-    while (job != null &&
-      (job.getOutcome() != Pre21JobHistoryConstants.Values.SUCCESS ||
-        job.getSubmissionTime() < 0)) {
-      if (LOG.isDebugEnabled()) {
-        String reason = null;
-        if (job.getOutcome() != Pre21JobHistoryConstants.Values.SUCCESS) {
-          reason = "STATE (" + job.getOutcome().name() + ") ";
-        }
-        if (job.getSubmissionTime() < 0) {
-          reason += "SUBMISSION-TIME (" + job.getSubmissionTime() + ")";
-        }
-        LOG.debug("Ignoring job " + job.getJobID() + " from the input trace."
-                  + " Reason: " + reason == null ? "N/A" : reason);
-      }
-      job = getNextJobFromTrace();
-    }
-    return null == job ? null : new FilterJobStory(job) {
-      @Override
-      public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
-        return new MinTaskInfo(this.job.getTaskInfo(taskType, taskNumber));
-      }
-    };
-  }
-
-
-  /**
-   * Obtain the error that caused the thread to exit unexpectedly.
-   */
-  public IOException error() {
-    return error;
-  }
-
-  /**
-   * Add is disabled.
-   * @throws UnsupportedOperationException
-   */
-  public void add(Void ignored) {
-    throw new UnsupportedOperationException(getClass().getName() +
-        " is at the start of the pipeline and accepts no events");
-  }
-
-  /**
-   * Start the reader thread, wait for latch if necessary.
-   */
-  public void start() {
-    rThread.start();
-  }
-
-  /**
-   * Wait for the reader thread to exhaust the job trace.
-   */
-  public void join(long millis) throws InterruptedException {
-    rThread.join(millis);
-  }
-
-  /**
-   * Interrupt the reader thread.
-   */
-  public void shutdown() {
-    rThread.interrupt();
-  }
-
-  /**
-   * Interrupt the reader thread. This requires no special consideration, as
-   * the thread has no pending work queue.
-   */
-  public void abort() {
-    // Currently no special work
-    rThread.interrupt();
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
deleted file mode 100644
index af7331c..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
+++ /dev/null
@@ -1,255 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.nio.channels.ClosedByInterruptException;
-import java.util.ArrayList;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Queue;
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.mapreduce.Job;
-
-/**
- * Component accepting submitted, running jobs and responsible for
- * monitoring jobs for success and failure. Once a job is submitted, it is
- * polled for status until complete. If a job is complete, then the monitor
- * thread returns immediately to the queue. If not, the monitor will sleep
- * for some duration.
- */
-class JobMonitor implements Gridmix.Component<Job> {
-
-  public static final Log LOG = LogFactory.getLog(JobMonitor.class);
-
-  private final Queue<Job> mJobs;
-  private final MonitorThread mThread;
-  private final BlockingQueue<Job> runningJobs;
-  private final long pollDelayMillis;
-  private Statistics statistics;
-  private boolean graceful = false;
-  private boolean shutdown = false;
-
-  public JobMonitor(Statistics statistics) {
-    this(5,TimeUnit.SECONDS, statistics);
-  }
-
-  /**
-   * Create a JobMonitor that sleeps for the specified duration after
-   * polling a still-running job.
-   * @param pollDelay Delay after polling a running job
-   * @param unit Time unit for pollDelaySec (rounded to milliseconds)
-   * @param statistics StatCollector , listener to job completion.
-   */
-  public JobMonitor(int pollDelay, TimeUnit unit, Statistics statistics) {
-    mThread = new MonitorThread();
-    runningJobs = new LinkedBlockingQueue<Job>();
-    mJobs = new LinkedList<Job>();
-    this.pollDelayMillis = TimeUnit.MILLISECONDS.convert(pollDelay, unit);
-    this.statistics = statistics;
-  }
-
-  /**
-   * Add a job to the polling queue.
-   */
-  public void add(Job job) throws InterruptedException {
-    runningJobs.put(job);
-  }
-
-  /**
-   * Add a submission failed job , such that it can be communicated
-   * back to serial.
-   * TODO: Cleaner solution for this problem
-   * @param job
-   */
-  public void submissionFailed(Job job) {
-    LOG.info("Job submission failed notification for job " + job.getJobID());
-    this.statistics.add(job);
-  }
-
-  /**
-   * Temporary hook for recording job success.
-   */
-  protected void onSuccess(Job job) {
-    LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " success");
-  }
-
-  /**
-   * Temporary hook for recording job failure.
-   */
-  protected void onFailure(Job job) {
-    LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " failure");
-  }
-
-  /**
-   * If shutdown before all jobs have completed, any still-running jobs
-   * may be extracted from the component.
-   * @throws IllegalStateException If monitoring thread is still running.
-   * @return Any jobs submitted and not known to have completed.
-   */
-  List<Job> getRemainingJobs() {
-    if (mThread.isAlive()) {
-      LOG.warn("Internal error: Polling running monitor for jobs");
-    }
-    synchronized (mJobs) {
-      return new ArrayList<Job>(mJobs);
-    }
-  }
-
-  /**
-   * Monitoring thread pulling running jobs from the component and into
-   * a queue to be polled for status.
-   */
-  private class MonitorThread extends Thread {
-
-    public MonitorThread() {
-      super("GridmixJobMonitor");
-    }
-
-    /**
-     * Check a job for success or failure.
-     */
-    public void process(Job job) throws IOException, InterruptedException {
-      if (job.isSuccessful()) {
-        onSuccess(job);
-      } else {
-        onFailure(job);
-      }
-    }
-
-    @Override
-    public void run() {
-      boolean graceful;
-      boolean shutdown;
-      while (true) {
-        try {
-          synchronized (mJobs) {
-            graceful = JobMonitor.this.graceful;
-            shutdown = JobMonitor.this.shutdown;
-            runningJobs.drainTo(mJobs);
-          }
-
-          // shutdown conditions; either shutdown requested and all jobs
-          // have completed or abort requested and there are recently
-          // submitted jobs not in the monitored set
-          if (shutdown) {
-            if (!graceful) {
-              while (!runningJobs.isEmpty()) {
-                synchronized (mJobs) {
-                  runningJobs.drainTo(mJobs);
-                }
-              }
-              break;
-            } else if (mJobs.isEmpty()) {
-              break;
-            }
-          }
-          while (!mJobs.isEmpty()) {
-            Job job;
-            synchronized (mJobs) {
-              job = mJobs.poll();
-            }
-            try {
-              if (job.isComplete()) {
-                process(job);
-                statistics.add(job);
-                continue;
-              }
-            } catch (IOException e) {
-              if (e.getCause() instanceof ClosedByInterruptException) {
-                // Job doesn't throw InterruptedException, but RPC socket layer
-                // is blocking and may throw a wrapped Exception if this thread
-                // is interrupted. Since the lower level cleared the flag,
-                // reset it here
-                Thread.currentThread().interrupt();
-              } else {
-                LOG.warn("Lost job " + (null == job.getJobName()
-                     ? "<unknown>" : job.getJobName()), e);
-                continue;
-              }
-            }
-            synchronized (mJobs) {
-              if (!mJobs.offer(job)) {
-                LOG.error("Lost job " + (null == job.getJobName()
-                     ? "<unknown>" : job.getJobName())); // should never
-                                                         // happen
-              }
-            }
-            break;
-          }
-          try {
-            TimeUnit.MILLISECONDS.sleep(pollDelayMillis);
-          } catch (InterruptedException e) {
-            shutdown = true;
-            continue;
-          }
-        } catch (Throwable e) {
-          LOG.warn("Unexpected exception: ", e);
-        }
-      }
-    }
-  }
-
-  /**
-   * Start the internal, monitoring thread.
-   */
-  public void start() {
-    mThread.start();
-  }
-
-  /**
-   * Wait for the monitor to halt, assuming shutdown or abort have been
-   * called. Note that, since submission may be sporatic, this will hang
-   * if no form of shutdown has been requested.
-   */
-  public void join(long millis) throws InterruptedException {
-    mThread.join(millis);
-  }
-
-  /**
-   * Drain all submitted jobs to a queue and stop the monitoring thread.
-   * Upstream submitter is assumed dead.
-   */
-  public void abort() {
-    synchronized (mJobs) {
-      graceful = false;
-      shutdown = true;
-    }
-    mThread.interrupt();
-  }
-
-  /**
-   * When all monitored jobs have completed, stop the monitoring thread.
-   * Upstream submitter is assumed dead.
-   */
-  public void shutdown() {
-    synchronized (mJobs) {
-      graceful = true;
-      shutdown = true;
-    }
-    mThread.interrupt();
-  }
-}
-
-
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
deleted file mode 100644
index 62dd9fa..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
+++ /dev/null
@@ -1,196 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.nio.channels.ClosedByInterruptException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.RejectedExecutionException;
-import java.util.concurrent.Semaphore;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Component accepting deserialized job traces, computing split data, and
- * submitting to the cluster on deadline. Each job added from an upstream
- * factory must be submitted to the cluster by the deadline recorded on it.
- * Once submitted, jobs must be added to a downstream component for
- * monitoring.
- */
-class JobSubmitter implements Gridmix.Component<GridmixJob> {
-
-  public static final Log LOG = LogFactory.getLog(JobSubmitter.class);
-
-  private final Semaphore sem;
-  private final Statistics statistics;
-  private final FilePool inputDir;
-  private final JobMonitor monitor;
-  private final ExecutorService sched;
-  private volatile boolean shutdown = false;
-
-  /**
-   * Initialize the submission component with downstream monitor and pool of
-   * files from which split data may be read.
-   * @param monitor Monitor component to which jobs should be passed
-   * @param threads Number of submission threads
-   *   See {@link Gridmix#GRIDMIX_SUB_THR}.
-   * @param queueDepth Max depth of pending work queue
-   *   See {@link Gridmix#GRIDMIX_QUE_DEP}.
-   * @param inputDir Set of files from which split data may be mined for
-   *   synthetic jobs.
-   * @param statistics
-   */
-  public JobSubmitter(JobMonitor monitor, int threads, int queueDepth,
-      FilePool inputDir, Statistics statistics) {
-    sem = new Semaphore(queueDepth);
-    sched = new ThreadPoolExecutor(threads, threads, 0L,
-        TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>());
-    this.inputDir = inputDir;
-    this.monitor = monitor;
-    this.statistics = statistics;
-  }
-
-  /**
-   * Runnable wrapping a job to be submitted to the cluster.
-   */
-  private class SubmitTask implements Runnable {
-
-    final GridmixJob job;
-    public SubmitTask(GridmixJob job) {
-      this.job = job;
-    }
-    public void run() {
-      try {
-        // pre-compute split information
-        try {
-          job.buildSplits(inputDir);
-        } catch (IOException e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
-                   + job.getUgi(), e);
-          monitor.submissionFailed(job.getJob());
-          return;
-        } catch (Exception e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
-                   + job.getUgi(), e);
-          monitor.submissionFailed(job.getJob());
-          return;
-        }
-        // Sleep until deadline
-        long nsDelay = job.getDelay(TimeUnit.NANOSECONDS);
-        while (nsDelay > 0) {
-          TimeUnit.NANOSECONDS.sleep(nsDelay);
-          nsDelay = job.getDelay(TimeUnit.NANOSECONDS);
-        }
-        try {
-          // submit job
-          monitor.add(job.call());
-          statistics.addJobStats(job.getJob(), job.getJobDesc());
-          LOG.debug("SUBMIT " + job + "@" + System.currentTimeMillis() +
-              " (" + job.getJob().getJobID() + ")");
-        } catch (IOException e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
-                   + job.getUgi(), e);
-          if (e.getCause() instanceof ClosedByInterruptException) {
-            throw new InterruptedException("Failed to submit " +
-                job.getJob().getJobName());
-          }
-          monitor.submissionFailed(job.getJob());
-        } catch (ClassNotFoundException e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName(), e);
-          monitor.submissionFailed(job.getJob());
-        }
-      } catch (InterruptedException e) {
-        // abort execution, remove splits if nesc
-        // TODO release ThdLoc
-        GridmixJob.pullDescription(job.id());
-        Thread.currentThread().interrupt();
-        monitor.submissionFailed(job.getJob());
-      } catch(Exception e) {
-        //Due to some exception job wasnt submitted.
-        LOG.info(" Job " + job.getJob().getJobID() + " submission failed " , e);
-        monitor.submissionFailed(job.getJob());
-      } finally {
-        sem.release();
-      }
-    }
-  }
-
-  /**
-   * Enqueue the job to be submitted per the deadline associated with it.
-   */
-  public void add(final GridmixJob job) throws InterruptedException {
-    final boolean addToQueue = !shutdown;
-    if (addToQueue) {
-      final SubmitTask task = new SubmitTask(job);
-      sem.acquire();
-      try {
-        sched.execute(task);
-      } catch (RejectedExecutionException e) {
-        sem.release();
-      }
-    }
-  }
-
-  /**
-   * (Re)scan the set of input files from which splits are derived.
-   * @throws java.io.IOException
-   */
-  public void refreshFilePool() throws IOException {
-    inputDir.refresh();
-  }
-
-  /**
-   * Does nothing, as the threadpool is already initialized and waiting for
-   * work from the upstream factory.
-   */
-  public void start() { }
-
-  /**
-   * Continue running until all queued jobs have been submitted to the
-   * cluster.
-   */
-  public void join(long millis) throws InterruptedException {
-    if (!shutdown) {
-      throw new IllegalStateException("Cannot wait for active submit thread");
-    }
-    sched.awaitTermination(millis, TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Finish all jobs pending submission, but do not accept new work.
-   */
-  public void shutdown() {
-    // complete pending tasks, but accept no new tasks
-    shutdown = true;
-    sched.shutdown();
-  }
-
-  /**
-   * Discard pending work, including precomputed work waiting to be
-   * submitted.
-   */
-  public void abort() {
-    //pendingJobs.clear();
-    shutdown = true;
-    sched.shutdownNow();
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
deleted file mode 100644
index 74cd9ad..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
+++ /dev/null
@@ -1,549 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskInputOutputContext;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-
-/**
- * Synthetic job generated from a trace description.
- */
-class LoadJob extends GridmixJob {
-
-  public static final Log LOG = LogFactory.getLog(LoadJob.class);
-
-  public LoadJob(final Configuration conf, long submissionMillis, 
-                 final JobStory jobdesc, Path outRoot, UserGroupInformation ugi,
-                 final int seq) throws IOException {
-    super(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
-  }
-
-  public Job call() throws IOException, InterruptedException,
-                           ClassNotFoundException {
-    ugi.doAs(
-      new PrivilegedExceptionAction<Job>() {
-        public Job run() throws IOException, ClassNotFoundException,
-                                InterruptedException {
-          job.setMapperClass(LoadMapper.class);
-          job.setReducerClass(LoadReducer.class);
-          job.setNumReduceTasks(jobdesc.getNumberReduces());
-          job.setMapOutputKeyClass(GridmixKey.class);
-          job.setMapOutputValueClass(GridmixRecord.class);
-          job.setSortComparatorClass(GridmixKey.Comparator.class);
-          job.setGroupingComparatorClass(SpecGroupingComparator.class);
-          job.setInputFormatClass(LoadInputFormat.class);
-          job.setOutputFormatClass(RawBytesOutputFormat.class);
-          job.setPartitionerClass(DraftPartitioner.class);
-          job.setJarByClass(LoadJob.class);
-          job.getConfiguration().setBoolean(Job.USED_GENERIC_PARSER, true);
-          FileOutputFormat.setOutputPath(job, outdir);
-          job.submit();
-          return job;
-        }
-      });
-
-    return job;
-  }
-
-  @Override
-  protected boolean canEmulateCompression() {
-    return true;
-  }
-  
-  /**
-   * This is a progress based resource usage matcher.
-   */
-  @SuppressWarnings("unchecked")
-  static class ResourceUsageMatcherRunner extends Thread {
-    private final ResourceUsageMatcher matcher;
-    private final Progressive progress;
-    private final long sleepTime;
-    private static final String SLEEP_CONFIG = 
-      "gridmix.emulators.resource-usage.sleep-duration";
-    private static final long DEFAULT_SLEEP_TIME = 100; // 100ms
-    
-    ResourceUsageMatcherRunner(final TaskInputOutputContext context, 
-                               ResourceUsageMetrics metrics) {
-      Configuration conf = context.getConfiguration();
-      
-      // set the resource calculator plugin
-      Class<? extends ResourceCalculatorPlugin> clazz =
-        conf.getClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN,
-                      null, ResourceCalculatorPlugin.class);
-      ResourceCalculatorPlugin plugin = 
-        ResourceCalculatorPlugin.getResourceCalculatorPlugin(clazz, conf);
-      
-      // set the other parameters
-      this.sleepTime = conf.getLong(SLEEP_CONFIG, DEFAULT_SLEEP_TIME);
-      progress = new Progressive() {
-        @Override
-        public float getProgress() {
-          return context.getProgress();
-        }
-      };
-      
-      // instantiate a resource-usage-matcher
-      matcher = new ResourceUsageMatcher();
-      matcher.configure(conf, plugin, metrics, progress);
-    }
-    
-    protected void match() throws Exception {
-      // match the resource usage
-      matcher.matchResourceUsage();
-    }
-    
-    @Override
-    public void run() {
-      LOG.info("Resource usage matcher thread started.");
-      try {
-        while (progress.getProgress() < 1) {
-          // match
-          match();
-          
-          // sleep for some time
-          try {
-            Thread.sleep(sleepTime);
-          } catch (Exception e) {}
-        }
-        
-        // match for progress = 1
-        match();
-        LOG.info("Resource usage emulation complete! Matcher exiting");
-      } catch (Exception e) {
-        LOG.info("Exception while running the resource-usage-emulation matcher"
-                 + " thread! Exiting.", e);
-      }
-    }
-  }
-  
-  // Makes sure that the TaskTracker doesn't kill the map/reduce tasks while
-  // they are emulating
-  private static class StatusReporter extends Thread {
-    private TaskAttemptContext context;
-    StatusReporter(TaskAttemptContext context) {
-      this.context = context;
-    }
-    
-    @Override
-    public void run() {
-      LOG.info("Status reporter thread started.");
-      try {
-        while (context.getProgress() < 1) {
-          // report progress
-          context.progress();
-
-          // sleep for some time
-          try {
-            Thread.sleep(100); // sleep for 100ms
-          } catch (Exception e) {}
-        }
-        
-        LOG.info("Status reporter thread exiting");
-      } catch (Exception e) {
-        LOG.info("Exception while running the status reporter thread!", e);
-      }
-    }
-  }
-  
-  public static class LoadMapper
-  extends Mapper<NullWritable, GridmixRecord, GridmixKey, GridmixRecord> {
-
-    private double acc;
-    private double ratio;
-    private final ArrayList<RecordFactory> reduces =
-      new ArrayList<RecordFactory>();
-    private final Random r = new Random();
-
-    private final GridmixKey key = new GridmixKey();
-    private final GridmixRecord val = new GridmixRecord();
-
-    private ResourceUsageMatcherRunner matcher = null;
-    private StatusReporter reporter = null;
-    
-    @Override
-    protected void setup(Context ctxt) 
-    throws IOException, InterruptedException {
-      final Configuration conf = ctxt.getConfiguration();
-      final LoadSplit split = (LoadSplit) ctxt.getInputSplit();
-      final int maps = split.getMapCount();
-      final long[] reduceBytes = split.getOutputBytes();
-      final long[] reduceRecords = split.getOutputRecords();
-
-      // enable gridmix map output record for compression
-      final boolean emulateMapOutputCompression = 
-        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
-        && conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);
-      float compressionRatio = 1.0f;
-      if (emulateMapOutputCompression) {
-        compressionRatio = 
-          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);
-        LOG.info("GridMix is configured to use a compression ratio of " 
-                 + compressionRatio + " for the map output data.");
-        key.setCompressibility(true, compressionRatio);
-        val.setCompressibility(true, compressionRatio);
-      }
-      
-      long totalRecords = 0L;
-      final int nReduces = ctxt.getNumReduceTasks();
-      if (nReduces > 0) {
-        int idx = 0;
-        int id = split.getId();
-        for (int i = 0; i < nReduces; ++i) {
-          final GridmixKey.Spec spec = new GridmixKey.Spec();
-          if (i == id) {
-            spec.bytes_out = split.getReduceBytes(idx);
-            spec.rec_out = split.getReduceRecords(idx);
-            spec.setResourceUsageSpecification(
-                   split.getReduceResourceUsageMetrics(idx));
-            ++idx;
-            id += maps;
-          }
-          
-          // set the map output bytes such that the final reduce input bytes 
-          // match the expected value obtained from the original job
-          long mapOutputBytes = reduceBytes[i];
-          if (emulateMapOutputCompression) {
-            mapOutputBytes /= compressionRatio;
-          }
-          reduces.add(new IntermediateRecordFactory(
-              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, 
-                                   5*1024),
-              i, reduceRecords[i], spec, conf));
-          totalRecords += reduceRecords[i];
-        }
-      } else {
-        long mapOutputBytes = reduceBytes[0];
-        if (emulateMapOutputCompression) {
-          mapOutputBytes /= compressionRatio;
-        }
-        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],
-                                         conf, 5*1024));
-        totalRecords = reduceRecords[0];
-      }
-      final long splitRecords = split.getInputRecords();
-      int missingRecSize = 
-        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);
-      final long inputRecords = 
-        (splitRecords <= 0 && split.getLength() >= 0)
-        ? Math.max(1, split.getLength() / missingRecSize)
-        : splitRecords;
-      ratio = totalRecords / (1.0 * inputRecords);
-      acc = 0.0;
-      
-      matcher = new ResourceUsageMatcherRunner(ctxt, 
-                      split.getMapResourceUsageMetrics());
-      matcher.setDaemon(true);
-      
-      // start the status reporter thread
-      reporter = new StatusReporter(ctxt);
-      reporter.setDaemon(true);
-      reporter.start();
-    }
-
-    @Override
-    public void map(NullWritable ignored, GridmixRecord rec,
-                    Context context) throws IOException, InterruptedException {
-      acc += ratio;
-      while (acc >= 1.0 && !reduces.isEmpty()) {
-        key.setSeed(r.nextLong());
-        val.setSeed(r.nextLong());
-        final int idx = r.nextInt(reduces.size());
-        final RecordFactory f = reduces.get(idx);
-        if (!f.next(key, val)) {
-          reduces.remove(idx);
-          continue;
-        }
-        context.write(key, val);
-        acc -= 1.0;
-        
-        // match inline
-        try {
-          matcher.match();
-        } catch (Exception e) {
-          LOG.debug("Error in resource usage emulation! Message: ", e);
-        }
-      }
-    }
-
-    @Override
-    public void cleanup(Context context) 
-    throws IOException, InterruptedException {
-      for (RecordFactory factory : reduces) {
-        key.setSeed(r.nextLong());
-        while (factory.next(key, val)) {
-          context.write(key, val);
-          key.setSeed(r.nextLong());
-          
-          // match inline
-          try {
-            matcher.match();
-          } catch (Exception e) {
-            LOG.debug("Error in resource usage emulation! Message: ", e);
-          }
-        }
-      }
-      
-      // start the matcher thread since the map phase ends here
-      matcher.start();
-    }
-  }
-
-  public static class LoadReducer
-  extends Reducer<GridmixKey,GridmixRecord,NullWritable,GridmixRecord> {
-
-    private final Random r = new Random();
-    private final GridmixRecord val = new GridmixRecord();
-
-    private double acc;
-    private double ratio;
-    private RecordFactory factory;
-
-    private ResourceUsageMatcherRunner matcher = null;
-    private StatusReporter reporter = null;
-    
-    @Override
-    protected void setup(Context context)
-    throws IOException, InterruptedException {
-      if (!context.nextKey() 
-          || context.getCurrentKey().getType() != GridmixKey.REDUCE_SPEC) {
-        throw new IOException("Missing reduce spec");
-      }
-      long outBytes = 0L;
-      long outRecords = 0L;
-      long inRecords = 0L;
-      ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-      for (GridmixRecord ignored : context.getValues()) {
-        final GridmixKey spec = context.getCurrentKey();
-        inRecords += spec.getReduceInputRecords();
-        outBytes += spec.getReduceOutputBytes();
-        outRecords += spec.getReduceOutputRecords();
-        if (spec.getReduceResourceUsageMetrics() != null) {
-          metrics = spec.getReduceResourceUsageMetrics();
-        }
-      }
-      if (0 == outRecords && inRecords > 0) {
-        LOG.info("Spec output bytes w/o records. Using input record count");
-        outRecords = inRecords;
-      }
-      
-      // enable gridmix reduce output record for compression
-      Configuration conf = context.getConfiguration();
-      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
-          && FileOutputFormat.getCompressOutput(context)) {
-        float compressionRatio = 
-          CompressionEmulationUtil
-            .getReduceOutputCompressionEmulationRatio(conf);
-        LOG.info("GridMix is configured to use a compression ratio of " 
-                 + compressionRatio + " for the reduce output data.");
-        val.setCompressibility(true, compressionRatio);
-        
-        // Set the actual output data size to make sure that the actual output 
-        // data size is same after compression
-        outBytes /= compressionRatio;
-      }
-      
-      factory =
-        new AvgRecordFactory(outBytes, outRecords, 
-                             context.getConfiguration(), 5*1024);
-      ratio = outRecords / (1.0 * inRecords);
-      acc = 0.0;
-      
-      matcher = new ResourceUsageMatcherRunner(context, metrics);
-      
-      // start the status reporter thread
-      reporter = new StatusReporter(context);
-      reporter.start();
-    }
-    @Override
-    protected void reduce(GridmixKey key, Iterable<GridmixRecord> values,
-                          Context context) 
-    throws IOException, InterruptedException {
-      for (GridmixRecord ignored : values) {
-        acc += ratio;
-        while (acc >= 1.0 && factory.next(null, val)) {
-          context.write(NullWritable.get(), val);
-          acc -= 1.0;
-          
-          // match inline
-          try {
-            matcher.match();
-          } catch (Exception e) {
-            LOG.debug("Error in resource usage emulation! Message: ", e);
-          }
-        }
-      }
-    }
-    @Override
-    protected void cleanup(Context context)
-    throws IOException, InterruptedException {
-      val.setSeed(r.nextLong());
-      while (factory.next(null, val)) {
-        context.write(NullWritable.get(), val);
-        val.setSeed(r.nextLong());
-        
-        // match inline
-        try {
-          matcher.match();
-        } catch (Exception e) {
-          LOG.debug("Error in resource usage emulation! Message: ", e);
-        }
-      }
-    }
-  }
-
-  static class LoadRecordReader
-  extends RecordReader<NullWritable,GridmixRecord> {
-
-    private RecordFactory factory;
-    private final Random r = new Random();
-    private final GridmixRecord val = new GridmixRecord();
-
-    public LoadRecordReader() { }
-
-    @Override
-    public void initialize(InputSplit genericSplit, TaskAttemptContext ctxt)
-    throws IOException, InterruptedException {
-      final LoadSplit split = (LoadSplit)genericSplit;
-      final Configuration conf = ctxt.getConfiguration();
-      factory = 
-        new ReadRecordFactory(split.getLength(), split.getInputRecords(), 
-                              new FileQueue(split, conf), conf);
-    }
-
-    @Override
-    public boolean nextKeyValue() throws IOException {
-      val.setSeed(r.nextLong());
-      return factory.next(null, val);
-    }
-    @Override
-    public float getProgress() throws IOException {
-      return factory.getProgress();
-    }
-    @Override
-    public NullWritable getCurrentKey() {
-      return NullWritable.get();
-    }
-    @Override
-    public GridmixRecord getCurrentValue() {
-      return val;
-    }
-    @Override
-    public void close() throws IOException {
-      factory.close();
-    }
-  }
-
-  static class LoadInputFormat
-  extends InputFormat<NullWritable,GridmixRecord> {
-
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      return pullDescription(jobCtxt);
-    }
-    @Override
-    public RecordReader<NullWritable,GridmixRecord> createRecordReader(
-        InputSplit split, final TaskAttemptContext taskContext)
-        throws IOException {
-      return new LoadRecordReader();
-    }
-  }
-
-  @Override
-  void buildSplits(FilePool inputDir) throws IOException {
-    long mapInputBytesTotal = 0L;
-    long mapOutputBytesTotal = 0L;
-    long mapOutputRecordsTotal = 0L;
-    final JobStory jobdesc = getJobDesc();
-    if (null == jobdesc) {
-      return;
-    }
-    final int maps = jobdesc.getNumberMaps();
-    final int reds = jobdesc.getNumberReduces();
-    for (int i = 0; i < maps; ++i) {
-      final TaskInfo info = jobdesc.getTaskInfo(TaskType.MAP, i);
-      mapInputBytesTotal += info.getInputBytes();
-      mapOutputBytesTotal += info.getOutputBytes();
-      mapOutputRecordsTotal += info.getOutputRecords();
-    }
-    final double[] reduceRecordRatio = new double[reds];
-    final double[] reduceByteRatio = new double[reds];
-    for (int i = 0; i < reds; ++i) {
-      final TaskInfo info = jobdesc.getTaskInfo(TaskType.REDUCE, i);
-      reduceByteRatio[i] = info.getInputBytes() / (1.0 * mapOutputBytesTotal);
-      reduceRecordRatio[i] =
-        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);
-    }
-    final InputStriper striper = new InputStriper(inputDir, mapInputBytesTotal);
-    final List<InputSplit> splits = new ArrayList<InputSplit>();
-    for (int i = 0; i < maps; ++i) {
-      final int nSpec = reds / maps + ((reds % maps) > i ? 1 : 0);
-      final long[] specBytes = new long[nSpec];
-      final long[] specRecords = new long[nSpec];
-      final ResourceUsageMetrics[] metrics = new ResourceUsageMetrics[nSpec];
-      for (int j = 0; j < nSpec; ++j) {
-        final TaskInfo info =
-          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);
-        specBytes[j] = info.getOutputBytes();
-        specRecords[j] = info.getOutputRecords();
-        metrics[j] = info.getResourceUsageMetrics();
-        if (LOG.isDebugEnabled()) {
-          LOG.debug(String.format("SPEC(%d) %d -> %d %d %d", id(), i,
-                    i + j * maps, info.getOutputRecords(), 
-                    info.getOutputBytes()));
-        }
-      }
-      final TaskInfo info = jobdesc.getTaskInfo(TaskType.MAP, i);
-      splits.add(
-        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), 
-                      maps, i, info.getInputBytes(), info.getInputRecords(),
-                      info.getOutputBytes(), info.getOutputRecords(),
-                      reduceByteRatio, reduceRecordRatio, specBytes, 
-                      specRecords, info.getResourceUsageMetrics(),
-                      metrics));
-    }
-    pushDescription(id(), splits);
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
deleted file mode 100644
index 27e7547..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
+++ /dev/null
@@ -1,180 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-class LoadSplit extends CombineFileSplit {
-  private int id;
-  private int nSpec;
-  private int maps;
-  private int reduces;
-  private long inputRecords;
-  private long outputBytes;
-  private long outputRecords;
-  private long maxMemory;
-  private double[] reduceBytes = new double[0];
-  private double[] reduceRecords = new double[0];
-
-  // Spec for reduces id mod this
-  private long[] reduceOutputBytes = new long[0];
-  private long[] reduceOutputRecords = new long[0];
-
-  private ResourceUsageMetrics mapMetrics;
-  private ResourceUsageMetrics[] reduceMetrics;
-
-  LoadSplit() {
-    super();
-  }
-
-  public LoadSplit(CombineFileSplit cfsplit, int maps, int id, long inputBytes, 
-                   long inputRecords, long outputBytes, long outputRecords, 
-                   double[] reduceBytes, double[] reduceRecords, 
-                   long[] reduceOutputBytes, long[] reduceOutputRecords,
-                   ResourceUsageMetrics metrics,
-                   ResourceUsageMetrics[] rMetrics)
-  throws IOException {
-    super(cfsplit);
-    this.id = id;
-    this.maps = maps;
-    reduces = reduceBytes.length;
-    this.inputRecords = inputRecords;
-    this.outputBytes = outputBytes;
-    this.outputRecords = outputRecords;
-    this.reduceBytes = reduceBytes;
-    this.reduceRecords = reduceRecords;
-    nSpec = reduceOutputBytes.length;
-    this.reduceOutputBytes = reduceOutputBytes;
-    this.reduceOutputRecords = reduceOutputRecords;
-    this.mapMetrics = metrics;
-    this.reduceMetrics = rMetrics;
-  }
-
-  public int getId() {
-    return id;
-  }
-  public int getMapCount() {
-    return maps;
-  }
-  public long getInputRecords() {
-    return inputRecords;
-  }
-  public long[] getOutputBytes() {
-    if (0 == reduces) {
-      return new long[] { outputBytes };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputBytes * reduceBytes[i]);
-    }
-    return ret;
-  }
-  public long[] getOutputRecords() {
-    if (0 == reduces) {
-      return new long[] { outputRecords };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputRecords * reduceRecords[i]);
-    }
-    return ret;
-  }
-  public long getReduceBytes(int i) {
-    return reduceOutputBytes[i];
-  }
-  public long getReduceRecords(int i) {
-    return reduceOutputRecords[i];
-  }
-  
-  public ResourceUsageMetrics getMapResourceUsageMetrics() {
-    return mapMetrics;
-  }
-  
-  public ResourceUsageMetrics getReduceResourceUsageMetrics(int i) {
-    return reduceMetrics[i];
-  }
-  
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    WritableUtils.writeVInt(out, id);
-    WritableUtils.writeVInt(out, maps);
-    WritableUtils.writeVLong(out, inputRecords);
-    WritableUtils.writeVLong(out, outputBytes);
-    WritableUtils.writeVLong(out, outputRecords);
-    WritableUtils.writeVLong(out, maxMemory);
-    WritableUtils.writeVInt(out, reduces);
-    for (int i = 0; i < reduces; ++i) {
-      out.writeDouble(reduceBytes[i]);
-      out.writeDouble(reduceRecords[i]);
-    }
-    WritableUtils.writeVInt(out, nSpec);
-    for (int i = 0; i < nSpec; ++i) {
-      WritableUtils.writeVLong(out, reduceOutputBytes[i]);
-      WritableUtils.writeVLong(out, reduceOutputRecords[i]);
-    }
-    mapMetrics.write(out);
-    int numReduceMetrics = (reduceMetrics == null) ? 0 : reduceMetrics.length;
-    WritableUtils.writeVInt(out, numReduceMetrics);
-    for (int i = 0; i < numReduceMetrics; ++i) {
-      reduceMetrics[i].write(out);
-    }
-  }
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    id = WritableUtils.readVInt(in);
-    maps = WritableUtils.readVInt(in);
-    inputRecords = WritableUtils.readVLong(in);
-    outputBytes = WritableUtils.readVLong(in);
-    outputRecords = WritableUtils.readVLong(in);
-    maxMemory = WritableUtils.readVLong(in);
-    reduces = WritableUtils.readVInt(in);
-    if (reduceBytes.length < reduces) {
-      reduceBytes = new double[reduces];
-      reduceRecords = new double[reduces];
-    }
-    for (int i = 0; i < reduces; ++i) {
-      reduceBytes[i] = in.readDouble();
-      reduceRecords[i] = in.readDouble();
-    }
-    nSpec = WritableUtils.readVInt(in);
-    if (reduceOutputBytes.length < nSpec) {
-      reduceOutputBytes = new long[nSpec];
-      reduceOutputRecords = new long[nSpec];
-    }
-    for (int i = 0; i < nSpec; ++i) {
-      reduceOutputBytes[i] = WritableUtils.readVLong(in);
-      reduceOutputRecords[i] = WritableUtils.readVLong(in);
-    }
-    mapMetrics = new ResourceUsageMetrics();
-    mapMetrics.readFields(in);
-    int numReduceMetrics = WritableUtils.readVInt(in);
-    reduceMetrics = new ResourceUsageMetrics[numReduceMetrics];
-    for (int i = 0; i < numReduceMetrics; ++i) {
-      reduceMetrics[i] = new ResourceUsageMetrics();
-      reduceMetrics[i].readFields(in);
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Progressive.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Progressive.java
deleted file mode 100644
index 4f1399e..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Progressive.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-/**
- * Used to track progress of tasks.
- */
-public interface Progressive {
-  public float getProgress();
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
deleted file mode 100644
index 497108a..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
+++ /dev/null
@@ -1,332 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Random;
-import java.net.URI;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PositionedReadable;
-import org.apache.hadoop.fs.Seekable;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * Pseudo local file system that generates random data for any file on the fly
- * instead of storing files on disk. So opening same file multiple times will
- * not give same file content. There are no directories in this file system
- * other than the root and all the files are under root i.e. "/". All file URIs
- * on pseudo local file system should be of the format <code>
- * pseudo:///&lt;name&gt;.&lt;fileSize&gt;</code> where name is a unique name
- * and &lt;fileSize&gt; is a number representing the size of the file in bytes.
- */
-class PseudoLocalFs extends FileSystem {
-  Path home;
-  /**
-   * The creation time and modification time of all files in
-   * {@link PseudoLocalFs} is same.
-   */
-  private static final long TIME = System.currentTimeMillis();
-  private static final String HOME_DIR = "/";
-  private static final long BLOCK_SIZE  = 4 * 1024 * 1024L; // 4 MB
-  private static final int DEFAULT_BUFFER_SIZE = 1024  * 1024; // 1MB
-
-  static final URI NAME = URI.create("pseudo:///");
-
-  PseudoLocalFs() {
-    this(new Path(HOME_DIR));
-  }
-
-  PseudoLocalFs(Path home) {
-    super();
-    this.home = home;
-  }
-
-  @Override
-  public URI getUri() {
-    return NAME;
-  }
-
-  @Override
-  public Path getHomeDirectory() {
-    return home;
-  }
-
-  @Override
-  public Path getWorkingDirectory() {
-    return getHomeDirectory();
-  }
-
-  /**
-   * Generates a valid pseudo local file path from the given <code>fileId</code>
-   * and <code>fileSize</code>.
-   * @param fileId unique file id string
-   * @param fileSize file size
-   * @return the generated relative path
-   */
-  static Path generateFilePath(String fileId, long fileSize) {
-    return new Path(fileId + "." + fileSize);
-  }
-
-  /**
-   * Creating a pseudo local file is nothing but validating the file path.
-   * Actual data of the file is generated on the fly when client tries to open
-   * the file for reading.
-   * @param path file path to be created
-   */
-  @Override
-  public FSDataOutputStream create(Path path) throws IOException {
-    try {
-      validateFileNameFormat(path);
-    } catch (FileNotFoundException e) {
-      throw new IOException("File creation failed for " + path);
-    }
-    return null;
-  }
-
-  /**
-   * Validate if the path provided is of expected format of Pseudo Local File
-   * System based files.
-   * @param path file path
-   * @return the file size
-   * @throws FileNotFoundException
-   */
-  long validateFileNameFormat(Path path) throws FileNotFoundException {
-    path = path.makeQualified(this);
-    boolean valid = true;
-    long fileSize = 0;
-    if (!path.toUri().getScheme().equals(getUri().getScheme())) {
-      valid = false;
-    } else {
-      String[] parts = path.toUri().getPath().split("\\.");
-      try {
-        fileSize = Long.valueOf(parts[parts.length - 1]);
-        valid = (fileSize >= 0);
-      } catch (NumberFormatException e) {
-        valid = false;
-      }
-    }
-    if (!valid) {
-      throw new FileNotFoundException("File " + path
-          + " does not exist in pseudo local file system");
-    }
-    return fileSize;
-  }
-
-  /**
-   * @See create(Path) for details
-   */
-  @Override
-  public FSDataInputStream open(Path path, int bufferSize) throws IOException {
-    long fileSize = validateFileNameFormat(path);
-    InputStream in = new RandomInputStream(fileSize, bufferSize);
-    return new FSDataInputStream(in);
-  }
-
-  /**
-   * @See create(Path) for details
-   */
-  @Override
-  public FSDataInputStream open(Path path) throws IOException {
-    return open(path, DEFAULT_BUFFER_SIZE);
-  }
-
-  @Override
-  public FileStatus getFileStatus(Path path) throws IOException {
-    long fileSize = validateFileNameFormat(path);
-    return new FileStatus(fileSize, false, 1, BLOCK_SIZE, TIME, path);
-  }
-
-  @Override
-  public boolean exists(Path path) {
-    try{
-      validateFileNameFormat(path);
-    } catch (FileNotFoundException e) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public FSDataOutputStream create(Path path, FsPermission permission,
-      boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
-    return create(path);
-  }
-
-  @Override
-  public FileStatus[] listStatus(Path path) throws FileNotFoundException,
-      IOException {
-    return new FileStatus[] {getFileStatus(path)};
-  }
-
-  /**
-   * Input Stream that generates specified number of random bytes.
-   */
-  static class RandomInputStream extends InputStream
-      implements Seekable, PositionedReadable {
-
-    private final Random r = new Random();
-    private BytesWritable val = null;
-    private int positionInVal = 0;// current position in the buffer 'val'
-
-    private long totalSize = 0;// total number of random bytes to be generated
-    private long curPos = 0;// current position in this stream
-
-    /**
-     * @param size total number of random bytes to be generated in this stream
-     * @param bufferSize the buffer size. An internal buffer array of length
-     * <code>bufferSize</code> is created. If <code>bufferSize</code> is not a
-     * positive number, then a default value of 1MB is used.
-     */
-    RandomInputStream(long size, int bufferSize) {
-      totalSize = size;
-      if (bufferSize <= 0) {
-        bufferSize = DEFAULT_BUFFER_SIZE;
-      }
-      val = new BytesWritable(new byte[bufferSize]);
-    }
-
-    @Override
-    public int read() throws IOException {
-      byte[] b = new byte[1];
-      if (curPos < totalSize) {
-        if (positionInVal < val.getLength()) {// use buffered byte
-          b[0] = val.getBytes()[positionInVal++];
-          ++curPos;
-        } else {// generate data
-          int num = read(b);
-          if (num < 0) {
-            return num;
-          }
-        }
-      } else {
-        return -1;
-      }
-      return b[0];
-    }
-
-    @Override
-    public int read(byte[] bytes) throws IOException {
-      return read(bytes, 0, bytes.length);
-    }
-
-    @Override
-    public int read(byte[] bytes, int off, int len) throws IOException {
-      if (curPos == totalSize) {
-        return -1;// EOF
-      }
-      int numBytes = len;
-      if (numBytes > (totalSize - curPos)) {// position in file is close to EOF
-        numBytes = (int)(totalSize - curPos);
-      }
-      if (numBytes > (val.getLength() - positionInVal)) {
-        // need to generate data into val
-        r.nextBytes(val.getBytes());
-        positionInVal = 0;
-      }
-
-      System.arraycopy(val.getBytes(), positionInVal, bytes, off, numBytes);
-      curPos += numBytes;
-      positionInVal += numBytes;
-      return numBytes;
-    }
-
-    @Override
-    public int available() {
-      return (int)(val.getLength() - positionInVal);
-    }
-
-    @Override
-    public int read(long position, byte[] buffer, int offset, int length)
-        throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer, int offset, int length)
-        throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    /**
-     * Get the current position in this stream/pseudo-file
-     * @return the position in this stream/pseudo-file
-     * @throws IOException
-     */
-    @Override
-    public long getPos() throws IOException {
-      return curPos;
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public boolean seekToNewSource(long targetPos) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public FSDataOutputStream append(Path path, int bufferSize,
-      Progressable progress) throws IOException {
-    throw new UnsupportedOperationException("Append is not supported"
-        + " in pseudo local file system.");
-  }
-
-  @Override
-  public boolean mkdirs(Path f, FsPermission permission) throws IOException {
-    throw new UnsupportedOperationException("Mkdirs is not supported"
-        + " in pseudo local file system.");
-  }
-
-  @Override
-  public boolean rename(Path src, Path dst) throws IOException {
-    throw new UnsupportedOperationException("Rename is not supported"
-        + " in pseudo local file system.");
-  }
-
-  @Override
-  public boolean delete(Path path, boolean recursive) {
-    throw new UnsupportedOperationException("File deletion is not supported "
-        + "in pseudo local file system.");
-  }
-
-  @Override
-  public void setWorkingDirectory(Path newDir) {
-    throw new UnsupportedOperationException("SetWorkingDirectory "
-        + "is not supported in pseudo local file system.");
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
deleted file mode 100644
index 95e8f90..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-/**
- * Random algorithms.
- */
-public class RandomAlgorithms {
-  
-  private interface IndexMapper {
-    int get(int pos);
-    void swap(int a, int b);
-    int getSize();
-    void reset();
-  }
-
-  /**
-   * A sparse index mapping table - useful when we want to
-   * non-destructively permute a small fraction of a large array.
-   */
-  private static class SparseIndexMapper implements IndexMapper {
-    Map<Integer, Integer> mapping = new HashMap<Integer, Integer>();
-    int size;
-    
-    SparseIndexMapper(int size) { 
-      this.size = size;
-    }
-    
-    public int get(int pos) {
-      Integer mapped = mapping.get(pos);
-      if (mapped == null) return pos;
-      return mapped;
-    }
-
-    public void swap(int a, int b) {
-      if (a == b) return;
-      int valA = get(a);
-      int valB = get(b);
-      if (b == valA) {
-        mapping.remove(b);
-      } else {
-        mapping.put(b, valA);
-      }
-      if (a == valB) {
-        mapping.remove(a);
-      } else {
-        mapping.put(a, valB);
-      }
-    }
-    
-    public int getSize() {
-      return size;
-    }
-    
-    public void reset() {
-      mapping.clear();
-    }
-  }
-
-  /**
-   * A dense index mapping table - useful when we want to
-   * non-destructively permute a large fraction of an array.
-   */
-  private static class DenseIndexMapper implements IndexMapper {
-    int[] mapping;
-
-    DenseIndexMapper(int size) {
-      mapping = new int[size];
-      for (int i=0; i<size; ++i) {
-        mapping[i] = i;
-      }
-    }
-
-    public int get(int pos) {
-      if ( (pos < 0) || (pos>=mapping.length) ) {
-        throw new IndexOutOfBoundsException();
-      }
-      return mapping[pos];
-    }
-
-    public void swap(int a, int b) {
-      if (a == b) return;
-      int valA = get(a);
-      int valB = get(b);
-      mapping[a]=valB;
-      mapping[b]=valA;
-    }
-    
-    public int getSize() {
-      return mapping.length;
-    }
-    
-    public void reset() {
-      return;
-    }
-  }
-
-  /**
-   * Iteratively pick random numbers from pool 0..n-1. Each number can only be
-   * picked once.
-   */
-  public static class Selector {
-    private IndexMapper mapping;
-    private int n;
-    private Random rand;
-
-    /**
-     * Constructor.
-     * 
-     * @param n
-     *          The pool of integers: 0..n-1.
-     * @param selPcnt
-     *          Percentage of selected numbers. This is just a hint for internal
-     *          memory optimization.
-     * @param rand
-     *          Random number generator.
-     */
-    public Selector(int n, double selPcnt, Random rand) {
-      if (n <= 0) {
-        throw new IllegalArgumentException("n should be positive");
-      }
-      
-      boolean sparse = (n > 200) && (selPcnt < 0.1);
-      
-      this.n = n;
-      mapping = (sparse) ? new SparseIndexMapper(n) : new DenseIndexMapper(n);
-      this.rand = rand;
-    }
-    
-    /**
-     * Select the next random number.
-     * @return Random number selected. Or -1 if the remaining pool is empty.
-     */
-    public int next() {
-      switch (n) {
-      case 0: return -1;
-      case 1: 
-      {
-        int index = mapping.get(0);
-        --n;
-        return index;
-      }
-      default:
-      {
-        int pos = rand.nextInt(n);
-        int index = mapping.get(pos);
-        mapping.swap(pos, --n);
-        return index;
-      }
-      }
-    }
-
-    /**
-     * Get the remaining random number pool size.
-     */
-    public int getPoolSize() {
-      return n;
-    }
-    
-    /**
-     * Reset the selector for reuse usage.
-     */
-    public void reset() {
-      mapping.reset();
-      n = mapping.getSize();
-    }
-  }
-  
-  
-  /**
-   * Selecting m random integers from 0..n-1.
-   * @return An array of selected integers.
-   */
-  public static int[] select(int m, int n, Random rand) {
-    if (m >= n) {
-      int[] ret = new int[n];
-      for (int i=0; i<n; ++i) {
-        ret[i] = i;
-      }
-      return ret;
-    }
-    
-    Selector selector = new Selector(n, (float)m/n, rand);
-    int[] selected = new int[m];
-    for (int i=0; i<m; ++i) {
-      selected[i] = selector.next();
-    }
-    return selected;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
deleted file mode 100644
index 877d434..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
+++ /dev/null
@@ -1,147 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.Arrays;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.commons.lang.RandomStringUtils;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * A random text generator. The words are simply sequences of alphabets.
- */
-class RandomTextDataGenerator {
-  static final Log LOG = LogFactory.getLog(RandomTextDataGenerator.class);
-  
-  /**
-   * Configuration key for random text data generator's list size.
-   */
-  static final String GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE = 
-    "gridmix.datagenerator.randomtext.listsize";
-  
-  /**
-   * Configuration key for random text data generator's word size.
-   */
-  static final String GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE = 
-    "gridmix.datagenerator.randomtext.wordsize";
-  
-  /**
-   * Default random text data generator's list size.
-   */
-  static final int DEFAULT_LIST_SIZE = 200;
-  
-  /**
-   * Default random text data generator's word size.
-   */
-  static final int DEFAULT_WORD_SIZE = 10;
-  
-  /**
-   * Default random text data generator's seed.
-   */
-  static final long DEFAULT_SEED = 0L;
-  
-  /**
-   * A list of random words
-   */
-  private String[] words;
-  private Random random;
-  
-  /**
-   * Constructor for {@link RandomTextDataGenerator} with default seed.
-   * @param size the total number of words to consider.
-   * @param wordSize Size of each word
-   */
-  RandomTextDataGenerator(int size, int wordSize) {
-    this(size, DEFAULT_SEED , wordSize);
-  }
-  
-  /**
-   * Constructor for {@link RandomTextDataGenerator}.
-   * @param size the total number of words to consider.
-   * @param seed Random number generator seed for repeatability
-   * @param wordSize Size of each word
-   */
-  RandomTextDataGenerator(int size, Long seed, int wordSize) {
-    random = new Random(seed);
-    words = new String[size];
-    
-    //TODO change the default with the actual stats
-    //TODO do u need varied sized words?
-    for (int i = 0; i < size; ++i) {
-      words[i] = 
-        RandomStringUtils.random(wordSize, 0, 0, true, false, null, random);
-    }
-  }
-  
-  /**
-   * Get the configured random text data generator's list size.
-   */
-  static int getRandomTextDataGeneratorListSize(Configuration conf) {
-    return conf.getInt(GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, DEFAULT_LIST_SIZE);
-  }
-  
-  /**
-   * Set the random text data generator's list size.
-   */
-  static void setRandomTextDataGeneratorListSize(Configuration conf, 
-                                                 int listSize) {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Random text data generator is configured to use a dictionary " 
-                + " with " + listSize + " words");
-    }
-    conf.setInt(GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, listSize);
-  }
-  
-  /**
-   * Get the configured random text data generator word size.
-   */
-  static int getRandomTextDataGeneratorWordSize(Configuration conf) {
-    return conf.getInt(GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, DEFAULT_WORD_SIZE);
-  }
-  
-  /**
-   * Set the random text data generator word size.
-   */
-  static void setRandomTextDataGeneratorWordSize(Configuration conf, 
-                                                 int wordSize) {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Random text data generator is configured to use a dictionary " 
-                + " with words of length " + wordSize);
-    }
-    conf.setInt(GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, wordSize);
-  }
-  
-  /**
-   * Returns a randomly selected word from a list of random words.
-   */
-  String getRandomWord() {
-    int index = random.nextInt(words.length);
-    return words[index];
-  }
-  
-  /**
-   * This is mainly for testing.
-   */
-  List<String> getRandomWords() {
-    return Arrays.asList(words);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
deleted file mode 100644
index 2cb806e..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IOUtils;
-
-/**
- * For every record consumed, read key + val bytes from the stream provided.
- */
-class ReadRecordFactory extends RecordFactory {
-
-  /**
-   * Size of internal, scratch buffer to read from internal stream.
-   */
-  public static final String GRIDMIX_READ_BUF_SIZE = "gridmix.read.buffer.size";
-
-  private final byte[] buf;
-  private final InputStream src;
-  private final RecordFactory factory;
-
-  /**
-   * @param targetBytes Expected byte count.
-   * @param targetRecords Expected record count.
-   * @param src Stream to read bytes.
-   * @param conf Used to establish read buffer size. @see #GRIDMIX_READ_BUF_SIZE
-   */
-  public ReadRecordFactory(long targetBytes, long targetRecords,
-      InputStream src, Configuration conf) {
-    this(new AvgRecordFactory(targetBytes, targetRecords, conf), src, conf);
-  }
-
-  /**
-   * @param factory Factory to draw record sizes.
-   * @param src Stream to read bytes.
-   * @param conf Used to establish read buffer size. @see #GRIDMIX_READ_BUF_SIZE
-   */
-  public ReadRecordFactory(RecordFactory factory, InputStream src,
-      Configuration conf) {
-    this.src = src;
-    this.factory = factory;
-    buf = new byte[conf.getInt(GRIDMIX_READ_BUF_SIZE, 64 * 1024)];
-  }
-
-  @Override
-  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-    if (!factory.next(key, val)) {
-      return false;
-    }
-    for (int len = (null == key ? 0 : key.getSize()) + val.getSize();
-         len > 0; len -= buf.length) {
-      IOUtils.readFully(src, buf, 0, Math.min(buf.length, len));
-    }
-    return true;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return factory.getProgress();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.cleanup(null, src);
-    factory.close();
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
deleted file mode 100644
index 7abcf78..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-/**
- * Interface for producing records as inputs and outputs to tasks.
- */
-abstract class RecordFactory implements Closeable {
-
-  /**
-   * Transform the given record or perform some operation.
-   * @return true if the record should be emitted.
-   */
-  public abstract boolean next(GridmixKey key, GridmixRecord val)
-    throws IOException;
-
-  /**
-   * Estimate of exhausted record capacity.
-   */
-  public abstract float getProgress() throws IOException;
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
deleted file mode 100644
index d1b1481..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-
- class ReplayJobFactory extends JobFactory<Statistics.ClusterStats> {
-  public static final Log LOG = LogFactory.getLog(ReplayJobFactory.class);
-
-  /**
-   * Creating a new instance does not start the thread.
-   *
-   * @param submitter   Component to which deserialized jobs are passed
-   * @param jobProducer Job story producer
-   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch     Directory into which to write output from simulated jobs
-   * @param conf        Config passed to all jobs to be submitted
-   * @param startFlag   Latch released from main to start pipeline
-   * @param resolver
-   * @throws java.io.IOException
-   */
-  public ReplayJobFactory(
-    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
-    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
-    throws IOException {
-    super(submitter, jobProducer, scratch, conf, startFlag, resolver);
-  }
-
-   
-    @Override
-  public Thread createReaderThread() {
-    return new ReplayReaderThread("ReplayJobFactory");
-  }
-
-   /**
-    * @param item
-    */
-   public void update(Statistics.ClusterStats item) {
-   }
-
-   private class ReplayReaderThread extends Thread {
-
-    public ReplayReaderThread(String threadName) {
-      super(threadName);
-    }
-
-
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-        final long initTime = TimeUnit.MILLISECONDS.convert(
-          System.nanoTime(), TimeUnit.NANOSECONDS);
-        LOG.info("START REPLAY @ " + initTime);
-        long first = -1;
-        long last = -1;
-        while (!Thread.currentThread().isInterrupted()) {
-          try {
-            final JobStory job = getNextJobFiltered();
-            if (null == job) {
-              return;
-            }
-            if (first < 0) {
-              first = job.getSubmissionTime();
-            }
-            final long current = job.getSubmissionTime();
-            if (current < last) {
-              LOG.warn("Job " + job.getJobID() + " out of order");
-              continue;
-            }
-            last = current;
-            submitter.add(
-              jobCreator.createGridmixJob(
-                conf, initTime + Math.round(rateFactor * (current - first)),
-                job, scratch,
-                userResolver.getTargetUgi(
-                  UserGroupInformation.createRemoteUser(job.getUser())), 
-                sequence.getAndIncrement()));
-          } catch (IOException e) {
-            error = e;
-            return;
-          }
-        }
-      } catch (InterruptedException e) {
-        // exit thread; ignore any jobs remaining in the trace
-      } finally {
-        IOUtils.cleanup(null, jobProducer);
-      }
-    }
-  }
-
-   /**
-    * Start the reader thread, wait for latch if necessary.
-    */
-   @Override
-   public void start() {
-     this.rThread.start();
-   }
-
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
deleted file mode 100644
index db643de..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
+++ /dev/null
@@ -1,138 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.LineReader;
-
-import java.io.IOException;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-
-public class RoundRobinUserResolver implements UserResolver {
-  public static final Log LOG = LogFactory.getLog(RoundRobinUserResolver.class);
-
-  private int uidx = 0;
-  private List<UserGroupInformation> users = Collections.emptyList();
-
-  /**
-   *  Mapping between user names of original cluster and UGIs of proxy users of
-   *  simulated cluster
-   */
-  private final HashMap<String,UserGroupInformation> usercache =
-      new HashMap<String,UserGroupInformation>();
-  
-  /**
-   * Userlist assumes one user per line.
-   * Each line in users-list-file is of the form &lt;username&gt;[,group]* 
-   * <br> Group names are ignored(they are not parsed at all).
-   */
-  private List<UserGroupInformation> parseUserList(URI userUri, 
-                                                   Configuration conf) 
-  throws IOException {
-    if (null == userUri) {
-      return Collections.emptyList();
-    }
-    
-    final Path userloc = new Path(userUri.toString());
-    final Text rawUgi = new Text();
-    final FileSystem fs = userloc.getFileSystem(conf);
-    final ArrayList<UserGroupInformation> ugiList =
-        new ArrayList<UserGroupInformation>();
-
-    LineReader in = null;
-    try {
-      in = new LineReader(fs.open(userloc));
-      while (in.readLine(rawUgi) > 0) {//line is of the form username[,group]*
-        // e is end position of user name in this line
-        int e = rawUgi.find(",");
-        if (rawUgi.getLength() == 0 || e == 0) {
-          throw new IOException("Missing username: " + rawUgi);
-        }
-        if (e == -1) {
-          e = rawUgi.getLength();
-        }
-        final String username = Text.decode(rawUgi.getBytes(), 0, e);
-        UserGroupInformation ugi = null;
-        try {
-          ugi = UserGroupInformation.createProxyUser(username,
-                    UserGroupInformation.getLoginUser());
-        } catch (IOException ioe) {
-          LOG.error("Error while creating a proxy user " ,ioe);
-        }
-        if (ugi != null) {
-          ugiList.add(ugi);
-        }
-        // No need to parse groups, even if they exist. Go to next line
-      }
-    } finally {
-      if (in != null) {
-        in.close();
-      }
-    }
-    return ugiList;
-  }
-
-  @Override
-  public synchronized boolean setTargetUsers(URI userloc, Configuration conf)
-  throws IOException {
-    uidx = 0;
-    users = parseUserList(userloc, conf);
-    if (users.size() == 0) {
-      throw new IOException(buildEmptyUsersErrorMsg(userloc));
-    }
-    usercache.clear();
-    return true;
-  }
-
-  static String buildEmptyUsersErrorMsg(URI userloc) {
-    return "Empty user list is not allowed for RoundRobinUserResolver. Provided"
-    + " user resource URI '" + userloc + "' resulted in an empty user list.";
-  }
-
-  @Override
-  public synchronized UserGroupInformation getTargetUgi(
-    UserGroupInformation ugi) {
-    // UGI of proxy user
-    UserGroupInformation targetUGI = usercache.get(ugi.getUserName());
-    if (targetUGI == null) {
-      targetUGI = users.get(uidx++ % users.size());
-      usercache.put(ugi.getUserName(), targetUGI);
-    }
-    return targetUGI;
-  }
-
-  /**
-   * {@inheritDoc}
-   * <p>
-   * {@link RoundRobinUserResolver} needs to map the users in the
-   * trace to the provided list of target users. So user list is needed.
-   */
-  public boolean needsTargetUsersList() {
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
deleted file mode 100644
index 3301cbd..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
+++ /dev/null
@@ -1,178 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.locks.Condition;
-
-public class SerialJobFactory extends JobFactory<JobStats> {
-
-  public static final Log LOG = LogFactory.getLog(SerialJobFactory.class);
-  private final Condition jobCompleted = lock.newCondition();
-
-  /**
-   * Creating a new instance does not start the thread.
-   *
-   * @param submitter   Component to which deserialized jobs are passed
-   * @param jobProducer Job story producer
-   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch     Directory into which to write output from simulated jobs
-   * @param conf        Config passed to all jobs to be submitted
-   * @param startFlag   Latch released from main to start pipeline
-   * @throws java.io.IOException
-   */
-  public SerialJobFactory(
-    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
-    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
-    throws IOException {
-    super(submitter, jobProducer, scratch, conf, startFlag, resolver);
-  }
-
-  @Override
-  public Thread createReaderThread() {
-    return new SerialReaderThread("SerialJobFactory");
-  }
-
-  private class SerialReaderThread extends Thread {
-
-    public SerialReaderThread(String threadName) {
-      super(threadName);
-    }
-
-    /**
-     * SERIAL : In this scenario .  method waits on notification ,
-     * that a submitted job is actually completed. Logic is simple.
-     * ===
-     * while(true) {
-     * wait till previousjob is completed.
-     * break;
-     * }
-     * submit newJob.
-     * previousJob = newJob;
-     * ==
-     */
-    @Override
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-        LOG.info("START SERIAL @ " + System.currentTimeMillis());
-        GridmixJob prevJob;
-        while (!Thread.currentThread().isInterrupted()) {
-          final JobStory job;
-          try {
-            job = getNextJobFiltered();
-            if (null == job) {
-              return;
-            }
-            if (LOG.isDebugEnabled()) {
-              LOG.debug(
-                "Serial mode submitting job " + job.getName());
-            }
-            prevJob = jobCreator.createGridmixJob(
-              conf, 0L, job, scratch, 
-              userResolver.getTargetUgi(
-                UserGroupInformation.createRemoteUser(job.getUser())),
-              sequence.getAndIncrement());
-
-            lock.lock();
-            try {
-              LOG.info(" Submitted the job " + prevJob);
-              submitter.add(prevJob);
-            } finally {
-              lock.unlock();
-            }
-          } catch (IOException e) {
-            error = e;
-            //If submission of current job fails , try to submit the next job.
-            return;
-          }
-
-          if (prevJob != null) {
-            //Wait till previous job submitted is completed.
-            lock.lock();
-            try {
-              while (true) {
-                try {
-                  jobCompleted.await();
-                } catch (InterruptedException ie) {
-                  LOG.error(
-                    " Error in SerialJobFactory while waiting for job completion ",
-                    ie);
-                  return;
-                }
-                if (LOG.isDebugEnabled()) {
-                  LOG.info(" job " + job.getName() + " completed ");
-                }
-                break;
-              }
-            } finally {
-              lock.unlock();
-            }
-            prevJob = null;
-          }
-        }
-      } catch (InterruptedException e) {
-        return;
-      } finally {
-        IOUtils.cleanup(null, jobProducer);
-      }
-    }
-
-  }
-
-  /**
-   * SERIAL. Once you get notification from StatsCollector about the job
-   * completion ,simply notify the waiting thread.
-   *
-   * @param item
-   */
-  @Override
-  public void update(Statistics.JobStats item) {
-    //simply notify in case of serial submissions. We are just bothered
-    //if submitted job is completed or not.
-    lock.lock();
-    try {
-      jobCompleted.signalAll();
-    } finally {
-      lock.unlock();
-    }
-  }
-
-  /**
-   * Start the reader thread, wait for latch if necessary.
-   */
-  @Override
-  public void start() {
-    LOG.info(" Starting Serial submission ");
-    this.rThread.start();
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SleepJob.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
deleted file mode 100644
index a9f2999..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
+++ /dev/null
@@ -1,411 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.TaskStatus;
-import org.apache.hadoop.mapred.gridmix.RandomAlgorithms.Selector;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-public class SleepJob extends GridmixJob {
-  public static final Log LOG = LogFactory.getLog(SleepJob.class);
-  private static final ThreadLocal <Random> rand = 
-    new ThreadLocal <Random> () {
-        @Override protected Random initialValue() {
-            return new Random();
-    }
-  };
-  
-  public static final String SLEEPJOB_MAPTASK_ONLY="gridmix.sleep.maptask-only";
-  private final boolean mapTasksOnly;
-  private final int fakeLocations;
-  private final String[] hosts;
-  private final Selector selector;
-  
-  /**
-   * Interval at which to report progress, in seconds.
-   */
-  public static final String GRIDMIX_SLEEP_INTERVAL = "gridmix.sleep.interval";
-  public static final String GRIDMIX_SLEEP_MAX_MAP_TIME = 
-    "gridmix.sleep.max-map-time";
-  public static final String GRIDMIX_SLEEP_MAX_REDUCE_TIME = 
-    "gridmix.sleep.max-reduce-time";
-
-  private final long mapMaxSleepTime, reduceMaxSleepTime;
-
-  public SleepJob(Configuration conf, long submissionMillis, JobStory jobdesc,
-      Path outRoot, UserGroupInformation ugi, int seq, int numLocations,
-      String[] hosts) throws IOException {
-    super(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
-    this.fakeLocations = numLocations;
-    this.hosts = hosts;
-    this.selector = (fakeLocations > 0)? new Selector(hosts.length, (float) fakeLocations
-        / hosts.length, rand.get()) : null;
-    this.mapTasksOnly = conf.getBoolean(SLEEPJOB_MAPTASK_ONLY, false);
-    mapMaxSleepTime = conf.getLong(GRIDMIX_SLEEP_MAX_MAP_TIME, Long.MAX_VALUE);
-    reduceMaxSleepTime = conf.getLong(GRIDMIX_SLEEP_MAX_REDUCE_TIME,
-        Long.MAX_VALUE);
-  }
-
-  @Override
-  protected boolean canEmulateCompression() {
-    return false;
-  }
-  
-  @Override
-  public Job call()
-    throws IOException, InterruptedException, ClassNotFoundException {
-    ugi.doAs(
-      new PrivilegedExceptionAction<Job>() {
-        public Job run()
-          throws IOException, ClassNotFoundException, InterruptedException {
-          job.setMapperClass(SleepMapper.class);
-          job.setReducerClass(SleepReducer.class);
-          job.setNumReduceTasks((mapTasksOnly) ? 0 : jobdesc.getNumberReduces());
-          job.setMapOutputKeyClass(GridmixKey.class);
-          job.setMapOutputValueClass(NullWritable.class);
-          job.setSortComparatorClass(GridmixKey.Comparator.class);
-          job.setGroupingComparatorClass(SpecGroupingComparator.class);
-          job.setInputFormatClass(SleepInputFormat.class);
-          job.setOutputFormatClass(NullOutputFormat.class);
-          job.setPartitionerClass(DraftPartitioner.class);
-          job.setJarByClass(SleepJob.class);
-          job.getConfiguration().setBoolean(Job.USED_GENERIC_PARSER, true);
-          job.submit();
-          return job;
-
-        }
-      });
-
-    return job;
-  }
-
-  public static class SleepMapper
-  extends Mapper<LongWritable, LongWritable, GridmixKey, NullWritable> {
-
-    @Override
-    public void map(LongWritable key, LongWritable value, Context context)
-    throws IOException, InterruptedException {
-      context.setStatus("Sleeping... " + value.get() + " ms left");
-      long now = System.currentTimeMillis();
-      if (now < key.get()) {
-        TimeUnit.MILLISECONDS.sleep(key.get() - now);
-      }
-    }
-
-    @Override
-    public void cleanup(Context context)
-    throws IOException, InterruptedException {
-      final int nReds = context.getNumReduceTasks();
-      if (nReds > 0) {
-        final SleepSplit split = (SleepSplit) context.getInputSplit();
-        int id = split.getId();
-        final int nMaps = split.getNumMaps();
-        //This is a hack to pass the sleep duration via Gridmix key
-        //TODO: We need to come up with better solution for this.
-        
-        final GridmixKey key = new GridmixKey(GridmixKey.REDUCE_SPEC, 0, 0L);
-        for (int i = id, idx = 0; i < nReds; i += nMaps) {
-          key.setPartition(i);
-          key.setReduceOutputBytes(split.getReduceDurations(idx++));
-          id += nReds;
-          context.write(key, NullWritable.get());
-        }
-      }
-    }
-
-  }
-
-  public static class SleepReducer
-  extends Reducer<GridmixKey, NullWritable, NullWritable, NullWritable> {
-
-    private long duration = 0L;
-
-    @Override
-    protected void setup(Context context)
-    throws IOException, InterruptedException {
-      if (!context.nextKey() ||
-        context.getCurrentKey().getType() != GridmixKey.REDUCE_SPEC) {
-        throw new IOException("Missing reduce spec");
-      }
-      for (NullWritable ignored : context.getValues()) {
-        final GridmixKey spec = context.getCurrentKey();
-        duration += spec.getReduceOutputBytes();
-      }
-      long sleepInterval = 
-        context.getConfiguration().getLong(GRIDMIX_SLEEP_INTERVAL, 5);
-      final long RINTERVAL = 
-        TimeUnit.MILLISECONDS.convert(sleepInterval, TimeUnit.SECONDS);
-      //This is to stop accumulating deviation from expected sleep time
-      //over a period of time.
-      long start = System.currentTimeMillis();
-      long slept = 0L;
-      long sleep = 0L;
-      while (slept < duration) {
-        final long rem = duration - slept;
-        sleep = Math.min(rem, RINTERVAL);
-        context.setStatus("Sleeping... " + rem + " ms left");
-        TimeUnit.MILLISECONDS.sleep(sleep);
-        slept = System.currentTimeMillis() - start;
-      }
-    }
-
-    @Override
-    protected void cleanup(Context context)
-    throws IOException, InterruptedException {
-      final String msg = "Slept for " + duration;
-      LOG.info(msg);
-      context.setStatus(msg);
-    }
-  }
-
-  public static class SleepInputFormat
-  extends InputFormat<LongWritable, LongWritable> {
-
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      return pullDescription(jobCtxt);
-    }
-
-    @Override
-    public RecordReader<LongWritable, LongWritable> createRecordReader(
-      InputSplit split, final TaskAttemptContext context)
-      throws IOException, InterruptedException {
-      final long duration = split.getLength();
-      long sleepInterval = 
-    	  context.getConfiguration().getLong(GRIDMIX_SLEEP_INTERVAL, 5);
-      final long RINTERVAL = 
-        TimeUnit.MILLISECONDS.convert(sleepInterval, TimeUnit.SECONDS);
-      if (RINTERVAL <= 0) {
-        throw new IOException(
-          "Invalid " + GRIDMIX_SLEEP_INTERVAL + ": " + RINTERVAL);
-      }
-      return new RecordReader<LongWritable, LongWritable>() {
-        long start = -1;
-        long slept = 0L;
-        long sleep = 0L;
-        final LongWritable key = new LongWritable();
-        final LongWritable val = new LongWritable();
-
-        @Override
-        public boolean nextKeyValue() throws IOException {
-          if (start == -1) {
-            start = System.currentTimeMillis();
-          }
-          slept += sleep;
-          sleep = Math.min(duration - slept, RINTERVAL);
-          key.set(slept + sleep + start);
-          val.set(duration - slept);
-          return slept < duration;
-        }
-
-        @Override
-        public float getProgress() throws IOException {
-          return slept / ((float) duration);
-        }
-
-        @Override
-        public LongWritable getCurrentKey() {
-          return key;
-        }
-
-        @Override
-        public LongWritable getCurrentValue() {
-          return val;
-        }
-
-        @Override
-        public void close() throws IOException {
-          final String msg = "Slept for " + duration;
-          LOG.info(msg);
-        }
-
-        public void initialize(InputSplit split, TaskAttemptContext ctxt) {
-        }
-      };
-    }
-  }
-
-  public static class SleepSplit extends InputSplit implements Writable {
-    private int id;
-    private int nSpec;
-    private int nMaps;
-    private long sleepDuration;
-    private long[] reduceDurations = new long[0];
-    private String[] locations = new String[0];
-
-    public SleepSplit() {
-    }
-
-    public SleepSplit(
-      int id, long sleepDuration, long[] reduceDurations, int nMaps,
-      String[] locations) {
-      this.id = id;
-      this.sleepDuration = sleepDuration;
-      nSpec = reduceDurations.length;
-      this.reduceDurations = reduceDurations;
-      this.nMaps = nMaps;
-      this.locations = locations;
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      WritableUtils.writeVInt(out, id);
-      WritableUtils.writeVLong(out, sleepDuration);
-      WritableUtils.writeVInt(out, nMaps);
-      WritableUtils.writeVInt(out, nSpec);
-      for (int i = 0; i < nSpec; ++i) {
-        WritableUtils.writeVLong(out, reduceDurations[i]);
-      }
-      WritableUtils.writeVInt(out, locations.length);
-      for (int i = 0; i < locations.length; ++i) {
-        Text.writeString(out, locations[i]);
-      }
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      id = WritableUtils.readVInt(in);
-      sleepDuration = WritableUtils.readVLong(in);
-      nMaps = WritableUtils.readVInt(in);
-      nSpec = WritableUtils.readVInt(in);
-      if (reduceDurations.length < nSpec) {
-        reduceDurations = new long[nSpec];
-      }
-      for (int i = 0; i < nSpec; ++i) {
-        reduceDurations[i] = WritableUtils.readVLong(in);
-      }
-      final int nLoc = WritableUtils.readVInt(in);
-      if (nLoc != locations.length) {
-        locations = new String[nLoc];
-      }
-      for (int i = 0; i < nLoc; ++i) {
-        locations[i] = Text.readString(in);
-      }
-    }
-
-    @Override
-    public long getLength() {
-      return sleepDuration;
-    }
-
-    public int getId() {
-      return id;
-    }
-
-    public int getNumMaps() {
-      return nMaps;
-    }
-
-    public long getReduceDurations(int i) {
-      return reduceDurations[i];
-    }
-
-    @Override
-    public String[] getLocations() {
-      return locations;
-    }
-  }
-
-  private TaskAttemptInfo getSuccessfulAttemptInfo(TaskType type, int task) {
-    TaskAttemptInfo ret;
-    for (int i = 0; true; ++i) {
-      // Rumen should make up an attempt if it's missing. Or this won't work
-      // at all. It's hard to discern what is happening in there.
-      ret = jobdesc.getTaskAttemptInfo(type, task, i);
-      if (ret.getRunState() == TaskStatus.State.SUCCEEDED) {
-        break;
-      }
-    }
-    if(ret.getRunState() != TaskStatus.State.SUCCEEDED) {
-      LOG.warn("No sucessful attempts tasktype " + type +" task "+ task);
-    }
-
-    return ret;
-  }
-
-  @Override
-  void buildSplits(FilePool inputDir) throws IOException {
-    final List<InputSplit> splits = new ArrayList<InputSplit>();
-    final int reds = (mapTasksOnly) ? 0 : jobdesc.getNumberReduces();
-    final int maps = jobdesc.getNumberMaps();
-    for (int i = 0; i < maps; ++i) {
-      final int nSpec = reds / maps + ((reds % maps) > i ? 1 : 0);
-      final long[] redDurations = new long[nSpec];
-      for (int j = 0; j < nSpec; ++j) {
-        final ReduceTaskAttemptInfo info =
-          (ReduceTaskAttemptInfo) getSuccessfulAttemptInfo(TaskType.REDUCE, 
-                                                           i + j * maps);
-        // Include only merge/reduce time
-        redDurations[j] = Math.min(reduceMaxSleepTime, info.getMergeRuntime()
-            + info.getReduceRuntime());
-        if (LOG.isDebugEnabled()) {
-          LOG.debug(
-            String.format(
-              "SPEC(%d) %d -> %d %d/%d", id(), i, i + j * maps, redDurations[j],
-              info.getRuntime()));
-        }
-      }
-      final TaskAttemptInfo info = getSuccessfulAttemptInfo(TaskType.MAP, i);
-      ArrayList<String> locations = new ArrayList<String>(fakeLocations);
-      if (fakeLocations > 0) {
-        selector.reset();
-      }
-      for (int k=0; k<fakeLocations; ++k) {
-        int index = selector.next();
-        if (index < 0) break;
-        locations.add(hosts[index]);
-      }
-
-      splits.add(new SleepSplit(i,
-          Math.min(info.getRuntime(), mapMaxSleepTime), redDurations, maps,
-          locations.toArray(new String[locations.size()])));    }
-    pushDescription(id(), splits);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StatListener.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StatListener.java
deleted file mode 100644
index 2a0f74f..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StatListener.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-/**
- * Stat listener.
- * @param <T>
- */
-interface StatListener<T>{
-
-  /**
-   * 
-   * @param item
-   */
-  void update(T item);
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Statistics.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Statistics.java
deleted file mode 100644
index 54f1730..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Statistics.java
+++ /dev/null
@@ -1,330 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.gridmix.Gridmix.Component;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.Condition;
-import java.util.concurrent.locks.ReentrantLock;
-
-/**
- * Component collecting the stats required by other components
- * to make decisions.
- * Single thread Collector tries to collec the stats.
- * Each of thread poll updates certain datastructure(Currently ClusterStats).
- * Components interested in these datastructure, need to register.
- * StatsCollector notifies each of the listeners.
- */
-public class Statistics implements Component<Job> {
-  public static final Log LOG = LogFactory.getLog(Statistics.class);
-
-  private final StatCollector statistics = new StatCollector();
-  private JobClient cluster;
-
-  //List of cluster status listeners.
-  private final List<StatListener<ClusterStats>> clusterStatlisteners =
-    new CopyOnWriteArrayList<StatListener<ClusterStats>>();
-
-  //List of job status listeners.
-  private final List<StatListener<JobStats>> jobStatListeners =
-    new CopyOnWriteArrayList<StatListener<JobStats>>();
-
-  //List of jobids and noofMaps for each job
-  private static final Map<Integer, JobStats> jobMaps =
-    new ConcurrentHashMap<Integer,JobStats>();
-
-  private int completedJobsInCurrentInterval = 0;
-  private final int jtPollingInterval;
-  private volatile boolean shutdown = false;
-  private final int maxJobCompletedInInterval;
-  private static final String MAX_JOBS_COMPLETED_IN_POLL_INTERVAL_KEY =
-    "gridmix.max-jobs-completed-in-poll-interval";
-  private final ReentrantLock lock = new ReentrantLock();
-  private final Condition jobCompleted = lock.newCondition();
-  private final CountDownLatch startFlag;
-
-  public Statistics(
-    final Configuration conf, int pollingInterval, CountDownLatch startFlag)
-    throws IOException, InterruptedException {
-      UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-      this.cluster = ugi.doAs(new PrivilegedExceptionAction<JobClient>() {
-        public JobClient run() throws IOException {
-          return new JobClient(new JobConf(conf));
-        }
-      });
-
-    this.jtPollingInterval = pollingInterval;
-    maxJobCompletedInInterval = conf.getInt(
-      MAX_JOBS_COMPLETED_IN_POLL_INTERVAL_KEY, 1);
-    this.startFlag = startFlag;
-  }
-
-  public void addJobStats(Job job, JobStory jobdesc) {
-    int seq = GridmixJob.getJobSeqId(job);
-    if (seq < 0) {
-      LOG.info("Not tracking job " + job.getJobName()
-               + " as seq id is less than zero: " + seq);
-      return;
-    }
-    
-    int maps = 0;
-    int reds = 0;
-    if (jobdesc == null) {
-      throw new IllegalArgumentException(
-        " JobStory not available for job " + job.getJobName());
-    } else {
-      maps = jobdesc.getNumberMaps();
-      reds = jobdesc.getNumberReduces();
-    }
-    JobStats stats = new JobStats(maps, reds, job);
-    jobMaps.put(seq,stats);
-  }
-
-  /**
-   * Used by JobMonitor to add the completed job.
-   */
-  @Override
-  public void add(Job job) {
-    //This thread will be notified initially by jobmonitor incase of
-    //data generation. Ignore that as we are getting once the input is
-    //generated.
-    if (!statistics.isAlive()) {
-      return;
-    }
-    JobStats stat = jobMaps.remove(GridmixJob.getJobSeqId(job));
-
-    if (stat == null) return;
-    
-    completedJobsInCurrentInterval++;
-    //check if we have reached the maximum level of job completions.
-    if (completedJobsInCurrentInterval >= maxJobCompletedInInterval) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(
-          " Reached maximum limit of jobs in a polling interval " +
-            completedJobsInCurrentInterval);
-      }
-      completedJobsInCurrentInterval = 0;
-      lock.lock();
-      try {
-        //Job is completed notify all the listeners.
-        for (StatListener<JobStats> l : jobStatListeners) {
-          l.update(stat);
-        }
-        this.jobCompleted.signalAll();
-      } finally {
-        lock.unlock();
-      }
-    }
-  }
-
-  //TODO: We have just 2 types of listeners as of now . If no of listeners
-  //increase then we should move to map kind of model.
-
-  public void addClusterStatsObservers(StatListener<ClusterStats> listener) {
-    clusterStatlisteners.add(listener);
-  }
-
-  public void addJobStatsListeners(StatListener<JobStats> listener) {
-    this.jobStatListeners.add(listener);
-  }
-
-  /**
-   * Attempt to start the service.
-   */
-  @Override
-  public void start() {
-    statistics.start();
-  }
-
-  private class StatCollector extends Thread {
-
-    StatCollector() {
-      super("StatsCollectorThread");
-    }
-
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-      } catch (InterruptedException ie) {
-        LOG.error(
-          "Statistics Error while waiting for other threads to get ready ", ie);
-        return;
-      }
-      while (!shutdown) {
-        lock.lock();
-        try {
-          jobCompleted.await(jtPollingInterval, TimeUnit.MILLISECONDS);
-        } catch (InterruptedException ie) {
-          if (!shutdown) {
-            LOG.error("Statistics interrupt while waiting for completion of "
-                + "a job.", ie);
-          }
-          return;
-        } finally {
-          lock.unlock();
-        }
-
-        //Fetch cluster data only if required.i.e .
-        // only if there are clusterStats listener.
-        if (clusterStatlisteners.size() > 0) {
-          try {
-            ClusterStatus clusterStatus = cluster.getClusterStatus();
-            updateAndNotifyClusterStatsListeners(clusterStatus);
-          } catch (IOException e) {
-            LOG.error(
-              "Statistics io exception while polling JT ", e);
-            return;
-          }
-        }
-      }
-    }
-
-    private void updateAndNotifyClusterStatsListeners(
-      ClusterStatus clusterStatus) {
-      ClusterStats stats = ClusterStats.getClusterStats();
-      stats.setClusterMetric(clusterStatus);
-      for (StatListener<ClusterStats> listener : clusterStatlisteners) {
-        listener.update(stats);
-      }
-    }
-
-  }
-
-  /**
-   * Wait until the service completes. It is assumed that either a
-   * {@link #shutdown} or {@link #abort} has been requested.
-   */
-  @Override
-  public void join(long millis) throws InterruptedException {
-    statistics.join(millis);
-  }
-
-  @Override
-  public void shutdown() {
-    shutdown = true;
-    jobMaps.clear();
-    clusterStatlisteners.clear();
-    jobStatListeners.clear();
-    statistics.interrupt();
-  }
-
-  @Override
-  public void abort() {
-    shutdown = true;
-    jobMaps.clear();
-    clusterStatlisteners.clear();
-    jobStatListeners.clear();
-    statistics.interrupt();
-  }
-
-  /**
-   * Class to encapsulate the JobStats information.
-   * Current we just need information about completedJob.
-   * TODO: In future we need to extend this to send more information.
-   */
-  static class JobStats {
-    private int noOfMaps;
-    private int noOfReds;
-    private Job job;
-
-    public JobStats(int noOfMaps,int numOfReds, Job job){
-      this.job = job;
-      this.noOfMaps = noOfMaps;
-      this.noOfReds = numOfReds;
-    }
-    public int getNoOfMaps() {
-      return noOfMaps;
-    }
-    public int getNoOfReds() {
-      return noOfReds;
-    }
-
-    /**
-     * Returns the job ,
-     * We should not use job.getJobID it returns null in 20.1xx.
-     * Use (GridmixJob.getJobSeqId(job)) instead
-     * @return job
-     */
-    public Job getJob() {
-      return job;
-    }
-  }
-
-  static class ClusterStats {
-    private ClusterStatus status = null;
-    private static ClusterStats stats = new ClusterStats();
-
-    private ClusterStats() {
-
-    }
-
-    /**
-     * @return stats
-     */
-    static ClusterStats getClusterStats() {
-      return stats;
-    }
-
-    /**
-     * @param metrics
-     */
-    void setClusterMetric(ClusterStatus metrics) {
-      this.status = metrics;
-    }
-
-    /**
-     * @return metrics
-     */
-    public ClusterStatus getStatus() {
-      return status;
-    }
-
-    int getNumRunningJob() {
-      return jobMaps.size();
-    }
-
-    /**
-     * @return runningWatitingJobs
-     */
-    static Collection<JobStats> getRunningJobStats() {
-      return jobMaps.values();
-    }
-
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
deleted file mode 100644
index d78d631..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
+++ /dev/null
@@ -1,472 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-public class StressJobFactory extends JobFactory<Statistics.ClusterStats> {
-  public static final Log LOG = LogFactory.getLog(StressJobFactory.class);
-
-  private final LoadStatus loadStatus = new LoadStatus();
-  /**
-   * The minimum ratio between pending+running map tasks (aka. incomplete map
-   * tasks) and cluster map slot capacity for us to consider the cluster is
-   * overloaded. For running maps, we only count them partially. Namely, a 40%
-   * completed map is counted as 0.6 map tasks in our calculation.
-   */
-  private static final float OVERLOAD_MAPTASK_MAPSLOT_RATIO = 2.0f;
-  public static final String CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO=
-      "gridmix.throttle.maps.task-to-slot-ratio";
-  final float overloadMapTaskMapSlotRatio;
-
-  /**
-   * The minimum ratio between pending+running reduce tasks (aka. incomplete
-   * reduce tasks) and cluster reduce slot capacity for us to consider the
-   * cluster is overloaded. For running reduces, we only count them partially.
-   * Namely, a 40% completed reduce is counted as 0.6 reduce tasks in our
-   * calculation.
-   */
-  private static final float OVERLOAD_REDUCETASK_REDUCESLOT_RATIO = 2.5f;
-  public static final String CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO=
-    "gridmix.throttle.reduces.task-to-slot-ratio";
-  final float overloadReduceTaskReduceSlotRatio;
-
-  /**
-   * The maximum share of the cluster's mapslot capacity that can be counted
-   * toward a job's incomplete map tasks in overload calculation.
-   */
-  private static final float MAX_MAPSLOT_SHARE_PER_JOB=0.1f;
-  public static final String CONF_MAX_MAPSLOT_SHARE_PER_JOB=
-    "gridmix.throttle.maps.max-slot-share-per-job";  
-  final float maxMapSlotSharePerJob;
-  
-  /**
-   * The maximum share of the cluster's reduceslot capacity that can be counted
-   * toward a job's incomplete reduce tasks in overload calculation.
-   */
-  private static final float MAX_REDUCESLOT_SHARE_PER_JOB=0.1f;
-  public static final String CONF_MAX_REDUCESLOT_SHARE_PER_JOB=
-    "gridmix.throttle.reducess.max-slot-share-per-job";  
-  final float maxReduceSlotSharePerJob;
-
-  /**
-   * The ratio of the maximum number of pending+running jobs over the number of
-   * task trackers.
-   */
-  private static final float MAX_JOB_TRACKER_RATIO=1.0f;
-  public static final String CONF_MAX_JOB_TRACKER_RATIO=
-    "gridmix.throttle.jobs-to-tracker-ratio";  
-  final float maxJobTrackerRatio;
-
-  /**
-   * Creating a new instance does not start the thread.
-   *
-   * @param submitter   Component to which deserialized jobs are passed
-   * @param jobProducer Stream of job traces with which to construct a
-   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch     Directory into which to write output from simulated jobs
-   * @param conf        Config passed to all jobs to be submitted
-   * @param startFlag   Latch released from main to start pipeline
-   * @throws java.io.IOException
-   */
-  public StressJobFactory(
-    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
-    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
-    throws IOException {
-    super(
-      submitter, jobProducer, scratch, conf, startFlag, resolver);
-    overloadMapTaskMapSlotRatio = conf.getFloat(
-        CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO, OVERLOAD_MAPTASK_MAPSLOT_RATIO);
-    overloadReduceTaskReduceSlotRatio = conf.getFloat(
-        CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO, 
-        OVERLOAD_REDUCETASK_REDUCESLOT_RATIO);
-    maxMapSlotSharePerJob = conf.getFloat(
-        CONF_MAX_MAPSLOT_SHARE_PER_JOB, MAX_MAPSLOT_SHARE_PER_JOB);
-    maxReduceSlotSharePerJob = conf.getFloat(
-        CONF_MAX_REDUCESLOT_SHARE_PER_JOB, MAX_REDUCESLOT_SHARE_PER_JOB);
-    maxJobTrackerRatio = conf.getFloat(
-        CONF_MAX_JOB_TRACKER_RATIO, MAX_JOB_TRACKER_RATIO);
-  }
-
-  public Thread createReaderThread() {
-    return new StressReaderThread("StressJobFactory");
-  }
-
-  /*
-  * Worker thread responsible for reading descriptions, assigning sequence
-  * numbers, and normalizing time.
-  */
-  private class StressReaderThread extends Thread {
-
-    public StressReaderThread(String name) {
-      super(name);
-    }
-
-    /**
-     * STRESS: Submits the job in STRESS mode.
-     * while(JT is overloaded) {
-     * wait();
-     * }
-     * If not overloaded , get number of slots available.
-     * Keep submitting the jobs till ,total jobs  is sufficient to
-     * load the JT.
-     * That is submit  (Sigma(no of maps/Job)) > (2 * no of slots available)
-     */
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-        LOG.info("START STRESS @ " + System.currentTimeMillis());
-        while (!Thread.currentThread().isInterrupted()) {
-          try {
-            while (loadStatus.overloaded()) {
-              if (LOG.isDebugEnabled()) {
-                LOG.debug("Cluster overloaded in run! Sleeping...");
-              }
-              // sleep 
-              try {
-                Thread.sleep(1000);
-              } catch (InterruptedException ie) {
-                return;
-              }
-            }
-
-            while (!loadStatus.overloaded()) {
-              if (LOG.isDebugEnabled()) {
-                LOG.debug("Cluster underloaded in run! Stressing...");
-              }
-              try {
-                //TODO This in-line read can block submission for large jobs.
-                final JobStory job = getNextJobFiltered();
-                if (null == job) {
-                  return;
-                }
-                if (LOG.isDebugEnabled()) {
-                  LOG.debug("Job Selected: " + job.getJobID());
-                }
-                submitter.add(
-                  jobCreator.createGridmixJob(
-                    conf, 0L, job, scratch, 
-                    userResolver.getTargetUgi(
-                      UserGroupInformation.createRemoteUser(job.getUser())), 
-                    sequence.getAndIncrement()));
-                // TODO: We need to take care of scenario when one map/reduce
-                // takes more than 1 slot.
-                
-                // Lock the loadjob as we are making updates
-                int incompleteMapTasks = (int) calcEffectiveIncompleteMapTasks(
-                                                 loadStatus.getMapCapacity(), 
-                                                 job.getNumberMaps(), 0.0f);
-                loadStatus.decrementMapLoad(incompleteMapTasks);
-                
-                int incompleteReduceTasks = 
-                  (int) calcEffectiveIncompleteReduceTasks(
-                          loadStatus.getReduceCapacity(), 
-                          job.getNumberReduces(), 0.0f);
-                loadStatus.decrementReduceLoad(incompleteReduceTasks);
-                  
-                loadStatus.decrementJobLoad(1);
-              } catch (IOException e) {
-                LOG.error("Error while submitting the job ", e);
-                error = e;
-                return;
-              }
-
-            }
-          } finally {
-            // do nothing
-          }
-        }
-      } catch (InterruptedException e) {
-        return;
-      } finally {
-        IOUtils.cleanup(null, jobProducer);
-      }
-    }
-  }
-
-  /**
-   * STRESS Once you get the notification from StatsCollector.Collect the
-   * clustermetrics. Update current loadStatus with new load status of JT.
-   *
-   * @param item
-   */
-  @Override
-  public void update(Statistics.ClusterStats item) {
-    ClusterStatus clusterMetrics = item.getStatus();
-    try {
-      checkLoadAndGetSlotsToBackfill(item, clusterMetrics);
-    } catch (Exception e) {
-      LOG.error("Couldn't get the new Status",e);
-    }
-  }
-
-  float calcEffectiveIncompleteMapTasks(int mapSlotCapacity,
-      int numMaps, float mapProgress) {
-    float maxEffIncompleteMapTasks = Math.max(1.0f, mapSlotCapacity
-        * maxMapSlotSharePerJob);
-    float mapProgressAdjusted = Math.max(Math.min(mapProgress, 1.0f), 0.0f);
-    return Math.min(maxEffIncompleteMapTasks, 
-                    numMaps * (1.0f - mapProgressAdjusted));
-  }
-
-  float calcEffectiveIncompleteReduceTasks(int reduceSlotCapacity,
-      int numReduces, float reduceProgress) {
-    float maxEffIncompleteReduceTasks = Math.max(1.0f, reduceSlotCapacity
-        * maxReduceSlotSharePerJob);
-    float reduceProgressAdjusted = 
-      Math.max(Math.min(reduceProgress, 1.0f), 0.0f);
-    return Math.min(maxEffIncompleteReduceTasks, 
-                    numReduces * (1.0f - reduceProgressAdjusted));
-  }
-
-  /**
-   * We try to use some light-weight mechanism to determine cluster load.
-   *
-   * @param stats
-   * @param clusterStatus Cluster status
-   * @throws java.io.IOException
-   */
-  private void checkLoadAndGetSlotsToBackfill(
-    ClusterStats stats, ClusterStatus clusterStatus) throws IOException, InterruptedException {
-    
-    // update the max cluster capacity incase its updated
-    int mapCapacity = clusterStatus.getMaxMapTasks();
-    loadStatus.updateMapCapacity(mapCapacity);
-    
-    int reduceCapacity = clusterStatus.getMaxReduceTasks();
-    
-    loadStatus.updateReduceCapacity(reduceCapacity);
-    
-    int numTrackers = clusterStatus.getTaskTrackers();
-    
-    int jobLoad = 
-      (int) (maxJobTrackerRatio * numTrackers) - stats.getNumRunningJob();
-    loadStatus.updateJobLoad(jobLoad);
-    if (loadStatus.getJobLoad() <= 0) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is "
-                  + Boolean.TRUE.toString() + " NumJobsBackfill is "
-                  + loadStatus.getJobLoad());
-      }
-      return; // stop calculation because we know it is overloaded.
-    }
-
-    float incompleteMapTasks = 0; // include pending & running map tasks.
-    for (JobStats job : ClusterStats.getRunningJobStats()) {
-      float mapProgress = job.getJob().mapProgress();
-      int noOfMaps = job.getNoOfMaps();
-      incompleteMapTasks += 
-        calcEffectiveIncompleteMapTasks(mapCapacity, noOfMaps, mapProgress);
-    }
-    
-    int mapSlotsBackFill = 
-      (int) ((overloadMapTaskMapSlotRatio * mapCapacity) - incompleteMapTasks);
-    loadStatus.updateMapLoad(mapSlotsBackFill);
-    
-    if (loadStatus.getMapLoad() <= 0) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(System.currentTimeMillis() + " [MAP-LOAD] Overloaded is "
-                  + Boolean.TRUE.toString() + " MapSlotsBackfill is "
-                  + loadStatus.getMapLoad());
-      }
-      return; // stop calculation because we know it is overloaded.
-    }
-
-    float incompleteReduceTasks = 0; // include pending & running reduce tasks.
-    for (JobStats job : ClusterStats.getRunningJobStats()) {
-      // Cached the num-reds value in JobStats
-      int noOfReduces = job.getNoOfReds();
-      if (noOfReduces > 0) {
-        float reduceProgress = job.getJob().reduceProgress();
-        incompleteReduceTasks += 
-          calcEffectiveIncompleteReduceTasks(reduceCapacity, noOfReduces, 
-                                             reduceProgress);
-      }
-    }
-    
-    int reduceSlotsBackFill = 
-      (int)((overloadReduceTaskReduceSlotRatio * reduceCapacity) 
-             - incompleteReduceTasks);
-    loadStatus.updateReduceLoad(reduceSlotsBackFill);
-    if (loadStatus.getReduceLoad() <= 0) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(System.currentTimeMillis() + " [REDUCE-LOAD] Overloaded is "
-                  + Boolean.TRUE.toString() + " ReduceSlotsBackfill is "
-                  + loadStatus.getReduceLoad());
-      }
-      return; // stop calculation because we know it is overloaded.
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug(System.currentTimeMillis() + " [OVERALL] Overloaded is "
-                + Boolean.FALSE.toString() + "Current load Status is " 
-                + loadStatus);
-    }
-  }
-
-  static class LoadStatus {
-    /**
-     * Additional number of map slots that can be requested before
-     * declaring (by Gridmix STRESS mode) the cluster as overloaded. 
-     */
-    private volatile int mapSlotsBackfill;
-    
-    /**
-     * Determines the total map slot capacity of the cluster.
-     */
-    private volatile int mapSlotCapacity;
-    
-    /**
-     * Additional number of reduce slots that can be requested before
-     * declaring (by Gridmix STRESS mode) the cluster as overloaded.
-     */
-    private volatile int reduceSlotsBackfill;
-    
-    /**
-     * Determines the total reduce slot capacity of the cluster.
-     */
-    private volatile int reduceSlotCapacity;
-
-    /**
-     * Determines the max count of running jobs in the cluster.
-     */
-    private volatile int numJobsBackfill;
-    
-    // set the default to true
-    private AtomicBoolean overloaded = new AtomicBoolean(true);
-
-    /**
-     * Construct the LoadStatus in an unknown state - assuming the cluster is
-     * overloaded by setting numSlotsBackfill=0.
-     */
-    LoadStatus() {
-      mapSlotsBackfill = 0;
-      reduceSlotsBackfill = 0;
-      numJobsBackfill = 0;
-      
-      mapSlotCapacity = -1;
-      reduceSlotCapacity = -1;
-    }
-    
-    public synchronized int getMapLoad() {
-      return mapSlotsBackfill;
-    }
-    
-    public synchronized int getMapCapacity() {
-      return mapSlotCapacity;
-    }
-    
-    public synchronized int getReduceLoad() {
-      return reduceSlotsBackfill;
-    }
-    
-    public synchronized int getReduceCapacity() {
-      return reduceSlotCapacity;
-    }
-    
-    public synchronized int getJobLoad() {
-      return numJobsBackfill;
-    }
-    
-    public synchronized void decrementMapLoad(int mapSlotsConsumed) {
-      this.mapSlotsBackfill -= mapSlotsConsumed;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void decrementReduceLoad(int reduceSlotsConsumed) {
-      this.reduceSlotsBackfill -= reduceSlotsConsumed;
-      updateOverloadStatus();
-    }
-
-    public synchronized void decrementJobLoad(int numJobsConsumed) {
-      this.numJobsBackfill -= numJobsConsumed;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateMapCapacity(int mapSlotsCapacity) {
-      this.mapSlotCapacity = mapSlotsCapacity;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateReduceCapacity(int reduceSlotsCapacity) {
-      this.reduceSlotCapacity = reduceSlotsCapacity;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateMapLoad(int mapSlotsBackfill) {
-      this.mapSlotsBackfill = mapSlotsBackfill;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateReduceLoad(int reduceSlotsBackfill) {
-      this.reduceSlotsBackfill = reduceSlotsBackfill;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateJobLoad(int numJobsBackfill) {
-      this.numJobsBackfill = numJobsBackfill;
-      updateOverloadStatus();
-    }
-    
-    private synchronized void updateOverloadStatus() {
-      overloaded.set((mapSlotsBackfill <= 0) || (reduceSlotsBackfill <= 0)
-                     || (numJobsBackfill <= 0));
-    }
-    
-    public synchronized boolean overloaded() {
-      return overloaded.get();
-    }
-    
-    public synchronized String toString() {
-    // TODO Use StringBuilder instead
-      return " Overloaded = " + overloaded()
-             + ", MapSlotBackfill = " + mapSlotsBackfill 
-             + ", MapSlotCapacity = " + mapSlotCapacity
-             + ", ReduceSlotBackfill = " + reduceSlotsBackfill 
-             + ", ReduceSlotCapacity = " + reduceSlotCapacity
-             + ", NumJobsBackfill = " + numJobsBackfill;
-    }
-  }
-
-  /**
-   * Start the reader thread, wait for latch if necessary.
-   */
-  @Override
-  public void start() {
-    LOG.info(" Starting Stress submission ");
-    this.rThread.start();
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
deleted file mode 100644
index d0d552a..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Resolves all UGIs to the submitting user.
- */
-public class SubmitterUserResolver implements UserResolver {
-  public static final Log LOG = LogFactory.getLog(SubmitterUserResolver.class);
-  
-  private UserGroupInformation ugi = null;
-
-  public SubmitterUserResolver() throws IOException {
-    LOG.info(" Current user resolver is SubmitterUserResolver ");
-    ugi = UserGroupInformation.getLoginUser();
-  }
-
-  public synchronized boolean setTargetUsers(URI userdesc, Configuration conf)
-      throws IOException {
-    return false;
-  }
-
-  public synchronized UserGroupInformation getTargetUgi(
-      UserGroupInformation ugi) {
-    return this.ugi;
-  }
-
-  /**
-   * {@inheritDoc}
-   * <p>
-   * Since {@link SubmitterUserResolver} returns the user name who is running
-   * gridmix, it doesn't need a target list of users.
-   */
-  public boolean needsTargetUsersList() {
-    return false;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Summarizer.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
deleted file mode 100644
index 16026f2..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-
-/**
- * Summarizes various aspects of a {@link Gridmix} run.
- */
-class Summarizer {
-  private ExecutionSummarizer executionSummarizer;
-  private ClusterSummarizer clusterSummarizer;
-  protected static final String NA = "N/A";
-  
-  Summarizer() {
-    this(new String[]{NA});
-  }
-  
-  Summarizer(String[] args) {
-    executionSummarizer = new ExecutionSummarizer(args);
-    clusterSummarizer = new ClusterSummarizer();
-  }
-  
-  ExecutionSummarizer getExecutionSummarizer() {
-    return executionSummarizer;
-  }
-  
-  ClusterSummarizer getClusterSummarizer() {
-    return clusterSummarizer;
-  }
-  
-  void start(Configuration conf) {
-    executionSummarizer.start(conf);
-    clusterSummarizer.start(conf);
-  }
-  
-  /**
-   * This finalizes the summarizer.
-   */
-  @SuppressWarnings("unchecked")
-  void finalize(JobFactory factory, String path, long size, 
-                UserResolver resolver, DataStatistics stats, Configuration conf)
-  throws IOException {
-    executionSummarizer.finalize(factory, path, size, resolver, stats, conf);
-  }
-  
-  /**
-   * Summarizes the current {@link Gridmix} run and the cluster used. 
-   */
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(executionSummarizer.toString());
-    builder.append(clusterSummarizer.toString());
-    return builder.toString();
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/UserResolver.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
deleted file mode 100644
index ca8c98b..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Maps users in the trace to a set of valid target users on the test cluster.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public interface UserResolver {
-
-  /**
-   * Configure the user map given the URI and configuration. The resolver's
-   * contract will define how the resource will be interpreted, but the default
-   * will typically interpret the URI as a {@link org.apache.hadoop.fs.Path}
-   * listing target users.
-   * This method should be called only if {@link #needsTargetUsersList()}
-   * returns true.
-   * @param userdesc URI from which user information may be loaded per the
-   * subclass contract.
-   * @param conf The tool configuration.
-   * @return true if the resource provided was used in building the list of
-   * target users
-   */
-  public boolean setTargetUsers(URI userdesc, Configuration conf)
-    throws IOException;
-
-  /**
-   * Map the given UGI to another per the subclass contract.
-   * @param ugi User information from the trace.
-   */
-  public UserGroupInformation getTargetUgi(UserGroupInformation ugi);
-
-  /**
-   * Indicates whether this user resolver needs a list of target users to be
-   * provided.
-   *
-   * @return true if a list of target users is to be provided for this
-   * user resolver
-   */
-  public boolean needsTargetUsersList();
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
deleted file mode 100644
index 8f4af1a..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
+++ /dev/null
@@ -1,315 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-/**
- * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the cumulative CPU 
- * usage by performing certain CPU intensive operations. Performing such CPU 
- * intensive operations essentially uses up some CPU. Every 
- * {@link ResourceUsageEmulatorPlugin} is configured with a feedback module i.e 
- * a {@link ResourceCalculatorPlugin}, to monitor the resource usage.</p>
- * 
- * <p>{@link CumulativeCpuUsageEmulatorPlugin} emulates the CPU usage in steps. 
- * The frequency of emulation can be configured via 
- * {@link #CPU_EMULATION_PROGRESS_INTERVAL}.
- * CPU usage values are matched via emulation only on the interval boundaries.
- * </p>
- *  
- * {@link CumulativeCpuUsageEmulatorPlugin} is a wrapper program for managing 
- * the CPU usage emulation feature. It internally uses an emulation algorithm 
- * (called as core and described using {@link CpuUsageEmulatorCore}) for 
- * performing the actual emulation. Multiple calls to this core engine should 
- * use up some amount of CPU.<br>
- * 
- * <p>{@link CumulativeCpuUsageEmulatorPlugin} provides a calibration feature 
- * via {@link #initialize(Configuration, ResourceUsageMetrics, 
- *                        ResourceCalculatorPlugin, Progressive)} to calibrate 
- *  the plugin and its core for the underlying hardware. As a result of 
- *  calibration, every call to the emulation engine's core should roughly use up
- *  1% of the total usage value to be emulated. This makes sure that the 
- *  underlying hardware is profiled before use and that the plugin doesn't 
- *  accidently overuse the CPU. With 1% as the unit emulation target value for 
- *  the core engine, there will be roughly 100 calls to the engine resulting in 
- *  roughly 100 calls to the feedback (resource usage monitor) module. 
- *  Excessive usage of the feedback module is discouraged as 
- *  it might result into excess CPU usage resulting into no real CPU emulation.
- *  </p>
- */
-public class CumulativeCpuUsageEmulatorPlugin 
-implements ResourceUsageEmulatorPlugin {
-  protected CpuUsageEmulatorCore emulatorCore;
-  private ResourceCalculatorPlugin monitor;
-  private Progressive progress;
-  private boolean enabled = true;
-  private float emulationInterval; // emulation interval
-  private long targetCpuUsage = 0;
-  private float lastSeenProgress = 0;
-  private long lastSeenCpuUsageCpuUsage = 0;
-  
-  // Configuration parameters
-  public static final String CPU_EMULATION_PROGRESS_INTERVAL = 
-    "gridmix.emulators.resource-usage.cpu.emulation-interval";
-  private static final float DEFAULT_EMULATION_FREQUENCY = 0.1F; // 10 times
-
-  /**
-   * This is the core CPU usage emulation algorithm. This is the core engine
-   * which actually performs some CPU intensive operations to consume some
-   * amount of CPU. Multiple calls of {@link #compute()} should help the 
-   * plugin emulate the desired level of CPU usage. This core engine can be
-   * calibrated using the {@link #calibrate(ResourceCalculatorPlugin, long)}
-   * API to suit the underlying hardware better. It also can be used to optimize
-   * the emulation cycle.
-   */
-  public interface CpuUsageEmulatorCore {
-    /**
-     * Performs some computation to use up some CPU.
-     */
-    public void compute();
-    
-    /**
-     * Allows the core to calibrate itself.
-     */
-    public void calibrate(ResourceCalculatorPlugin monitor, 
-                          long totalCpuUsage);
-  }
-  
-  /**
-   * This is the core engine to emulate the CPU usage. The only responsibility 
-   * of this class is to perform certain math intensive operations to make sure 
-   * that some desired value of CPU is used.
-   */
-  public static class DefaultCpuUsageEmulator implements CpuUsageEmulatorCore {
-    // number of times to loop for performing the basic unit computation
-    private int numIterations;
-    private final Random random;
-    
-    /**
-     * This is to fool the JVM and make it think that we need the value 
-     * stored in the unit computation i.e {@link #compute()}. This will prevent
-     * the JVM from optimizing the code.
-     */
-    protected double returnValue;
-    
-    /**
-     * Initialized the {@link DefaultCpuUsageEmulator} with default values. 
-     * Note that the {@link DefaultCpuUsageEmulator} should be calibrated 
-     * (see {@link #calibrate(ResourceCalculatorPlugin, long)}) when initialized
-     * using this constructor.
-     */
-    public DefaultCpuUsageEmulator() {
-      this(-1);
-    }
-    
-    DefaultCpuUsageEmulator(int numIterations) {
-      this.numIterations = numIterations;
-      random = new Random();
-    }
-    
-    /**
-     * This will consume some desired level of CPU. This API will try to use up
-     * 'X' percent of the target cumulative CPU usage. Currently X is set to 
-     * 10%.
-     */
-    public void compute() {
-      for (int i = 0; i < numIterations; ++i) {
-        performUnitComputation();
-      }
-    }
-    
-    // Perform unit computation. The complete CPU emulation will be based on 
-    // multiple invocations to this unit computation module.
-    protected void performUnitComputation() {
-      //TODO can this be configurable too. Users/emulators should be able to 
-      // pick and choose what MATH operations to run.
-      // Example :
-      //           BASIC : ADD, SUB, MUL, DIV
-      //           ADV   : SQRT, SIN, COSIN..
-      //           COMPO : (BASIC/ADV)*
-      // Also define input generator. For now we can use the random number 
-      // generator. Later this can be changed to accept multiple sources.
-      
-      int randomData = random.nextInt();
-      int randomDataCube = randomData * randomData * randomData;
-      double randomDataCubeRoot = Math.cbrt(randomData);
-      returnValue = Math.log(Math.tan(randomDataCubeRoot 
-                                      * Math.exp(randomDataCube)) 
-                             * Math.sqrt(randomData));
-    }
-    
-    /**
-     * This will calibrate the algorithm such that a single invocation of
-     * {@link #compute()} emulates roughly 1% of the total desired resource 
-     * usage value.
-     */
-    public void calibrate(ResourceCalculatorPlugin monitor, 
-                          long totalCpuUsage) {
-      long initTime = monitor.getProcResourceValues().getCumulativeCpuTime();
-      
-      long defaultLoopSize = 0;
-      long finalTime = initTime;
-      
-      //TODO Make this configurable
-      while (finalTime - initTime < 100) { // 100 ms
-        ++defaultLoopSize;
-        performUnitComputation(); //perform unit computation
-        finalTime = monitor.getProcResourceValues().getCumulativeCpuTime();
-      }
-      
-      long referenceRuntime = finalTime - initTime;
-      
-      // time for one loop = (final-time - init-time) / total-loops
-      float timePerLoop = ((float)referenceRuntime) / defaultLoopSize;
-      
-      // compute the 1% of the total CPU usage desired
-      //TODO Make this configurable
-      long onePercent = totalCpuUsage / 100;
-      
-      // num-iterations for 1% = (total-desired-usage / 100) / time-for-one-loop
-      numIterations = Math.max(1, (int)((float)onePercent/timePerLoop));
-      
-      System.out.println("Calibration done. Basic computation runtime : " 
-          + timePerLoop + " milliseconds. Optimal number of iterations (1%): " 
-          + numIterations);
-    }
-  }
-  
-  public CumulativeCpuUsageEmulatorPlugin() {
-    this(new DefaultCpuUsageEmulator());
-  }
-  
-  /**
-   * For testing.
-   */
-  public CumulativeCpuUsageEmulatorPlugin(CpuUsageEmulatorCore core) {
-    emulatorCore = core;
-  }
-  
-  // Note that this weighing function uses only the current progress. In future,
-  // this might depend on progress, emulation-interval and expected target.
-  private float getWeightForProgressInterval(float progress) {
-    // we want some kind of exponential growth function that gives less weight
-    // on lower progress boundaries but high (exact emulation) near progress 
-    // value of 1.
-    // so here is how the current growth function looks like
-    //    progress    weight
-    //      0.1       0.0001
-    //      0.2       0.0016
-    //      0.3       0.0081
-    //      0.4       0.0256
-    //      0.5       0.0625
-    //      0.6       0.1296
-    //      0.7       0.2401
-    //      0.8       0.4096
-    //      0.9       0.6561
-    //      1.0       1.000
-    
-    return progress * progress * progress * progress;
-  }
-  
-  @Override
-  //TODO Multi-threading for speedup?
-  public void emulate() throws IOException, InterruptedException {
-    if (enabled) {
-      float currentProgress = progress.getProgress();
-      if (lastSeenProgress < currentProgress 
-          && ((currentProgress - lastSeenProgress) >= emulationInterval
-              || currentProgress == 1)) {
-        // Estimate the final cpu usage
-        //
-        //   Consider the following
-        //     Cl/Cc/Cp : Last/Current/Projected Cpu usage
-        //     Pl/Pc/Pp : Last/Current/Projected progress
-        //   Then
-        //     (Cp-Cc)/(Pp-Pc) = (Cc-Cl)/(Pc-Pl)
-        //   Solving this for Cp, we get
-        //     Cp = Cc + (1-Pc)*(Cc-Cl)/Pc-Pl)
-        //   Note that (Cc-Cl)/(Pc-Pl) is termed as 'rate' in the following 
-        //   section
-        
-        long currentCpuUsage = 
-          monitor.getProcResourceValues().getCumulativeCpuTime();
-        // estimate the cpu usage rate
-        float rate = (currentCpuUsage - lastSeenCpuUsageCpuUsage)
-                     / (currentProgress - lastSeenProgress);
-        long projectedUsage = 
-          currentCpuUsage + (long)((1 - currentProgress) * rate);
-        
-        if (projectedUsage < targetCpuUsage) {
-          // determine the correction factor between the current usage and the
-          // expected usage and add some weight to the target
-          long currentWeighedTarget = 
-            (long)(targetCpuUsage 
-                   * getWeightForProgressInterval(currentProgress));
-          
-          while (monitor.getProcResourceValues().getCumulativeCpuTime() 
-                 < currentWeighedTarget) {
-            emulatorCore.compute();
-            // sleep for 100ms
-            try {
-              Thread.sleep(100);
-            } catch (InterruptedException ie) {
-              String message = 
-                "CumulativeCpuUsageEmulatorPlugin got interrupted. Exiting.";
-              throw new RuntimeException(message);
-            }
-          }
-        }
-        
-        // set the last seen progress
-        lastSeenProgress = progress.getProgress();
-        // set the last seen usage
-        lastSeenCpuUsageCpuUsage = 
-          monitor.getProcResourceValues().getCumulativeCpuTime();
-      }
-    }
-  }
-
-  @Override
-  public void initialize(Configuration conf, ResourceUsageMetrics metrics,
-                         ResourceCalculatorPlugin monitor,
-                         Progressive progress) {
-    // get the target CPU usage
-    targetCpuUsage = metrics.getCumulativeCpuUsage();
-    if (targetCpuUsage <= 0 ) {
-      enabled = false;
-      return;
-    } else {
-      enabled = true;
-    }
-    
-    this.monitor = monitor;
-    this.progress = progress;
-    emulationInterval =  conf.getFloat(CPU_EMULATION_PROGRESS_INTERVAL, 
-                                       DEFAULT_EMULATION_FREQUENCY);
-    
-    // calibrate the core cpu-usage utility
-    emulatorCore.calibrate(monitor, targetCpuUsage);
-    
-    // initialize the states
-    lastSeenProgress = 0;
-    lastSeenCpuUsageCpuUsage = 0;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
deleted file mode 100644
index 7d40cfd..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * <p>Each resource to be emulated should have a corresponding implementation 
- * class that implements {@link ResourceUsageEmulatorPlugin}.</p>
- * <br><br>
- * {@link ResourceUsageEmulatorPlugin} will be configured using the 
- * {@link #initialize(Configuration, ResourceUsageMetrics, 
- *                    ResourceCalculatorPlugin, Progressive)} call.
- * Every 
- * {@link ResourceUsageEmulatorPlugin} is also configured with a feedback module
- * i.e a {@link ResourceCalculatorPlugin}, to monitor the current resource 
- * usage. {@link ResourceUsageMetrics} decides the final resource usage value to
- * emulate. {@link Progressive} keeps track of the task's progress.</p>
- * 
- * <br><br>
- * 
- * For configuring GridMix to load and and use a resource usage emulator, 
- * see {@link ResourceUsageMatcher}. 
- */
-public interface ResourceUsageEmulatorPlugin {
-  /**
-   * Initialize the plugin. This might involve
-   *   - initializing the variables
-   *   - calibrating the plugin
-   */
-  void initialize(Configuration conf, ResourceUsageMetrics metrics, 
-                  ResourceCalculatorPlugin monitor,
-                  Progressive progress);
-
-  /**
-   * Emulate the resource usage to match the usage target. The plugin can use
-   * the given {@link ResourceCalculatorPlugin} to query for the current 
-   * resource usage.
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  void emulate() throws IOException, InterruptedException;
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
deleted file mode 100644
index 917cd09..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.util.ReflectionUtils;
-
-/**
- * <p>This is the driver class for managing all the resource usage emulators.
- * {@link ResourceUsageMatcher} expects a comma separated list of 
- * {@link ResourceUsageEmulatorPlugin} implementations specified using 
- * {@link #RESOURCE_USAGE_EMULATION_PLUGINS} as the configuration parameter.</p>
- * 
- * <p>Note that the order in which the emulators are invoked is same as the 
- * order in which they are configured.
- */
-public class ResourceUsageMatcher {
-  /**
-   * Configuration key to set resource usage emulators.
-   */
-  public static final String RESOURCE_USAGE_EMULATION_PLUGINS =
-    "gridmix.emulators.resource-usage.plugins";
-  
-  private List<ResourceUsageEmulatorPlugin> emulationPlugins = 
-    new ArrayList<ResourceUsageEmulatorPlugin>();
-  
-  /**
-   * Configure the {@link ResourceUsageMatcher} to load the configured plugins
-   * and initialize them.
-   */
-  @SuppressWarnings("unchecked")
-  public void configure(Configuration conf, ResourceCalculatorPlugin monitor, 
-                        ResourceUsageMetrics metrics, Progressive progress) {
-    Class[] plugins = conf.getClasses(RESOURCE_USAGE_EMULATION_PLUGINS);
-    if (plugins == null) {
-      System.out.println("No resource usage emulator plugins configured.");
-    } else {
-      for (Class clazz : plugins) {
-        if (clazz != null) {
-          if (ResourceUsageEmulatorPlugin.class.isAssignableFrom(clazz)) {
-            ResourceUsageEmulatorPlugin plugin = 
-              (ResourceUsageEmulatorPlugin) ReflectionUtils.newInstance(clazz, 
-                                                                        conf);
-            emulationPlugins.add(plugin);
-          } else {
-            throw new RuntimeException("Misconfigured resource usage plugins. " 
-                + "Class " + clazz.getClass().getName() + " is not a resource "
-                + "usage plugin as it does not extend "
-                + ResourceUsageEmulatorPlugin.class.getName());
-          }
-        }
-      }
-    }
-
-    // initialize the emulators once all the configured emulator plugins are
-    // loaded
-    for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
-      emulator.initialize(conf, metrics, monitor, progress);
-    }
-  }
-  
-  public void matchResourceUsage() throws Exception {
-    for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
-      // match the resource usage
-      emulator.emulate();
-    }
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
deleted file mode 100644
index a50358a..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
+++ /dev/null
@@ -1,258 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-/**
- * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the total heap 
- * usage by loading the JVM heap memory. Adding smaller chunks of data to the 
- * heap will essentially use up some heap space thus forcing the JVM to expand 
- * its heap and thus resulting into increase in the heap usage.</p>
- * 
- * <p>{@link TotalHeapUsageEmulatorPlugin} emulates the heap usage in steps. 
- * The frequency of emulation can be configured via 
- * {@link #HEAP_EMULATION_PROGRESS_INTERVAL}.
- * Heap usage values are matched via emulation only at specific interval 
- * boundaries.
- * </p>
- *  
- * {@link TotalHeapUsageEmulatorPlugin} is a wrapper program for managing 
- * the heap usage emulation feature. It internally uses an emulation algorithm 
- * (called as core and described using {@link HeapUsageEmulatorCore}) for 
- * performing the actual emulation. Multiple calls to this core engine should 
- * use up some amount of heap.
- */
-public class TotalHeapUsageEmulatorPlugin 
-implements ResourceUsageEmulatorPlugin {
-  // Configuration parameters
-  //  the core engine to emulate heap usage
-  protected HeapUsageEmulatorCore emulatorCore;
-  //  the progress bar
-  private Progressive progress;
-  //  decides if this plugin can emulate heap usage or not
-  private boolean enabled = true;
-  //  the progress boundaries/interval where emulation should be done
-  private float emulationInterval;
-  //  target heap usage to emulate
-  private long targetHeapUsageInMB = 0;
-  
-  /**
-   * The frequency (based on task progress) with which memory-emulation code is
-   * run. If the value is set to 0.1 then the emulation will happen at 10% of 
-   * the task's progress. The default value of this parameter is 
-   * {@link #DEFAULT_EMULATION_PROGRESS_INTERVAL}.
-   */
-  public static final String HEAP_EMULATION_PROGRESS_INTERVAL = 
-    "gridmix.emulators.resource-usage.heap.emulation-interval";
-  
-  // Default value for emulation interval
-  private static final float DEFAULT_EMULATION_PROGRESS_INTERVAL = 0.1F; // 10 %
-
-  private float prevEmulationProgress = 0F;
-  
-  /**
-   * The minimum buffer reserved for other non-emulation activities.
-   */
-  public static final String MIN_HEAP_FREE_RATIO = 
-    "gridmix.emulators.resource-usage.heap.min-free-ratio";
-  
-  private float minFreeHeapRatio;
-  
-  private static final float DEFAULT_MIN_FREE_HEAP_RATIO = 0.3F;
-  
-  /**
-   * Determines the unit increase per call to the core engine's load API. This
-   * is expressed as a percentage of the difference between the expected total 
-   * heap usage and the current usage. 
-   */
-  public static final String HEAP_LOAD_RATIO = 
-    "gridmix.emulators.resource-usage.heap.load-ratio";
-  
-  private float heapLoadRatio;
-  
-  private static final float DEFAULT_HEAP_LOAD_RATIO = 0.1F;
-  
-  public static int ONE_MB = 1024 * 1024;
-  
-  /**
-   * Defines the core heap usage emulation algorithm. This engine is expected
-   * to perform certain memory intensive operations to consume some
-   * amount of heap. {@link #load(long)} should load the current heap and 
-   * increase the heap usage by the specified value. This core engine can be 
-   * initialized using the {@link #initialize(ResourceCalculatorPlugin, long)} 
-   * API to suit the underlying hardware better.
-   */
-  public interface HeapUsageEmulatorCore {
-    /**
-     * Performs some memory intensive operations to use up some heap.
-     */
-    public void load(long sizeInMB);
-    
-    /**
-     * Initialize the core.
-     */
-    public void initialize(ResourceCalculatorPlugin monitor, 
-                           long totalHeapUsageInMB);
-    
-    /**
-     * Reset the resource usage
-     */
-    public void reset();
-  }
-  
-  /**
-   * This is the core engine to emulate the heap usage. The only responsibility 
-   * of this class is to perform certain memory intensive operations to make 
-   * sure that some desired value of heap is used.
-   */
-  public static class DefaultHeapUsageEmulator 
-  implements HeapUsageEmulatorCore {
-    // store the unit loads in a list
-    protected static ArrayList<Object> heapSpace = new ArrayList<Object>();
-    
-    /**
-     * Increase heap usage by current process by the given amount.
-     * This is done by creating objects each of size 1MB.
-     */
-    public void load(long sizeInMB) {
-      for (long i = 0; i < sizeInMB; ++i) {
-        // Create another String object of size 1MB
-        heapSpace.add((Object)new byte[ONE_MB]);
-      }
-    }
-    
-    /**
-     * This will initialize the core and check if the core can emulate the 
-     * desired target on the underlying hardware.
-     */
-    public void initialize(ResourceCalculatorPlugin monitor, 
-                           long totalHeapUsageInMB) {
-      long maxPhysicalMemoryInMB = monitor.getPhysicalMemorySize() / ONE_MB ;
-      if(maxPhysicalMemoryInMB < totalHeapUsageInMB) {
-        throw new RuntimeException("Total heap the can be used is " 
-            + maxPhysicalMemoryInMB 
-            + " bytes while the emulator is configured to emulate a total of " 
-            + totalHeapUsageInMB + " bytes");
-      }
-    }
-    
-    /**
-     * Clear references to all the GridMix-allocated special objects so that 
-     * heap usage is reduced.
-     */
-    @Override
-    public void reset() {
-      heapSpace.clear();
-    }
-  }
-  
-  public TotalHeapUsageEmulatorPlugin() {
-    this(new DefaultHeapUsageEmulator());
-  }
-  
-  /**
-   * For testing.
-   */
-  public TotalHeapUsageEmulatorPlugin(HeapUsageEmulatorCore core) {
-    emulatorCore = core;
-  }
-  
-  protected long getTotalHeapUsageInMB() {
-    return Runtime.getRuntime().totalMemory() / ONE_MB;
-  }
-  
-  protected long getMaxHeapUsageInMB() {
-    return Runtime.getRuntime().maxMemory() / ONE_MB;
-  }
-  
-  @Override
-  public void emulate() throws IOException, InterruptedException {
-    if (enabled) {
-      float currentProgress = progress.getProgress();
-      if (prevEmulationProgress < currentProgress 
-          && ((currentProgress - prevEmulationProgress) >= emulationInterval
-              || currentProgress == 1)) {
-
-        long maxHeapSizeInMB = getMaxHeapUsageInMB();
-        long committedHeapSizeInMB = getTotalHeapUsageInMB();
-        
-        // Increase committed heap usage, if needed
-        // Using a linear weighing function for computing the expected usage
-        long expectedHeapUsageInMB = 
-          Math.min(maxHeapSizeInMB,
-                   (long) (targetHeapUsageInMB * currentProgress));
-        if (expectedHeapUsageInMB < maxHeapSizeInMB
-            && committedHeapSizeInMB < expectedHeapUsageInMB) {
-          long bufferInMB = (long)(minFreeHeapRatio * expectedHeapUsageInMB);
-          long currentDifferenceInMB = 
-            expectedHeapUsageInMB - committedHeapSizeInMB;
-          long currentIncrementLoadSizeInMB = 
-                (long)(currentDifferenceInMB * heapLoadRatio);
-          // Make sure that at least 1 MB is incremented.
-          currentIncrementLoadSizeInMB = 
-            Math.max(1, currentIncrementLoadSizeInMB);
-          while (committedHeapSizeInMB + bufferInMB < expectedHeapUsageInMB) {
-            // add blocks in order of X% of the difference, X = 10% by default
-            emulatorCore.load(currentIncrementLoadSizeInMB);
-            committedHeapSizeInMB = getTotalHeapUsageInMB();
-          }
-        }
-        
-        // store the emulation progress boundary
-        prevEmulationProgress = currentProgress;
-      }
-      
-      // reset the core so that the garbage is reclaimed
-      emulatorCore.reset();
-    }
-  }
-
-  @Override
-  public void initialize(Configuration conf, ResourceUsageMetrics metrics,
-                         ResourceCalculatorPlugin monitor,
-                         Progressive progress) {
-    // get the target heap usage
-    targetHeapUsageInMB = metrics.getHeapUsage() / ONE_MB;
-    if (targetHeapUsageInMB <= 0 ) {
-      enabled = false;
-      return;
-    } else {
-      // calibrate the core heap-usage utility
-      emulatorCore.initialize(monitor, targetHeapUsageInMB);
-      enabled = true;
-    }
-    
-    this.progress = progress;
-    emulationInterval = 
-      conf.getFloat(HEAP_EMULATION_PROGRESS_INTERVAL, 
-                    DEFAULT_EMULATION_PROGRESS_INTERVAL);
-    
-    minFreeHeapRatio = conf.getFloat(MIN_HEAP_FREE_RATIO, 
-                                     DEFAULT_MIN_FREE_HEAP_RATIO);
-    
-    heapLoadRatio = conf.getFloat(HEAP_LOAD_RATIO, DEFAULT_HEAP_LOAD_RATIO);
-    
-    prevEmulationProgress = 0;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/data/wordcount.json.gz b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/data/wordcount.json.gz
deleted file mode 100644
index c5cc40e7618a5ec775ccd0a827e1d46ea120e0b2..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 1452
zcmV;d1ylMTiwFq8l%-4p19xw7WMgl2Zgehcb8l_{?V3w(+c*$_@A(ym&rZsRL`i#d
zon+A>NekO)4@D8kHeG9@*Gkl)L6QGnN|Yr^l&vJJyDpGi;&5g-Lvr|WNax4NU@(jm
zFG@W2FdY0f7~XuCUXQPaqg?P$w0L)3wDM1P2tWWFWIz+6I{*L_N?A&iQfCvcB3hZ=
zxB)E$pQB_oiL#IhQQ*dNH@#1D*_Xi&-ozVI<|b_J2@8m9?gJ4MVgg1&fpL>EnEay}
z&0UcH;y**lV*U_|7_P@3x9u)tG>BQlKX3M<rOUi1%$vi^_eWd7CQ4Z~mNzT9b~B%y
zChqe7%{q$C=yx}WeR{Txf?;K|b2y|@S$m<EZWJ9G5mf0UH08bu$FtkX<YG3fU6jKp
z=cN2=&$QeDDZEX#Ui<X22!XEh9!_B`jS{v)v{|jww-1b_Q|cBn3mC`(s{AugC`QAU
z`P%bo!@Nr-nHqdH>KWsS^Dc}tHl<6RQN{W}%-zchA2P-@ihc??v=PQw)sXu@1dv*V
zuj9fCsp6@rmC#kT6iR<5Q5L7I(MWQs6GdSr7NUw*QBb0l*D3aVpROb!ArL9kC3Rki
z9tU5`ozR_v1Xz|`YQza$(}ag|RqhUD*SHTfDI16K-WZN}|E=vY_}U5}-=Z_-vMlY$
z@uAm<Ud}Wj>du2<)Jy>>hf0yMs$BRu>Ln3;E{WhVlW4odyGdj@fE<EE@GvAYEek>8
zP$V*KXyE-M0?^z;BF8pOZ0rXSunhouLDbYbP7vAhQfPq4hQ|jYIng>mRHZ>1L{bWM
zfT&p2eh}qz;yf2b=x9MifDNo35D~*B<{?0Y4g(@cup<rX7lQ~v6M}sp!WJfbID{+?
zcCwd47?{ZJ!%$P{I4OjI0rV+EY+xQAh2$(#C{*V_8$we0s1PdF^rVVt$Anm>jSI)+
znmq9`$I0b9{W6#AyUa~5&Tl6de+0ZNV42P@g;(;l`&<<y-|c88$0{v;Hj&*(>%yJH
zB#4mZ;N3`r5CTN@Vy&gp!8Iucm1<qWZrdF=>nW%pv>@)G8MJIddVmIz#V@3NP-}Wq
zxz_abQm|QwS7|+9dl8wWF{<@Z9eNv0au#(INco&dY)Z35;3-?uFriVLN}R-5xZJ36
zB1*R*i<s+QyJC59-83zhM7S(kBetQ*6B)<vnvU_@>dfe5-0Wh0`~TNxhRui6gE+%6
zZTk@QnQ@r<tOsTqwONxwzZYQL#nurOWBQet39)$yU~R)zU{wjC?!{6mRg9$P3YT;D
z*;hE+35m(o#rWn3Fu{j}``(i*>{##+n82@?c*R8Peq#O!m?$e5zEWJuL;FaRYiZ1N
z7j2a~DR$^g>!s_}>g;So-T0b*a~D-{29T(m_*i%!RGF^)8$Y(?();nJv1na+30<;t
z8q;v)hU|T`kXL|LaZ6rorRyv&#l~W%PtGr!rB>j6_kt`aUQrRk*g^mk+cYe`($y6>
z;JHvLovK>f?=uoNy!XOytt<$$U_p~luAhm)Hs9f1-IqNRd;4%C`!j&q-G6odtoZ(F
zsTOeX`HuXLPK8n7`3#t1i;rGp4YiE5Tq7-GE!SAfSj)AcWvt~|&sfWvXc_NuO`1=t
z9oCkXv6gFF%UH{`qh)Nis)jc2<X45Y%j>n(>sj>bzILyaDxz+xGbGpSh|n>S8j{r7
zcUsn}FF7syt)Z5&mTRPCtmPVO8Ed&Vw2ZY}>ltfV6D?yc*Or#CmTOzfSj)AeW!!gK
ziU+Rxtb8I2f8%LU{*P%)lX>FNbRNxea*GF_F9Uc6+Gjo2yiDBX_KA#hwfuHd%+L0H
zLA3sx(M()&%l9&I&t3Xi%8JYR^;`M6Et<t7H6;AHTq#$VYu+fU=ItksCqDrWrmn)J
GD*ynGGs_|X

diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
deleted file mode 100644
index 413dfd9..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.tools.rumen.JobStory;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.concurrent.CountDownLatch;
-
-
-/**
- * Component generating random job traces for testing on a single node.
- */
-class DebugJobFactory {
-
-  interface Debuggable {
-    ArrayList<JobStory> getSubmitted();
-  }
-
-  public static JobFactory getFactory(
-    JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-    CountDownLatch startFlag, UserResolver resolver) throws IOException {
-    GridmixJobSubmissionPolicy policy = GridmixJobSubmissionPolicy.getPolicy(
-      conf, GridmixJobSubmissionPolicy.STRESS);
-    if (policy == GridmixJobSubmissionPolicy.REPLAY) {
-      return new DebugReplayJobFactory(
-        submitter, scratch, numJobs, conf, startFlag, resolver);
-    } else if (policy == GridmixJobSubmissionPolicy.STRESS) {
-      return new DebugStressJobFactory(
-        submitter, scratch, numJobs, conf, startFlag, resolver);
-    } else if (policy == GridmixJobSubmissionPolicy.SERIAL) {
-      return new DebugSerialJobFactory(
-        submitter, scratch, numJobs, conf, startFlag, resolver);
-
-    }
-    return null;
-  }
-
-  static class DebugReplayJobFactory extends ReplayJobFactory
-    implements Debuggable {
-    public DebugReplayJobFactory(
-      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-      CountDownLatch startFlag, UserResolver resolver) throws IOException {
-      super(
-        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
-        startFlag, resolver);
-    }
-
-    @Override
-    public ArrayList<JobStory> getSubmitted() {
-      return ((DebugJobProducer) jobProducer).submitted;
-    }
-
-  }
-
-  static class DebugSerialJobFactory extends SerialJobFactory
-    implements Debuggable {
-    public DebugSerialJobFactory(
-      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-      CountDownLatch startFlag, UserResolver resolver) throws IOException {
-      super(
-        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
-        startFlag, resolver);
-    }
-
-    @Override
-    public ArrayList<JobStory> getSubmitted() {
-      return ((DebugJobProducer) jobProducer).submitted;
-    }
-  }
-
-  static class DebugStressJobFactory extends StressJobFactory
-    implements Debuggable {
-    public DebugStressJobFactory(
-      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-      CountDownLatch startFlag, UserResolver resolver) throws IOException {
-      super(
-        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
-        startFlag, resolver);
-    }
-
-    @Override
-    public ArrayList<JobStory> getSubmitted() {
-      return ((DebugJobProducer) jobProducer).submitted;
-    }
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
deleted file mode 100644
index fca29af..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
+++ /dev/null
@@ -1,313 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.mapred.TaskStatus.State;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.MapTaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants.Values;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.InputSplit;
-
-import java.util.ArrayList;
-import java.util.Random;
-import java.util.Arrays;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicLong;
-import java.util.concurrent.TimeUnit;
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-
-public class DebugJobProducer implements JobStoryProducer {
-  public static final Log LOG = LogFactory.getLog(DebugJobProducer.class);
-  final ArrayList<JobStory> submitted;
-  private final Configuration conf;
-  private final AtomicInteger numJobs;
-
-  public DebugJobProducer(int numJobs, Configuration conf) {
-    super();
-    MockJob.reset();
-    this.conf = conf;
-    this.numJobs = new AtomicInteger(numJobs);
-    this.submitted = new ArrayList<JobStory>();
-  }
-
-  @Override
-  public JobStory getNextJob() throws IOException {
-    if (numJobs.getAndDecrement() > 0) {
-      final MockJob ret = new MockJob(conf);
-      submitted.add(ret);
-      return ret;
-    }
-    return null;
-  }
-
-  @Override
-  public void close() {
-  }
-
-
-  static double[] getDistr(Random r, double mindist, int size) {
-    assert 0.0 <= mindist && mindist <= 1.0;
-    final double min = mindist / size;
-    final double rem = 1.0 - min * size;
-    final double[] tmp = new double[size];
-    for (int i = 0; i < tmp.length - 1; ++i) {
-      tmp[i] = r.nextDouble() * rem;
-    }
-    tmp[tmp.length - 1] = rem;
-    Arrays.sort(tmp);
-
-    final double[] ret = new double[size];
-    ret[0] = tmp[0] + min;
-    for (int i = 1; i < size; ++i) {
-      ret[i] = tmp[i] - tmp[i - 1] + min;
-    }
-    return ret;
-  }
-
-
-  /**
-   * Generate random task data for a synthetic job.
-   */
-  static class MockJob implements JobStory {
-
-    static final int MIN_REC = 1 << 14;
-    static final int MIN_BYTES = 1 << 20;
-    static final int VAR_REC = 1 << 14;
-    static final int VAR_BYTES = 4 << 20;
-    static final int MAX_MAP = 5;
-    static final int MAX_RED = 3;
-    final Configuration conf;
-
-    static void initDist(
-      Random r, double min, int[] recs, long[] bytes, long tot_recs,
-      long tot_bytes) {
-      final double[] recs_dist = getDistr(r, min, recs.length);
-      final double[] bytes_dist = getDistr(r, min, recs.length);
-      long totalbytes = 0L;
-      int totalrecs = 0;
-      for (int i = 0; i < recs.length; ++i) {
-        recs[i] = (int) Math.round(tot_recs * recs_dist[i]);
-        bytes[i] = Math.round(tot_bytes * bytes_dist[i]);
-        totalrecs += recs[i];
-        totalbytes += bytes[i];
-      }
-      // Add/remove excess
-      recs[0] += totalrecs - tot_recs;
-      bytes[0] += totalbytes - tot_bytes;
-      if (LOG.isInfoEnabled()) {
-        LOG.info(
-          "DIST: " + Arrays.toString(recs) + " " + tot_recs + "/" + totalrecs +
-            " " + Arrays.toString(bytes) + " " + tot_bytes + "/" + totalbytes);
-      }
-    }
-
-    private static final AtomicInteger seq = new AtomicInteger(0);
-    // set timestamp in the past
-    private static final AtomicLong timestamp = new AtomicLong(
-      System.currentTimeMillis() - TimeUnit.MILLISECONDS.convert(
-        60, TimeUnit.DAYS));
-
-    private final int id;
-    private final String name;
-    private final int[] m_recsIn, m_recsOut, r_recsIn, r_recsOut;
-    private final long[] m_bytesIn, m_bytesOut, r_bytesIn, r_bytesOut;
-    private final long submitTime;
-
-    public MockJob(Configuration conf) {
-      final Random r = new Random();
-      final long seed = r.nextLong();
-      r.setSeed(seed);
-      id = seq.getAndIncrement();
-      name = String.format("MOCKJOB%06d", id);
-
-      this.conf = conf;
-      LOG.info(name + " (" + seed + ")");
-      submitTime = timestamp.addAndGet(
-        TimeUnit.MILLISECONDS.convert(
-          r.nextInt(10), TimeUnit.SECONDS));
-
-      m_recsIn = new int[r.nextInt(MAX_MAP) + 1];
-      m_bytesIn = new long[m_recsIn.length];
-      m_recsOut = new int[m_recsIn.length];
-      m_bytesOut = new long[m_recsIn.length];
-
-      r_recsIn = new int[r.nextInt(MAX_RED) + 1];
-      r_bytesIn = new long[r_recsIn.length];
-      r_recsOut = new int[r_recsIn.length];
-      r_bytesOut = new long[r_recsIn.length];
-
-      // map input
-      final long map_recs = r.nextInt(VAR_REC) + MIN_REC;
-      final long map_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
-      initDist(r, 0.5, m_recsIn, m_bytesIn, map_recs, map_bytes);
-
-      // shuffle
-      final long shuffle_recs = r.nextInt(VAR_REC) + MIN_REC;
-      final long shuffle_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
-      initDist(r, 0.5, m_recsOut, m_bytesOut, shuffle_recs, shuffle_bytes);
-      initDist(r, 0.8, r_recsIn, r_bytesIn, shuffle_recs, shuffle_bytes);
-
-      // reduce output
-      final long red_recs = r.nextInt(VAR_REC) + MIN_REC;
-      final long red_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
-      initDist(r, 0.5, r_recsOut, r_bytesOut, red_recs, red_bytes);
-
-      if (LOG.isDebugEnabled()) {
-        int iMapBTotal = 0, oMapBTotal = 0, iRedBTotal = 0, oRedBTotal = 0;
-        int iMapRTotal = 0, oMapRTotal = 0, iRedRTotal = 0, oRedRTotal = 0;
-        for (int i = 0; i < m_recsIn.length; ++i) {
-          iMapRTotal += m_recsIn[i];
-          iMapBTotal += m_bytesIn[i];
-          oMapRTotal += m_recsOut[i];
-          oMapBTotal += m_bytesOut[i];
-        }
-        for (int i = 0; i < r_recsIn.length; ++i) {
-          iRedRTotal += r_recsIn[i];
-          iRedBTotal += r_bytesIn[i];
-          oRedRTotal += r_recsOut[i];
-          oRedBTotal += r_bytesOut[i];
-        }
-        LOG.debug(
-          String.format(
-            "%s: M (%03d) %6d/%10d -> %6d/%10d" +
-              " R (%03d) %6d/%10d -> %6d/%10d @%d", name, m_bytesIn.length,
-            iMapRTotal, iMapBTotal, oMapRTotal, oMapBTotal, r_bytesIn.length,
-            iRedRTotal, iRedBTotal, oRedRTotal, oRedBTotal, submitTime));
-      }
-    }
-    @Override
-   public String getName() {
-     return name;
-    }
-
-   @Override
-   public String getUser() {
-     // Obtain user name from job configuration, if available.
-     // Otherwise use dummy user names.
-     String user = conf.get(MRJobConfig.USER_NAME);
-     if (user == null) {
-       user = String.format("foobar%d", id);
-     }
-     GridmixTestUtils.createHomeAndStagingDirectory(user, (JobConf)conf);
-     return user;
-   }
-
-   @Override
-   public JobID getJobID() {
-     return new JobID("job_mock_" + name, id);
-    }
-
-    @Override
-   public Values getOutcome() {
-     return Values.SUCCESS;
-    }
-
-   @Override
-   public long getSubmissionTime() {
-     return submitTime;
-   }
-
-   @Override
-   public int getNumberMaps() {
-     return m_bytesIn.length;
-   }
-
-   @Override
-   public int getNumberReduces() {
-     return r_bytesIn.length;
-   }
-    
-    @Override
-    public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
-      switch (taskType) {
-        case MAP:
-          return new TaskInfo(m_bytesIn[taskNumber], m_recsIn[taskNumber],
-              m_bytesOut[taskNumber], m_recsOut[taskNumber], -1);
-        case REDUCE:
-          return new TaskInfo(r_bytesIn[taskNumber], r_recsIn[taskNumber],
-              r_bytesOut[taskNumber], r_recsOut[taskNumber], -1);
-        default:
-          throw new IllegalArgumentException("Not interested");
-      }
-    }
-
-    @Override
-    public InputSplit[] getInputSplits() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public TaskAttemptInfo getTaskAttemptInfo(
-      TaskType taskType, int taskNumber, int taskAttemptNumber) {
-      switch (taskType) {
-        case MAP:
-          return new MapTaskAttemptInfo(
-            State.SUCCEEDED, 
-            new TaskInfo(
-              m_bytesIn[taskNumber], m_recsIn[taskNumber],
-              m_bytesOut[taskNumber], m_recsOut[taskNumber], -1),
-            100);
-
-        case REDUCE:
-          return new ReduceTaskAttemptInfo(
-            State.SUCCEEDED, 
-            new TaskInfo(
-              r_bytesIn[taskNumber], r_recsIn[taskNumber],
-              r_bytesOut[taskNumber], r_recsOut[taskNumber], -1),
-            100, 100, 100);
-      }
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public TaskAttemptInfo getMapTaskAttemptInfoAdjusted(
-      int taskNumber, int taskAttemptNumber, int locality) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public org.apache.hadoop.mapred.JobConf getJobConf() {
-      return new JobConf(conf);
-    }
-
-    @Override
-    public String getQueueName() {
-      String qName = "q" + ((id % 2) + 1);
-      return qName;
-    }
-    
-    public static void reset() {
-      seq.set(0);
-      timestamp.set(System.currentTimeMillis() - TimeUnit.MILLISECONDS.convert(
-        60, TimeUnit.DAYS));
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
deleted file mode 100644
index 8b727d2..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.CommonConfigurationKeys;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;
-import org.apache.hadoop.security.Groups;
-
-import java.io.IOException;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-public class GridmixTestUtils {
-  static final Path DEST = new Path("/gridmix");
-  static FileSystem dfs = null;
-  static MiniDFSCluster dfsCluster = null;
-  static MiniMRCluster mrCluster = null;
-
-  public static void initCluster() throws IOException {
-    Configuration conf = new Configuration();
-    conf.set("mapred.queue.names", "default,q1,q2");
-    dfsCluster = new MiniDFSCluster(conf, 3, true, null);
-    dfs = dfsCluster.getFileSystem();
-    conf.set(JTConfig.JT_RETIREJOBS, "false");
-    mrCluster = new MiniMRCluster(3, dfs.getUri().toString(), 1, null, null, 
-                                  new JobConf(conf));
-  }
-
-  public static void shutdownCluster() throws IOException {
-    if (mrCluster != null) {
-      mrCluster.shutdown();
-    }
-    if (dfsCluster != null) {
-      dfsCluster.shutdown();
-    }
-  }
-
-  /**
-   * Methods to generate the home directory for dummy users.
-   *
-   * @param conf
-   */
-  public static void createHomeAndStagingDirectory(String user, JobConf conf) {
-    try {
-      FileSystem fs = dfsCluster.getFileSystem();
-      String path = "/user/" + user;
-      Path homeDirectory = new Path(path);
-      if(fs.exists(homeDirectory)) {
-        fs.delete(homeDirectory,true);
-      }
-      TestGridmixSubmission.LOG.info(
-        "Creating Home directory : " + homeDirectory);
-      fs.mkdirs(homeDirectory);
-      changePermission(user,homeDirectory, fs);
-      Path stagingArea = 
-        new Path(conf.get("mapreduce.jobtracker.staging.root.dir",
-                          "/tmp/hadoop/mapred/staging"));
-      TestGridmixSubmission.LOG.info(
-        "Creating Staging root directory : " + stagingArea);
-      fs.mkdirs(stagingArea);
-      fs.setPermission(stagingArea, new FsPermission((short) 0777));
-    } catch (IOException ioe) {
-      ioe.printStackTrace();
-    }
-  }
-
-  static void changePermission(String user, Path homeDirectory, FileSystem fs)
-    throws IOException {
-    fs.setOwner(homeDirectory, user, "");
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
deleted file mode 100644
index 51071a0..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
+++ /dev/null
@@ -1,564 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil.RandomTextDataMapper;
-import org.apache.hadoop.mapred.gridmix.GenerateData.GenSplit;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-
-import static org.junit.Assert.*;
-import org.junit.Test;
-
-/**
- * Test {@link CompressionEmulationUtil}
- */
-public class TestCompressionEmulationUtils {
-  //TODO Remove this once LocalJobRunner can run Gridmix.
-  static class CustomInputFormat extends GenerateData.GenDataFormat {
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      // get the total data to be generated
-      long toGen =
-        jobCtxt.getConfiguration().getLong(GenerateData.GRIDMIX_GEN_BYTES, -1);
-      if (toGen < 0) {
-        throw new IOException("Invalid/missing generation bytes: " + toGen);
-      }
-      // get the total number of mappers configured
-      int totalMappersConfigured =
-        jobCtxt.getConfiguration().getInt(MRJobConfig.NUM_MAPS, -1);
-      if (totalMappersConfigured < 0) {
-        throw new IOException("Invalid/missing num mappers: " 
-                              + totalMappersConfigured);
-      }
-      
-      final long bytesPerTracker = toGen / totalMappersConfigured;
-      final ArrayList<InputSplit> splits = 
-        new ArrayList<InputSplit>(totalMappersConfigured);
-      for (int i = 0; i < totalMappersConfigured; ++i) {
-        splits.add(new GenSplit(bytesPerTracker, 
-                   new String[] { "tracker_local" }));
-      }
-      return splits;
-    }
-  }
-  
-  /**
-   * Test {@link RandomTextDataMapper} via {@link CompressionEmulationUtil}.
-   */
-  @Test
-  public void testRandomCompressedTextDataGenerator() throws Exception {
-    int wordSize = 10;
-    int listSize = 20;
-    long dataSize = 10*1024*1024;
-    
-    Configuration conf = new Configuration();
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    
-    // configure the RandomTextDataGenerator to generate desired sized data
-    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, 
-                listSize);
-    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, 
-                wordSize);
-    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);
-    conf.set("mapreduce.job.hdfs-servers", "");
-    
-    FileSystem lfs = FileSystem.getLocal(conf);
-    
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = new Path(rootTempDir, "TestRandomCompressedTextDataGenr");
-    lfs.delete(tempDir, true);
-    
-    runDataGenJob(conf, tempDir);
-    
-    // validate the output data
-    FileStatus[] files = 
-      lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());
-    long size = 0;
-    long maxLineSize = 0;
-    
-    for (FileStatus status : files) {
-      InputStream in = 
-        CompressionEmulationUtil
-          .getPossiblyDecompressedInputStream(status.getPath(), conf, 0);
-      BufferedReader reader = new BufferedReader(new InputStreamReader(in));
-      String line = reader.readLine();
-      if (line != null) {
-        long lineSize = line.getBytes().length;
-        if (lineSize > maxLineSize) {
-          maxLineSize = lineSize;
-        }
-        while (line != null) {
-          for (String word : line.split("\\s")) {
-            size += word.getBytes().length;
-          }
-          line = reader.readLine();
-        }
-      }
-      reader.close();
-    }
-
-    assertTrue(size >= dataSize);
-    assertTrue(size <= dataSize + maxLineSize);
-  }
-  
-  /**
-   * Runs a GridMix data-generation job.
-   */
-  private static void runDataGenJob(Configuration conf, Path tempDir) 
-  throws IOException, ClassNotFoundException, InterruptedException {
-    JobClient client = new JobClient(conf);
-    
-    // get the local job runner
-    conf.setInt(MRJobConfig.NUM_MAPS, 1);
-    
-    Job job = new Job(conf);
-    
-    CompressionEmulationUtil.configure(job);
-    job.setInputFormatClass(CustomInputFormat.class);
-    
-    // set the output path
-    FileOutputFormat.setOutputPath(job, tempDir);
-    
-    // submit and wait for completion
-    job.submit();
-    int ret = job.waitForCompletion(true) ? 0 : 1;
-
-    assertEquals("Job Failed", 0, ret);
-  }
-  
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate random text data 
-   * with the desired compression ratio. This involves
-   *   - using {@link CompressionEmulationUtil} to configure the MR job for 
-   *     generating the random text data with the desired compression ratio
-   *   - running the MR job
-   *   - test {@link RandomTextDataGenerator}'s output and match the output size
-   *     (compressed) with the expected compression ratio.
-   */
-  private void testCompressionRatioConfigure(float ratio)
-  throws Exception {
-    long dataSize = 10*1024*1024;
-    
-    Configuration conf = new Configuration();
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    
-    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);
-    conf.set("mapreduce.job.hdfs-servers", "");
-    
-    float expectedRatio = CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;
-    if (ratio > 0) {
-      // set the compression ratio in the conf
-      CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);
-      expectedRatio = 
-        CompressionEmulationUtil.standardizeCompressionRatio(ratio);
-    }
-    
-    // invoke the utility to map from ratio to word-size
-    CompressionEmulationUtil.setupDataGeneratorConfig(conf);
-    
-    FileSystem lfs = FileSystem.getLocal(conf);
-    
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = 
-      new Path(rootTempDir, "TestCustomRandomCompressedTextDataGenr");
-    lfs.delete(tempDir, true);
-    
-    runDataGenJob(conf, tempDir);
-    
-    // validate the output data
-    FileStatus[] files = 
-      lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());
-    long size = 0;
-    
-    for (FileStatus status : files) {
-      size += status.getLen();
-    }
-
-    float compressionRatio = ((float)size)/dataSize;
-    float stdRatio = 
-      CompressionEmulationUtil.standardizeCompressionRatio(compressionRatio);
-    
-    assertEquals(expectedRatio, stdRatio, 0.0D);
-  }
-  
-  /**
-   * Test compression ratio with multiple compression ratios.
-   */
-  @Test
-  public void testCompressionRatios() throws Exception {
-    // test default compression ratio i.e 0.5
-    testCompressionRatioConfigure(0F);
-    // test for a sample compression ratio of 0.2
-    testCompressionRatioConfigure(0.2F);
-    // test for a sample compression ratio of 0.4
-    testCompressionRatioConfigure(0.4F);
-    // test for a sample compression ratio of 0.65
-    testCompressionRatioConfigure(0.65F);
-    // test for a compression ratio of 0.682 which should be standardized
-    // to round(0.682) i.e 0.68
-    testCompressionRatioConfigure(0.682F);
-    // test for a compression ratio of 0.567 which should be standardized
-    // to round(0.567) i.e 0.57
-    testCompressionRatioConfigure(0.567F);
-    
-    // test with a compression ratio of 0.01 which less than the min supported
-    // value of 0.07
-    boolean failed = false;
-    try {
-      testCompressionRatioConfigure(0.01F);
-    } catch (RuntimeException re) {
-      failed = true;
-    }
-    assertTrue("Compression ratio min value (0.07) check failed!", failed);
-    
-    // test with a compression ratio of 0.01 which less than the max supported
-    // value of 0.68
-    failed = false;
-    try {
-      testCompressionRatioConfigure(0.7F);
-    } catch (RuntimeException re) {
-      failed = true;
-    }
-    assertTrue("Compression ratio max value (0.68) check failed!", failed);
-  }
-  
-  /**
-   * Test compression ratio standardization.
-   */
-  @Test
-  public void testCompressionRatioStandardization() throws Exception {
-    assertEquals(0.55F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.55F), 0.0D);
-    assertEquals(0.65F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.652F), 0.0D);
-    assertEquals(0.78F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.777F), 0.0D);
-    assertEquals(0.86F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.855F), 0.0D);
-  }
-  
-  /**
-   * Test map input compression ratio configuration utilities.
-   */
-  @Test
-  public void testInputCompressionRatioConfiguration() throws Exception {
-    Configuration conf = new Configuration();
-    float ratio = 0.567F;
-    CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);
-    assertEquals(ratio, 
-        CompressionEmulationUtil.getMapInputCompressionEmulationRatio(conf), 
-        0.0D);
-  }
-  
-  /**
-   * Test map output compression ratio configuration utilities.
-   */
-  @Test
-  public void testIntermediateCompressionRatioConfiguration() 
-  throws Exception {
-    Configuration conf = new Configuration();
-    float ratio = 0.567F;
-    CompressionEmulationUtil.setMapOutputCompressionEmulationRatio(conf, ratio);
-    assertEquals(ratio, 
-        CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf), 
-        0.0D);
-  }
-  
-  /**
-   * Test reduce output compression ratio configuration utilities.
-   */
-  @Test
-  public void testOutputCompressionRatioConfiguration() throws Exception {
-    Configuration conf = new Configuration();
-    float ratio = 0.567F;
-    CompressionEmulationUtil.setReduceOutputCompressionEmulationRatio(conf, 
-                                                                      ratio);
-    assertEquals(ratio, 
-        CompressionEmulationUtil.getReduceOutputCompressionEmulationRatio(conf),
-        0.0D);
-  }
-  
-  /**
-   * Test compressible {@link GridmixRecord}.
-   */
-  @Test
-  public void testCompressibleGridmixRecord() throws IOException {
-    JobConf conf = new JobConf();
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    
-    FileSystem lfs = FileSystem.getLocal(conf);
-    int dataSize = 1024 * 1024 * 10; // 10 MB
-    float ratio = 0.357F;
-    
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = new Path(rootTempDir, 
-                            "TestPossiblyCompressibleGridmixRecord");
-    lfs.delete(tempDir, true);
-    
-    // define a compressible GridmixRecord
-    GridmixRecord record = new GridmixRecord(dataSize, 0);
-    record.setCompressibility(true, ratio); // enable compression
-    
-    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, 
-                  CompressionCodec.class);
-    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);
-    
-    // write the record to a file
-    Path recordFile = new Path(tempDir, "record");
-    OutputStream outStream = CompressionEmulationUtil
-                               .getPossiblyCompressedOutputStream(recordFile, 
-                                                                  conf);    
-    DataOutputStream out = new DataOutputStream(outStream);
-    record.write(out);
-    out.close();
-    outStream.close();
-    
-    // open the compressed stream for reading
-    Path actualRecordFile = recordFile.suffix(".gz");
-    InputStream in = 
-      CompressionEmulationUtil
-        .getPossiblyDecompressedInputStream(actualRecordFile, conf, 0);
-    
-    // get the compressed file size
-    long compressedFileSize = lfs.listStatus(actualRecordFile)[0].getLen();
-    
-    GridmixRecord recordRead = new GridmixRecord();
-    recordRead.readFields(new DataInputStream(in));
-    
-    assertEquals("Record size mismatch in a compressible GridmixRecord",
-                 dataSize, recordRead.getSize());
-    assertTrue("Failed to generate a compressible GridmixRecord",
-               recordRead.getSize() > compressedFileSize);
-    
-    // check if the record can generate data with the desired compression ratio
-    float seenRatio = ((float)compressedFileSize)/dataSize;
-    assertEquals(CompressionEmulationUtil.standardizeCompressionRatio(ratio), 
-        CompressionEmulationUtil.standardizeCompressionRatio(seenRatio), 1.0D);
-  }
-  
-  /**
-   * Test 
-   * {@link CompressionEmulationUtil#isCompressionEmulationEnabled(
-   *          org.apache.hadoop.conf.Configuration)}.
-   */
-  @Test
-  public void testIsCompressionEmulationEnabled() {
-    Configuration conf = new Configuration();
-    // Check default values
-    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
-    
-    // Check disabled
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    assertFalse(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
-    
-    // Check enabled
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
-  }
-  
-  /**
-   * Test 
-   * {@link CompressionEmulationUtil#getPossiblyDecompressedInputStream(Path, 
-   *                                   Configuration, long)}
-   *  and
-   *  {@link CompressionEmulationUtil#getPossiblyCompressedOutputStream(Path, 
-   *                                    Configuration)}.
-   */
-  @Test
-  public void testPossiblyCompressedDecompressedStreams() throws IOException {
-    JobConf conf = new JobConf();
-    FileSystem lfs = FileSystem.getLocal(conf);
-    String inputLine = "Hi Hello!";
-
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(FileOutputFormat.COMPRESS, true);
-    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, 
-                  CompressionCodec.class);
-
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir =
-      new Path(rootTempDir, "TestPossiblyCompressedDecompressedStreams");
-    lfs.delete(tempDir, true);
-
-    // create a compressed file
-    Path compressedFile = new Path(tempDir, "test");
-    OutputStream out = 
-      CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, 
-                                                                 conf);
-    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));
-    writer.write(inputLine);
-    writer.close();
-    
-    // now read back the data from the compressed stream
-    compressedFile = compressedFile.suffix(".gz");
-    InputStream in = 
-      CompressionEmulationUtil
-        .getPossiblyDecompressedInputStream(compressedFile, conf, 0);
-    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
-    String readLine = reader.readLine();
-    assertEquals("Compression/Decompression error", inputLine, readLine);
-    reader.close();
-  }
-  
-  /**
-   * Test if 
-   * {@link CompressionEmulationUtil#configureCompressionEmulation(
-   *        org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.JobConf)}
-   *  can extract compression related configuration parameters.
-   */
-  @Test
-  public void testExtractCompressionConfigs() {
-    JobConf source = new JobConf();
-    JobConf target = new JobConf();
-    
-    // set the default values
-    source.setBoolean(FileOutputFormat.COMPRESS, false);
-    source.set(FileOutputFormat.COMPRESS_CODEC, "MyDefaultCodec");
-    source.set(FileOutputFormat.COMPRESS_TYPE, "MyDefaultType");
-    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false); 
-    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, "MyDefaultCodec2");
-    
-    CompressionEmulationUtil.configureCompressionEmulation(source, target);
-    
-    // check default values
-    assertFalse(target.getBoolean(FileOutputFormat.COMPRESS, true));
-    assertEquals("MyDefaultCodec", target.get(FileOutputFormat.COMPRESS_CODEC));
-    assertEquals("MyDefaultType", target.get(FileOutputFormat.COMPRESS_TYPE));
-    assertFalse(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true));
-    assertEquals("MyDefaultCodec2", 
-                 target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));
-    assertFalse(CompressionEmulationUtil
-                .isInputCompressionEmulationEnabled(target));
-    
-    // set new values
-    source.setBoolean(FileOutputFormat.COMPRESS, true);
-    source.set(FileOutputFormat.COMPRESS_CODEC, "MyCodec");
-    source.set(FileOutputFormat.COMPRESS_TYPE, "MyType");
-    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true); 
-    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, "MyCodec2");
-    org.apache.hadoop.mapred.FileInputFormat.setInputPaths(source, "file.gz");
-    
-    target = new JobConf(); // reset
-    CompressionEmulationUtil.configureCompressionEmulation(source, target);
-    
-    // check new values
-    assertTrue(target.getBoolean(FileOutputFormat.COMPRESS, false));
-    assertEquals("MyCodec", target.get(FileOutputFormat.COMPRESS_CODEC));
-    assertEquals("MyType", target.get(FileOutputFormat.COMPRESS_TYPE));
-    assertTrue(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));
-    assertEquals("MyCodec2", 
-                 target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));
-    assertTrue(CompressionEmulationUtil
-               .isInputCompressionEmulationEnabled(target));
-  }
-  
-  /**
-   * Test of {@link FileQueue} can identify compressed file and provide
-   * readers to extract uncompressed data only if input-compression is enabled.
-   */
-  @Test
-  public void testFileQueueDecompression() throws IOException {
-    JobConf conf = new JobConf();
-    FileSystem lfs = FileSystem.getLocal(conf);
-    String inputLine = "Hi Hello!";
-    
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);
-    org.apache.hadoop.mapred.FileOutputFormat.setOutputCompressorClass(conf, 
-                                                GzipCodec.class);
-
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = new Path(rootTempDir, "TestFileQueueDecompression");
-    lfs.delete(tempDir, true);
-
-    // create a compressed file
-    Path compressedFile = new Path(tempDir, "test");
-    OutputStream out = 
-      CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, 
-                                                                 conf);
-    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));
-    writer.write(inputLine);
-    writer.close();
-    
-    compressedFile = compressedFile.suffix(".gz");
-    // now read back the data from the compressed stream using FileQueue
-    long fileSize = lfs.listStatus(compressedFile)[0].getLen();
-    CombineFileSplit split = 
-      new CombineFileSplit(new Path[] {compressedFile}, new long[] {fileSize});
-    FileQueue queue = new FileQueue(split, conf);
-    byte[] bytes = new byte[inputLine.getBytes().length];
-    queue.read(bytes);
-    queue.close();
-    String readLine = new String(bytes);
-    assertEquals("Compression/Decompression error", inputLine, readLine);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFilePool.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFilePool.java
deleted file mode 100644
index 4be90c6..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFilePool.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.OutputStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.Random;
-
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-public class TestFilePool {
-
-  static final Log LOG = LogFactory.getLog(TestFileQueue.class);
-  static final int NFILES = 26;
-  static final Path base = getBaseDir();
-
-  static Path getBaseDir() {
-    try {
-      final Configuration conf = new Configuration();
-      final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-      return new Path(System.getProperty("test.build.data", "/tmp"),
-          "testFilePool").makeQualified(fs);
-    } catch (IOException e) {
-      fail();
-    }
-    return null;
-  }
-
-  @BeforeClass
-  public static void setup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    fs.delete(base, true);
-    final Random r = new Random();
-    final long seed = r.nextLong();
-    r.setSeed(seed);
-    LOG.info("seed: " + seed);
-    fs.mkdirs(base);
-    for (int i = 0; i < NFILES; ++i) {
-      Path file = base;
-      for (double d = 0.6; d > 0.0; d *= 0.8) {
-        if (r.nextDouble() < d) {
-          file = new Path(base, Integer.toString(r.nextInt(3)));
-          continue;
-        }
-        break;
-      }
-      OutputStream out = null;
-      try {
-        out = fs.create(new Path(file, "" + (char)('A' + i)));
-        final byte[] b = new byte[1024];
-        Arrays.fill(b, (byte)('A' + i));
-        for (int len = ((i % 13) + 1) * 1024; len > 0; len -= 1024) {
-          out.write(b);
-        }
-      } finally {
-        if (out != null) {
-          out.close();
-        }
-      }
-    }
-  }
-
-  @AfterClass
-  public static void cleanup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    fs.delete(base, true);
-  }
-
-  @Test
-  public void testUnsuitable() throws Exception {
-    try {
-      final Configuration conf = new Configuration();
-      // all files 13k or less
-      conf.setLong(FilePool.GRIDMIX_MIN_FILE, 14 * 1024);
-      final FilePool pool = new FilePool(conf, base);
-      pool.refresh();
-    } catch (IOException e) {
-      return;
-    }
-    fail();
-  }
-
-  @Test
-  public void testPool() throws Exception {
-    final Random r = new Random();
-    final Configuration conf = new Configuration();
-    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
-    final FilePool pool = new FilePool(conf, base);
-    pool.refresh();
-    final ArrayList<FileStatus> files = new ArrayList<FileStatus>();
-
-    // ensure 1k, 2k files excluded
-    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
-    assertEquals(expectedPoolSize, pool.getInputFiles(Long.MAX_VALUE, files));
-    assertEquals(NFILES - 4, files.size());
-
-    // exact match
-    files.clear();
-    assertEquals(expectedPoolSize, pool.getInputFiles(expectedPoolSize, files));
-
-    // match random within 12k
-    files.clear();
-    final long rand = r.nextInt(expectedPoolSize);
-    assertTrue("Missed: " + rand,
-        (NFILES / 2) * 1024 > rand - pool.getInputFiles(rand, files));
-
-    // all files
-    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 0);
-    pool.refresh();
-    files.clear();
-    assertEquals((NFILES / 2 * (NFILES / 2 + 1)) * 1024,
-        pool.getInputFiles(Long.MAX_VALUE, files));
-  }
-
-  void checkSplitEq(FileSystem fs, CombineFileSplit split, long bytes)
-      throws Exception {
-    long splitBytes = 0L;
-    HashSet<Path> uniq = new HashSet<Path>();
-    for (int i = 0; i < split.getNumPaths(); ++i) {
-      splitBytes += split.getLength(i);
-      assertTrue(
-          split.getLength(i) <= fs.getFileStatus(split.getPath(i)).getLen());
-      assertFalse(uniq.contains(split.getPath(i)));
-      uniq.add(split.getPath(i));
-    }
-    assertEquals(bytes, splitBytes);
-  }
-
-  @Test
-  public void testStriper() throws Exception {
-    final Random r = new Random();
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
-    final FilePool pool = new FilePool(conf, base) {
-      @Override
-      public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
-          throws IOException {
-        return new BlockLocation[] { new BlockLocation() };
-      }
-    };
-    pool.refresh();
-
-    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
-    final InputStriper striper = new InputStriper(pool, expectedPoolSize);
-    int last = 0;
-    for (int i = 0; i < expectedPoolSize;
-        last = Math.min(expectedPoolSize - i, r.nextInt(expectedPoolSize))) {
-      checkSplitEq(fs, striper.splitFor(pool, last, 0), last);
-      i += last;
-    }
-    final InputStriper striper2 = new InputStriper(pool, expectedPoolSize);
-    checkSplitEq(fs, striper2.splitFor(pool, expectedPoolSize, 0),
-        expectedPoolSize);
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFileQueue.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
deleted file mode 100644
index a4668ee..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.util.Arrays;
-
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-public class TestFileQueue {
-
-  static final Log LOG = LogFactory.getLog(TestFileQueue.class);
-  static final int NFILES = 4;
-  static final int BLOCK = 256;
-  static final Path[] paths = new Path[NFILES];
-  static final String[] loc = new String[NFILES];
-  static final long[] start = new long[NFILES];
-  static final long[] len = new long[NFILES];
-
-  @BeforeClass
-  public static void setup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    final Path p = new Path(System.getProperty("test.build.data", "/tmp"),
-        "testFileQueue").makeQualified(fs);
-    fs.delete(p, true);
-    final byte[] b = new byte[BLOCK];
-    for (int i = 0; i < NFILES; ++i) {
-      Arrays.fill(b, (byte)('A' + i));
-      paths[i] = new Path(p, "" + (char)('A' + i));
-      OutputStream f = null;
-      try {
-        f = fs.create(paths[i]);
-        f.write(b);
-      } finally {
-        if (f != null) {
-          f.close();
-        }
-      }
-    }
-  }
-
-  @AfterClass
-  public static void cleanup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    final Path p = new Path(System.getProperty("test.build.data", "/tmp"),
-        "testFileQueue").makeQualified(fs);
-    fs.delete(p, true);
-  }
-
-  static ByteArrayOutputStream fillVerif() throws IOException {
-    final byte[] b = new byte[BLOCK];
-    final ByteArrayOutputStream out = new ByteArrayOutputStream();
-    for (int i = 0; i < NFILES; ++i) {
-      Arrays.fill(b, (byte)('A' + i));
-      out.write(b, 0, (int)len[i]);
-    }
-    return out;
-  }
-
-  @Test
-  public void testRepeat() throws Exception {
-    final Configuration conf = new Configuration();
-    Arrays.fill(loc, "");
-    Arrays.fill(start, 0L);
-    Arrays.fill(len, BLOCK);
-
-    final ByteArrayOutputStream out = fillVerif();
-    final FileQueue q =
-      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
-    final byte[] verif = out.toByteArray();
-    final byte[] check = new byte[2 * NFILES * BLOCK];
-    q.read(check, 0, NFILES * BLOCK);
-    assertArrayEquals(verif, Arrays.copyOf(check, NFILES * BLOCK));
-
-    final byte[] verif2 = new byte[2 * NFILES * BLOCK];
-    System.arraycopy(verif, 0, verif2, 0, verif.length);
-    System.arraycopy(verif, 0, verif2, verif.length, verif.length);
-    q.read(check, 0, 2 * NFILES * BLOCK);
-    assertArrayEquals(verif2, check);
-
-  }
-
-  @Test
-  public void testUneven() throws Exception {
-    final Configuration conf = new Configuration();
-    Arrays.fill(loc, "");
-    Arrays.fill(start, 0L);
-    Arrays.fill(len, BLOCK);
-
-    final int B2 = BLOCK / 2;
-    for (int i = 0; i < NFILES; i += 2) {
-      start[i] += B2;
-      len[i] -= B2;
-    }
-    final FileQueue q =
-      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
-    final ByteArrayOutputStream out = fillVerif();
-    final byte[] verif = out.toByteArray();
-    final byte[] check = new byte[NFILES / 2 * BLOCK + NFILES / 2 * B2];
-    q.read(check, 0, verif.length);
-    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));
-    q.read(check, 0, verif.length);
-    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));
-  }
-
-  @Test
-  public void testEmpty() throws Exception {
-    final Configuration conf = new Configuration();
-    // verify OK if unused
-    final FileQueue q = new FileQueue(new CombineFileSplit(
-          new Path[0], new long[0], new long[0], new String[0]), conf);
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
deleted file mode 100644
index 422ec12..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
+++ /dev/null
@@ -1,453 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.DummyResourceCalculatorPlugin;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.gridmix.DebugJobProducer.MockJob;
-import org.apache.hadoop.mapred.gridmix.TestHighRamJob.DummyGridmixJob;
-import org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators.FakeProgressive;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin.DefaultHeapUsageEmulator;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-/**
- * Test Gridmix memory emulation.
- */
-public class TestGridmixMemoryEmulation {
-  /**
-   * This is a dummy class that fakes heap usage.
-   */
-  private static class FakeHeapUsageEmulatorCore 
-  extends DefaultHeapUsageEmulator {
-    private int numCalls = 0;
-    
-    @Override
-    public void load(long sizeInMB) {
-      ++numCalls;
-      super.load(sizeInMB);
-    }
-    
-    // Get the total number of times load() was invoked
-    int getNumCalls() {
-      return numCalls;
-    }
-    
-    // Get the total number of 1mb objects stored within
-    long getHeapUsageInMB() {
-      return heapSpace.size();
-    }
-    
-    @Override
-    public void reset() {
-      // no op to stop emulate() from resetting
-    }
-    
-    /**
-     * For re-testing purpose.
-     */
-    void resetFake() {
-      numCalls = 0;
-      super.reset();
-    }
-  }
-
-  /**
-   * This is a dummy class that fakes the heap usage emulator plugin.
-   */
-  private static class FakeHeapUsageEmulatorPlugin 
-  extends TotalHeapUsageEmulatorPlugin {
-    private FakeHeapUsageEmulatorCore core;
-    
-    public FakeHeapUsageEmulatorPlugin(FakeHeapUsageEmulatorCore core) {
-      super(core);
-      this.core = core;
-    }
-    
-    @Override
-    protected long getMaxHeapUsageInMB() {
-      return Long.MAX_VALUE / ONE_MB;
-    }
-    
-    @Override
-    protected long getTotalHeapUsageInMB() {
-      return core.getHeapUsageInMB();
-    }
-  }
-  
-  /**
-   * Test {@link TotalHeapUsageEmulatorPlugin}'s core heap usage emulation 
-   * engine.
-   */
-  @Test
-  public void testHeapUsageEmulator() throws IOException {
-    FakeHeapUsageEmulatorCore heapEmulator = new FakeHeapUsageEmulatorCore();
-    
-    long testSizeInMB = 10; // 10 mb
-    long previousHeap = heapEmulator.getHeapUsageInMB();
-    heapEmulator.load(testSizeInMB);
-    long currentHeap = heapEmulator.getHeapUsageInMB();
-    
-    // check if the heap has increased by expected value
-    assertEquals("Default heap emulator failed to load 10mb", 
-                 previousHeap + testSizeInMB, currentHeap);
-    
-    // test reset
-    heapEmulator.resetFake();
-    assertEquals("Default heap emulator failed to reset", 
-                 0, heapEmulator.getHeapUsageInMB());
-  }
-
-  /**
-   * Test {@link TotalHeapUsageEmulatorPlugin}.
-   */
-  @Test
-  public void testTotalHeapUsageEmulatorPlugin() throws Exception {
-    Configuration conf = new Configuration();
-    // set the dummy resource calculator for testing
-    ResourceCalculatorPlugin monitor = new DummyResourceCalculatorPlugin();
-    long maxHeapUsage = 1024 * TotalHeapUsageEmulatorPlugin.ONE_MB; // 1GB
-    conf.setLong(DummyResourceCalculatorPlugin.MAXPMEM_TESTING_PROPERTY, 
-                 maxHeapUsage);
-    monitor.setConf(conf);
-    
-    // no buffer to be reserved
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
-    // only 1 call to be made per cycle
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
-    long targetHeapUsageInMB = 200; // 200mb
-    
-    // fake progress indicator
-    FakeProgressive fakeProgress = new FakeProgressive();
-    
-    // fake heap usage generator
-    FakeHeapUsageEmulatorCore fakeCore = new FakeHeapUsageEmulatorCore();
-    
-    // a heap usage emulator with fake core
-    FakeHeapUsageEmulatorPlugin heapPlugin = 
-      new FakeHeapUsageEmulatorPlugin(fakeCore);
-    
-    // test with invalid or missing resource usage value
-    ResourceUsageMetrics invalidUsage = 
-      TestResourceUsageEmulators.createMetrics(0);
-    heapPlugin.initialize(conf, invalidUsage, null, null);
-    
-    // test if disabled heap emulation plugin's emulate() call is a no-operation
-    // this will test if the emulation plugin is disabled or not
-    int numCallsPre = fakeCore.getNumCalls();
-    long heapUsagePre = fakeCore.getHeapUsageInMB();
-    heapPlugin.emulate();
-    int numCallsPost = fakeCore.getNumCalls();
-    long heapUsagePost = fakeCore.getHeapUsageInMB();
-    
-    //  test if no calls are made heap usage emulator core
-    assertEquals("Disabled heap usage emulation plugin works!", 
-                 numCallsPre, numCallsPost);
-    //  test if no calls are made heap usage emulator core
-    assertEquals("Disabled heap usage emulation plugin works!", 
-                 heapUsagePre, heapUsagePost);
-    
-    // test with wrong/invalid configuration
-    Boolean failed = null;
-    invalidUsage = 
-      TestResourceUsageEmulators.createMetrics(maxHeapUsage 
-                                   + TotalHeapUsageEmulatorPlugin.ONE_MB);
-    try {
-      heapPlugin.initialize(conf, invalidUsage, monitor, null);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull("Fail case failure!", failed);
-    assertTrue("Expected failure!", failed); 
-    
-    // test with valid resource usage value
-    ResourceUsageMetrics metrics = 
-      TestResourceUsageEmulators.createMetrics(targetHeapUsageInMB 
-                                   * TotalHeapUsageEmulatorPlugin.ONE_MB);
-    
-    // test with default emulation interval
-    // in every interval, the emulator will add 100% of the expected usage 
-    // (since gridmix.emulators.resource-usage.heap.load-ratio=1)
-    // so at 10%, emulator will add 10% (difference), at 20% it will add 10% ...
-    // So to emulate 200MB, it will add
-    //   20mb + 20mb + 20mb + 20mb + .. = 200mb 
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 
-                          10);
-    
-    // test with custom value for emulation interval of 20%
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL,
-                  0.2F);
-    //  40mb + 40mb + 40mb + 40mb + 40mb = 200mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 5);
-    
-    // test with custom value of free heap ratio and load ratio = 1
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.5F);
-    //  40mb + 0mb + 80mb + 0mb + 0mb = 120mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 120, 2);
-    
-    // test with custom value of heap load ratio and min free heap ratio = 0
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
-    // 20mb (call#1) + 20mb (call#1) + 20mb (call#2) + 20mb (call#2) +.. = 200mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 
-                          10);
-    
-    // test with custom value of free heap ratio = 0.3 and load ratio = 0.5
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.25F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);
-    // 20mb (call#1) + 20mb (call#1) + 30mb (call#2) + 0mb (call#2) 
-    // + 30mb (call#3) + 0mb (call#3) + 35mb (call#4) + 0mb (call#4)
-    // + 37mb (call#5) + 0mb (call#5) = 162mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 162, 6);
-    
-    // test if emulation interval boundary is respected
-    fakeProgress = new FakeProgressive(); // initialize
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL,
-                  0.25F);
-    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    fakeCore.resetFake();
-    // take a snapshot after the initialization
-    long initHeapUsage = fakeCore.getHeapUsageInMB();
-    long initNumCallsUsage = fakeCore.getNumCalls();
-    // test with 0 progress
-    testEmulationBoundary(0F, fakeCore, fakeProgress, heapPlugin, initHeapUsage, 
-                          initNumCallsUsage, "[no-op, 0 progress]");
-    // test with 24% progress
-    testEmulationBoundary(0.24F, fakeCore, fakeProgress, heapPlugin, 
-                          initHeapUsage, initNumCallsUsage, 
-                          "[no-op, 24% progress]");
-    // test with 25% progress
-    testEmulationBoundary(0.25F, fakeCore, fakeProgress, heapPlugin, 
-        targetHeapUsageInMB / 4, 1, "[op, 25% progress]");
-    // test with 80% progress
-    testEmulationBoundary(0.80F, fakeCore, fakeProgress, heapPlugin, 
-        (targetHeapUsageInMB * 4) / 5, 2, "[op, 80% progress]");
-    
-    // now test if the final call with 100% progress ramps up the heap usage
-    testEmulationBoundary(1F, fakeCore, fakeProgress, heapPlugin, 
-        targetHeapUsageInMB, 3, "[op, 100% progress]");
-  }
-
-  // test whether the heap usage emulator achieves the desired target using
-  // desired calls to the underling core engine.
-  private static void testEmulationAccuracy(Configuration conf, 
-                        FakeHeapUsageEmulatorCore fakeCore,
-                        ResourceCalculatorPlugin monitor,
-                        ResourceUsageMetrics metrics,
-                        TotalHeapUsageEmulatorPlugin heapPlugin,
-                        long expectedTotalHeapUsageInMB,
-                        long expectedTotalNumCalls)
-  throws Exception {
-    FakeProgressive fakeProgress = new FakeProgressive();
-    fakeCore.resetFake();
-    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    int numLoops = 0;
-    while (fakeProgress.getProgress() < 1) {
-      ++numLoops;
-      float progress = numLoops / 100.0F;
-      fakeProgress.setProgress(progress);
-      heapPlugin.emulate();
-    }
-    
-    // test if the resource plugin shows the expected usage
-    assertEquals("Cumulative heap usage emulator plugin failed (total usage)!", 
-                 expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 1L);
-    // test if the resource plugin shows the expected num calls
-    assertEquals("Cumulative heap usage emulator plugin failed (num calls)!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-  }
-
-  // tests if the heap usage emulation plugin emulates only at the expected
-  // progress gaps
-  private static void testEmulationBoundary(float progress, 
-      FakeHeapUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, 
-      TotalHeapUsageEmulatorPlugin heapPlugin, long expectedTotalHeapUsageInMB, 
-      long expectedTotalNumCalls, String info) throws Exception {
-    fakeProgress.setProgress(progress);
-    heapPlugin.emulate();
-    // test heap usage
-    assertEquals("Emulation interval test for heap usage failed " + info + "!", 
-                 expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 0L);
-    // test num calls
-    assertEquals("Emulation interval test for heap usage failed " + info + "!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-  }
-  
-  /**
-   * Test the specified task java heap options.
-   */
-  @SuppressWarnings("deprecation")
-  private void testJavaHeapOptions(String mapOptions, 
-      String reduceOptions, String taskOptions, String defaultMapOptions, 
-      String defaultReduceOptions, String defaultTaskOptions, 
-      String expectedMapOptions, String expectedReduceOptions, 
-      String expectedTaskOptions) throws Exception {
-    Configuration simulatedConf = new Configuration();
-    // reset the configuration parameters
-    simulatedConf.unset(MRJobConfig.MAP_JAVA_OPTS);
-    simulatedConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);
-    simulatedConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);
-    
-    // set the default map task options
-    if (defaultMapOptions != null) {
-      simulatedConf.set(MRJobConfig.MAP_JAVA_OPTS, defaultMapOptions);
-    }
-    // set the default reduce task options
-    if (defaultReduceOptions != null) {
-      simulatedConf.set(MRJobConfig.REDUCE_JAVA_OPTS, defaultReduceOptions);
-    }
-    // set the default task options
-    if (defaultTaskOptions != null) {
-      simulatedConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, defaultTaskOptions);
-    }
-    
-    Configuration originalConf = new Configuration();
-    // reset the configuration parameters
-    originalConf.unset(MRJobConfig.MAP_JAVA_OPTS);
-    originalConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);
-    originalConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);
-    
-    // set the map task options
-    if (mapOptions != null) {
-      originalConf.set(MRJobConfig.MAP_JAVA_OPTS, mapOptions);
-    }
-    // set the reduce task options
-    if (reduceOptions != null) {
-      originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, reduceOptions);
-    }
-    // set the task options
-    if (taskOptions != null) {
-      originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, taskOptions);
-    }
-    
-    // configure the task jvm's heap options
-    GridmixJob.configureTaskJVMOptions(originalConf, simulatedConf);
-    
-    assertEquals("Map heap options mismatch!", expectedMapOptions, 
-                 simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));
-    assertEquals("Reduce heap options mismatch!", expectedReduceOptions, 
-                 simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));
-    assertEquals("Task heap options mismatch!", expectedTaskOptions, 
-                 simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));
-  }
-  
-  /**
-   * Test task-level java heap options configuration in {@link GridmixJob}.
-   */
-  @Test
-  public void testJavaHeapOptions() throws Exception {
-    // test missing opts
-    testJavaHeapOptions(null, null, null, null, null, null, null, null, 
-                        null);
-    
-    // test original heap opts and missing default opts
-    testJavaHeapOptions("-Xms10m", "-Xms20m", "-Xms30m", null, null, null,
-                        null, null, null);
-    
-    // test missing opts with default opts
-    testJavaHeapOptions(null, null, null, "-Xms10m", "-Xms20m", "-Xms30m",
-                        "-Xms10m", "-Xms20m", "-Xms30m");
-    
-    // test empty option
-    testJavaHeapOptions("", "", "", null, null, null, null, null, null);
-    
-    // test empty default option and no original heap options
-    testJavaHeapOptions(null, null, null, "", "", "", "", "", "");
-    
-    // test empty opts and default opts
-    testJavaHeapOptions("", "", "", "-Xmx10m -Xms1m", "-Xmx50m -Xms2m", 
-                        "-Xms2m -Xmx100m", "-Xmx10m -Xms1m", "-Xmx50m -Xms2m", 
-                        "-Xms2m -Xmx100m");
-    
-    // test custom heap opts with no default opts
-    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx30m", null, null, null,
-                        "-Xmx10m", "-Xmx20m", "-Xmx30m");
-    
-    // test heap opts with default opts (multiple value)
-    testJavaHeapOptions("-Xms5m -Xmx200m", "-Xms15m -Xmx300m", 
-                        "-Xms25m -Xmx50m", "-XXabc", "-XXxyz", "-XXdef", 
-                        "-XXabc -Xmx200m", "-XXxyz -Xmx300m", "-XXdef -Xmx50m");
-    
-    // test heap opts with default opts (duplication of -Xmx)
-    testJavaHeapOptions("-Xms5m -Xmx200m", "-Xms15m -Xmx300m", 
-                        "-Xms25m -Xmx50m", "-XXabc -Xmx500m", "-XXxyz -Xmx600m",
-                        "-XXdef -Xmx700m", "-XXabc -Xmx200m", "-XXxyz -Xmx300m",
-                        "-XXdef -Xmx50m");
-    
-    // test heap opts with default opts (single value)
-    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx50m", "-Xms2m", 
-                        "-Xms3m", "-Xms5m", "-Xms2m -Xmx10m", "-Xms3m -Xmx20m",
-                        "-Xms5m -Xmx50m");
-    
-    // test heap opts with default opts (duplication of -Xmx)
-    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx50m", "-Xmx2m", 
-                        "-Xmx3m", "-Xmx5m", "-Xmx10m", "-Xmx20m", "-Xmx50m");
-  }
-  
-  /**
-   * Test disabled task heap options configuration in {@link GridmixJob}.
-   */
-  @Test
-  @SuppressWarnings("deprecation")
-  public void testJavaHeapOptionsDisabled() throws Exception {
-    Configuration gridmixConf = new Configuration();
-    gridmixConf.setBoolean(GridmixJob.GRIDMIX_TASK_JVM_OPTIONS_ENABLE, false);
-    
-    // set the default values of simulated job
-    gridmixConf.set(MRJobConfig.MAP_JAVA_OPTS, "-Xmx1m");
-    gridmixConf.set(MRJobConfig.REDUCE_JAVA_OPTS, "-Xmx2m");
-    gridmixConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, "-Xmx3m");
-    
-    // set the default map and reduce task options for original job
-    final JobConf originalConf = new JobConf();
-    originalConf.set(MRJobConfig.MAP_JAVA_OPTS, "-Xmx10m");
-    originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, "-Xmx20m");
-    originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, "-Xmx30m");
-    
-    // define a mock job
-    MockJob story = new MockJob(originalConf) {
-      public JobConf getJobConf() {
-        return originalConf;
-      }
-    };
-    
-    GridmixJob job = new DummyGridmixJob(gridmixConf, story);
-    Job simulatedJob = job.getJob();
-    Configuration simulatedConf = simulatedJob.getConfiguration();
-    
-    assertEquals("Map heap options works when disabled!", "-Xmx1m", 
-                 simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));
-    assertEquals("Reduce heap options works when disabled!", "-Xmx2m", 
-                 simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));
-    assertEquals("Task heap options works when disabled!", "-Xmx3m", 
-                 simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
deleted file mode 100644
index 2f3ce70..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
+++ /dev/null
@@ -1,278 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Random;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-
-public class TestGridmixRecord {
-  private static final Log LOG = LogFactory.getLog(TestGridmixRecord.class);
-
-  static void lengthTest(GridmixRecord x, GridmixRecord y, int min,
-      int max) throws Exception {
-    final Random r = new Random();
-    final long seed = r.nextLong();
-    r.setSeed(seed);
-    LOG.info("length: " + seed);
-    final DataInputBuffer in = new DataInputBuffer();
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      setSerialize(x, r.nextLong(), i, out1);
-      // check write
-      assertEquals(i, out1.getLength());
-      // write to stream
-      x.write(out2);
-      // check read
-      in.reset(out1.getData(), 0, out1.getLength());
-      y.readFields(in);
-      assertEquals(i, x.getSize());
-      assertEquals(i, y.getSize());
-    }
-    // check stream read
-    in.reset(out2.getData(), 0, out2.getLength());
-    for (int i = min; i < max; ++i) {
-      y.readFields(in);
-      assertEquals(i, y.getSize());
-    }
-  }
-
-  static void randomReplayTest(GridmixRecord x, GridmixRecord y, int min,
-      int max) throws Exception {
-    final Random r = new Random();
-    final long seed = r.nextLong();
-    r.setSeed(seed);
-    LOG.info("randReplay: " + seed);
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      final int s = out1.getLength();
-      x.setSeed(r.nextLong());
-      x.setSize(i);
-      x.write(out1);
-      assertEquals(i, out1.getLength() - s);
-    }
-    final DataInputBuffer in = new DataInputBuffer();
-    in.reset(out1.getData(), 0, out1.getLength());
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    // deserialize written records, write to separate buffer
-    for (int i = min; i < max; ++i) {
-      final int s = in.getPosition();
-      y.readFields(in);
-      assertEquals(i, in.getPosition() - s);
-      y.write(out2);
-    }
-    // verify written contents match
-    assertEquals(out1.getLength(), out2.getLength());
-    // assumes that writes will grow buffer deterministically
-    assertEquals("Bad test", out1.getData().length, out2.getData().length);
-    assertArrayEquals(out1.getData(), out2.getData());
-  }
-
-  static void eqSeedTest(GridmixRecord x, GridmixRecord y, int max)
-      throws Exception {
-    final Random r = new Random();
-    final long s = r.nextLong();
-    r.setSeed(s);
-    LOG.info("eqSeed: " + s);
-    assertEquals(x.fixedBytes(), y.fixedBytes());
-    final int min = x.fixedBytes() + 1;
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      final long seed = r.nextLong();
-      setSerialize(x, seed, i, out1);
-      setSerialize(y, seed, i, out2);
-      assertEquals(x, y);
-      assertEquals(x.hashCode(), y.hashCode());
-
-      // verify written contents match
-      assertEquals(out1.getLength(), out2.getLength());
-      // assumes that writes will grow buffer deterministically
-      assertEquals("Bad test", out1.getData().length, out2.getData().length);
-      assertArrayEquals(out1.getData(), out2.getData());
-    }
-  }
-
-  static void binSortTest(GridmixRecord x, GridmixRecord y, int min,
-      int max, WritableComparator cmp) throws Exception {
-    final Random r = new Random();
-    final long s = r.nextLong();
-    r.setSeed(s);
-    LOG.info("sort: " + s);
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      final long seed1 = r.nextLong();
-      setSerialize(x, seed1, i, out1);
-      assertEquals(0, x.compareSeed(seed1, Math.max(0, i - x.fixedBytes())));
-
-      final long seed2 = r.nextLong();
-      setSerialize(y, seed2, i, out2);
-      assertEquals(0, y.compareSeed(seed2, Math.max(0, i - x.fixedBytes())));
-
-      // for eq sized records, ensure byte cmp where req
-      final int chk = WritableComparator.compareBytes(
-          out1.getData(), 0, out1.getLength(),
-          out2.getData(), 0, out2.getLength());
-      assertEquals(chk, x.compareTo(y));
-      assertEquals(chk, cmp.compare(
-            out1.getData(), 0, out1.getLength(),
-            out2.getData(), 0, out2.getLength()));
-      // write second copy, compare eq
-      final int s1 = out1.getLength();
-      x.write(out1);
-      assertEquals(0, cmp.compare(out1.getData(), 0, s1,
-            out1.getData(), s1, out1.getLength() - s1));
-      final int s2 = out2.getLength();
-      y.write(out2);
-      assertEquals(0, cmp.compare(out2.getData(), 0, s2,
-            out2.getData(), s2, out2.getLength() - s2));
-      assertEquals(chk, cmp.compare(out1.getData(), 0, s1,
-            out2.getData(), s2, out2.getLength() - s2));
-    }
-  }
-
-  static void checkSpec(GridmixKey a, GridmixKey b) throws Exception {
-    final Random r = new Random();
-    final long s = r.nextLong();
-    r.setSeed(s);
-    LOG.info("spec: " + s);
-    final DataInputBuffer in = new DataInputBuffer();
-    final DataOutputBuffer out = new DataOutputBuffer();
-    a.setType(GridmixKey.REDUCE_SPEC);
-    b.setType(GridmixKey.REDUCE_SPEC);
-    for (int i = 0; i < 100; ++i) {
-      final int in_rec = r.nextInt(Integer.MAX_VALUE);
-      a.setReduceInputRecords(in_rec);
-      final int out_rec = r.nextInt(Integer.MAX_VALUE);
-      a.setReduceOutputRecords(out_rec);
-      final int out_bytes = r.nextInt(Integer.MAX_VALUE);
-      a.setReduceOutputBytes(out_bytes);
-      final int min = WritableUtils.getVIntSize(in_rec)
-                    + WritableUtils.getVIntSize(out_rec)
-                    + WritableUtils.getVIntSize(out_bytes)
-                    + WritableUtils.getVIntSize(0);
-      assertEquals(min + 2, a.fixedBytes()); // meta + vint min
-      final int size = r.nextInt(1024) + a.fixedBytes() + 1;
-      setSerialize(a, r.nextLong(), size, out);
-      assertEquals(size, out.getLength());
-      assertTrue(a.equals(a));
-      assertEquals(0, a.compareTo(a));
-
-      in.reset(out.getData(), 0, out.getLength());
-
-      b.readFields(in);
-      assertEquals(size, b.getSize());
-      assertEquals(in_rec, b.getReduceInputRecords());
-      assertEquals(out_rec, b.getReduceOutputRecords());
-      assertEquals(out_bytes, b.getReduceOutputBytes());
-      assertTrue(a.equals(b));
-      assertEquals(0, a.compareTo(b));
-      assertEquals(a.hashCode(), b.hashCode());
-    }
-  }
-
-  static void setSerialize(GridmixRecord x, long seed, int size,
-      DataOutputBuffer out) throws IOException {
-    x.setSeed(seed);
-    x.setSize(size);
-    out.reset();
-    x.write(out);
-  }
-
-  @Test
-  public void testKeySpec() throws Exception {
-    final int min = 6;
-    final int max = 300;
-    final GridmixKey a = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);
-    final GridmixKey b = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);
-    lengthTest(a, b, min, max);
-    randomReplayTest(a, b, min, max);
-    binSortTest(a, b, min, max, new GridmixKey.Comparator());
-    // 2 fixed GR bytes, 1 type, 3 spec
-    eqSeedTest(a, b, max);
-    checkSpec(a, b);
-  }
-
-  @Test
-  public void testKeyData() throws Exception {
-    final int min = 2;
-    final int max = 300;
-    final GridmixKey a = new GridmixKey(GridmixKey.DATA, 1, 0L);
-    final GridmixKey b = new GridmixKey(GridmixKey.DATA, 1, 0L);
-    lengthTest(a, b, min, max);
-    randomReplayTest(a, b, min, max);
-    binSortTest(a, b, min, max, new GridmixKey.Comparator());
-    // 2 fixed GR bytes, 1 type
-    eqSeedTest(a, b, 300);
-  }
-
-  @Test
-  public void testBaseRecord() throws Exception {
-    final int min = 1;
-    final int max = 300;
-    final GridmixRecord a = new GridmixRecord();
-    final GridmixRecord b = new GridmixRecord();
-    lengthTest(a, b, min, max);
-    randomReplayTest(a, b, min, max);
-    binSortTest(a, b, min, max, new GridmixRecord.Comparator());
-    // 2 fixed GR bytes
-    eqSeedTest(a, b, 300);
-  }
-
-  public static void main(String[] argv) throws Exception {
-    boolean fail = false;
-    final TestGridmixRecord test = new TestGridmixRecord();
-    try { test.testKeySpec(); } catch (Exception e) {
-      fail = true;
-      e.printStackTrace();
-    }
-    try {test.testKeyData(); } catch (Exception e) {
-      fail = true;
-      e.printStackTrace();
-    }
-    try {test.testBaseRecord(); } catch (Exception e) {
-      fail = true;
-      e.printStackTrace();
-    }
-    System.exit(fail ? -1 : 0);
-  }
-
-  static void printDebug(GridmixRecord a, GridmixRecord b) throws IOException {
-    DataOutputBuffer out = new DataOutputBuffer();
-    a.write(out);
-    System.out.println("A " +
-        Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));
-    out.reset();
-    b.write(out);
-    System.out.println("B " +
-        Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
deleted file mode 100644
index 64af603..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
+++ /dev/null
@@ -1,377 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.CommonConfigurationKeys;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.UtilsForTests;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.junit.Test;
-
-/**
- * Test {@link ExecutionSummarizer} and {@link ClusterSummarizer}.
- */
-public class TestGridmixSummary {
-  
-  /**
-   * Test {@link DataStatistics}.
-   */
-  @Test
-  public void testDataStatistics() throws Exception {
-    // test data-statistics getters with compression enabled
-    DataStatistics stats = new DataStatistics(10, 2, true);
-    assertEquals("Data size mismatch", 10, stats.getDataSize());
-    assertEquals("Num files mismatch", 2, stats.getNumFiles());
-    assertTrue("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test data-statistics getters with compression disabled
-    stats = new DataStatistics(100, 5, false);
-    assertEquals("Data size mismatch", 100, stats.getDataSize());
-    assertEquals("Num files mismatch", 5, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test publish data stats
-    Configuration conf = new Configuration();
-    Path rootTempDir = new Path(System.getProperty("test.build.data", "/tmp"));
-    Path testDir = new Path(rootTempDir, "testDataStatistics");
-    FileSystem fs = testDir.getFileSystem(conf);
-    fs.delete(testDir, true);
-    Path testInputDir = new Path(testDir, "test");
-    fs.mkdirs(testInputDir);
-    
-    // test empty folder (compression = true)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    Boolean failed = null;
-    try {
-      GenerateData.publishDataStatistics(testInputDir, 1024L, conf);
-      failed = false;
-    } catch (RuntimeException e) {
-      failed = true;
-    }
-    assertNotNull("Expected failure!", failed);
-    assertTrue("Compression data publishing error", failed);
-    
-    // test with empty folder (compression = off)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    stats = GenerateData.publishDataStatistics(testInputDir, 1024L, conf);
-    assertEquals("Data size mismatch", 0, stats.getDataSize());
-    assertEquals("Num files mismatch", 0, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test with some plain input data (compression = off)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    Path inputDataFile = new Path(testInputDir, "test");
-    long size = 
-      UtilsForTests.createTmpFileDFS(fs, inputDataFile, 
-          FsPermission.createImmutable((short)777), "hi hello bye").size();
-    stats = GenerateData.publishDataStatistics(testInputDir, -1, conf);
-    assertEquals("Data size mismatch", size, stats.getDataSize());
-    assertEquals("Num files mismatch", 1, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test with some plain input data (compression = on)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    failed = null;
-    try {
-      GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
-      failed = false;
-    } catch (RuntimeException e) {
-      failed = true;
-    }
-    assertNotNull("Expected failure!", failed);
-    assertTrue("Compression data publishing error", failed);
-    
-    // test with some compressed input data (compression = off)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    fs.delete(inputDataFile, false);
-    inputDataFile = new Path(testInputDir, "test.gz");
-    size = 
-      UtilsForTests.createTmpFileDFS(fs, inputDataFile, 
-          FsPermission.createImmutable((short)777), "hi hello").size();
-    stats =  GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
-    assertEquals("Data size mismatch", size, stats.getDataSize());
-    assertEquals("Num files mismatch", 1, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test with some compressed input data (compression = on)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    stats = GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
-    assertEquals("Data size mismatch", size, stats.getDataSize());
-    assertEquals("Num files mismatch", 1, stats.getNumFiles());
-    assertTrue("Compression configuration mismatch", stats.isDataCompressed());
-  }
-  
-  /**
-   * A fake {@link JobFactory}.
-   */
-  @SuppressWarnings("unchecked")
-  private static class FakeJobFactory extends JobFactory {
-    /**
-     * A fake {@link JobStoryProducer} for {@link FakeJobFactory}.
-     */
-    private static class FakeJobStoryProducer implements JobStoryProducer {
-      @Override
-      public void close() throws IOException {
-      }
-
-      @Override
-      public JobStory getNextJob() throws IOException {
-        return null;
-      }
-    }
-    
-    FakeJobFactory(Configuration conf) {
-      super(null, new FakeJobStoryProducer(), null, conf, null, null);
-    }
-    
-    @Override
-    public void update(Object item) {
-    }
-    
-    @Override
-    protected Thread createReaderThread() {
-      return new Thread();
-    }
-  }
-  
-  /**
-   * Test {@link ExecutionSummarizer}.
-   */
-  @Test
-  @SuppressWarnings("unchecked")
-  public void testExecutionSummarizer() throws IOException {
-    Configuration conf = new Configuration();
-    
-    ExecutionSummarizer es = new ExecutionSummarizer();
-    assertEquals("ExecutionSummarizer init failed", 
-                 Summarizer.NA, es.getCommandLineArgsString());
-    
-    long startTime = System.currentTimeMillis();
-    // test configuration parameters
-    String[] initArgs = new String[] {"-Xmx20m", "-Dtest.args='test'"};
-    es = new ExecutionSummarizer(initArgs);
-    
-    assertEquals("ExecutionSummarizer init failed", 
-                 "-Xmx20m -Dtest.args='test'", 
-                 es.getCommandLineArgsString());
-    
-    // test start time
-    assertTrue("Start time mismatch", es.getStartTime() >= startTime);
-    assertTrue("Start time mismatch", 
-               es.getStartTime() <= System.currentTimeMillis());
-    
-    // test start() of ExecutionSummarizer
-    es.update(null);
-    assertEquals("ExecutionSummarizer init failed", 0, 
-                 es.getSimulationStartTime());
-    testExecutionSummarizer(0, 0, 0, 0, 0, 0, es);
-    
-    long simStartTime = System.currentTimeMillis();
-    es.start(null);
-    assertTrue("Simulation start time mismatch", 
-               es.getSimulationStartTime() >= simStartTime);
-    assertTrue("Simulation start time mismatch", 
-               es.getSimulationStartTime() <= System.currentTimeMillis());
-    
-    // test with job stats
-    JobStats stats = generateFakeJobStats(1, 10, true);
-    es.update(stats);
-    testExecutionSummarizer(1, 10, 0, 1, 1, 0, es);
-    
-    // test with failed job 
-    stats = generateFakeJobStats(5, 1, false);
-    es.update(stats);
-    testExecutionSummarizer(6, 11, 0, 2, 1, 1, es);
-    
-    // test finalize
-    //  define a fake job factory
-    JobFactory factory = new FakeJobFactory(conf);
-    
-    // fake the num jobs in trace
-    factory.numJobsInTrace = 3;
-    
-    Path rootTempDir = new Path(System.getProperty("test.build.data", "/tmp"));
-    Path testDir = new Path(rootTempDir, "testGridmixSummary");
-    Path testTraceFile = new Path(testDir, "test-trace.json");
-    FileSystem fs = FileSystem.getLocal(conf);
-    fs.create(testTraceFile).close();
-    
-    // finalize the summarizer
-    UserResolver resolver = new RoundRobinUserResolver();
-    DataStatistics dataStats = new DataStatistics(100, 2, true);
-    String policy = GridmixJobSubmissionPolicy.REPLAY.name();
-    conf.set(GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, policy);
-    es.finalize(factory, testTraceFile.toString(), 1024L, resolver, dataStats, 
-                conf);
-    
-    // test num jobs in trace
-    assertEquals("Mismtach in num jobs in trace", 3, es.getNumJobsInTrace());
-    
-    // test trace signature
-    String tid = 
-      ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
-    assertEquals("Mismatch in trace signature", 
-                 tid, es.getInputTraceSignature());
-    // test trace location
-    Path qPath = fs.makeQualified(testTraceFile);
-    assertEquals("Mismatch in trace filename", 
-                 qPath.toString(), es.getInputTraceLocation());
-    // test expected data size
-    assertEquals("Mismatch in expected data size", 
-                 "1.0k", es.getExpectedDataSize());
-    // test input data statistics
-    assertEquals("Mismatch in input data statistics", 
-                 ExecutionSummarizer.stringifyDataStatistics(dataStats), 
-                 es.getInputDataStatistics());
-    // test user resolver
-    assertEquals("Mismatch in user resolver", 
-                 resolver.getClass().getName(), es.getUserResolver());
-    // test policy
-    assertEquals("Mismatch in policy", policy, es.getJobSubmissionPolicy());
-    
-    // test data stringification using large data
-    es.finalize(factory, testTraceFile.toString(), 1024*1024*1024*10L, resolver,
-                dataStats, conf);
-    assertEquals("Mismatch in expected data size", 
-                 "10.0g", es.getExpectedDataSize());
-    
-    // test trace signature uniqueness
-    //  touch the trace file
-    fs.delete(testTraceFile, false);
-    //  sleep for 1 sec
-    try {
-      Thread.sleep(1000);
-    } catch (InterruptedException ie) {}
-    fs.create(testTraceFile).close();
-    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, 
-                conf);
-    // test missing expected data size
-    assertEquals("Mismatch in trace data size", 
-                 Summarizer.NA, es.getExpectedDataSize());
-    assertFalse("Mismatch in trace signature", 
-                tid.equals(es.getInputTraceSignature()));
-    // get the new identifier
-    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
-    assertEquals("Mismatch in trace signature", 
-                 tid, es.getInputTraceSignature());
-    
-    testTraceFile = new Path(testDir, "test-trace2.json");
-    fs.create(testTraceFile).close();
-    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, 
-                conf);
-    assertFalse("Mismatch in trace signature", 
-                tid.equals(es.getInputTraceSignature()));
-    // get the new identifier
-    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
-    assertEquals("Mismatch in trace signature", 
-                 tid, es.getInputTraceSignature());
-    
-    // finalize trace identifier '-' input
-    es.finalize(factory, "-", 0L, resolver, dataStats, conf);
-    assertEquals("Mismatch in trace signature",
-                 Summarizer.NA, es.getInputTraceSignature());
-    assertEquals("Mismatch in trace file location", 
-                 Summarizer.NA, es.getInputTraceLocation());
-  }
-  
-  // test the ExecutionSummarizer
-  private static void testExecutionSummarizer(int numMaps, int numReds,
-      int totalJobsInTrace, int totalJobSubmitted, int numSuccessfulJob, 
-      int numFailedJobs, ExecutionSummarizer es) {
-    assertEquals("ExecutionSummarizer test failed [num-maps]", 
-                 numMaps, es.getNumMapTasksLaunched());
-    assertEquals("ExecutionSummarizer test failed [num-reducers]", 
-                 numReds, es.getNumReduceTasksLaunched());
-    assertEquals("ExecutionSummarizer test failed [num-jobs-in-trace]", 
-                 totalJobsInTrace, es.getNumJobsInTrace());
-    assertEquals("ExecutionSummarizer test failed [num-submitted jobs]", 
-                 totalJobSubmitted, es.getNumSubmittedJobs());
-    assertEquals("ExecutionSummarizer test failed [num-successful-jobs]", 
-                 numSuccessfulJob, es.getNumSuccessfulJobs());
-    assertEquals("ExecutionSummarizer test failed [num-failed jobs]", 
-                 numFailedJobs, es.getNumFailedJobs());
-  }
-  
-  // generate fake job stats
-  @SuppressWarnings("deprecation")
-  private static JobStats generateFakeJobStats(final int numMaps, 
-      final int numReds, final boolean isSuccessful) 
-  throws IOException {
-    // A fake job 
-    Job fakeJob = new Job() {
-      @Override
-      public int getNumReduceTasks() {
-        return numReds;
-      };
-      
-      @Override
-      public boolean isSuccessful() throws IOException, InterruptedException {
-        return isSuccessful;
-      };
-    };
-    return new JobStats(numMaps, numReds, fakeJob);
-  }
-  
-  /**
-   * Test {@link ClusterSummarizer}.
-   */
-  @Test
-  @SuppressWarnings("deprecation")
-  public void testClusterSummarizer() throws IOException {
-    ClusterSummarizer cs = new ClusterSummarizer();
-    Configuration conf = new Configuration();
-    
-    String jt = "test-jt:1234";
-    String nn = "test-nn:5678";
-    conf.set(JTConfig.JT_IPC_ADDRESS, jt);
-    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, nn);
-    cs.start(conf);
-    
-    assertEquals("JT name mismatch", jt, cs.getJobTrackerInfo());
-    assertEquals("NN name mismatch", nn, cs.getNamenodeInfo());
-    
-    ClusterStats cstats = ClusterStats.getClusterStats();
-    conf.set(JTConfig.JT_IPC_ADDRESS, "local");
-    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "local");
-    JobClient jc = new JobClient(conf);
-    cstats.setClusterMetric(jc.getClusterStatus());
-    
-    cs.update(cstats);
-    
-    // test
-    assertEquals("Cluster summary test failed!", 1, cs.getMaxMapTasks());
-    assertEquals("Cluster summary test failed!", 1, cs.getMaxReduceTasks());
-    assertEquals("Cluster summary test failed!", 1, cs.getNumActiveTrackers());
-    assertEquals("Cluster summary test failed!", 0, 
-                 cs.getNumBlacklistedTrackers());
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
deleted file mode 100644
index 5523d73..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
+++ /dev/null
@@ -1,195 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.gridmix.DebugJobProducer.MockJob;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.junit.Test;
-
-/**
- * Test if Gridmix correctly configures the simulated job's configuration for
- * high ram job properties.
- */
-public class TestHighRamJob {
-  /**
-   * A dummy {@link GridmixJob} that opens up the simulated job for testing.
-   */
-  protected static class DummyGridmixJob extends GridmixJob {
-    public DummyGridmixJob(Configuration conf, JobStory desc) 
-    throws IOException {
-      super(conf, System.currentTimeMillis(), desc, new Path("test"), 
-            UserGroupInformation.getCurrentUser(), -1);
-    }
-    
-    /**
-     * Do nothing since this is a dummy gridmix job.
-     */
-    @Override
-    public Job call() throws Exception {
-      return null;
-    }
-    
-    @Override
-    protected boolean canEmulateCompression() {
-      // return false as we don't need compression
-      return false;
-    }
-    
-    protected Job getJob() {
-      // open the simulated job for testing
-      return job;
-    }
-  }
-  
-  private static void testHighRamConfig(long jobMapMB, long jobReduceMB, 
-      long clusterMapMB, long clusterReduceMB, long simulatedClusterMapMB, 
-      long simulatedClusterReduceMB, long expectedMapMB, long expectedReduceMB, 
-      Configuration gConf) 
-  throws IOException {
-    Configuration simulatedJobConf = new Configuration(gConf);
-    simulatedJobConf.setLong(MRConfig.MAPMEMORY_MB, simulatedClusterMapMB);
-    simulatedJobConf.setLong(MRConfig.REDUCEMEMORY_MB, 
-                             simulatedClusterReduceMB);
-    
-    // define a source conf
-    Configuration sourceConf = new Configuration();
-    
-    // configure the original job
-    sourceConf.setLong(MRJobConfig.MAP_MEMORY_MB, jobMapMB);
-    sourceConf.setLong(MRConfig.MAPMEMORY_MB, clusterMapMB);
-    sourceConf.setLong(MRJobConfig.REDUCE_MEMORY_MB, jobReduceMB);
-    sourceConf.setLong(MRConfig.REDUCEMEMORY_MB, clusterReduceMB);
-    
-    // define a mock job
-    MockJob story = new MockJob(sourceConf);
-    
-    GridmixJob job = new DummyGridmixJob(simulatedJobConf, story);
-    Job simulatedJob = job.getJob();
-    Configuration simulatedConf = simulatedJob.getConfiguration();
-    
-    // check if the high ram properties are not set
-    assertEquals(expectedMapMB, 
-                 simulatedConf.getLong(MRJobConfig.MAP_MEMORY_MB,
-                                       JobConf.DISABLED_MEMORY_LIMIT));
-    assertEquals(expectedReduceMB, 
-                 simulatedConf.getLong(MRJobConfig.REDUCE_MEMORY_MB, 
-                                       JobConf.DISABLED_MEMORY_LIMIT));
-  }
-  
-  /**
-   * Tests high ram job properties configuration.
-   */
-  @SuppressWarnings("deprecation")
-  @Test
-  public void testHighRamFeatureEmulation() throws IOException {
-    // define the gridmix conf
-    Configuration gridmixConf = new Configuration();
-    
-    // test : check high ram emulation disabled
-    gridmixConf.setBoolean(GridmixJob.GRIDMIX_HIGHRAM_EMULATION_ENABLE, false);
-    testHighRamConfig(10, 20, 5, 10, JobConf.DISABLED_MEMORY_LIMIT, 
-                      JobConf.DISABLED_MEMORY_LIMIT, 
-                      JobConf.DISABLED_MEMORY_LIMIT, 
-                      JobConf.DISABLED_MEMORY_LIMIT, gridmixConf);
-    
-    // test : check with high ram enabled (default) and no scaling
-    gridmixConf = new Configuration();
-    // set the deprecated max memory limit
-    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
-                        20*1024*1024);
-    testHighRamConfig(10, 20, 5, 10, 5, 10, 10, 20, gridmixConf);
-    
-    // test : check with high ram enabled and scaling
-    gridmixConf = new Configuration();
-    // set the new max map/reduce memory limits
-    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 100);
-    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 300);
-    testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-    
-    // test : check with high ram enabled and map memory scaling mismatch 
-    //        (deprecated)
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
-                        70*1024*1024);
-    Boolean failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding map memory limit "
-               + "(deprecation)!", failed);
-    
-    // test : check with high ram enabled and reduce memory scaling mismatch 
-    //        (deprecated)
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
-                        150*1024*1024);
-    failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding reduce memory limit "
-               + "(deprecation)!", failed);
-    
-    // test : check with high ram enabled and scaling mismatch on map limits
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 70);
-    failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding map memory limit!", failed);
-    
-    // test : check with high ram enabled and scaling mismatch on reduce 
-    //        limits
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 200);
-    failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding reduce memory limit!", failed);
-  }
-}
\ No newline at end of file
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
deleted file mode 100644
index b9c2728..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
+++ /dev/null
@@ -1,233 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.junit.Test;
-
-/**
- * Test the basic functionality of PseudoLocalFs
- */
-public class TestPseudoLocalFs {
-
-  /**
-   * Test if a file on PseudoLocalFs of a specific size can be opened and read.
-   * Validate the size of the data read.
-   * Test the read methods of {@link PseudoLocalFs.RandomInputStream}.
-   * @throws Exception
-   */
-  @Test
-  public void testPseudoLocalFsFileSize() throws Exception {
-    long fileSize = 10000;
-    Path path = PseudoLocalFs.generateFilePath("myPsedoFile", fileSize);
-    PseudoLocalFs pfs = new PseudoLocalFs();
-    pfs.create(path);
-
-    // Read 1 byte at a time and validate file size.
-    InputStream in = pfs.open(path, 0);
-    long totalSize = 0;
-
-    while (in.read() >= 0) {
-      ++totalSize;
-    }
-    in.close();
-    assertEquals("File size mismatch with read().", fileSize, totalSize);
-
-    // Read data from PseudoLocalFs-based file into buffer to
-    // validate read(byte[]) and file size.
-    in = pfs.open(path, 0);
-    totalSize = 0;
-    byte[] b = new byte[1024];
-    int bytesRead = in.read(b);
-    while (bytesRead >= 0) {
-      totalSize += bytesRead;
-      bytesRead = in.read(b);
-    }
-    assertEquals("File size mismatch with read(byte[]).", fileSize, totalSize);
-  }
-
-  /**
-   * Validate if file status is obtained for correctly formed file paths on
-   * PseudoLocalFs and also verify if appropriate exception is thrown for
-   * invalid file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which getFileStatus() is to be called
-   * @param shouldSucceed <code>true</code> if getFileStatus() should succeed
-   * @throws IOException
-   */
-  private void validateGetFileStatus(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean expectedExceptionSeen = false;
-    FileStatus stat = null;
-    try {
-      stat = pfs.getFileStatus(path);
-    } catch(FileNotFoundException e) {
-      expectedExceptionSeen = true;
-    }
-    if (shouldSucceed) {
-      assertFalse("getFileStatus() has thrown Exception for valid file name "
-                  + path, expectedExceptionSeen);
-      assertNotNull("Missing file status for a valid file.", stat);
-
-      // validate fileSize
-      String[] parts = path.toUri().getPath().split("\\.");
-      long expectedFileSize = Long.valueOf(parts[parts.length - 1]);
-      assertEquals("Invalid file size.", expectedFileSize, stat.getLen());
-    } else {
-      assertTrue("getFileStatus() did not throw Exception for invalid file "
-                 + " name " + path, expectedExceptionSeen);
-    }
-  }
-
-  /**
-   * Validate if file creation succeeds for correctly formed file paths on
-   * PseudoLocalFs and also verify if appropriate exception is thrown for
-   * invalid file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which create() is to be called
-   * @param shouldSucceed <code>true</code> if create() should succeed
-   * @throws IOException
-   */
-  private void validateCreate(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean expectedExceptionSeen = false;
-    try {
-      pfs.create(path);
-    } catch(IOException e) {
-      expectedExceptionSeen = true;
-    }
-    if (shouldSucceed) {
-      assertFalse("create() has thrown Exception for valid file name "
-                  + path, expectedExceptionSeen);
-    } else {
-      assertTrue("create() did not throw Exception for invalid file name "
-                 + path, expectedExceptionSeen);
-    }
-  }
-
-  /**
-   * Validate if opening of file succeeds for correctly formed file paths on
-   * PseudoLocalFs and also verify if appropriate exception is thrown for
-   * invalid file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which open() is to be called
-   * @param shouldSucceed <code>true</code> if open() should succeed
-   * @throws IOException
-   */
-  private void validateOpen(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean expectedExceptionSeen = false;
-    try {
-      pfs.open(path);
-    } catch(IOException e) {
-      expectedExceptionSeen = true;
-    }
-    if (shouldSucceed) {
-      assertFalse("open() has thrown Exception for valid file name "
-                  + path, expectedExceptionSeen);
-    } else {
-      assertTrue("open() did not throw Exception for invalid file name "
-                 + path, expectedExceptionSeen);
-    }
-  }
-
-  /**
-   * Validate if exists() returns <code>true</code> for correctly formed file
-   * paths on PseudoLocalFs and returns <code>false</code> for improperly
-   * formed file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which exists() is to be called
-   * @param shouldSucceed expected return value of exists(&lt;path&gt;)
-   * @throws IOException
-   */
-  private void validateExists(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean ret = pfs.exists(path);
-    if (shouldSucceed) {
-      assertTrue("exists() returned false for valid file name " + path, ret);
-    } else {
-      assertFalse("exists() returned true for invalid file name " + path, ret);
-    }
-  }
-
-  /**
-   *  Test Pseudo Local File System methods like getFileStatus(), create(),
-   *  open(), exists() for <li> valid file paths and <li> invalid file paths.
-   * @throws IOException
-   */
-  @Test
-  public void testPseudoLocalFsFileNames() throws IOException {
-    PseudoLocalFs pfs = new PseudoLocalFs();
-    Configuration conf = new Configuration();
-    conf.setClass("fs.pseudo.impl", PseudoLocalFs.class, FileSystem.class);
-
-    Path path = new Path("pseudo:///myPsedoFile.1234");
-    FileSystem testFs = path.getFileSystem(conf);
-    assertEquals("Failed to obtain a pseudo local file system object from path",
-                 pfs.getUri().getScheme(), testFs.getUri().getScheme());
-
-    // Validate PseudoLocalFS operations on URI of some other file system
-    path = new Path("file:///myPsedoFile.12345");
-    validateGetFileStatus(pfs, path, false);
-    validateCreate(pfs, path, false);
-    validateOpen(pfs, path, false);
-    validateExists(pfs, path, false);
-
-    path = new Path("pseudo:///myPsedoFile");//.<fileSize> missing
-    validateGetFileStatus(pfs, path, false);
-    validateCreate(pfs, path, false);
-    validateOpen(pfs, path, false);
-    validateExists(pfs, path, false);
-
-    // thing after final '.' is not a number
-    path = new Path("pseudo:///myPsedoFile.txt");
-    validateGetFileStatus(pfs, path, false);
-    validateCreate(pfs, path, false);
-    validateOpen(pfs, path, false);
-    validateExists(pfs, path, false);
-
-    // Generate valid file name(relative path) and validate operations on it
-    long fileSize = 231456;
-    path = PseudoLocalFs.generateFilePath("my.Psedo.File", fileSize);
-    // Validate the above generateFilePath()
-    assertEquals("generateFilePath() failed.", fileSize,
-                 pfs.validateFileNameFormat(path));
-
-    validateGetFileStatus(pfs, path, true);
-    validateCreate(pfs, path, true);
-    validateOpen(pfs, path, true);
-    validateExists(pfs, path, true);
-
-    // Validate operations on valid qualified path
-    path = new Path("myPsedoFile.1237");
-    path = path.makeQualified(pfs);
-    validateGetFileStatus(pfs, path, true);
-    validateCreate(pfs, path, true);
-    validateOpen(pfs, path, true);
-    validateExists(pfs, path, true);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
deleted file mode 100644
index cd55483..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
+++ /dev/null
@@ -1,134 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.junit.Test;
-
-import com.sun.tools.javac.code.Attribute.Array;
-
-public class TestRandomAlgorithm {
-  private static final int[][] parameters = new int[][] {
-    {5, 1, 1}, 
-    {10, 1, 2},
-    {10, 2, 2},
-    {20, 1, 3},
-    {20, 2, 3},
-    {20, 3, 3},
-    {100, 3, 10},
-    {100, 3, 100},
-    {100, 3, 1000},
-    {100, 3, 10000},
-    {100, 3, 100000},
-    {100, 3, 1000000}
-  };
-  
-  private List<Integer> convertIntArray(int[] from) {
-    List<Integer> ret = new ArrayList<Integer>(from.length);
-    for (int v : from) {
-      ret.add(v);
-    }
-    return ret;
-  }
-  
-  private void testRandomSelectSelector(int niter, int m, int n) {
-    RandomAlgorithms.Selector selector = new RandomAlgorithms.Selector(n,
-        (double) m / n, new Random());
-    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(
-        niter);
-    for (int i = 0; i < niter; ++i, selector.reset()) {
-      int[] result = new int[m];
-      for (int j = 0; j < m; ++j) {
-        int v = selector.next();
-        if (v < 0)
-          break;
-        result[j]=v;
-      }
-      Arrays.sort(result);
-      List<Integer> resultAsList = convertIntArray(result);
-      Integer count = results.get(resultAsList);
-      if (count == null) {
-        results.put(resultAsList, 1);
-      } else {
-        results.put(resultAsList, ++count);
-      }
-    }
-
-    verifyResults(results, m, n);
-  }
-
-  private void testRandomSelect(int niter, int m, int n) {
-    Random random = new Random();
-    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(
-        niter);
-    for (int i = 0; i < niter; ++i) {
-      int[] result = RandomAlgorithms.select(m, n, random);
-      Arrays.sort(result);
-      List<Integer> resultAsList = convertIntArray(result);
-      Integer count = results.get(resultAsList);
-      if (count == null) {
-        results.put(resultAsList, 1);
-      } else {
-        results.put(resultAsList, ++count);
-      }
-    }
-
-    verifyResults(results, m, n);
-  }
-
-  private void verifyResults(Map<List<Integer>, Integer> results, int m, int n) {
-    if (n>=10) {
-      assertTrue(results.size() >= Math.min(m, 2));
-    }
-    for (List<Integer> result : results.keySet()) {
-      assertEquals(m, result.size());
-      Set<Integer> seen = new HashSet<Integer>();
-      for (int v : result) {
-        System.out.printf("%d ", v);
-        assertTrue((v >= 0) && (v < n));
-        assertTrue(seen.add(v));
-      }
-      System.out.printf(" ==> %d\n", results.get(result));
-    }
-    System.out.println("====");
-  }
-  
-  @Test
-  public void testRandomSelect() {
-    for (int[] param : parameters) {
-    testRandomSelect(param[0], param[1], param[2]);
-    }
-  }
-  
-  @Test
-  public void testRandomSelectSelector() {
-    for (int[] param : parameters) {
-      testRandomSelectSelector(param[0], param[1], param[2]);
-      }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
deleted file mode 100644
index e302db5..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator;
-
-import static org.junit.Assert.*;
-import org.junit.Test;
-
-/**
- * Test {@link RandomTextDataGenerator}.
- */
-public class TestRandomTextDataGenerator {
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate random words of 
-   * desired size.
-   */
-  @Test
-  public void testRandomTextDataGenerator() {
-    RandomTextDataGenerator rtdg = new RandomTextDataGenerator(10, 0L, 5);
-    List<String> words = rtdg.getRandomWords();
-
-    // check the size
-    assertEquals("List size mismatch", 10, words.size());
-
-    // check the words
-    Set<String> wordsSet = new HashSet<String>(words);
-    assertEquals("List size mismatch due to duplicates", 10, wordsSet.size());
-
-    // check the word lengths
-    for (String word : wordsSet) {
-      assertEquals("Word size mismatch", 5, word.length());
-    }
-  }
-  
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate same words given the
-   * same list-size, word-length and seed.
-   */
-  @Test
-  public void testRandomTextDataGeneratorRepeatability() {
-    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 0L, 5);
-    List<String> words1 = rtdg1.getRandomWords();
-
-    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);
-    List<String> words2 = rtdg2.getRandomWords();
-    
-    assertTrue("List mismatch", words1.equals(words2));
-  }
-  
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate different words given 
-   * different seeds.
-   */
-  @Test
-  public void testRandomTextDataGeneratorUniqueness() {
-    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 1L, 5);
-    Set<String> words1 = new HashSet(rtdg1.getRandomWords());
-
-    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);
-    Set<String> words2 = new HashSet(rtdg2.getRandomWords());
-    
-    assertFalse("List size mismatch across lists", words1.equals(words2));
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
deleted file mode 100644
index 2ab2444..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
+++ /dev/null
@@ -1,81 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.Random;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.DataOutputBuffer;
-
-public class TestRecordFactory {
-  private static final Log LOG = LogFactory.getLog(TestRecordFactory.class);
-
-  public static void testFactory(long targetBytes, long targetRecs)
-      throws Exception {
-    final Configuration conf = new Configuration();
-    final GridmixKey key = new GridmixKey();
-    final GridmixRecord val = new GridmixRecord();
-    LOG.info("Target bytes/records: " + targetBytes + "/" + targetRecs);
-    final RecordFactory f = new AvgRecordFactory(targetBytes, targetRecs, conf);
-    targetRecs = targetRecs <= 0 && targetBytes >= 0
-      ? Math.max(1,
-                 targetBytes 
-                 / conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 
-                               64 * 1024))
-      : targetRecs;
-
-    long records = 0L;
-    final DataOutputBuffer out = new DataOutputBuffer();
-    while (f.next(key, val)) {
-      ++records;
-      key.write(out);
-      val.write(out);
-    }
-    assertEquals(targetRecs, records);
-    assertEquals(targetBytes, out.getLength());
-  }
-
-  @Test
-  public void testRandom() throws Exception {
-    final Random r = new Random();
-    final long targetBytes = r.nextInt(1 << 20) + 3 * (1 << 14);
-    final long targetRecs = r.nextInt(1 << 14);
-    testFactory(targetBytes, targetRecs);
-  }
-
-  @Test
-  public void testAvg() throws Exception {
-    final Random r = new Random();
-    final long avgsize = r.nextInt(1 << 10) + 1;
-    final long targetRecs = r.nextInt(1 << 14);
-    testFactory(targetRecs * avgsize, targetRecs);
-  }
-
-  @Test
-  public void testZero() throws Exception {
-    final Random r = new Random();
-    final long targetBytes = r.nextInt(1 << 20);
-    testFactory(targetBytes, 0);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
deleted file mode 100644
index 35db026..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
+++ /dev/null
@@ -1,613 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.StatusReporter;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.mapreduce.TaskInputOutputContext;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-import org.apache.hadoop.mapreduce.task.MapContextImpl;
-import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.ProcResourceValues;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.mapred.DummyResourceCalculatorPlugin;
-import org.apache.hadoop.mapred.gridmix.LoadJob.ResourceUsageMatcherRunner;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageEmulatorPlugin;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin.DefaultCpuUsageEmulator;
-
-/**
- * Test Gridmix's resource emulator framework and supported plugins.
- */
-public class TestResourceUsageEmulators {
-  /**
-   * A {@link ResourceUsageEmulatorPlugin} implementation for testing purpose.
-   * It essentially creates a file named 'test' in the test directory.
-   */
-  static class TestResourceUsageEmulatorPlugin 
-  implements ResourceUsageEmulatorPlugin {
-    static final Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp"));
-    static final Path tempDir = 
-      new Path(rootTempDir, "TestResourceUsageEmulatorPlugin");
-    static final String DEFAULT_IDENTIFIER = "test";
-    
-    private Path touchPath = null;
-    private FileSystem fs = null;
-    
-    @Override
-    public void emulate() throws IOException, InterruptedException {
-      // add some time between 2 calls to emulate()
-      try {
-        Thread.sleep(1000); // sleep for 1s
-      } catch (Exception e){}
-      
-      try {
-        fs.delete(touchPath, false); // delete the touch file
-        //TODO Search for a better touch utility
-        fs.create(touchPath).close(); // recreate it
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    protected String getIdentifier() {
-      return DEFAULT_IDENTIFIER;
-    }
-    
-    private static Path getFilePath(String id) {
-      return new Path(tempDir, id);
-    }
-    
-    private static Path getInitFilePath(String id) {
-      return new Path(tempDir, id + ".init");
-    }
-    
-    @Override
-    public void initialize(Configuration conf, ResourceUsageMetrics metrics,
-        ResourceCalculatorPlugin monitor, Progressive progress) {
-      // add some time between 2 calls to initialize()
-      try {
-        Thread.sleep(1000); // sleep for 1s
-      } catch (Exception e){}
-      
-      try {
-        fs = FileSystem.getLocal(conf);
-        
-        Path initPath = getInitFilePath(getIdentifier());
-        fs.delete(initPath, false); // delete the old file
-        fs.create(initPath).close(); // create a new one
-        
-        touchPath = getFilePath(getIdentifier());
-        fs.delete(touchPath, false);
-      } catch (Exception e) {
-        
-      } finally {
-        if (fs != null) {
-          try {
-            fs.deleteOnExit(tempDir);
-          } catch (IOException ioe){}
-        }
-      }
-    }
-    
-    // test if the emulation framework successfully loaded this plugin
-    static long testInitialization(String id, Configuration conf) 
-    throws IOException {
-      Path testPath = getInitFilePath(id);
-      FileSystem fs = FileSystem.getLocal(conf);
-      return fs.exists(testPath) 
-             ? fs.getFileStatus(testPath).getModificationTime() 
-             : 0;
-    }
-    
-    // test if the emulation framework successfully loaded this plugin
-    static long testEmulation(String id, Configuration conf) 
-    throws IOException {
-      Path testPath = getFilePath(id);
-      FileSystem fs = FileSystem.getLocal(conf);
-      return fs.exists(testPath) 
-             ? fs.getFileStatus(testPath).getModificationTime() 
-             : 0;
-    }
-  }
-  
-  /**
-   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
-   * a file named 'others' in the test directory.
-   */
-  static class TestOthers extends TestResourceUsageEmulatorPlugin {
-    static final String ID = "others";
-    
-    @Override
-    protected String getIdentifier() {
-      return ID;
-    }
-  }
-  
-  /**
-   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
-   * a file named 'cpu' in the test directory.
-   */
-  static class TestCpu extends TestResourceUsageEmulatorPlugin {
-    static final String ID = "cpu";
-    
-    @Override
-    protected String getIdentifier() {
-      return ID;
-    }
-  }
-  
-  /**
-   * Test {@link ResourceUsageMatcher}.
-   */
-  @Test
-  public void testResourceUsageMatcher() throws Exception {
-    ResourceUsageMatcher matcher = new ResourceUsageMatcher();
-    Configuration conf = new Configuration();
-    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
-                  TestResourceUsageEmulatorPlugin.class, 
-                  ResourceUsageEmulatorPlugin.class);
-    long currentTime = System.currentTimeMillis();
-    
-    matcher.configure(conf, null, null, null);
-    
-    matcher.matchResourceUsage();
-    
-    String id = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;
-    long result = 
-      TestResourceUsageEmulatorPlugin.testInitialization(id, conf);
-    assertTrue("Resource usage matcher failed to initialize the configured"
-               + " plugin", result > currentTime);
-    result = TestResourceUsageEmulatorPlugin.testEmulation(id, conf);
-    assertTrue("Resource usage matcher failed to load and emulate the"
-               + " configured plugin", result > currentTime);
-    
-    // test plugin order to first emulate cpu and then others
-    conf.setStrings(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
-                    TestCpu.class.getName() + "," + TestOthers.class.getName());
-    
-    matcher.configure(conf, null, null, null);
-
-    // test the initialization order
-    long time1 = 
-           TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);
-    long time2 = 
-           TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, 
-                                                              conf);
-    assertTrue("Resource usage matcher failed to initialize the configured"
-               + " plugins in order", time1 < time2);
-    
-    matcher.matchResourceUsage();
-
-    // Note that the cpu usage emulator plugin is configured 1st and then the
-    // others plugin.
-    time1 = 
-      TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);
-    time2 = 
-      TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, 
-                                                         conf);
-    assertTrue("Resource usage matcher failed to load the configured plugins", 
-               time1 < time2);
-  }
-  
-  /**
-   * Fakes the cumulative usage using {@link FakeCpuUsageEmulatorCore}.
-   */
-  static class FakeResourceUsageMonitor extends DummyResourceCalculatorPlugin {
-    private FakeCpuUsageEmulatorCore core;
-    
-    public FakeResourceUsageMonitor(FakeCpuUsageEmulatorCore core) {
-      this.core = core;
-    }
-    
-    /**
-     * A dummy CPU usage monitor. Every call to 
-     * {@link ResourceCalculatorPlugin#getCumulativeCpuTime()} will return the 
-     * value of {@link FakeCpuUsageEmulatorCore#getNumCalls()}.
-     */
-    @Override
-    public long getCumulativeCpuTime() {
-      return core.getCpuUsage();
-    }
-
-    /**
-     * Returns a {@link ProcResourceValues} with cumulative cpu usage  
-     * computed using {@link #getCumulativeCpuTime()}.
-     */
-    @Override
-    public ProcResourceValues getProcResourceValues() {
-      long usageValue = getCumulativeCpuTime();
-      return new ProcResourceValues(usageValue, -1, -1);
-    }
-  }
-  
-  /**
-   * A dummy {@link Progressive} implementation that allows users to set the
-   * progress for testing. The {@link Progressive#getProgress()} call will 
-   * return the last progress value set using 
-   * {@link FakeProgressive#setProgress(float)}.
-   */
-  static class FakeProgressive implements Progressive {
-    private float progress = 0F;
-    @Override
-    public float getProgress() {
-      return progress;
-    }
-    
-    void setProgress(float progress) {
-      this.progress = progress;
-    }
-  }
-  
-  /**
-   * A dummy reporter for {@link LoadJob.ResourceUsageMatcherRunner}.
-   */
-  private static class DummyReporter extends StatusReporter {
-    private Progressive progress;
-    
-    DummyReporter(Progressive progress) {
-      this.progress = progress;
-    }
-    
-    @Override
-    public org.apache.hadoop.mapreduce.Counter getCounter(Enum<?> name) {
-      return null;
-    }
-    
-    @Override
-    public org.apache.hadoop.mapreduce.Counter getCounter(String group,
-                                                          String name) {
-      return null;
-    }
-    
-    @Override
-    public void progress() {
-    }
-    
-    @Override
-    public float getProgress() {
-      return progress.getProgress();
-    }
-    
-    @Override
-    public void setStatus(String status) {
-    }
-  }
-  
-  // Extends ResourceUsageMatcherRunner for testing.
-  @SuppressWarnings("unchecked")
-  private static class FakeResourceUsageMatcherRunner 
-  extends ResourceUsageMatcherRunner {
-    FakeResourceUsageMatcherRunner(TaskInputOutputContext context, 
-                                   ResourceUsageMetrics metrics) {
-      super(context, metrics);
-    }
-    
-    // test ResourceUsageMatcherRunner
-    void test() throws Exception {
-      super.match();
-    }
-  }
-  
-  /**
-   * Test {@link LoadJob.ResourceUsageMatcherRunner}.
-   */
-  @Test
-  @SuppressWarnings("unchecked")
-  public void testResourceUsageMatcherRunner() throws Exception {
-    Configuration conf = new Configuration();
-    FakeProgressive progress = new FakeProgressive();
-    
-    // set the resource calculator plugin
-    conf.setClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN,
-                  DummyResourceCalculatorPlugin.class, 
-                  ResourceCalculatorPlugin.class);
-    // set the resources
-    // set the resource implementation class
-    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
-                  TestResourceUsageEmulatorPlugin.class, 
-                  ResourceUsageEmulatorPlugin.class);
-    
-    long currentTime = System.currentTimeMillis();
-    
-    // initialize the matcher class
-    TaskAttemptID id = new TaskAttemptID("test", 1, TaskType.MAP, 1, 1);
-    StatusReporter reporter = new DummyReporter(progress);
-    TaskInputOutputContext context = 
-      new MapContextImpl(conf, id, null, null, null, reporter, null);
-    FakeResourceUsageMatcherRunner matcher = 
-      new FakeResourceUsageMatcherRunner(context, null);
-    
-    // check if the matcher initialized the plugin
-    String identifier = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;
-    long initTime = 
-      TestResourceUsageEmulatorPlugin.testInitialization(identifier, conf);
-    assertTrue("ResourceUsageMatcherRunner failed to initialize the"
-               + " configured plugin", initTime > currentTime);
-    
-    // check the progress
-    assertEquals("Progress mismatch in ResourceUsageMatcherRunner", 
-                 0, progress.getProgress(), 0D);
-    
-    // call match() and check progress
-    progress.setProgress(0.01f);
-    currentTime = System.currentTimeMillis();
-    matcher.test();
-    long emulateTime = 
-      TestResourceUsageEmulatorPlugin.testEmulation(identifier, conf);
-    assertTrue("ProgressBasedResourceUsageMatcher failed to load and emulate"
-               + " the configured plugin", emulateTime > currentTime);
-  }
-  
-  /**
-   * Test {@link CumulativeCpuUsageEmulatorPlugin}'s core CPU usage emulation 
-   * engine.
-   */
-  @Test
-  public void testCpuUsageEmulator() throws IOException {
-    // test CpuUsageEmulator calibration with fake resource calculator plugin
-    long target = 100000L; // 100 secs
-    int unitUsage = 50;
-    FakeCpuUsageEmulatorCore fakeCpuEmulator = new FakeCpuUsageEmulatorCore();
-    fakeCpuEmulator.setUnitUsage(unitUsage);
-    FakeResourceUsageMonitor fakeMonitor = 
-      new FakeResourceUsageMonitor(fakeCpuEmulator);
-    
-    // calibrate for 100ms
-    fakeCpuEmulator.calibrate(fakeMonitor, target);
-    
-    // by default, CpuUsageEmulator.calibrate() will consume 100ms of CPU usage
-    assertEquals("Fake calibration failed", 
-                 100, fakeMonitor.getCumulativeCpuTime());
-    assertEquals("Fake calibration failed", 
-                 100, fakeCpuEmulator.getCpuUsage());
-    // by default, CpuUsageEmulator.performUnitComputation() will be called 
-    // twice
-    assertEquals("Fake calibration failed", 
-                 2, fakeCpuEmulator.getNumCalls());
-  }
-  
-  /**
-   * This is a dummy class that fakes CPU usage.
-   */
-  private static class FakeCpuUsageEmulatorCore 
-  extends DefaultCpuUsageEmulator {
-    private int numCalls = 0;
-    private int unitUsage = 1;
-    private int cpuUsage = 0;
-    
-    @Override
-    protected void performUnitComputation() {
-      ++numCalls;
-      cpuUsage += unitUsage;
-    }
-    
-    int getNumCalls() {
-      return numCalls;
-    }
-    
-    int getCpuUsage() {
-      return cpuUsage;
-    }
-    
-    void reset() {
-      numCalls = 0;
-      cpuUsage = 0;
-    }
-    
-    void setUnitUsage(int unitUsage) {
-      this.unitUsage = unitUsage;
-    }
-  }
-  
-  // Creates a ResourceUsageMetrics object from the target usage
-  static ResourceUsageMetrics createMetrics(long target) {
-    ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-    metrics.setCumulativeCpuUsage(target);
-    metrics.setVirtualMemoryUsage(target);
-    metrics.setPhysicalMemoryUsage(target);
-    metrics.setHeapUsage(target);
-    return metrics;
-  }
-  
-  /**
-   * Test {@link CumulativeCpuUsageEmulatorPlugin}.
-   */
-  @Test
-  public void testCumulativeCpuUsageEmulatorPlugin() throws Exception {
-    Configuration conf = new Configuration();
-    long targetCpuUsage = 1000L;
-    int unitCpuUsage = 50;
-    
-    // fake progress indicator
-    FakeProgressive fakeProgress = new FakeProgressive();
-    
-    // fake cpu usage generator
-    FakeCpuUsageEmulatorCore fakeCore = new FakeCpuUsageEmulatorCore();
-    fakeCore.setUnitUsage(unitCpuUsage);
-    
-    // a cumulative cpu usage emulator with fake core
-    CumulativeCpuUsageEmulatorPlugin cpuPlugin = 
-      new CumulativeCpuUsageEmulatorPlugin(fakeCore);
-    
-    // test with invalid or missing resource usage value
-    ResourceUsageMetrics invalidUsage = createMetrics(0);
-    cpuPlugin.initialize(conf, invalidUsage, null, null);
-    
-    // test if disabled cpu emulation plugin's emulate() call is a no-operation
-    // this will test if the emulation plugin is disabled or not
-    int numCallsPre = fakeCore.getNumCalls();
-    long cpuUsagePre = fakeCore.getCpuUsage();
-    cpuPlugin.emulate();
-    int numCallsPost = fakeCore.getNumCalls();
-    long cpuUsagePost = fakeCore.getCpuUsage();
-    
-    //  test if no calls are made cpu usage emulator core
-    assertEquals("Disabled cumulative CPU usage emulation plugin works!", 
-                 numCallsPre, numCallsPost);
-    
-    //  test if no calls are made cpu usage emulator core
-    assertEquals("Disabled cumulative CPU usage emulation plugin works!", 
-                 cpuUsagePre, cpuUsagePost);
-    
-    // test with valid resource usage value
-    ResourceUsageMetrics metrics = createMetrics(targetCpuUsage);
-    
-    // fake monitor
-    ResourceCalculatorPlugin monitor = new FakeResourceUsageMonitor(fakeCore);
-    
-    // test with default emulation interval
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, 
-                          targetCpuUsage, targetCpuUsage / unitCpuUsage);
-    
-    // test with custom value for emulation interval of 20%
-    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
-                  0.2F);
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, 
-                          targetCpuUsage, targetCpuUsage / unitCpuUsage);
-    
-    // test if emulation interval boundary is respected (unit usage = 1)
-    //  test the case where the current progress is less than threshold
-    fakeProgress = new FakeProgressive(); // initialize
-    fakeCore.reset();
-    fakeCore.setUnitUsage(1);
-    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
-                  0.25F);
-    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    // take a snapshot after the initialization
-    long initCpuUsage = monitor.getCumulativeCpuTime();
-    long initNumCalls = fakeCore.getNumCalls();
-    // test with 0 progress
-    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, 
-                          initNumCalls, "[no-op, 0 progress]");
-    // test with 24% progress
-    testEmulationBoundary(0.24F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[no-op, 24% progress]");
-    // test with 25% progress
-    //  target = 1000ms, target emulation at 25% = 250ms, 
-    //  weighed target = 1000 * 0.25^4 (we are using progress^4 as the weight)
-    //                 ~ 4
-    //  but current usage = init-usage = 100, hence expected = 100
-    testEmulationBoundary(0.25F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[op, 25% progress]");
-    
-    // test with 80% progress
-    //  target = 1000ms, target emulation at 80% = 800ms, 
-    //  weighed target = 1000 * 0.25^4 (we are using progress^4 as the weight)
-    //                 ~ 410
-    //  current-usage = init-usage = 100, hence expected-usage = 410
-    testEmulationBoundary(0.80F, fakeCore, fakeProgress, cpuPlugin, 410, 410, 
-                          "[op, 80% progress]");
-    
-    // now test if the final call with 100% progress ramps up the CPU usage
-    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage,
-                          targetCpuUsage, "[op, 100% progress]");
-    
-    // test if emulation interval boundary is respected (unit usage = 50)
-    //  test the case where the current progress is less than threshold
-    fakeProgress = new FakeProgressive(); // initialize
-    fakeCore.reset();
-    fakeCore.setUnitUsage(unitCpuUsage);
-    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
-                  0.40F);
-    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    // take a snapshot after the initialization
-    initCpuUsage = monitor.getCumulativeCpuTime();
-    initNumCalls = fakeCore.getNumCalls();
-    // test with 0 progress
-    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, 
-                          initNumCalls, "[no-op, 0 progress]");
-    // test with 39% progress
-    testEmulationBoundary(0.39F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[no-op, 39% progress]");
-    // test with 40% progress
-    //  target = 1000ms, target emulation at 40% = 4000ms, 
-    //  weighed target = 1000 * 0.40^4 (we are using progress^4 as the weight)
-    //                 ~ 26
-    // current-usage = init-usage = 100, hence expected-usage = 100
-    testEmulationBoundary(0.40F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[op, 40% progress]");
-    
-    // test with 90% progress
-    //  target = 1000ms, target emulation at 90% = 900ms, 
-    //  weighed target = 1000 * 0.90^4 (we are using progress^4 as the weight)
-    //                 ~ 657
-    //  current-usage = init-usage = 100, hence expected-usage = 657 but 
-    //  the fake-core increases in steps of 50, hence final target = 700
-    testEmulationBoundary(0.90F, fakeCore, fakeProgress, cpuPlugin, 700, 
-                          700 / unitCpuUsage, "[op, 90% progress]");
-    
-    // now test if the final call with 100% progress ramps up the CPU usage
-    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage,
-                          targetCpuUsage / unitCpuUsage, "[op, 100% progress]");
-  }
-  
-  // test whether the CPU usage emulator achieves the desired target using
-  // desired calls to the underling core engine.
-  private static void testEmulationAccuracy(Configuration conf, 
-                        FakeCpuUsageEmulatorCore fakeCore,
-                        ResourceCalculatorPlugin monitor,
-                        ResourceUsageMetrics metrics,
-                        CumulativeCpuUsageEmulatorPlugin cpuPlugin,
-                        long expectedTotalCpuUsage, long expectedTotalNumCalls) 
-  throws Exception {
-    FakeProgressive fakeProgress = new FakeProgressive();
-    fakeCore.reset();
-    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    int numLoops = 0;
-    while (fakeProgress.getProgress() < 1) {
-      ++numLoops;
-      float progress = (float)numLoops / 100;
-      fakeProgress.setProgress(progress);
-      cpuPlugin.emulate();
-    }
-    
-    // test if the resource plugin shows the expected invocations
-    assertEquals("Cumulative cpu usage emulator plugin failed (num calls)!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-    // test if the resource plugin shows the expected usage
-    assertEquals("Cumulative cpu usage emulator plugin failed (total usage)!", 
-                 expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);
-  }
-  
-  // tests if the CPU usage emulation plugin emulates only at the expected
-  // progress gaps
-  private static void testEmulationBoundary(float progress, 
-      FakeCpuUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, 
-      CumulativeCpuUsageEmulatorPlugin cpuPlugin, long expectedTotalCpuUsage, 
-      long expectedTotalNumCalls, String info) throws Exception {
-    fakeProgress.setProgress(progress);
-    cpuPlugin.emulate();
-    
-    assertEquals("Emulation interval test for cpu usage failed " + info + "!", 
-                 expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);
-    assertEquals("Emulation interval test for num calls failed " + info + "!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestUserResolve.java b/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
deleted file mode 100644
index 8050f33..0000000
--- a/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
+++ /dev/null
@@ -1,172 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.security.UserGroupInformation;
-
-public class TestUserResolve {
-
-  private static Path rootDir = null;
-  private static Configuration conf = null;
-  private static FileSystem fs = null;
-
-  @BeforeClass
-  public static void createRootDir() throws IOException {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    rootDir = new Path(new Path(System.getProperty("test.build.data", "/tmp"))
-                 .makeQualified(fs), "gridmixUserResolve");
-  }
-
-  /**
-   * Creates users file with the content as the String usersFileContent.
-   * @param usersFilePath    the path to the file that is to be created
-   * @param usersFileContent Content of users file
-   * @throws IOException
-   */
-  private static void writeUserList(Path usersFilePath, String usersFileContent)
-      throws IOException {
-
-    FSDataOutputStream out = null;
-    try {
-      out = fs.create(usersFilePath, true);
-      out.writeBytes(usersFileContent);
-    } finally {
-      if (out != null) {
-        out.close();
-      }
-    }
-  }
-
-  /**
-   * Validate RoundRobinUserResolver's behavior for bad user resource file.
-   * RoundRobinUserResolver.setTargetUsers() should throw proper Exception for
-   * the cases like
-   * <li> non existent user resource file and
-   * <li> empty user resource file
-   * 
-   * @param rslv              The RoundRobinUserResolver object
-   * @param userRsrc          users file
-   * @param expectedErrorMsg  expected error message
-   */
-  private void validateBadUsersFile(UserResolver rslv, URI userRsrc,
-      String expectedErrorMsg) {
-    boolean fail = false;
-    try {
-      rslv.setTargetUsers(userRsrc, conf);
-    } catch (IOException e) {
-      assertTrue("Exception message from RoundRobinUserResolver is wrong",
-          e.getMessage().equals(expectedErrorMsg));
-      fail = true;
-    }
-    assertTrue("User list required for RoundRobinUserResolver", fail);
-  }
-
-  /**
-   * Validate the behavior of {@link RoundRobinUserResolver} for different
-   * user resource files like
-   * <li> Empty user resource file
-   * <li> Non existent user resource file
-   * <li> User resource file with valid content
-   * @throws Exception
-   */
-  @Test
-  public void testRoundRobinResolver() throws Exception {
-
-    final UserResolver rslv = new RoundRobinUserResolver();
-    Path usersFilePath = new Path(rootDir, "users");
-    URI userRsrc = new URI(usersFilePath.toString());
-
-    // Check if the error message is as expected for non existent
-    // user resource file.
-    fs.delete(usersFilePath, false);
-    String expectedErrorMsg = "File " + userRsrc + " does not exist";
-    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);
-
-    // Check if the error message is as expected for empty user resource file
-    writeUserList(usersFilePath, "");// creates empty users file
-    expectedErrorMsg =
-        RoundRobinUserResolver.buildEmptyUsersErrorMsg(userRsrc);
-    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);
-
-    // Create user resource file with valid content like older users list file
-    // with usernames and groups
-    writeUserList(usersFilePath,
-        "user0,groupA,groupB,groupC\nuser1,groupA,groupC\n");
-    validateValidUsersFile(rslv, userRsrc);
-
-    // Create user resource file with valid content with
-    // usernames with groups and without groups
-    writeUserList(usersFilePath, "user0,groupA,groupB\nuser1,");
-    validateValidUsersFile(rslv, userRsrc);
-
-    // Create user resource file with valid content with
-    // usernames without groups
-    writeUserList(usersFilePath, "user0\nuser1");
-    validateValidUsersFile(rslv, userRsrc);
-  }
-
-  // Validate RoundRobinUserResolver for the case of
-  // user resource file with valid content.
-  private void validateValidUsersFile(UserResolver rslv, URI userRsrc)
-      throws IOException {
-    assertTrue(rslv.setTargetUsers(userRsrc, conf));
-    UserGroupInformation ugi1 = UserGroupInformation.createRemoteUser("hfre0");
-    assertEquals("user0", rslv.getTargetUgi(ugi1).getUserName());
-    assertEquals("user1", 
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre1"))
-            .getUserName());
-    assertEquals("user0",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre2"))
-            .getUserName());
-    assertEquals("user0", rslv.getTargetUgi(ugi1).getUserName());
-    assertEquals("user1",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre3"))
-            .getUserName());
-
-    // Verify if same user comes again, its mapped user name should be
-    // correct even though UGI is constructed again.
-    assertEquals("user0", rslv.getTargetUgi(
-        UserGroupInformation.createRemoteUser("hfre0")).getUserName());
-    assertEquals("user0",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre5"))
-            .getUserName());
-    assertEquals("user0",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre0"))
-            .getUserName());
-  }
-
-  @Test
-  public void testSubmitterResolver() throws Exception {
-    final UserResolver rslv = new SubmitterUserResolver();
-    assertFalse(rslv.needsTargetUsersList());
-    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-    assertEquals(ugi, rslv.getTargetUgi((UserGroupInformation)null));
-  }
-}
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index 1e41ad4..e7d849b 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -215,6 +215,11 @@
         <artifactId>hadoop-mapreduce-examples</artifactId>
         <version>${project.version}</version>
       </dependency>
+      <dependency>
+        <groupId>org.apache.hadoop</groupId>
+        <artifactId>hadoop-gridmix</artifactId>
+        <version>${project.version}</version>
+      </dependency>
 
       <dependency>
         <groupId>org.apache.hadoop</groupId>
diff --git a/hadoop-tools/hadoop-gridmix/pom.xml b/hadoop-tools/hadoop-gridmix/pom.xml
new file mode 100644
index 0000000..231b63a
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/pom.xml
@@ -0,0 +1,131 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<project>
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.hadoop</groupId>
+    <artifactId>hadoop-project</artifactId>
+    <version>2.0.0-cdh4.1.0-SNAPSHOT</version>
+    <relativePath>../../hadoop-project</relativePath>
+  </parent>
+  <groupId>org.apache.hadoop</groupId>
+  <artifactId>hadoop-gridmix</artifactId>
+  <version>2.0.0-cdh4.1.0-SNAPSHOT</version>
+  <description>Apache Hadoop Gridmix</description>
+  <name>Apache Hadoop Gridmix</name>
+  <packaging>jar</packaging>
+
+  <properties>
+    <hadoop.log.dir>${project.build.directory}/log</hadoop.log.dir>
+  </properties>
+
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-annotations</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-hs</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-core</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-common</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-rumen</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-common</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-server-tests</artifactId>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+  </dependencies>
+
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-antrun-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>create-log-dir</id>
+            <phase>process-test-resources</phase>
+            <goals>
+              <goal>run</goal>
+            </goals>
+            <configuration>
+              <target>
+                <delete dir="${test.build.data}"/>
+                <mkdir dir="${test.build.data}"/>
+                <mkdir dir="${hadoop.log.dir}"/>
+              </target>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+         <configuration>
+          <archive>
+           <manifest>
+            <mainClass>org.apache.hadoop.tools.HadoopArchives</mainClass>
+           </manifest>
+         </archive>
+        </configuration>
+       </plugin>
+    </plugins>
+  </build>
+</project>
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
new file mode 100644
index 0000000..9ba6e9a
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
@@ -0,0 +1,123 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * Given byte and record targets, emit roughly equal-sized records satisfying
+ * the contract.
+ */
+class AvgRecordFactory extends RecordFactory {
+
+  /**
+   * Percentage of record for key data.
+   */
+  public static final String GRIDMIX_KEY_FRC = "gridmix.key.fraction";
+  public static final String GRIDMIX_MISSING_REC_SIZE = 
+    "gridmix.missing.rec.size";
+
+
+  private final long targetBytes;
+  private final long targetRecords;
+  private final long step;
+  private final int avgrec;
+  private final int keyLen;
+  private long accBytes = 0L;
+  private long accRecords = 0L;
+  private int unspilledBytes = 0;
+  private int minSpilledBytes = 0;
+
+  /**
+   * @param targetBytes Expected byte count.
+   * @param targetRecords Expected record count.
+   * @param conf Used to resolve edge cases @see #GRIDMIX_KEY_FRC
+   */
+  public AvgRecordFactory(long targetBytes, long targetRecords,
+      Configuration conf) {
+    this(targetBytes, targetRecords, conf, 0);
+  }
+  
+  /**
+   * @param minSpilledBytes Minimum amount of data expected per record
+   */
+  public AvgRecordFactory(long targetBytes, long targetRecords,
+      Configuration conf, int minSpilledBytes) {
+    this.targetBytes = targetBytes;
+    this.targetRecords = targetRecords <= 0 && this.targetBytes >= 0
+      ? Math.max(1,
+          this.targetBytes / conf.getInt(GRIDMIX_MISSING_REC_SIZE, 64 * 1024))
+      : targetRecords;
+    final long tmp = this.targetBytes / this.targetRecords;
+    step = this.targetBytes - this.targetRecords * tmp;
+    avgrec = (int) Math.min(Integer.MAX_VALUE, tmp + 1);
+    keyLen = Math.max(1,
+        (int)(tmp * Math.min(1.0f, conf.getFloat(GRIDMIX_KEY_FRC, 0.1f))));
+    this.minSpilledBytes = minSpilledBytes;
+  }
+
+  @Override
+  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
+    if (accBytes >= targetBytes) {
+      return false;
+    }
+    final int reclen = accRecords++ >= step ? avgrec - 1 : avgrec;
+    final int len = (int) Math.min(targetBytes - accBytes, reclen);
+    
+    unspilledBytes += len;
+    
+    // len != reclen?
+    if (key != null) {
+      if (unspilledBytes < minSpilledBytes && accRecords < targetRecords) {
+        key.setSize(1);
+        val.setSize(1);
+        accBytes += key.getSize() + val.getSize();
+        unspilledBytes -= (key.getSize() + val.getSize());
+      } else {
+        key.setSize(keyLen);
+        val.setSize(unspilledBytes - key.getSize());
+        accBytes += unspilledBytes;
+        unspilledBytes = 0;
+      }
+    } else {
+      if (unspilledBytes < minSpilledBytes && accRecords < targetRecords) {
+        val.setSize(1);
+        accBytes += val.getSize();
+        unspilledBytes -= val.getSize();
+      } else {
+        val.setSize(unspilledBytes);
+        accBytes += unspilledBytes;
+        unspilledBytes = 0;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public float getProgress() throws IOException {
+    return Math.min(1.0f, accBytes / ((float)targetBytes));
+  }
+
+  @Override
+  public void close() throws IOException {
+    // noop
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
new file mode 100644
index 0000000..9568171
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.lang.time.FastDateFormat;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
+
+/**
+ * Summarizes the Hadoop cluster used in this {@link Gridmix} run. 
+ * Statistics that are reported are
+ * <ul>
+ *   <li>Total number of active trackers in the cluster</li>
+ *   <li>Total number of blacklisted trackers in the cluster</li>
+ *   <li>Max map task capacity of the cluster</li>
+ *   <li>Max reduce task capacity of the cluster</li>
+ * </ul>
+ * 
+ * Apart from these statistics, {@link JobTracker} and {@link FileSystem} 
+ * addresses are also recorded in the summary.
+ */
+class ClusterSummarizer implements StatListener<ClusterStats> {
+  static final Log LOG = LogFactory.getLog(ClusterSummarizer.class);
+  
+  private int numBlacklistedTrackers;
+  private int numActiveTrackers;
+  private int maxMapTasks;
+  private int maxReduceTasks;
+  private String jobTrackerInfo = Summarizer.NA;
+  private String namenodeInfo = Summarizer.NA;
+  
+  @Override
+  @SuppressWarnings("deprecation")
+  public void update(ClusterStats item) {
+    try {
+      numBlacklistedTrackers = item.getStatus().getBlacklistedTrackers();
+      numActiveTrackers = item.getStatus().getTaskTrackers();
+      maxMapTasks = item.getStatus().getMaxMapTasks();
+      maxReduceTasks = item.getStatus().getMaxReduceTasks();
+    } catch (Exception e) {
+      long time = System.currentTimeMillis();
+      LOG.info("Error in processing cluster status at " 
+               + FastDateFormat.getInstance().format(time));
+    }
+  }
+  
+  /**
+   * Summarizes the cluster used for this {@link Gridmix} run.
+   */
+  @Override
+  public String toString() {
+    StringBuilder builder = new StringBuilder();
+    builder.append("Cluster Summary:-");
+    builder.append("\nJobTracker: ").append(getJobTrackerInfo());
+    builder.append("\nFileSystem: ").append(getNamenodeInfo());
+    builder.append("\nNumber of blacklisted trackers: ")
+           .append(getNumBlacklistedTrackers());
+    builder.append("\nNumber of active trackers: ")
+           .append(getNumActiveTrackers());
+    builder.append("\nMax map task capacity: ")
+           .append(getMaxMapTasks());
+    builder.append("\nMax reduce task capacity: ").append(getMaxReduceTasks());
+    builder.append("\n\n");
+    return builder.toString();
+  }
+  
+  void start(Configuration conf) {
+    jobTrackerInfo = conf.get(JTConfig.JT_IPC_ADDRESS);
+    namenodeInfo = conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY);
+  }
+  
+  // Getters
+  protected int getNumBlacklistedTrackers() {
+    return numBlacklistedTrackers;
+  }
+  
+  protected int getNumActiveTrackers() {
+    return numActiveTrackers;
+  }
+  
+  protected int getMaxMapTasks() {
+    return maxMapTasks;
+  }
+  
+  protected int getMaxReduceTasks() {
+    return maxReduceTasks;
+  }
+  
+  protected String getJobTrackerInfo() {
+    return jobTrackerInfo;
+  }
+  
+  protected String getNamenodeInfo() {
+    return namenodeInfo;
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
new file mode 100644
index 0000000..1308869
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
@@ -0,0 +1,574 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.io.compress.CompressionInputStream;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.io.compress.GzipCodec;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Utils;
+import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
+import org.apache.hadoop.mapred.gridmix.GenerateData.GenDataFormat;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ * This is a utility class for all the compression related modules.
+ */
+class CompressionEmulationUtil {
+  static final Log LOG = LogFactory.getLog(CompressionEmulationUtil.class);
+  
+  /**
+   * Enable compression usage in GridMix runs.
+   */
+  private static final String COMPRESSION_EMULATION_ENABLE = 
+    "gridmix.compression-emulation.enable";
+  
+  /**
+   * Enable input data decompression.
+   */
+  private static final String INPUT_DECOMPRESSION_EMULATION_ENABLE = 
+    "gridmix.compression-emulation.input-decompression.enable";
+  
+  /**
+   * Configuration property for setting the compression ratio for map input 
+   * data.
+   */
+  private static final String GRIDMIX_MAP_INPUT_COMPRESSION_RATIO = 
+    "gridmix.compression-emulation.map-input.decompression-ratio";
+  
+  /**
+   * Configuration property for setting the compression ratio of map output.
+   */
+  private static final String GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO = 
+    "gridmix.compression-emulation.map-output.compression-ratio";
+  
+  /**
+   * Configuration property for setting the compression ratio of reduce output.
+   */
+  private static final String GRIDMIX_REDUCE_OUTPUT_COMPRESSION_RATIO = 
+    "gridmix.compression-emulation.reduce-output.compression-ratio";
+  
+  /**
+   * Default compression ratio.
+   */
+  static final float DEFAULT_COMPRESSION_RATIO = 0.5F;
+  
+  private static final CompressionRatioLookupTable COMPRESSION_LOOKUP_TABLE = 
+    new CompressionRatioLookupTable();
+  
+  /**
+   * This is a {@link Mapper} implementation for generating random text data.
+   * It uses {@link RandomTextDataGenerator} for generating text data and the
+   * output files are compressed.
+   */
+  public static class RandomTextDataMapper
+  extends Mapper<NullWritable, LongWritable, Text, Text> {
+    private RandomTextDataGenerator rtg;
+
+    @Override
+    protected void setup(Context context)
+        throws IOException, InterruptedException {
+      Configuration conf = context.getConfiguration();
+      int listSize = 
+        RandomTextDataGenerator.getRandomTextDataGeneratorListSize(conf);
+      int wordSize = 
+        RandomTextDataGenerator.getRandomTextDataGeneratorWordSize(conf);
+      rtg = new RandomTextDataGenerator(listSize, wordSize);
+    }
+    
+    /**
+     * Emits random words sequence of desired size. Note that the desired output
+     * size is passed as the value parameter to this map.
+     */
+    @Override
+    public void map(NullWritable key, LongWritable value, Context context)
+    throws IOException, InterruptedException {
+      //TODO Control the extra data written ..
+      //TODO Should the key\tvalue\n be considered for measuring size?
+      //     Can counters like BYTES_WRITTEN be used? What will be the value of
+      //     such counters in LocalJobRunner?
+      for (long bytes = value.get(); bytes > 0;) {
+        String randomKey = rtg.getRandomWord();
+        String randomValue = rtg.getRandomWord();
+        context.write(new Text(randomKey), new Text(randomValue));
+        bytes -= (randomValue.getBytes().length + randomKey.getBytes().length);
+      }
+    }
+  }
+  
+  /**
+   * Configure the {@link Job} for enabling compression emulation.
+   */
+  static void configure(final Job job) throws IOException, InterruptedException,
+                                              ClassNotFoundException {
+    // set the random text mapper
+    job.setMapperClass(RandomTextDataMapper.class);
+    job.setNumReduceTasks(0);
+    job.setMapOutputKeyClass(Text.class);
+    job.setMapOutputValueClass(Text.class);
+    job.setInputFormatClass(GenDataFormat.class);
+    job.setJarByClass(GenerateData.class);
+
+    // set the output compression true
+    FileOutputFormat.setCompressOutput(job, true);
+    try {
+      FileInputFormat.addInputPath(job, new Path("ignored"));
+    } catch (IOException e) {
+      LOG.error("Error while adding input path ", e);
+    }
+  }
+
+  /**
+   * This is the lookup table for mapping compression ratio to the size of the 
+   * word in the {@link RandomTextDataGenerator}'s dictionary. 
+   * 
+   * Note that this table is computed (empirically) using a dictionary of 
+   * default length i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
+   */
+  private static class CompressionRatioLookupTable {
+    private static Map<Float, Integer> map = new HashMap<Float, Integer>(60);
+    private static final float MIN_RATIO = 0.07F;
+    private static final float MAX_RATIO = 0.68F;
+    
+    // add the empirically obtained data points in the lookup table
+    CompressionRatioLookupTable() {
+      map.put(.07F,30);
+      map.put(.08F,25);
+      map.put(.09F,60);
+      map.put(.10F,20);
+      map.put(.11F,70);
+      map.put(.12F,15);
+      map.put(.13F,80);
+      map.put(.14F,85);
+      map.put(.15F,90);
+      map.put(.16F,95);
+      map.put(.17F,100);
+      map.put(.18F,105);
+      map.put(.19F,110);
+      map.put(.20F,115);
+      map.put(.21F,120);
+      map.put(.22F,125);
+      map.put(.23F,130);
+      map.put(.24F,140);
+      map.put(.25F,145);
+      map.put(.26F,150);
+      map.put(.27F,155);
+      map.put(.28F,160);
+      map.put(.29F,170);
+      map.put(.30F,175);
+      map.put(.31F,180);
+      map.put(.32F,190);
+      map.put(.33F,195);
+      map.put(.34F,205);
+      map.put(.35F,215);
+      map.put(.36F,225);
+      map.put(.37F,230);
+      map.put(.38F,240);
+      map.put(.39F,250);
+      map.put(.40F,260);
+      map.put(.41F,270);
+      map.put(.42F,280);
+      map.put(.43F,295);
+      map.put(.44F,310);
+      map.put(.45F,325);
+      map.put(.46F,335);
+      map.put(.47F,355);
+      map.put(.48F,375);
+      map.put(.49F,395);
+      map.put(.50F,420);
+      map.put(.51F,440);
+      map.put(.52F,465);
+      map.put(.53F,500);
+      map.put(.54F,525);
+      map.put(.55F,550);
+      map.put(.56F,600);
+      map.put(.57F,640);
+      map.put(.58F,680);
+      map.put(.59F,734);
+      map.put(.60F,813);
+      map.put(.61F,905);
+      map.put(.62F,1000);
+      map.put(.63F,1055);
+      map.put(.64F,1160);
+      map.put(.65F,1355);
+      map.put(.66F,1510);
+      map.put(.67F,1805);
+      map.put(.68F,2170);
+    }
+    
+    /**
+     * Returns the size of the word in {@link RandomTextDataGenerator}'s 
+     * dictionary that can generate text with the desired compression ratio.
+     * 
+     * @throws RuntimeException If ratio is less than {@value #MIN_RATIO} or 
+     *                          greater than {@value #MAX_RATIO}.
+     */
+    int getWordSizeForRatio(float ratio) {
+      ratio = standardizeCompressionRatio(ratio);
+      if (ratio >= MIN_RATIO && ratio <= MAX_RATIO) {
+        return map.get(ratio);
+      } else {
+        throw new RuntimeException("Compression ratio should be in the range [" 
+          + MIN_RATIO + "," + MAX_RATIO + "]. Configured compression ratio is " 
+          + ratio + ".");
+      }
+    }
+  }
+  
+  /**
+   * Setup the data generator's configuration to generate compressible random 
+   * text data with the desired compression ratio.
+   * Note that the compression ratio, if configured, will set the 
+   * {@link RandomTextDataGenerator}'s list-size and word-size based on 
+   * empirical values using the compression ratio set in the configuration. 
+   * 
+   * Hence to achieve the desired compression ratio, 
+   * {@link RandomTextDataGenerator}'s list-size will be set to the default 
+   * value i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
+   */
+  static void setupDataGeneratorConfig(Configuration conf) {
+    boolean compress = isCompressionEmulationEnabled(conf);
+    if (compress) {
+      float ratio = getMapInputCompressionEmulationRatio(conf);
+      LOG.info("GridMix is configured to generate compressed input data with "
+               + " a compression ratio of " + ratio);
+      int wordSize = COMPRESSION_LOOKUP_TABLE.getWordSizeForRatio(ratio);
+      RandomTextDataGenerator.setRandomTextDataGeneratorWordSize(conf, 
+                                                                 wordSize);
+
+      // since the compression ratios are computed using the default value of 
+      // list size
+      RandomTextDataGenerator.setRandomTextDataGeneratorListSize(conf, 
+          RandomTextDataGenerator.DEFAULT_LIST_SIZE);
+    }
+  }
+  
+  /**
+   * Returns a {@link RandomTextDataGenerator} that generates random 
+   * compressible text with the desired compression ratio.
+   */
+  static RandomTextDataGenerator getRandomTextDataGenerator(float ratio, 
+                                                            long seed) {
+    int wordSize = COMPRESSION_LOOKUP_TABLE.getWordSizeForRatio(ratio);
+    RandomTextDataGenerator rtg = 
+      new RandomTextDataGenerator(RandomTextDataGenerator.DEFAULT_LIST_SIZE, 
+            seed, wordSize);
+    return rtg;
+  }
+  
+  /** Publishes compression related data statistics. Following statistics are
+   * published
+   * <ul>
+   *   <li>Total compressed input data size</li>
+   *   <li>Number of compressed input data files</li>
+   *   <li>Compression Ratio</li>
+   *   <li>Text data dictionary size</li>
+   *   <li>Random text word size</li>
+   * </ul>
+   */
+  static DataStatistics publishCompressedDataStatistics(Path inputDir, 
+                          Configuration conf, long uncompressedDataSize) 
+  throws IOException {
+    FileSystem fs = inputDir.getFileSystem(conf);
+    CompressionCodecFactory compressionCodecs = 
+      new CompressionCodecFactory(conf);
+
+    // iterate over compressed files and sum up the compressed file sizes
+    long compressedDataSize = 0;
+    int numCompressedFiles = 0;
+    // obtain input data file statuses
+    FileStatus[] outFileStatuses = 
+      fs.listStatus(inputDir, new Utils.OutputFileUtils.OutputFilesFilter());
+    for (FileStatus status : outFileStatuses) {
+      // check if the input file is compressed
+      if (compressionCodecs != null) {
+        CompressionCodec codec = compressionCodecs.getCodec(status.getPath());
+        if (codec != null) {
+          ++numCompressedFiles;
+          compressedDataSize += status.getLen();
+        }
+      }
+    }
+
+    LOG.info("Gridmix is configured to use compressed input data.");
+    // publish the input data size
+    LOG.info("Total size of compressed input data : " 
+             + StringUtils.humanReadableInt(compressedDataSize));
+    LOG.info("Total number of compressed input data files : " 
+             + numCompressedFiles);
+
+    if (numCompressedFiles == 0) {
+      throw new RuntimeException("No compressed file found in the input" 
+          + " directory : " + inputDir.toString() + ". To enable compression"
+          + " emulation, run Gridmix either with "
+          + " an input directory containing compressed input file(s) or" 
+          + " use the -generate option to (re)generate it. If compression"
+          + " emulation is not desired, disable it by setting '" 
+          + COMPRESSION_EMULATION_ENABLE + "' to 'false'.");
+    }
+    
+    // publish compression ratio only if its generated in this gridmix run
+    if (uncompressedDataSize > 0) {
+      // compute the compression ratio
+      double ratio = ((double)compressedDataSize) / uncompressedDataSize;
+
+      // publish the compression ratio
+      LOG.info("Input Data Compression Ratio : " + ratio);
+    }
+    
+    return new DataStatistics(compressedDataSize, numCompressedFiles, true);
+  }
+  
+  /**
+   * Enables/Disables compression emulation.
+   * @param conf Target configuration where the parameter 
+   * {@value #COMPRESSION_EMULATION_ENABLE} will be set. 
+   * @param val The value to be set.
+   */
+  static void setCompressionEmulationEnabled(Configuration conf, boolean val) {
+    conf.setBoolean(COMPRESSION_EMULATION_ENABLE, val);
+  }
+  
+  /**
+   * Checks if compression emulation is enabled or not. Default is {@code true}.
+   */
+  static boolean isCompressionEmulationEnabled(Configuration conf) {
+    return conf.getBoolean(COMPRESSION_EMULATION_ENABLE, true);
+  }
+  
+  /**
+   * Enables/Disables input decompression emulation.
+   * @param conf Target configuration where the parameter 
+   * {@value #INPUT_DECOMPRESSION_EMULATION_ENABLE} will be set. 
+   * @param val The value to be set.
+   */
+  static void setInputCompressionEmulationEnabled(Configuration conf, 
+                                                  boolean val) {
+    conf.setBoolean(INPUT_DECOMPRESSION_EMULATION_ENABLE, val);
+  }
+  
+  /**
+   * Check if input decompression emulation is enabled or not. 
+   * Default is {@code false}.
+   */
+  static boolean isInputCompressionEmulationEnabled(Configuration conf) {
+    return conf.getBoolean(INPUT_DECOMPRESSION_EMULATION_ENABLE, false);
+  }
+  
+  /**
+   * Set the map input data compression ratio in the given conf.
+   */
+  static void setMapInputCompressionEmulationRatio(Configuration conf, 
+                                                   float ratio) {
+    conf.setFloat(GRIDMIX_MAP_INPUT_COMPRESSION_RATIO, ratio);
+  }
+  
+  /**
+   * Get the map input data compression ratio using the given configuration.
+   * If the compression ratio is not set in the configuration then use the 
+   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
+   */
+  static float getMapInputCompressionEmulationRatio(Configuration conf) {
+    return conf.getFloat(GRIDMIX_MAP_INPUT_COMPRESSION_RATIO, 
+                         DEFAULT_COMPRESSION_RATIO);
+  }
+  
+  /**
+   * Set the map output data compression ratio in the given configuration.
+   */
+  static void setMapOutputCompressionEmulationRatio(Configuration conf, 
+                                                    float ratio) {
+    conf.setFloat(GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO, ratio);
+  }
+  
+  /**
+   * Get the map output data compression ratio using the given configuration.
+   * If the compression ratio is not set in the configuration then use the 
+   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
+   */
+  static float getMapOutputCompressionEmulationRatio(Configuration conf) {
+    return conf.getFloat(GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO, 
+                         DEFAULT_COMPRESSION_RATIO);
+  }
+  
+  /**
+   * Set the reduce output data compression ratio in the given configuration.
+   */
+  static void setReduceOutputCompressionEmulationRatio(Configuration conf, 
+                                                       float ratio) {
+    conf.setFloat(GRIDMIX_REDUCE_OUTPUT_COMPRESSION_RATIO, ratio);
+  }
+  
+  /**
+   * Get the reduce output data compression ratio using the given configuration.
+   * If the compression ratio is not set in the configuration then use the 
+   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
+   */
+  static float getReduceOutputCompressionEmulationRatio(Configuration conf) {
+    return conf.getFloat(GRIDMIX_REDUCE_OUTPUT_COMPRESSION_RATIO, 
+                         DEFAULT_COMPRESSION_RATIO);
+  }
+  
+  /**
+   * Standardize the compression ratio i.e round off the compression ratio to
+   * only 2 significant digits.
+   */
+  static float standardizeCompressionRatio(float ratio) {
+    // round off to 2 significant digits
+    int significant = (int)Math.round(ratio * 100);
+    return ((float)significant)/100;
+  }
+  
+  /**
+   * Returns a {@link InputStream} for a file that might be compressed.
+   */
+  static InputStream getPossiblyDecompressedInputStream(Path file, 
+                                                        Configuration conf,
+                                                        long offset)
+  throws IOException {
+    FileSystem fs = file.getFileSystem(conf);
+    if (isCompressionEmulationEnabled(conf)
+        && isInputCompressionEmulationEnabled(conf)) {
+      CompressionCodecFactory compressionCodecs = 
+        new CompressionCodecFactory(conf);
+      CompressionCodec codec = compressionCodecs.getCodec(file);
+      if (codec != null) {
+        Decompressor decompressor = CodecPool.getDecompressor(codec);
+        if (decompressor != null) {
+          CompressionInputStream in = 
+            codec.createInputStream(fs.open(file), decompressor);
+          //TODO Seek doesnt work with compressed input stream. 
+          //     Use SplittableCompressionCodec?
+          return (InputStream)in;
+        }
+      }
+    }
+    FSDataInputStream in = fs.open(file);
+    in.seek(offset);
+    return (InputStream)in;
+  }
+  
+  /**
+   * Returns a {@link OutputStream} for a file that might need 
+   * compression.
+   */
+  static OutputStream getPossiblyCompressedOutputStream(Path file, 
+                                                        Configuration conf)
+  throws IOException {
+    FileSystem fs = file.getFileSystem(conf);
+    JobConf jConf = new JobConf(conf);
+    if (org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput(jConf)) {
+      // get the codec class
+      Class<? extends CompressionCodec> codecClass =
+        org.apache.hadoop.mapred.FileOutputFormat
+                                .getOutputCompressorClass(jConf, 
+                                                          GzipCodec.class);
+      // get the codec implementation
+      CompressionCodec codec = ReflectionUtils.newInstance(codecClass, conf);
+
+      // add the appropriate extension
+      file = file.suffix(codec.getDefaultExtension());
+
+      if (isCompressionEmulationEnabled(conf)) {
+        FSDataOutputStream fileOut = fs.create(file, false);
+        return new DataOutputStream(codec.createOutputStream(fileOut));
+      }
+    }
+    return fs.create(file, false);
+  }
+  
+  /**
+   * Extracts compression/decompression related configuration parameters from 
+   * the source configuration to the target configuration.
+   */
+  static void configureCompressionEmulation(Configuration source, 
+                                            Configuration target) {
+    // enable output compression
+    target.setBoolean(FileOutputFormat.COMPRESS, 
+        source.getBoolean(FileOutputFormat.COMPRESS, false));
+
+    // set the job output compression codec
+    String jobOutputCompressionCodec = 
+      source.get(FileOutputFormat.COMPRESS_CODEC);
+    if (jobOutputCompressionCodec != null) {
+      target.set(FileOutputFormat.COMPRESS_CODEC, jobOutputCompressionCodec);
+    }
+
+    // set the job output compression type
+    String jobOutputCompressionType = 
+      source.get(FileOutputFormat.COMPRESS_TYPE);
+    if (jobOutputCompressionType != null) {
+      target.set(FileOutputFormat.COMPRESS_TYPE, jobOutputCompressionType);
+    }
+
+    // enable map output compression
+    target.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,
+        source.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));
+
+    // set the map output compression codecs
+    String mapOutputCompressionCodec = 
+      source.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC);
+    if (mapOutputCompressionCodec != null) {
+      target.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, 
+                 mapOutputCompressionCodec);
+    }
+
+    // enable input decompression
+    //TODO replace with mapInputBytes and hdfsBytesRead
+    Path[] inputs = 
+      org.apache.hadoop.mapred.FileInputFormat
+         .getInputPaths(new JobConf(source));
+    boolean needsCompressedInput = false;
+    CompressionCodecFactory compressionCodecs = 
+      new CompressionCodecFactory(source);
+    for (Path input : inputs) {
+      CompressionCodec codec = compressionCodecs.getCodec(input);
+      if (codec != null) {
+        needsCompressedInput = true;
+      }
+    }
+    setInputCompressionEmulationEnabled(target, needsCompressedInput);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
new file mode 100644
index 0000000..8b80d42
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
@@ -0,0 +1,548 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.MD5Hash;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants;
+
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Emulation of Distributed Cache Usage in gridmix.
+ * <br> Emulation of Distributed Cache Load in gridmix will put load on
+ * TaskTrackers and affects execution time of tasks because of localization of
+ * distributed cache files by TaskTrackers.
+ * <br> Gridmix creates distributed cache files for simulated jobs by launching
+ * a MapReduce job {@link GenerateDistCacheData} in advance i.e. before
+ * launching simulated jobs.
+ * <br> The distributed cache file paths used in the original cluster are mapped
+ * to unique file names in the simulated cluster.
+ * <br> All HDFS-based distributed cache files generated by gridmix are
+ * public distributed cache files. But Gridmix makes sure that load incurred due
+ * to localization of private distributed cache files on the original cluster
+ * is also faithfully simulated. Gridmix emulates the load due to private
+ * distributed cache files by mapping private distributed cache files of
+ * different users in the original cluster to different public distributed cache
+ * files in the simulated cluster.
+ *
+ * <br> The configuration properties like
+ * {@link MRJobConfig#CACHE_FILES}, {@link MRJobConfig#CACHE_FILE_VISIBILITIES},
+ * {@link MRJobConfig#CACHE_FILES_SIZES} and
+ * {@link MRJobConfig#CACHE_FILE_TIMESTAMPS} obtained from trace are used to
+ *  decide
+ * <li> file size of each distributed cache file to be generated
+ * <li> whether a distributed cache file is already seen in this trace file
+ * <li> whether a distributed cache file was considered public or private.
+ * <br>
+ * <br> Gridmix configures these generated files as distributed cache files for
+ * the simulated jobs.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+class DistributedCacheEmulator {
+  private static final Log LOG =
+      LogFactory.getLog(DistributedCacheEmulator.class);
+
+  static final long AVG_BYTES_PER_MAP = 128 * 1024 * 1024L;// 128MB
+
+  // If at least 1 distributed cache file is missing in the expected
+  // distributed cache dir, Gridmix cannot proceed with emulation of
+  // distributed cache load.
+  int MISSING_DIST_CACHE_FILES_ERROR = 1;
+
+  private Path distCachePath;
+
+  /**
+   * Map between simulated cluster's distributed cache file paths and their
+   * file sizes. Unique distributed cache files are entered into this map.
+   * 2 distributed cache files are considered same if and only if their
+   * file paths, visibilities and timestamps are same.
+   */
+  private Map<String, Long> distCacheFiles = new HashMap<String, Long>();
+
+  /**
+   * Configuration property for whether gridmix should emulate
+   * distributed cache usage or not. Default value is true.
+   */
+  static final String GRIDMIX_EMULATE_DISTRIBUTEDCACHE =
+      "gridmix.distributed-cache-emulation.enable";
+
+  // Whether to emulate distributed cache usage or not
+  boolean emulateDistributedCache = true;
+
+  // Whether to generate distributed cache data or not
+  boolean generateDistCacheData = false;
+
+  Configuration conf; // gridmix configuration
+
+  // Pseudo local file system where local FS based distributed cache files are
+  // created by gridmix.
+  FileSystem pseudoLocalFs = null;
+
+  {
+    // Need to handle deprecation of these MapReduce-internal configuration
+    // properties as MapReduce doesn't handle their deprecation.
+    Configuration.addDeprecation("mapred.cache.files.filesizes",
+        new String[] {MRJobConfig.CACHE_FILES_SIZES});
+    Configuration.addDeprecation("mapred.cache.files.visibilities",
+        new String[] {MRJobConfig.CACHE_FILE_VISIBILITIES});
+  }
+
+  /**
+   * @param conf gridmix configuration
+   * @param ioPath &lt;ioPath&gt;/distributedCache/ is the gridmix Distributed
+   *               Cache directory
+   */
+  public DistributedCacheEmulator(Configuration conf, Path ioPath) {
+    this.conf = conf;
+    distCachePath = new Path(ioPath, "distributedCache");
+    this.conf.setClass("fs.pseudo.impl", PseudoLocalFs.class, FileSystem.class);
+  }
+
+  /**
+   * This is to be called before any other method of DistributedCacheEmulator.
+   * <br> Checks if emulation of distributed cache load is needed and is feasible.
+   *  Sets the flags generateDistCacheData and emulateDistributedCache to the
+   *  appropriate values.
+   * <br> Gridmix does not emulate distributed cache load if
+   * <ol><li> the specific gridmix job type doesn't need emulation of
+   * distributed cache load OR
+   * <li> the trace is coming from a stream instead of file OR
+   * <li> the distributed cache dir where distributed cache data is to be
+   * generated by gridmix is on local file system OR
+   * <li> execute permission is not there for any of the ascendant directories
+   * of &lt;ioPath&gt; till root. This is because for emulation of distributed
+   * cache load, distributed cache files created under
+   * &lt;ioPath/distributedCache/public/&gt; should be considered by hadoop
+   * as public distributed cache files.
+   * <li> creation of pseudo local file system fails.</ol>
+   * <br> For (2), (3), (4) and (5), generation of distributed cache data
+   * is also disabled.
+   * 
+   * @param traceIn trace file path. If this is '-', then trace comes from the
+   *                stream stdin.
+   * @param jobCreator job creator of gridmix jobs of a specific type
+   * @param generate  true if -generate option was specified
+   * @throws IOException
+   */
+  void init(String traceIn, JobCreator jobCreator, boolean generate)
+      throws IOException {
+    emulateDistributedCache = jobCreator.canEmulateDistCacheLoad()
+        && conf.getBoolean(GRIDMIX_EMULATE_DISTRIBUTEDCACHE, true);
+    generateDistCacheData = generate;
+
+    if (generateDistCacheData || emulateDistributedCache) {
+      if ("-".equals(traceIn)) {// trace is from stdin
+        LOG.warn("Gridmix will not emulate Distributed Cache load because "
+            + "the input trace source is a stream instead of file.");
+        emulateDistributedCache = generateDistCacheData = false;
+      } else if (FileSystem.getLocal(conf).getUri().getScheme().equals(
+          distCachePath.toUri().getScheme())) {// local FS
+        LOG.warn("Gridmix will not emulate Distributed Cache load because "
+            + "<iopath> provided is on local file system.");
+        emulateDistributedCache = generateDistCacheData = false;
+      } else {
+        // Check if execute permission is there for all the ascendant
+        // directories of distCachePath till root.
+        FileSystem fs = FileSystem.get(conf);
+        Path cur = distCachePath.getParent();
+        while (cur != null) {
+          if (cur.toString().length() > 0) {
+            FsPermission perm = fs.getFileStatus(cur).getPermission();
+            if (!perm.getOtherAction().and(FsAction.EXECUTE).equals(
+                FsAction.EXECUTE)) {
+              LOG.warn("Gridmix will not emulate Distributed Cache load "
+                  + "because the ascendant directory (of distributed cache "
+                  + "directory) " + cur + " doesn't have execute permission "
+                  + "for others.");
+              emulateDistributedCache = generateDistCacheData = false;
+              break;
+            }
+          }
+          cur = cur.getParent();
+        }
+      }
+    }
+
+    // Check if pseudo local file system can be created
+    try {
+      pseudoLocalFs = FileSystem.get(new URI("pseudo:///"), conf);
+    } catch (URISyntaxException e) {
+      LOG.warn("Gridmix will not emulate Distributed Cache load because "
+          + "creation of pseudo local file system failed.");
+      e.printStackTrace();
+      emulateDistributedCache = generateDistCacheData = false;
+      return;
+    }
+  }
+
+  /**
+   * @return true if gridmix should emulate distributed cache load
+   */
+  boolean shouldEmulateDistCacheLoad() {
+    return emulateDistributedCache;
+  }
+
+  /**
+   * @return true if gridmix should generate distributed cache data
+   */
+  boolean shouldGenerateDistCacheData() {
+    return generateDistCacheData;
+  }
+
+  /**
+   * @return the distributed cache directory path
+   */
+  Path getDistributedCacheDir() {
+    return distCachePath;
+  }
+
+  /**
+   * Create distributed cache directories.
+   * Also create a file that contains the list of distributed cache files
+   * that will be used as distributed cache files for all the simulated jobs.
+   * @param jsp job story producer for the trace
+   * @return exit code
+   * @throws IOException
+   */
+  int setupGenerateDistCacheData(JobStoryProducer jsp)
+      throws IOException {
+
+    createDistCacheDirectory();
+    return buildDistCacheFilesList(jsp);
+  }
+
+  /**
+   * Create distributed cache directory where distributed cache files will be
+   * created by the MapReduce job {@link GenerateDistCacheData#JOB_NAME}.
+   * @throws IOException
+   */
+  private void createDistCacheDirectory() throws IOException {
+    FileSystem fs = FileSystem.get(conf);
+    FileSystem.mkdirs(fs, distCachePath, new FsPermission((short) 0777));
+  }
+
+  /**
+   * Create the list of unique distributed cache files needed for all the
+   * simulated jobs and write the list to a special file.
+   * @param jsp job story producer for the trace
+   * @return exit code
+   * @throws IOException
+   */
+  private int buildDistCacheFilesList(JobStoryProducer jsp) throws IOException {
+    // Read all the jobs from the trace file and build the list of unique
+    // distributed cache files.
+    JobStory jobStory;
+    while ((jobStory = jsp.getNextJob()) != null) {
+      if (jobStory.getOutcome() == Pre21JobHistoryConstants.Values.SUCCESS && 
+         jobStory.getSubmissionTime() >= 0) {
+        updateHDFSDistCacheFilesList(jobStory);
+      }
+    }
+    jsp.close();
+
+    return writeDistCacheFilesList();
+  }
+
+  /**
+   * For the job to be simulated, identify the needed distributed cache files by
+   * mapping original cluster's distributed cache file paths to the simulated cluster's
+   * paths and add these paths in the map {@code distCacheFiles}.
+   *<br>
+   * JobStory should contain distributed cache related properties like
+   * <li> {@link MRJobConfig#CACHE_FILES}
+   * <li> {@link MRJobConfig#CACHE_FILE_VISIBILITIES}
+   * <li> {@link MRJobConfig#CACHE_FILES_SIZES}
+   * <li> {@link MRJobConfig#CACHE_FILE_TIMESTAMPS}
+   * <li> {@link MRJobConfig#CLASSPATH_FILES}
+   *
+   * <li> {@link MRJobConfig#CACHE_ARCHIVES}
+   * <li> {@link MRJobConfig#CACHE_ARCHIVES_VISIBILITIES}
+   * <li> {@link MRJobConfig#CACHE_ARCHIVES_SIZES}
+   * <li> {@link MRJobConfig#CACHE_ARCHIVES_TIMESTAMPS}
+   * <li> {@link MRJobConfig#CLASSPATH_ARCHIVES}
+   *
+   * <li> {@link MRJobConfig#CACHE_SYMLINK}
+   *
+   * @param jobdesc JobStory of original job obtained from trace
+   * @throws IOException
+   */
+  void updateHDFSDistCacheFilesList(JobStory jobdesc) throws IOException {
+
+    // Map original job's distributed cache file paths to simulated cluster's
+    // paths, to be used by this simulated job.
+    JobConf jobConf = jobdesc.getJobConf();
+
+    String[] files = jobConf.getStrings(MRJobConfig.CACHE_FILES);
+    if (files != null) {
+
+      String[] fileSizes = jobConf.getStrings(MRJobConfig.CACHE_FILES_SIZES);
+      String[] visibilities =
+        jobConf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES);
+      String[] timeStamps =
+        jobConf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS);
+
+      FileSystem fs = FileSystem.get(conf);
+      String user = jobConf.getUser();
+      for (int i = 0; i < files.length; i++) {
+        // Check if visibilities are available because older hadoop versions
+        // didn't have public, private Distributed Caches separately.
+        boolean visibility =
+            (visibilities == null) ? true : Boolean.valueOf(visibilities[i]);
+        if (isLocalDistCacheFile(files[i], user, visibility)) {
+          // local FS based distributed cache file.
+          // Create this file on the pseudo local FS on the fly (i.e. when the
+          // simulated job is submitted).
+          continue;
+        }
+        // distributed cache file on hdfs
+        String mappedPath = mapDistCacheFilePath(files[i], timeStamps[i],
+                                                 visibility, user);
+
+        // No need to add a distributed cache file path to the list if
+        // (1) the mapped path is already there in the list OR
+        // (2) the file with the mapped path already exists.
+        // In any of the above 2 cases, file paths, timestamps, file sizes and
+        // visibilities match. File sizes should match if file paths and
+        // timestamps match because single file path with single timestamp
+        // should correspond to a single file size.
+        if (distCacheFiles.containsKey(mappedPath) ||
+            fs.exists(new Path(mappedPath))) {
+          continue;
+        }
+        distCacheFiles.put(mappedPath, Long.valueOf(fileSizes[i]));
+      }
+    }
+  }
+
+  /**
+   * Check if the file path provided was constructed by MapReduce for a
+   * distributed cache file on local file system.
+   * @param filePath path of the distributed cache file
+   * @param user job submitter of the job for which &lt;filePath&gt; is a
+   *             distributed cache file
+   * @param visibility <code>true</code> for public distributed cache file
+   * @return true if the path provided is of a local file system based
+   *              distributed cache file
+   */
+  static boolean isLocalDistCacheFile(String filePath, String user,
+                                       boolean visibility) {
+    return (!visibility && filePath.contains(user + "/.staging"));
+  }
+
+  /**
+   * Map the HDFS based distributed cache file path from original cluster to
+   * a unique file name on the simulated cluster.
+   * <br> Unique  distributed file names on simulated cluster are generated
+   * using original cluster's <li>file path, <li>timestamp and <li> the
+   * job-submitter for private distributed cache file.
+   * <br> This implies that if on original cluster, a single HDFS file
+   * considered as two private distributed cache files for two jobs of
+   * different users, then the corresponding simulated jobs will have two
+   * different files of the same size in public distributed cache, one for each
+   * user. Both these simulated jobs will not share these distributed cache
+   * files, thus leading to the same load as seen in the original cluster.
+   * @param file distributed cache file path
+   * @param timeStamp time stamp of dist cachce file
+   * @param isPublic true if this distributed cache file is a public
+   *                 distributed cache file
+   * @param user job submitter on original cluster
+   * @return the mapped path on simulated cluster
+   */
+  private String mapDistCacheFilePath(String file, String timeStamp,
+      boolean isPublic, String user) {
+    String id = file + timeStamp;
+    if (!isPublic) {
+      // consider job-submitter for private distributed cache file
+      id = id.concat(user);
+    }
+    return new Path(distCachePath, MD5Hash.digest(id).toString()).toUri()
+               .getPath();
+  }
+
+  /**
+   * Write the list of distributed cache files in the decreasing order of
+   * file sizes into the sequence file. This file will be input to the job
+   * {@link GenerateDistCacheData}.
+   * Also validates if -generate option is missing and distributed cache files
+   * are missing.
+   * @return exit code
+   * @throws IOException
+   */
+  private int writeDistCacheFilesList()
+      throws IOException {
+    // Sort the distributed cache files in the decreasing order of file sizes.
+    List dcFiles = new ArrayList(distCacheFiles.entrySet());
+    Collections.sort(dcFiles, new Comparator() {
+      public int compare(Object dc1, Object dc2) {
+        return ((Comparable) ((Map.Entry) (dc2)).getValue())
+            .compareTo(((Map.Entry) (dc1)).getValue());
+      }
+    });
+
+    // write the sorted distributed cache files to the sequence file
+    FileSystem fs = FileSystem.get(conf);
+    Path distCacheFilesList = new Path(distCachePath, "_distCacheFiles.txt");
+    conf.set(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST,
+        distCacheFilesList.toString());
+    SequenceFile.Writer src_writer = SequenceFile.createWriter(fs, conf,
+        distCacheFilesList, LongWritable.class, BytesWritable.class,
+        SequenceFile.CompressionType.NONE);
+
+    // Total number of unique distributed cache files
+    int fileCount = dcFiles.size();
+    long byteCount = 0;// Total size of all distributed cache files
+    long bytesSync = 0;// Bytes after previous sync;used to add sync marker
+
+    for (Iterator it = dcFiles.iterator(); it.hasNext();) {
+      Map.Entry entry = (Map.Entry)it.next();
+      LongWritable fileSize =
+          new LongWritable(Long.valueOf(entry.getValue().toString()));
+      BytesWritable filePath =
+          new BytesWritable(entry.getKey().toString().getBytes());
+
+      byteCount += fileSize.get();
+      bytesSync += fileSize.get();
+      if (bytesSync > AVG_BYTES_PER_MAP) {
+        src_writer.sync();
+        bytesSync = fileSize.get();
+      }
+      src_writer.append(fileSize, filePath);
+    }
+    if (src_writer != null) {
+      src_writer.close();
+    }
+    // Set delete on exit for 'dist cache files list' as it is not needed later.
+    fs.deleteOnExit(distCacheFilesList);
+
+    conf.setInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, fileCount);
+    conf.setLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, byteCount);
+    LOG.info("Number of HDFS based distributed cache files to be generated is "
+        + fileCount + ". Total size of HDFS based distributed cache files "
+        + "to be generated is " + byteCount);
+
+    if (!shouldGenerateDistCacheData() && fileCount > 0) {
+      LOG.error("Missing " + fileCount + " distributed cache files under the "
+          + " directory\n" + distCachePath + "\nthat are needed for gridmix"
+          + " to emulate distributed cache load. Either use -generate\noption"
+          + " to generate distributed cache data along with input data OR "
+          + "disable\ndistributed cache emulation by configuring '"
+          + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE
+          + "' to false.");
+      return MISSING_DIST_CACHE_FILES_ERROR;
+    }
+    return 0;
+  }
+
+  /**
+   * If gridmix needs to emulate distributed cache load, then configure
+   * distributed cache files of a simulated job by mapping the original
+   * cluster's distributed cache file paths to the simulated cluster's paths and
+   * setting these mapped paths in the job configuration of the simulated job.
+   * <br>
+   * Configure local FS based distributed cache files through the property
+   * "tmpfiles" and hdfs based distributed cache files through the property
+   * {@link MRJobConfig#CACHE_FILES}.
+   * @param conf configuration for the simulated job to be run
+   * @param jobConf job configuration of original cluster's job, obtained from
+   *                trace
+   * @throws IOException
+   */
+  void configureDistCacheFiles(Configuration conf, JobConf jobConf)
+      throws IOException {
+    if (shouldEmulateDistCacheLoad()) {
+
+      String[] files = jobConf.getStrings(MRJobConfig.CACHE_FILES);
+      if (files != null) {
+        // hdfs based distributed cache files to be configured for simulated job
+        List<String> cacheFiles = new ArrayList<String>();
+        // local FS based distributed cache files to be configured for
+        // simulated job
+        List<String> localCacheFiles = new ArrayList<String>();
+
+        String[] visibilities =
+          jobConf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES);
+        String[] timeStamps =
+          jobConf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS);
+        String[] fileSizes = jobConf.getStrings(MRJobConfig.CACHE_FILES_SIZES);
+
+        String user = jobConf.getUser();
+        for (int i = 0; i < files.length; i++) {
+          // Check if visibilities are available because older hadoop versions
+          // didn't have public, private Distributed Caches separately.
+          boolean visibility =
+            (visibilities == null) ? true : Boolean.valueOf(visibilities[i]);
+          if (isLocalDistCacheFile(files[i], user, visibility)) {
+            // local FS based distributed cache file.
+            // Create this file on the pseudo local FS.
+            String fileId = MD5Hash.digest(files[i] + timeStamps[i]).toString();
+            long fileSize = Long.valueOf(fileSizes[i]);
+            Path mappedLocalFilePath =
+                PseudoLocalFs.generateFilePath(fileId, fileSize)
+                    .makeQualified(pseudoLocalFs.getUri(),
+                                   pseudoLocalFs.getWorkingDirectory());
+            pseudoLocalFs.create(mappedLocalFilePath);
+            localCacheFiles.add(mappedLocalFilePath.toUri().toString());
+          } else {
+            // hdfs based distributed cache file.
+            // Get the mapped HDFS path on simulated cluster
+            String mappedPath = mapDistCacheFilePath(files[i], timeStamps[i],
+                                                     visibility, user);
+            cacheFiles.add(mappedPath);
+          }
+        }
+        if (cacheFiles.size() > 0) {
+          // configure hdfs based distributed cache files for simulated job
+          conf.setStrings(MRJobConfig.CACHE_FILES,
+                          cacheFiles.toArray(new String[cacheFiles.size()]));
+        }
+        if (localCacheFiles.size() > 0) {
+          // configure local FS based distributed cache files for simulated job
+          conf.setStrings("tmpfiles", localCacheFiles.toArray(
+                                        new String[localCacheFiles.size()]));
+        }
+      }
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
new file mode 100644
index 0000000..2fcb39d
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Echos the UGI offered.
+ */
+public class EchoUserResolver implements UserResolver {
+  public static final Log LOG = LogFactory.getLog(Gridmix.class);
+
+  public EchoUserResolver() {
+    LOG.info(" Current user resolver is EchoUserResolver ");
+  }
+
+  public synchronized boolean setTargetUsers(URI userdesc, Configuration conf)
+  throws IOException {
+    return false;
+  }
+
+  public synchronized UserGroupInformation getTargetUgi(
+    UserGroupInformation ugi) {
+    return ugi;
+  }
+
+  /**
+   * {@inheritDoc}
+   * <br><br>
+   * Since {@link EchoUserResolver} simply returns the user's name passed as
+   * the argument, it doesn't need a target list of users.
+   */
+  public boolean needsTargetUsersList() {
+    return false;
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
new file mode 100644
index 0000000..fc362c5
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
@@ -0,0 +1,312 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+
+import org.apache.commons.lang.time.FastDateFormat;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.MD5Hash;
+import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
+import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ * Summarizes a {@link Gridmix} run. Statistics that are reported are
+ * <ul>
+ *   <li>Total number of jobs in the input trace</li>
+ *   <li>Trace signature</li>
+ *   <li>Total number of jobs processed from the input trace</li>
+ *   <li>Total number of jobs submitted</li>
+ *   <li>Total number of successful and failed jobs</li>
+ *   <li>Total number of map/reduce tasks launched</li>
+ *   <li>Gridmix start & end time</li>
+ *   <li>Total time for the Gridmix run (data-generation and simulation)</li>
+ *   <li>Gridmix Configuration (i.e job-type, submission-type, resolver)</li>
+ * </ul>
+ */
+class ExecutionSummarizer implements StatListener<JobStats> {
+  static final Log LOG = LogFactory.getLog(ExecutionSummarizer.class);
+  private static final FastDateFormat UTIL = FastDateFormat.getInstance();
+  
+  private int numJobsInInputTrace;
+  private int totalSuccessfulJobs;
+  private int totalFailedJobs;
+  private int totalMapTasksLaunched;
+  private int totalReduceTasksLaunched;
+  private long totalSimulationTime;
+  private long totalRuntime;
+  private final String commandLineArgs;
+  private long startTime;
+  private long endTime;
+  private long simulationStartTime;
+  private String inputTraceLocation;
+  private String inputTraceSignature;
+  private String jobSubmissionPolicy;
+  private String resolver;
+  private DataStatistics dataStats;
+  private String expectedDataSize;
+  
+  /**
+   * Basic constructor initialized with the runtime arguments. 
+   */
+  ExecutionSummarizer(String[] args) {
+    startTime = System.currentTimeMillis();
+    // flatten the args string and store it
+    commandLineArgs = 
+      org.apache.commons.lang.StringUtils.join(args, ' '); 
+  }
+  
+  /**
+   * Default constructor. 
+   */
+  ExecutionSummarizer() {
+    startTime = System.currentTimeMillis();
+    commandLineArgs = Summarizer.NA; 
+  }
+  
+  void start(Configuration conf) {
+    simulationStartTime = System.currentTimeMillis();
+  }
+  
+  private void processJobState(JobStats stats) throws Exception {
+    Job job = stats.getJob();
+    if (job.isSuccessful()) {
+      ++totalSuccessfulJobs;
+    } else {
+      ++totalFailedJobs;
+    }
+  }
+  
+  private void processJobTasks(JobStats stats) throws Exception {
+    totalMapTasksLaunched += stats.getNoOfMaps();
+    Job job = stats.getJob();
+    totalReduceTasksLaunched += job.getNumReduceTasks();
+  }
+  
+  private void process(JobStats stats) {
+    try {
+      // process the job run state
+      processJobState(stats);
+      
+      // process the tasks information
+      processJobTasks(stats);
+    } catch (Exception e) {
+      LOG.info("Error in processing job " + stats.getJob().getJobID() + ".");
+    }
+  }
+  
+  @Override
+  public void update(JobStats item) {
+    // process only if the simulation has started
+    if (simulationStartTime > 0) {
+      process(item);
+      totalSimulationTime = 
+        System.currentTimeMillis() - getSimulationStartTime();
+    }
+  }
+  
+  // Generates a signature for the trace file based on
+  //   - filename
+  //   - modification time
+  //   - file length
+  //   - owner
+  protected static String getTraceSignature(String input) throws IOException {
+    Path inputPath = new Path(input);
+    FileSystem fs = inputPath.getFileSystem(new Configuration());
+    FileStatus status = fs.getFileStatus(inputPath);
+    Path qPath = fs.makeQualified(status.getPath());
+    String traceID = status.getModificationTime() + qPath.toString()
+                     + status.getOwner() + status.getLen();
+    return MD5Hash.digest(traceID).toString();
+  }
+  
+  @SuppressWarnings("unchecked")
+  void finalize(JobFactory factory, String inputPath, long dataSize, 
+                UserResolver userResolver, DataStatistics stats,
+                Configuration conf) 
+  throws IOException {
+    numJobsInInputTrace = factory.numJobsInTrace;
+    endTime = System.currentTimeMillis();
+     if ("-".equals(inputPath)) {
+      inputTraceLocation = Summarizer.NA;
+      inputTraceSignature = Summarizer.NA;
+    } else {
+      Path inputTracePath = new Path(inputPath);
+      FileSystem fs = inputTracePath.getFileSystem(conf);
+      inputTraceLocation = fs.makeQualified(inputTracePath).toString();
+      inputTraceSignature = getTraceSignature(inputPath);
+    }
+    jobSubmissionPolicy = Gridmix.getJobSubmissionPolicy(conf).name();
+    resolver = userResolver.getClass().getName();
+    if (dataSize > 0) {
+      expectedDataSize = StringUtils.humanReadableInt(dataSize);
+    } else {
+      expectedDataSize = Summarizer.NA;
+    }
+    dataStats = stats;
+    totalRuntime = System.currentTimeMillis() - getStartTime();
+  }
+  
+  /**
+   * Summarizes the current {@link Gridmix} run.
+   */
+  @Override
+  public String toString() {
+    StringBuilder builder = new StringBuilder();
+    builder.append("Execution Summary:-");
+    builder.append("\nInput trace: ").append(getInputTraceLocation());
+    builder.append("\nInput trace signature: ")
+           .append(getInputTraceSignature());
+    builder.append("\nTotal number of jobs in trace: ")
+           .append(getNumJobsInTrace());
+    builder.append("\nExpected input data size: ")
+           .append(getExpectedDataSize());
+    builder.append("\nInput data statistics: ")
+           .append(getInputDataStatistics());
+    builder.append("\nTotal number of jobs processed: ")
+           .append(getNumSubmittedJobs());
+    builder.append("\nTotal number of successful jobs: ")
+           .append(getNumSuccessfulJobs());
+    builder.append("\nTotal number of failed jobs: ")
+           .append(getNumFailedJobs());
+    builder.append("\nTotal number of map tasks launched: ")
+           .append(getNumMapTasksLaunched());
+    builder.append("\nTotal number of reduce task launched: ")
+           .append(getNumReduceTasksLaunched());
+    builder.append("\nGridmix start time: ")
+           .append(UTIL.format(getStartTime()));
+    builder.append("\nGridmix end time: ").append(UTIL.format(getEndTime()));
+    builder.append("\nGridmix simulation start time: ")
+           .append(UTIL.format(getStartTime()));
+    builder.append("\nGridmix runtime: ")
+           .append(StringUtils.formatTime(getRuntime()));
+    builder.append("\nTime spent in initialization (data-gen etc): ")
+           .append(StringUtils.formatTime(getInitTime()));
+    builder.append("\nTime spent in simulation: ")
+           .append(StringUtils.formatTime(getSimulationTime()));
+    builder.append("\nGridmix configuration parameters: ")
+           .append(getCommandLineArgsString());
+    builder.append("\nGridmix job submission policy: ")
+           .append(getJobSubmissionPolicy());
+    builder.append("\nGridmix resolver: ").append(getUserResolver());
+    builder.append("\n\n");
+    return builder.toString();
+  }
+  
+  // Gets the stringified version of DataStatistics
+  static String stringifyDataStatistics(DataStatistics stats) {
+    if (stats != null) {
+      StringBuffer buffer = new StringBuffer();
+      String compressionStatus = stats.isDataCompressed() 
+                                 ? "Compressed" 
+                                 : "Uncompressed";
+      buffer.append(compressionStatus).append(" input data size: ");
+      buffer.append(StringUtils.humanReadableInt(stats.getDataSize()));
+      buffer.append(", ");
+      buffer.append("Number of files: ").append(stats.getNumFiles());
+
+      return buffer.toString();
+    } else {
+      return Summarizer.NA;
+    }
+  }
+  
+  // Getters
+  protected String getExpectedDataSize() {
+    return expectedDataSize;
+  }
+  
+  protected String getUserResolver() {
+    return resolver;
+  }
+  
+  protected String getInputDataStatistics() {
+    return stringifyDataStatistics(dataStats);
+  }
+  
+  protected String getInputTraceSignature() {
+    return inputTraceSignature;
+  }
+  
+  protected String getInputTraceLocation() {
+    return inputTraceLocation;
+  }
+  
+  protected int getNumJobsInTrace() {
+    return numJobsInInputTrace;
+  }
+  
+  protected int getNumSuccessfulJobs() {
+    return totalSuccessfulJobs;
+  }
+  
+  protected int getNumFailedJobs() {
+    return totalFailedJobs;
+  }
+  
+  protected int getNumSubmittedJobs() {
+    return totalSuccessfulJobs + totalFailedJobs;
+  }
+  
+  protected int getNumMapTasksLaunched() {
+    return totalMapTasksLaunched;
+  }
+  
+  protected int getNumReduceTasksLaunched() {
+    return totalReduceTasksLaunched;
+  }
+  
+  protected long getStartTime() {
+    return startTime;
+  }
+  
+  protected long getEndTime() {
+    return endTime;
+  }
+  
+  protected long getInitTime() {
+    return simulationStartTime - startTime;
+  }
+  
+  protected long getSimulationStartTime() {
+    return simulationStartTime;
+  }
+  
+  protected long getSimulationTime() {
+    return totalSimulationTime;
+  }
+  
+  protected long getRuntime() {
+    return totalRuntime;
+  }
+  
+  protected String getCommandLineArgsString() {
+    return commandLineArgs;
+  }
+  
+  protected String getJobSubmissionPolicy() {
+    return jobSubmissionPolicy;
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java
new file mode 100644
index 0000000..ba83bd9
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java
@@ -0,0 +1,301 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+
+import java.util.Arrays;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Random;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapred.gridmix.RandomAlgorithms.Selector;
+
+/**
+ * Class for caching a pool of input data to be used by synthetic jobs for
+ * simulating read traffic.
+ */
+class FilePool {
+
+  public static final Log LOG = LogFactory.getLog(FilePool.class);
+
+  /**
+   * The minimum file size added to the pool. Default 128MiB.
+   */
+  public static final String GRIDMIX_MIN_FILE = "gridmix.min.file.size";
+
+  /**
+   * The maximum size for files added to the pool. Defualts to 100TiB.
+   */
+  public static final String GRIDMIX_MAX_TOTAL = "gridmix.max.total.scan";
+
+  private Node root;
+  private final Path path;
+  private final FileSystem fs;
+  private final Configuration conf;
+  private final ReadWriteLock updateLock;
+
+  /**
+   * Initialize a filepool under the path provided, but do not populate the
+   * cache.
+   */
+  public FilePool(Configuration conf, Path input) throws IOException {
+    root = null;
+    this.conf = conf;
+    this.path = input;
+    this.fs = path.getFileSystem(conf);
+    updateLock = new ReentrantReadWriteLock();
+  }
+
+  /**
+   * Gather a collection of files at least as large as minSize.
+   * @return The total size of files returned.
+   */
+  public long getInputFiles(long minSize, Collection<FileStatus> files)
+      throws IOException {
+    updateLock.readLock().lock();
+    try {
+      return root.selectFiles(minSize, files);
+    } finally {
+      updateLock.readLock().unlock();
+    }
+  }
+
+  /**
+   * (Re)generate cache of input FileStatus objects.
+   */
+  public void refresh() throws IOException {
+    updateLock.writeLock().lock();
+    try {
+      root = new InnerDesc(fs, fs.getFileStatus(path),
+        new MinFileFilter(conf.getLong(GRIDMIX_MIN_FILE, 128 * 1024 * 1024),
+                          conf.getLong(GRIDMIX_MAX_TOTAL, 100L * (1L << 40))));
+      if (0 == root.getSize()) {
+        throw new IOException("Found no satisfactory file in " + path);
+      }
+    } finally {
+      updateLock.writeLock().unlock();
+    }
+  }
+
+  /**
+   * Get a set of locations for the given file.
+   */
+  public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
+      throws IOException {
+    // TODO cache
+    return fs.getFileBlockLocations(stat, start, len);
+  }
+
+  static abstract class Node {
+
+    protected final static Random rand = new Random();
+
+    /**
+     * Total size of files and directories under the current node.
+     */
+    abstract long getSize();
+
+    /**
+     * Return a set of files whose cumulative size is at least
+     * <tt>targetSize</tt>.
+     * TODO Clearly size is not the only criterion, e.g. refresh from
+     * generated data without including running task output, tolerance
+     * for permission issues, etc.
+     */
+    abstract long selectFiles(long targetSize, Collection<FileStatus> files)
+        throws IOException;
+  }
+
+  /**
+   * Files in current directory of this Node.
+   */
+  static class LeafDesc extends Node {
+    final long size;
+    final ArrayList<FileStatus> curdir;
+
+    LeafDesc(ArrayList<FileStatus> curdir, long size) {
+      this.size = size;
+      this.curdir = curdir;
+    }
+
+    @Override
+    public long getSize() {
+      return size;
+    }
+
+    @Override
+    public long selectFiles(long targetSize, Collection<FileStatus> files)
+        throws IOException {
+      if (targetSize >= getSize()) {
+        files.addAll(curdir);
+        return getSize();
+      }
+
+      Selector selector = new Selector(curdir.size(), (double) targetSize
+          / getSize(), rand);
+      
+      ArrayList<Integer> selected = new ArrayList<Integer>();
+      long ret = 0L;
+      do {
+        int index = selector.next();
+        selected.add(index);
+        ret += curdir.get(index).getLen();
+      } while (ret < targetSize);
+
+      for (Integer i : selected) {
+        files.add(curdir.get(i));
+      }
+
+      return ret;
+    }
+  }
+
+  /**
+   * A subdirectory of the current Node.
+   */
+  static class InnerDesc extends Node {
+    final long size;
+    final double[] dist;
+    final Node[] subdir;
+
+    private static final Comparator<Node> nodeComparator =
+      new Comparator<Node>() {
+          public int compare(Node n1, Node n2) {
+            return n1.getSize() < n2.getSize() ? -1
+                 : n1.getSize() > n2.getSize() ? 1 : 0;
+          }
+    };
+
+    InnerDesc(final FileSystem fs, FileStatus thisDir, MinFileFilter filter)
+        throws IOException {
+      long fileSum = 0L;
+      final ArrayList<FileStatus> curFiles = new ArrayList<FileStatus>();
+      final ArrayList<FileStatus> curDirs = new ArrayList<FileStatus>();
+      for (FileStatus stat : fs.listStatus(thisDir.getPath())) {
+        if (stat.isDirectory()) {
+          curDirs.add(stat);
+        } else if (filter.accept(stat)) {
+          curFiles.add(stat);
+          fileSum += stat.getLen();
+        }
+      }
+      ArrayList<Node> subdirList = new ArrayList<Node>();
+      if (!curFiles.isEmpty()) {
+        subdirList.add(new LeafDesc(curFiles, fileSum));
+      }
+      for (Iterator<FileStatus> i = curDirs.iterator();
+          !filter.done() && i.hasNext();) {
+        // add subdirectories
+        final Node d = new InnerDesc(fs, i.next(), filter);
+        final long dSize = d.getSize();
+        if (dSize > 0) {
+          fileSum += dSize;
+          subdirList.add(d);
+        }
+      }
+      size = fileSum;
+      LOG.debug(size + " bytes in " + thisDir.getPath());
+      subdir = subdirList.toArray(new Node[subdirList.size()]);
+      Arrays.sort(subdir, nodeComparator);
+      dist = new double[subdir.length];
+      for (int i = dist.length - 1; i > 0; --i) {
+        fileSum -= subdir[i].getSize();
+        dist[i] = fileSum / (1.0 * size);
+      }
+    }
+
+    @Override
+    public long getSize() {
+      return size;
+    }
+
+    @Override
+    public long selectFiles(long targetSize, Collection<FileStatus> files)
+        throws IOException {
+      long ret = 0L;
+      if (targetSize >= getSize()) {
+        // request larger than all subdirs; add everything
+        for (Node n : subdir) {
+          long added = n.selectFiles(targetSize, files);
+          ret += added;
+          targetSize -= added;
+        }
+        return ret;
+      }
+
+      // can satisfy request in proper subset of contents
+      // select random set, weighted by size
+      final HashSet<Node> sub = new HashSet<Node>();
+      do {
+        assert sub.size() < subdir.length;
+        final double r = rand.nextDouble();
+        int pos = Math.abs(Arrays.binarySearch(dist, r) + 1) - 1;
+        while (sub.contains(subdir[pos])) {
+          pos = (pos + 1) % subdir.length;
+        }
+        long added = subdir[pos].selectFiles(targetSize, files);
+        ret += added;
+        targetSize -= added;
+        sub.add(subdir[pos]);
+      } while (targetSize > 0);
+      return ret;
+    }
+  }
+
+  /**
+   * Filter enforcing the minFile/maxTotal parameters of the scan.
+   */
+  private static class MinFileFilter {
+
+    private long totalScan;
+    private final long minFileSize;
+
+    public MinFileFilter(long minFileSize, long totalScan) {
+      this.minFileSize = minFileSize;
+      this.totalScan = totalScan;
+    }
+    public boolean done() {
+      return totalScan <= 0;
+    }
+    public boolean accept(FileStatus stat) {
+      final boolean done = done();
+      if (!done && stat.getLen() >= minFileSize) {
+        totalScan -= stat.getLen();
+        return true;
+      }
+      return false;
+    }
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
new file mode 100644
index 0000000..2e4222c
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+
+/**
+ * Given a {@link org.apache.hadoop.mapreduce.lib.input.CombineFileSplit},
+ * circularly read through each input source.
+ */
+class FileQueue extends InputStream {
+
+  private int idx = -1;
+  private long curlen = -1L;
+  private InputStream input;
+  private final byte[] z = new byte[1];
+  private final Path[] paths;
+  private final long[] lengths;
+  private final long[] startoffset;
+  private final Configuration conf;
+
+  /**
+   * @param split Description of input sources.
+   * @param conf Used to resolve FileSystem instances.
+   */
+  public FileQueue(CombineFileSplit split, Configuration conf)
+      throws IOException {
+    this.conf = conf;
+    paths = split.getPaths();
+    startoffset = split.getStartOffsets();
+    lengths = split.getLengths();
+    nextSource();
+  }
+
+  protected void nextSource() throws IOException {
+    if (0 == paths.length) {
+      return;
+    }
+    if (input != null) {
+      input.close();
+    }
+    idx = (idx + 1) % paths.length;
+    curlen = lengths[idx];
+    final Path file = paths[idx];
+    input = 
+      CompressionEmulationUtil.getPossiblyDecompressedInputStream(file, 
+                                 conf, startoffset[idx]);
+  }
+
+  @Override
+  public int read() throws IOException {
+    final int tmp = read(z);
+    return tmp == -1 ? -1 : (0xFF & z[0]);
+  }
+
+  @Override
+  public int read(byte[] b) throws IOException {
+    return read(b, 0, b.length);
+  }
+
+  @Override
+  public int read(byte[] b, int off, int len) throws IOException {
+    int kvread = 0;
+    while (kvread < len) {
+      if (curlen <= 0) {
+        nextSource();
+        continue;
+      }
+      final int srcRead = (int) Math.min(len - kvread, curlen);
+      IOUtils.readFully(input, b, kvread, srcRead);
+      curlen -= srcRead;
+      kvread += srcRead;
+    }
+    return kvread;
+  }
+
+  @Override
+  public void close() throws IOException {
+    input.close();
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
new file mode 100644
index 0000000..41e937f
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
@@ -0,0 +1,412 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.OutputStream;
+import java.security.PrivilegedExceptionAction;
+import java.util.Arrays;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Random;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocatedFileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.fs.RemoteIterator;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Utils;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.StringUtils;
+
+// TODO can replace with form of GridmixJob
+class GenerateData extends GridmixJob {
+
+  /**
+   * Total bytes to write.
+   */
+  public static final String GRIDMIX_GEN_BYTES = "gridmix.gen.bytes";
+
+  /**
+   * Maximum size per file written.
+   */
+  public static final String GRIDMIX_GEN_CHUNK = "gridmix.gen.bytes.per.file";
+
+  /**
+   * Size of writes to output file.
+   */
+  public static final String GRIDMIX_VAL_BYTES = "gendata.val.bytes";
+
+  /**
+   * Status reporting interval, in megabytes.
+   */
+  public static final String GRIDMIX_GEN_INTERVAL = "gendata.interval.mb";
+
+  /**
+   * Blocksize of generated data.
+   */
+  public static final String GRIDMIX_GEN_BLOCKSIZE = "gridmix.gen.blocksize";
+
+  /**
+   * Replication of generated data.
+   */
+  public static final String GRIDMIX_GEN_REPLICATION = "gridmix.gen.replicas";
+  static final String JOB_NAME = "GRIDMIX_GENERATE_INPUT_DATA";
+
+  public GenerateData(Configuration conf, Path outdir, long genbytes)
+      throws IOException {
+    super(conf, 0L, JOB_NAME);
+    job.getConfiguration().setLong(GRIDMIX_GEN_BYTES, genbytes);
+    FileOutputFormat.setOutputPath(job, outdir);
+  }
+
+  /**
+   * Represents the input data characteristics.
+   */
+  static class DataStatistics {
+    private long dataSize;
+    private long numFiles;
+    private boolean isDataCompressed;
+    
+    DataStatistics(long dataSize, long numFiles, boolean isCompressed) {
+      this.dataSize = dataSize;
+      this.numFiles = numFiles;
+      this.isDataCompressed = isCompressed;
+    }
+    
+    long getDataSize() {
+      return dataSize;
+    }
+    
+    long getNumFiles() {
+      return numFiles;
+    }
+    
+    boolean isDataCompressed() {
+      return isDataCompressed;
+    }
+  }
+  
+  /**
+   * Publish the data statistics.
+   */
+  static DataStatistics publishDataStatistics(Path inputDir, long genBytes, 
+                                              Configuration conf) 
+  throws IOException {
+    if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)) {
+      return CompressionEmulationUtil.publishCompressedDataStatistics(inputDir, 
+                                        conf, genBytes);
+    } else {
+      return publishPlainDataStatistics(conf, inputDir);
+    }
+  }
+  
+  static DataStatistics publishPlainDataStatistics(Configuration conf, 
+                                                   Path inputDir) 
+  throws IOException {
+    FileSystem fs = inputDir.getFileSystem(conf);
+
+    // obtain input data file statuses
+    long dataSize = 0;
+    long fileCount = 0;
+    RemoteIterator<LocatedFileStatus> iter = fs.listFiles(inputDir, true);
+    PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();
+    while (iter.hasNext()) {
+      LocatedFileStatus lStatus = iter.next();
+      if (filter.accept(lStatus.getPath())) {
+        dataSize += lStatus.getLen();
+        ++fileCount;
+      }
+    }
+
+    // publish the plain data statistics
+    LOG.info("Total size of input data : " 
+             + StringUtils.humanReadableInt(dataSize));
+    LOG.info("Total number of input data files : " + fileCount);
+    
+    return new DataStatistics(dataSize, fileCount, false);
+  }
+  
+  @Override
+  public Job call() throws IOException, InterruptedException,
+                           ClassNotFoundException {
+    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+    ugi.doAs( new PrivilegedExceptionAction <Job>() {
+       public Job run() throws IOException, ClassNotFoundException,
+                               InterruptedException {
+         // check if compression emulation is enabled
+         if (CompressionEmulationUtil
+             .isCompressionEmulationEnabled(job.getConfiguration())) {
+           CompressionEmulationUtil.configure(job);
+         } else {
+           configureRandomBytesDataGenerator();
+         }
+         job.submit();
+         return job;
+       }
+       
+       private void configureRandomBytesDataGenerator() {
+        job.setMapperClass(GenDataMapper.class);
+        job.setNumReduceTasks(0);
+        job.setMapOutputKeyClass(NullWritable.class);
+        job.setMapOutputValueClass(BytesWritable.class);
+        job.setInputFormatClass(GenDataFormat.class);
+        job.setOutputFormatClass(RawBytesOutputFormat.class);
+        job.setJarByClass(GenerateData.class);
+        try {
+          FileInputFormat.addInputPath(job, new Path("ignored"));
+        } catch (IOException e) {
+          LOG.error("Error while adding input path ", e);
+        }
+      }
+    });
+    return job;
+  }
+  
+  @Override
+  protected boolean canEmulateCompression() {
+    return false;
+  }
+
+  public static class GenDataMapper
+      extends Mapper<NullWritable,LongWritable,NullWritable,BytesWritable> {
+
+    private BytesWritable val;
+    private final Random r = new Random();
+
+    @Override
+    protected void setup(Context context)
+        throws IOException, InterruptedException {
+      val = new BytesWritable(new byte[
+          context.getConfiguration().getInt(GRIDMIX_VAL_BYTES, 1024 * 1024)]);
+    }
+
+    @Override
+    public void map(NullWritable key, LongWritable value, Context context)
+        throws IOException, InterruptedException {
+      for (long bytes = value.get(); bytes > 0; bytes -= val.getLength()) {
+        r.nextBytes(val.getBytes());
+        val.setSize((int)Math.min(val.getLength(), bytes));
+        context.write(key, val);
+      }
+    }
+
+  }
+
+  static class GenDataFormat extends InputFormat<NullWritable,LongWritable> {
+
+    @Override
+    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
+      final JobClient client =
+        new JobClient(new JobConf(jobCtxt.getConfiguration()));
+      ClusterStatus stat = client.getClusterStatus(true);
+      final long toGen =
+        jobCtxt.getConfiguration().getLong(GRIDMIX_GEN_BYTES, -1);
+      if (toGen < 0) {
+        throw new IOException("Invalid/missing generation bytes: " + toGen);
+      }
+      final int nTrackers = stat.getTaskTrackers();
+      final long bytesPerTracker = toGen / nTrackers;
+      final ArrayList<InputSplit> splits = new ArrayList<InputSplit>(nTrackers);
+      final Pattern trackerPattern = Pattern.compile("tracker_([^:]*):.*");
+      final Matcher m = trackerPattern.matcher("");
+      for (String tracker : stat.getActiveTrackerNames()) {
+        m.reset(tracker);
+        if (!m.find()) {
+          System.err.println("Skipping node: " + tracker);
+          continue;
+        }
+        final String name = m.group(1);
+        splits.add(new GenSplit(bytesPerTracker, new String[] { name }));
+      }
+      return splits;
+    }
+
+    @Override
+    public RecordReader<NullWritable,LongWritable> createRecordReader(
+        InputSplit split, final TaskAttemptContext taskContext)
+        throws IOException {
+      return new RecordReader<NullWritable,LongWritable>() {
+        long written = 0L;
+        long write = 0L;
+        long RINTERVAL;
+        long toWrite;
+        final NullWritable key = NullWritable.get();
+        final LongWritable val = new LongWritable();
+
+        @Override
+        public void initialize(InputSplit split, TaskAttemptContext ctxt)
+            throws IOException, InterruptedException {
+          toWrite = split.getLength();
+          RINTERVAL = ctxt.getConfiguration().getInt(
+              GRIDMIX_GEN_INTERVAL, 10) << 20;
+        }
+        @Override
+        public boolean nextKeyValue() throws IOException {
+          written += write;
+          write = Math.min(toWrite - written, RINTERVAL);
+          val.set(write);
+          return written < toWrite;
+        }
+        @Override
+        public float getProgress() throws IOException {
+          return written / ((float)toWrite);
+        }
+        @Override
+        public NullWritable getCurrentKey() { return key; }
+        @Override
+        public LongWritable getCurrentValue() { return val; }
+        @Override
+        public void close() throws IOException {
+          taskContext.setStatus("Wrote " + toWrite);
+        }
+      };
+    }
+  }
+
+  static class GenSplit extends InputSplit implements Writable {
+    private long bytes;
+    private int nLoc;
+    private String[] locations;
+
+    public GenSplit() { }
+    public GenSplit(long bytes, String[] locations) {
+      this(bytes, locations.length, locations);
+    }
+    public GenSplit(long bytes, int nLoc, String[] locations) {
+      this.bytes = bytes;
+      this.nLoc = nLoc;
+      this.locations = Arrays.copyOf(locations, nLoc);
+    }
+    @Override
+    public long getLength() {
+      return bytes;
+    }
+    @Override
+    public String[] getLocations() {
+      return locations;
+    }
+    @Override
+    public void readFields(DataInput in) throws IOException {
+      bytes = in.readLong();
+      nLoc = in.readInt();
+      if (null == locations || locations.length < nLoc) {
+        locations = new String[nLoc];
+      }
+      for (int i = 0; i < nLoc; ++i) {
+        locations[i] = Text.readString(in);
+      }
+    }
+    @Override
+    public void write(DataOutput out) throws IOException {
+      out.writeLong(bytes);
+      out.writeInt(nLoc);
+      for (int i = 0; i < nLoc; ++i) {
+        Text.writeString(out, locations[i]);
+      }
+    }
+  }
+
+  static class RawBytesOutputFormat
+      extends FileOutputFormat<NullWritable,BytesWritable> {
+
+    @Override
+    public RecordWriter<NullWritable,BytesWritable> getRecordWriter(
+        TaskAttemptContext job) throws IOException {
+
+      return new ChunkWriter(getDefaultWorkFile(job, ""),
+          job.getConfiguration());
+    }
+
+    static class ChunkWriter extends RecordWriter<NullWritable,BytesWritable> {
+      private final Path outDir;
+      private final FileSystem fs;
+      private final int blocksize;
+      private final short replicas;
+      private final FsPermission genPerms = new FsPermission((short) 0777);
+      private final long maxFileBytes;
+
+      private long accFileBytes = 0L;
+      private long fileIdx = -1L;
+      private OutputStream fileOut = null;
+
+      public ChunkWriter(Path outDir, Configuration conf) throws IOException {
+        this.outDir = outDir;
+        fs = outDir.getFileSystem(conf);
+        blocksize = conf.getInt(GRIDMIX_GEN_BLOCKSIZE, 1 << 28);
+        replicas = (short) conf.getInt(GRIDMIX_GEN_REPLICATION, 3);
+        maxFileBytes = conf.getLong(GRIDMIX_GEN_CHUNK, 1L << 30);
+        nextDestination();
+      }
+      private void nextDestination() throws IOException {
+        if (fileOut != null) {
+          fileOut.close();
+        }
+        fileOut = fs.create(new Path(outDir, "segment-" + (++fileIdx)),
+                            genPerms, false, 64 * 1024, replicas, 
+                            blocksize, null);
+        accFileBytes = 0L;
+      }
+      @Override
+      public void write(NullWritable key, BytesWritable value)
+          throws IOException {
+        int written = 0;
+        final int total = value.getLength();
+        while (written < total) {
+          if (accFileBytes >= maxFileBytes) {
+            nextDestination();
+          }
+          final int write = (int)
+            Math.min(total - written, maxFileBytes - accFileBytes);
+          fileOut.write(value.getBytes(), written, write);
+          written += write;
+          accFileBytes += write;
+        }
+      }
+      @Override
+      public void close(TaskAttemptContext ctxt) throws IOException {
+        fileOut.close();
+      }
+    }
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
new file mode 100644
index 0000000..c90e17c
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
@@ -0,0 +1,259 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
+import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * GridmixJob that generates distributed cache files.
+ * {@link GenerateDistCacheData} expects a list of distributed cache files to be
+ * generated as input. This list is expected to be stored as a sequence file
+ * and the filename is expected to be configured using
+ * {@code gridmix.distcache.file.list}.
+ * This input file contains the list of distributed cache files and their sizes.
+ * For each record (i.e. file size and file path) in this input file,
+ * a file with the specific file size at the specific path is created.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+class GenerateDistCacheData extends GridmixJob {
+
+  /**
+   * Number of distributed cache files to be created by gridmix
+   */
+  static final String GRIDMIX_DISTCACHE_FILE_COUNT =
+      "gridmix.distcache.file.count";
+  /**
+   * Total number of bytes to be written to the distributed cache files by
+   * gridmix. i.e. Sum of sizes of all unique distributed cache files to be
+   * created by gridmix.
+   */
+  static final String GRIDMIX_DISTCACHE_BYTE_COUNT =
+      "gridmix.distcache.byte.count";
+  /**
+   * The special file created(and used) by gridmix, that contains the list of
+   * unique distributed cache files that are to be created and their sizes.
+   */
+  static final String GRIDMIX_DISTCACHE_FILE_LIST =
+      "gridmix.distcache.file.list";
+  static final String JOB_NAME = "GRIDMIX_GENERATE_DISTCACHE_DATA";
+
+  public GenerateDistCacheData(Configuration conf) throws IOException {
+    super(conf, 0L, JOB_NAME);
+  }
+
+  @Override
+  public Job call() throws IOException, InterruptedException,
+                           ClassNotFoundException {
+    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+    ugi.doAs( new PrivilegedExceptionAction <Job>() {
+       public Job run() throws IOException, ClassNotFoundException,
+                               InterruptedException {
+        job.setMapperClass(GenDCDataMapper.class);
+        job.setNumReduceTasks(0);
+        job.setMapOutputKeyClass(NullWritable.class);
+        job.setMapOutputValueClass(BytesWritable.class);
+        job.setInputFormatClass(GenDCDataFormat.class);
+        job.setOutputFormatClass(NullOutputFormat.class);
+        job.setJarByClass(GenerateDistCacheData.class);
+        try {
+          FileInputFormat.addInputPath(job, new Path("ignored"));
+        } catch (IOException e) {
+          LOG.error("Error while adding input path ", e);
+        }
+        job.submit();
+        return job;
+      }
+    });
+    return job;
+  }
+
+  @Override
+  protected boolean canEmulateCompression() {
+    return false;
+  }
+
+  public static class GenDCDataMapper
+      extends Mapper<LongWritable, BytesWritable, NullWritable, BytesWritable> {
+
+    private BytesWritable val;
+    private final Random r = new Random();
+    private FileSystem fs;
+
+    @Override
+    protected void setup(Context context)
+        throws IOException, InterruptedException {
+      val = new BytesWritable(new byte[context.getConfiguration().getInt(
+              GenerateData.GRIDMIX_VAL_BYTES, 1024 * 1024)]);
+      fs = FileSystem.get(context.getConfiguration());
+    }
+
+    // Create one distributed cache file with the needed file size.
+    // key is distributed cache file size and
+    // value is distributed cache file path.
+    @Override
+    public void map(LongWritable key, BytesWritable value, Context context)
+        throws IOException, InterruptedException {
+
+      String fileName = new String(value.getBytes(), 0, value.getLength());
+      Path path = new Path(fileName);
+
+      /**
+       * Create distributed cache file with the permissions 0755.
+       * Since the private distributed cache directory doesn't have execute
+       * permission for others, it is OK to set read permission for others for
+       * the files under that directory and still they will become 'private'
+       * distributed cache files on the simulated cluster.
+       */
+      FSDataOutputStream dos =
+          FileSystem.create(fs, path, new FsPermission((short)0755));
+
+      for (long bytes = key.get(); bytes > 0; bytes -= val.getLength()) {
+        r.nextBytes(val.getBytes());
+        val.setSize((int)Math.min(val.getLength(), bytes));
+        dos.write(val.getBytes(), 0, val.getLength());// Write to distCache file
+      }
+      dos.close();
+    }
+  }
+
+  /**
+   * InputFormat for GenerateDistCacheData.
+   * Input to GenerateDistCacheData is the special file(in SequenceFile format)
+   * that contains the list of distributed cache files to be generated along
+   * with their file sizes.
+   */
+  static class GenDCDataFormat
+      extends InputFormat<LongWritable, BytesWritable> {
+
+    // Split the special file that contains the list of distributed cache file
+    // paths and their file sizes such that each split corresponds to
+    // approximately same amount of distributed cache data to be generated.
+    // Consider numTaskTrackers * numMapSlotsPerTracker as the number of maps
+    // for this job, if there is lot of data to be generated.
+    @Override
+    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
+      final JobConf jobConf = new JobConf(jobCtxt.getConfiguration());
+      final JobClient client = new JobClient(jobConf);
+      ClusterStatus stat = client.getClusterStatus(true);
+      int numTrackers = stat.getTaskTrackers();
+      final int fileCount = jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);
+
+      // Total size of distributed cache files to be generated
+      final long totalSize = jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);
+      // Get the path of the special file
+      String distCacheFileList = jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);
+      if (fileCount < 0 || totalSize < 0 || distCacheFileList == null) {
+        throw new RuntimeException("Invalid metadata: #files (" + fileCount
+            + "), total_size (" + totalSize + "), filelisturi ("
+            + distCacheFileList + ")");
+      }
+
+      Path sequenceFile = new Path(distCacheFileList);
+      FileSystem fs = sequenceFile.getFileSystem(jobConf);
+      FileStatus srcst = fs.getFileStatus(sequenceFile);
+      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.
+      int numMapSlotsPerTracker = jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);
+      int numSplits = numTrackers * numMapSlotsPerTracker;
+
+      List<InputSplit> splits = new ArrayList<InputSplit>(numSplits);
+      LongWritable key = new LongWritable();
+      BytesWritable value = new BytesWritable();
+
+      // Average size of data to be generated by each map task
+      final long targetSize = Math.max(totalSize / numSplits,
+                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);
+      long splitStartPosition = 0L;
+      long splitEndPosition = 0L;
+      long acc = 0L;
+      long bytesRemaining = srcst.getLen();
+      SequenceFile.Reader reader = null;
+      try {
+        reader = new SequenceFile.Reader(fs, sequenceFile, jobConf);
+        while (reader.next(key, value)) {
+
+          // If adding this file would put this split past the target size,
+          // cut the last split and put this file in the next split.
+          if (acc + key.get() > targetSize && acc != 0) {
+            long splitSize = splitEndPosition - splitStartPosition;
+            splits.add(new FileSplit(
+                sequenceFile, splitStartPosition, splitSize, (String[])null));
+            bytesRemaining -= splitSize;
+            splitStartPosition = splitEndPosition;
+            acc = 0L;
+          }
+          acc += key.get();
+          splitEndPosition = reader.getPosition();
+        }
+      } finally {
+        if (reader != null) {
+          reader.close();
+        }
+      }
+      if (bytesRemaining != 0) {
+        splits.add(new FileSplit(
+            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));
+      }
+
+      return splits;
+    }
+
+    /**
+     * Returns a reader for this split of the distributed cache file list.
+     */
+    @Override
+    public RecordReader<LongWritable, BytesWritable> createRecordReader(
+        InputSplit split, final TaskAttemptContext taskContext)
+        throws IOException, InterruptedException {
+      return new SequenceFileRecordReader<LongWritable, BytesWritable>();
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
new file mode 100644
index 0000000..d8f3854
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
@@ -0,0 +1,717 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.PrintStream;
+import java.net.URI;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FsShell;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.tools.rumen.ZombieJobProducer;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Driver class for the Gridmix3 benchmark. Gridmix accepts a timestamped
+ * stream (trace) of job/task descriptions. For each job in the trace, the
+ * client will submit a corresponding, synthetic job to the target cluster at
+ * the rate in the original trace. The intent is to provide a benchmark that
+ * can be configured and extended to closely match the measured resource
+ * profile of actual, production loads.
+ */
+public class Gridmix extends Configured implements Tool {
+
+  public static final Log LOG = LogFactory.getLog(Gridmix.class);
+
+  /**
+   * Output (scratch) directory for submitted jobs. Relative paths are
+   * resolved against the path provided as input and absolute paths remain
+   * independent of it. The default is &quot;gridmix&quot;.
+   */
+  public static final String GRIDMIX_OUT_DIR = "gridmix.output.directory";
+
+  /**
+   * Number of submitting threads at the client and upper bound for
+   * in-memory split data. Submitting threads precompute InputSplits for
+   * submitted jobs. This limits the number of splits held in memory waiting
+   * for submission and also permits parallel computation of split data.
+   */
+  public static final String GRIDMIX_SUB_THR = "gridmix.client.submit.threads";
+
+  /**
+   * The depth of the queue of job descriptions. Before splits are computed,
+   * a queue of pending descriptions is stored in memoory. This parameter
+   * limits the depth of that queue.
+   */
+  public static final String GRIDMIX_QUE_DEP =
+    "gridmix.client.pending.queue.depth";
+
+  /**
+   * Multiplier to accelerate or decelerate job submission. As a crude means of
+   * sizing a job trace to a cluster, the time separating two jobs is
+   * multiplied by this factor.
+   */
+  public static final String GRIDMIX_SUB_MUL = "gridmix.submit.multiplier";
+
+  /**
+   * Class used to resolve users in the trace to the list of target users
+   * on the cluster.
+   */
+  public static final String GRIDMIX_USR_RSV = "gridmix.user.resolve.class";
+
+  /**
+   * Configuration property set in simulated job's configuration whose value is
+   * set to the corresponding original job's name. This is not configurable by
+   * gridmix user.
+   */
+  public static final String ORIGINAL_JOB_NAME =
+      "gridmix.job.original-job-name";
+  /**
+   * Configuration property set in simulated job's configuration whose value is
+   * set to the corresponding original job's id. This is not configurable by
+   * gridmix user.
+   */
+  public static final String ORIGINAL_JOB_ID = "gridmix.job.original-job-id";
+
+  private DistributedCacheEmulator distCacheEmulator;
+
+  // Submit data structures
+  private JobFactory factory;
+  private JobSubmitter submitter;
+  private JobMonitor monitor;
+  private Statistics statistics;
+  private Summarizer summarizer;
+
+  // Shutdown hook
+  private final Shutdown sdh = new Shutdown();
+
+  Gridmix(String[] args) {
+    summarizer = new Summarizer(args);
+  }
+  
+  public Gridmix() {
+    summarizer = new Summarizer();
+  }
+  
+  // Get the input data directory for Gridmix. Input directory is 
+  // <io-path>/input
+  static Path getGridmixInputDataPath(Path ioPath) {
+    return new Path(ioPath, "input");
+  }
+  
+  /**
+   * Write random bytes at the path &lt;inputDir&gt;.
+   * @see org.apache.hadoop.mapred.gridmix.GenerateData
+   */
+  protected void writeInputData(long genbytes, Path inputDir)
+      throws IOException, InterruptedException {
+    final Configuration conf = getConf();
+    
+    // configure the compression ratio if needed
+    CompressionEmulationUtil.setupDataGeneratorConfig(conf);
+    
+    final GenerateData genData = new GenerateData(conf, inputDir, genbytes);
+    LOG.info("Generating " + StringUtils.humanReadableInt(genbytes) +
+        " of test data...");
+    launchGridmixJob(genData);
+    
+    FsShell shell = new FsShell(conf);
+    try {
+      LOG.info("Changing the permissions for inputPath " + inputDir.toString());
+      shell.run(new String[] {"-chmod","-R","777", inputDir.toString()});
+    } catch (Exception e) {
+      LOG.error("Couldnt change the file permissions " , e);
+      throw new IOException(e);
+    }
+    
+    LOG.info("Input data generation successful.");
+  }
+
+  /**
+   * Write random bytes in the distributed cache files that will be used by all
+   * simulated jobs of current gridmix run, if files are to be generated.
+   * Do this as part of the MapReduce job {@link GenerateDistCacheData#JOB_NAME}
+   * @see org.apache.hadoop.mapred.gridmix.GenerateDistCacheData
+   */
+  protected void writeDistCacheData(Configuration conf)
+      throws IOException, InterruptedException {
+    int fileCount =
+        conf.getInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, -1);
+    if (fileCount > 0) {// generate distributed cache files
+      final GridmixJob genDistCacheData = new GenerateDistCacheData(conf);
+      LOG.info("Generating distributed cache data of size " + conf.getLong(
+          GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1));
+      launchGridmixJob(genDistCacheData);
+    }
+  }
+
+  // Launch Input/DistCache Data Generation job and wait for completion
+  void launchGridmixJob(GridmixJob job)
+      throws IOException, InterruptedException {
+    submitter.add(job);
+
+    // TODO add listeners, use for job dependencies
+    TimeUnit.SECONDS.sleep(10);
+    try {
+      job.getJob().waitForCompletion(false);
+    } catch (ClassNotFoundException e) {
+      throw new IOException("Internal error", e);
+    }
+    if (!job.getJob().isSuccessful()) {
+      throw new IOException(job.getJob().getJobName() + " job failed!");
+    }
+  }
+
+  /**
+   * Create an appropriate {@code JobStoryProducer} object for the
+   * given trace.
+   * 
+   * @param traceIn the path to the trace file. The special path
+   * &quot;-&quot; denotes the standard input stream.
+   *
+   * @param conf the configuration to be used.
+   *
+   * @throws IOException if there was an error.
+   */
+  protected JobStoryProducer createJobStoryProducer(String traceIn,
+      Configuration conf) throws IOException {
+    if ("-".equals(traceIn)) {
+      return new ZombieJobProducer(System.in, null);
+    }
+    return new ZombieJobProducer(new Path(traceIn), null, conf);
+  }
+
+  // get the gridmix job submission policy
+  protected static GridmixJobSubmissionPolicy getJobSubmissionPolicy(
+                                                Configuration conf) {
+    return GridmixJobSubmissionPolicy.getPolicy(conf, 
+                                        GridmixJobSubmissionPolicy.STRESS);
+  }
+  
+  /**
+   * Create each component in the pipeline and start it.
+   * @param conf Configuration data, no keys specific to this context
+   * @param traceIn Either a Path to the trace data or &quot;-&quot; for
+   *                stdin
+   * @param ioPath &lt;ioPath&gt;/input/ is the dir from which input data is
+   *               read and &lt;ioPath&gt;/distributedCache/ is the gridmix
+   *               distributed cache directory.
+   * @param scratchDir Path into which job output is written
+   * @param startFlag Semaphore for starting job trace pipeline
+   */
+  private void startThreads(Configuration conf, String traceIn, Path ioPath,
+      Path scratchDir, CountDownLatch startFlag, UserResolver userResolver)
+      throws IOException {
+    try {
+      Path inputDir = getGridmixInputDataPath(ioPath);
+      GridmixJobSubmissionPolicy policy = getJobSubmissionPolicy(conf);
+      LOG.info(" Submission policy is " + policy.name());
+      statistics = new Statistics(conf, policy.getPollingInterval(), startFlag);
+      monitor = createJobMonitor(statistics);
+      int noOfSubmitterThreads = 
+        (policy == GridmixJobSubmissionPolicy.SERIAL) 
+        ? 1
+        : Runtime.getRuntime().availableProcessors() + 1;
+
+      int numThreads = conf.getInt(GRIDMIX_SUB_THR, noOfSubmitterThreads);
+      int queueDep = conf.getInt(GRIDMIX_QUE_DEP, 5);
+      submitter = createJobSubmitter(monitor, numThreads, queueDep,
+                                     new FilePool(conf, inputDir), userResolver, 
+                                     statistics);
+      distCacheEmulator = new DistributedCacheEmulator(conf, ioPath);
+
+      factory = createJobFactory(submitter, traceIn, scratchDir, conf, 
+                                 startFlag, userResolver);
+      factory.jobCreator.setDistCacheEmulator(distCacheEmulator);
+
+      if (policy == GridmixJobSubmissionPolicy.SERIAL) {
+        statistics.addJobStatsListeners(factory);
+      } else {
+        statistics.addClusterStatsObservers(factory);
+      }
+
+      // add the gridmix run summarizer to the statistics
+      statistics.addJobStatsListeners(summarizer.getExecutionSummarizer());
+      statistics.addClusterStatsObservers(summarizer.getClusterSummarizer());
+      
+      monitor.start();
+      submitter.start();
+    }catch(Exception e) {
+      LOG.error(" Exception at start " ,e);
+      throw new IOException(e);
+    }
+   }
+
+  protected JobMonitor createJobMonitor(Statistics stats) throws IOException {
+    return new JobMonitor(stats);
+  }
+
+  protected JobSubmitter createJobSubmitter(JobMonitor monitor, int threads,
+      int queueDepth, FilePool pool, UserResolver resolver, 
+      Statistics statistics) throws IOException {
+    return new JobSubmitter(monitor, threads, queueDepth, pool, statistics);
+  }
+
+  protected JobFactory createJobFactory(JobSubmitter submitter, String traceIn,
+      Path scratchDir, Configuration conf, CountDownLatch startFlag, 
+      UserResolver resolver)
+      throws IOException {
+     return GridmixJobSubmissionPolicy.getPolicy(
+       conf, GridmixJobSubmissionPolicy.STRESS).createJobFactory(
+       submitter, createJobStoryProducer(traceIn, conf), scratchDir, conf,
+       startFlag, resolver);
+  }
+
+  private static UserResolver userResolver;
+
+  public UserResolver getCurrentUserResolver() {
+    return userResolver;
+  }
+  
+  public int run(final String[] argv) throws IOException, InterruptedException {
+    int val = -1;
+    final Configuration conf = getConf();
+    UserGroupInformation.setConfiguration(conf);
+    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+
+    val = ugi.doAs(new PrivilegedExceptionAction<Integer>() {
+      public Integer run() throws Exception {
+        return runJob(conf, argv);
+      }
+    });
+    
+    // print the gridmix summary if the run was successful
+    if (val == 0) {
+        // print the run summary
+        System.out.print("\n\n");
+        System.out.println(summarizer.toString());
+    }
+    
+    return val; 
+  }
+
+  private int runJob(Configuration conf, String[] argv)
+    throws IOException, InterruptedException {
+    if (argv.length < 2) {
+      printUsage(System.err);
+      return 1;
+    }
+    
+    // Should gridmix generate distributed cache data ?
+    boolean generate = false;
+    long genbytes = -1L;
+    String traceIn = null;
+    Path ioPath = null;
+    URI userRsrc = null;
+    userResolver = ReflectionUtils.newInstance(
+                     conf.getClass(GRIDMIX_USR_RSV, 
+                       SubmitterUserResolver.class,
+                       UserResolver.class), 
+                     conf);
+    try {
+      for (int i = 0; i < argv.length - 2; ++i) {
+        if ("-generate".equals(argv[i])) {
+          genbytes = StringUtils.TraditionalBinaryPrefix.string2long(argv[++i]);
+          generate = true;
+        } else if ("-users".equals(argv[i])) {
+          userRsrc = new URI(argv[++i]);
+        } else {
+          printUsage(System.err);
+          return 1;
+        }
+      }
+
+      if (userResolver.needsTargetUsersList()) {
+        if (userRsrc != null) {
+          if (!userResolver.setTargetUsers(userRsrc, conf)) {
+            LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
+          }
+        } else {
+          System.err.println("\n\n" + userResolver.getClass()
+              + " needs target user list. Use -users option." + "\n\n");
+          printUsage(System.err);
+          return 1;
+        }
+      } else if (userRsrc != null) {
+        LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
+      }
+
+      ioPath = new Path(argv[argv.length - 2]);
+      traceIn = argv[argv.length - 1];
+    } catch (Exception e) {
+      e.printStackTrace();
+      printUsage(System.err);
+      return 1;
+    }
+    return start(conf, traceIn, ioPath, genbytes, userResolver, generate);
+  }
+
+  /**
+   * 
+   * @param conf gridmix configuration
+   * @param traceIn trace file path(if it is '-', then trace comes from the
+   *                stream stdin)
+   * @param ioPath Working directory for gridmix. GenerateData job
+   *               will generate data in the directory &lt;ioPath&gt;/input/ and
+   *               distributed cache data is generated in the directory
+   *               &lt;ioPath&gt;/distributedCache/, if -generate option is
+   *               specified.
+   * @param genbytes size of input data to be generated under the directory
+   *                 &lt;ioPath&gt;/input/
+   * @param userResolver gridmix user resolver
+   * @param generate true if -generate option was specified
+   * @return exit code
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  int start(Configuration conf, String traceIn, Path ioPath, long genbytes,
+      UserResolver userResolver, boolean generate)
+      throws IOException, InterruptedException {
+    DataStatistics stats = null;
+    InputStream trace = null;
+    ioPath = ioPath.makeQualified(ioPath.getFileSystem(conf));
+
+    try {
+      Path scratchDir = new Path(ioPath, conf.get(GRIDMIX_OUT_DIR, "gridmix"));
+
+      // add shutdown hook for SIGINT, etc.
+      Runtime.getRuntime().addShutdownHook(sdh);
+      CountDownLatch startFlag = new CountDownLatch(1);
+      try {
+        // Create, start job submission threads
+        startThreads(conf, traceIn, ioPath, scratchDir, startFlag,
+                     userResolver);
+        
+        Path inputDir = getGridmixInputDataPath(ioPath);
+        
+        // Write input data if specified
+        if (genbytes > 0) {
+          writeInputData(genbytes, inputDir);
+        }
+        
+        // publish the data statistics
+        stats = GenerateData.publishDataStatistics(inputDir, genbytes, conf);
+        
+        // scan input dir contents
+        submitter.refreshFilePool();
+
+        // set up the needed things for emulation of various loads
+        int exitCode = setupEmulation(conf, traceIn, scratchDir, ioPath,
+                                      generate);
+        if (exitCode != 0) {
+          return exitCode;
+        }
+
+        // start the summarizer
+        summarizer.start(conf);
+        
+        factory.start();
+        statistics.start();
+      } catch (Throwable e) {
+        LOG.error("Startup failed", e);
+        if (factory != null) factory.abort(); // abort pipeline
+      } finally {
+        // signal for factory to start; sets start time
+        startFlag.countDown();
+      }
+      if (factory != null) {
+        // wait for input exhaustion
+        factory.join(Long.MAX_VALUE);
+        final Throwable badTraceException = factory.error();
+        if (null != badTraceException) {
+          LOG.error("Error in trace", badTraceException);
+          throw new IOException("Error in trace", badTraceException);
+        }
+        // wait for pending tasks to be submitted
+        submitter.shutdown();
+        submitter.join(Long.MAX_VALUE);
+        // wait for running tasks to complete
+        monitor.shutdown();
+        monitor.join(Long.MAX_VALUE);
+
+        statistics.shutdown();
+        statistics.join(Long.MAX_VALUE);
+
+      }
+    } finally {
+      if (factory != null) {
+        summarizer.finalize(factory, traceIn, genbytes, userResolver, stats, 
+                            conf);
+      }
+      IOUtils.cleanup(LOG, trace);
+    }
+    return 0;
+  }
+
+  /**
+   * Create gridmix output directory. Setup things for emulation of
+   * various loads, if needed.
+   * @param conf gridmix configuration
+   * @param traceIn trace file path(if it is '-', then trace comes from the
+   *                stream stdin)
+   * @param scratchDir gridmix output directory
+   * @param ioPath Working directory for gridmix.
+   * @param generate true if -generate option was specified
+   * @return exit code
+   * @throws IOException
+   * @throws InterruptedException 
+   */
+  private int setupEmulation(Configuration conf, String traceIn,
+      Path scratchDir, Path ioPath, boolean generate)
+      throws IOException, InterruptedException {
+    // create scratch directory(output directory of gridmix)
+    final FileSystem scratchFs = scratchDir.getFileSystem(conf);
+    FileSystem.mkdirs(scratchFs, scratchDir, new FsPermission((short) 0777));
+
+    // Setup things needed for emulation of distributed cache load
+    return setupDistCacheEmulation(conf, traceIn, ioPath, generate);
+    // Setup emulation of other loads like CPU load, Memory load
+  }
+
+  /**
+   * Setup gridmix for emulation of distributed cache load. This includes
+   * generation of distributed cache files, if needed.
+   * @param conf gridmix configuration
+   * @param traceIn trace file path(if it is '-', then trace comes from the
+   *                stream stdin)
+   * @param ioPath &lt;ioPath&gt;/input/ is the dir where input data (a) exists
+   *               or (b) is generated. &lt;ioPath&gt;/distributedCache/ is the
+   *               folder where distributed cache data (a) exists or (b) is to be
+   *               generated by gridmix.
+   * @param generate true if -generate option was specified
+   * @return exit code
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  private int setupDistCacheEmulation(Configuration conf, String traceIn,
+      Path ioPath, boolean generate) throws IOException, InterruptedException {
+    distCacheEmulator.init(traceIn, factory.jobCreator, generate);
+    int exitCode = 0;
+    if (distCacheEmulator.shouldGenerateDistCacheData() ||
+        distCacheEmulator.shouldEmulateDistCacheLoad()) {
+
+      JobStoryProducer jsp = createJobStoryProducer(traceIn, conf);
+      exitCode = distCacheEmulator.setupGenerateDistCacheData(jsp);
+      if (exitCode == 0) {
+        // If there are files to be generated, run a MapReduce job to generate
+        // these distributed cache files of all the simulated jobs of this trace.
+        writeDistCacheData(conf);
+      }
+    }
+    return exitCode;
+  }
+
+  /**
+   * Handles orderly shutdown by requesting that each component in the
+   * pipeline abort its progress, waiting for each to exit and killing
+   * any jobs still running on the cluster.
+   */
+  class Shutdown extends Thread {
+
+    static final long FAC_SLEEP = 1000;
+    static final long SUB_SLEEP = 4000;
+    static final long MON_SLEEP = 15000;
+
+    private void killComponent(Component<?> component, long maxwait) {
+      if (component == null) {
+        return;
+      }
+      component.abort();
+      try {
+        component.join(maxwait);
+      } catch (InterruptedException e) {
+        LOG.warn("Interrupted waiting for " + component);
+      }
+
+    }
+
+    @Override
+    public void run() {
+      LOG.info("Exiting...");
+      try {
+        killComponent(factory, FAC_SLEEP);   // read no more tasks
+        killComponent(submitter, SUB_SLEEP); // submit no more tasks
+        killComponent(monitor, MON_SLEEP);   // process remaining jobs here
+        killComponent(statistics,MON_SLEEP);
+      } finally {
+        if (monitor == null) {
+          return;
+        }
+        List<Job> remainingJobs = monitor.getRemainingJobs();
+        if (remainingJobs.isEmpty()) {
+          return;
+        }
+        LOG.info("Killing running jobs...");
+        for (Job job : remainingJobs) {
+          try {
+            if (!job.isComplete()) {
+              job.killJob();
+              LOG.info("Killed " + job.getJobName() + " (" + job.getJobID() + ")");
+            } else {
+              if (job.isSuccessful()) {
+                monitor.onSuccess(job);
+              } else {
+                monitor.onFailure(job);
+              }
+            }
+          } catch (IOException e) {
+            LOG.warn("Failure killing " + job.getJobName(), e);
+          } catch (Exception e) {
+            LOG.error("Unexcpected exception", e);
+          }
+        }
+        LOG.info("Done.");
+      }
+    }
+
+  }
+
+  public static void main(String[] argv) throws Exception {
+    int res = -1;
+    try {
+      res = ToolRunner.run(new Configuration(), new Gridmix(argv), argv);
+    } finally {
+      System.exit(res);
+    }
+  }
+
+  private String getEnumValues(Enum<?>[] e) {
+    StringBuilder sb = new StringBuilder();
+    String sep = "";
+    for (Enum<?> v : e) {
+      sb.append(sep);
+      sb.append(v.name());
+      sep = "|";
+    }
+    return sb.toString();
+  }
+  
+  private String getJobTypes() {
+    return getEnumValues(JobCreator.values());
+  }
+  
+  private String getSubmissionPolicies() {
+    return getEnumValues(GridmixJobSubmissionPolicy.values());
+  }
+  
+  protected void printUsage(PrintStream out) {
+    ToolRunner.printGenericCommandUsage(out);
+    out.println("Usage: gridmix [-generate <MiB>] [-users URI] [-Dname=value ...] <iopath> <trace>");
+    out.println("  e.g. gridmix -generate 100m foo -");
+    out.println("Options:");
+    out.println("   -generate <MiB> : Generate input data of size MiB under "
+        + "<iopath>/input/ and generate\n\t\t     distributed cache data under "
+        + "<iopath>/distributedCache/.");
+    out.println("   -users <usersResourceURI> : URI that contains the users list.");
+    out.println("Configuration parameters:");
+    out.println("   General parameters:");
+    out.printf("       %-48s : Output directory\n", GRIDMIX_OUT_DIR);
+    out.printf("       %-48s : Submitting threads\n", GRIDMIX_SUB_THR);
+    out.printf("       %-48s : Queued job desc\n", GRIDMIX_QUE_DEP);
+    out.printf("       %-48s : User resolution class\n", GRIDMIX_USR_RSV);
+    out.printf("       %-48s : Job types (%s)\n", JobCreator.GRIDMIX_JOB_TYPE, getJobTypes());
+    out.println("   Parameters related to job submission:");    
+    out.printf("       %-48s : Default queue\n",
+        GridmixJob.GRIDMIX_DEFAULT_QUEUE);
+    out.printf("       %-48s : Enable/disable using queues in trace\n",
+        GridmixJob.GRIDMIX_USE_QUEUE_IN_TRACE);
+    out.printf("       %-48s : Job submission policy (%s)\n",
+        GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, getSubmissionPolicies());
+    out.println("   Parameters specific for LOADJOB:");
+    out.printf("       %-48s : Key fraction of rec\n",
+        AvgRecordFactory.GRIDMIX_KEY_FRC);
+    out.println("   Parameters specific for SLEEPJOB:");
+    out.printf("       %-48s : Whether to ignore reduce tasks\n",
+        SleepJob.SLEEPJOB_MAPTASK_ONLY);
+    out.printf("       %-48s : Number of fake locations for map tasks\n",
+        JobCreator.SLEEPJOB_RANDOM_LOCATIONS);
+    out.printf("       %-48s : Maximum map task runtime in mili-sec\n",
+        SleepJob.GRIDMIX_SLEEP_MAX_MAP_TIME);
+    out.printf("       %-48s : Maximum reduce task runtime in mili-sec (merge+reduce)\n",
+        SleepJob.GRIDMIX_SLEEP_MAX_REDUCE_TIME);
+    out.println("   Parameters specific for STRESS submission throttling policy:");
+    out.printf("       %-48s : jobs vs task-tracker ratio\n",
+        StressJobFactory.CONF_MAX_JOB_TRACKER_RATIO);
+    out.printf("       %-48s : maps vs map-slot ratio\n",
+        StressJobFactory.CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO);
+    out.printf("       %-48s : reduces vs reduce-slot ratio\n",
+        StressJobFactory.CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO);
+    out.printf("       %-48s : map-slot share per job\n",
+        StressJobFactory.CONF_MAX_MAPSLOT_SHARE_PER_JOB);
+    out.printf("       %-48s : reduce-slot share per job\n",
+        StressJobFactory.CONF_MAX_REDUCESLOT_SHARE_PER_JOB);
+   }
+
+  /**
+   * Components in the pipeline must support the following operations for
+   * orderly startup and shutdown.
+   */
+  interface Component<T> {
+
+    /**
+     * Accept an item into this component from an upstream component. If
+     * shutdown or abort have been called, this may fail, depending on the
+     * semantics for the component.
+     */
+    void add(T item) throws InterruptedException;
+
+    /**
+     * Attempt to start the service.
+     */
+    void start();
+
+    /**
+     * Wait until the service completes. It is assumed that either a
+     * {@link #shutdown} or {@link #abort} has been requested.
+     */
+    void join(long millis) throws InterruptedException;
+
+    /**
+     * Shut down gracefully, finishing all pending work. Reject new requests.
+     */
+    void shutdown();
+
+    /**
+     * Shut down immediately, aborting any work in progress and discarding
+     * all pending work. It is legal to store pending work for another
+     * thread to process.
+     */
+    void abort();
+  }
+
+}
+
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
new file mode 100644
index 0000000..77ec697
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
@@ -0,0 +1,517 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Formatter;
+import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Delayed;
+import java.util.concurrent.TimeUnit;
+import java.security.PrivilegedExceptionAction;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.MRConfig;
+import org.apache.hadoop.mapreduce.Partitioner;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+import static org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser.extractMaxHeapOpts;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Synthetic job generated from a trace description.
+ */
+abstract class GridmixJob implements Callable<Job>, Delayed {
+
+  // Gridmix job name format is GRIDMIX<6 digit sequence number>
+  public static final String JOB_NAME_PREFIX = "GRIDMIX";
+  public static final Log LOG = LogFactory.getLog(GridmixJob.class);
+
+  private static final ThreadLocal<Formatter> nameFormat =
+    new ThreadLocal<Formatter>() {
+      @Override
+      protected Formatter initialValue() {
+        final StringBuilder sb =
+            new StringBuilder(JOB_NAME_PREFIX.length() + 6);
+        sb.append(JOB_NAME_PREFIX);
+        return new Formatter(sb);
+      }
+    };
+
+  protected final int seq;
+  protected final Path outdir;
+  protected final Job job;
+  protected final JobStory jobdesc;
+  protected final UserGroupInformation ugi;
+  protected final long submissionTimeNanos;
+  private static final ConcurrentHashMap<Integer,List<InputSplit>> descCache =
+     new ConcurrentHashMap<Integer,List<InputSplit>>();
+  protected static final String GRIDMIX_JOB_SEQ = "gridmix.job.seq";
+  protected static final String GRIDMIX_USE_QUEUE_IN_TRACE = 
+      "gridmix.job-submission.use-queue-in-trace";
+  protected static final String GRIDMIX_DEFAULT_QUEUE = 
+      "gridmix.job-submission.default-queue";
+  // configuration key to enable/disable High-Ram feature emulation
+  static final String GRIDMIX_HIGHRAM_EMULATION_ENABLE = 
+    "gridmix.highram-emulation.enable";
+  // configuration key to enable/disable task jvm options
+  static final String GRIDMIX_TASK_JVM_OPTIONS_ENABLE = 
+    "gridmix.task.jvm-options.enable";
+
+  private static void setJobQueue(Job job, String queue) {
+    if (queue != null) {
+      job.getConfiguration().set(MRJobConfig.QUEUE_NAME, queue);
+    }
+  }
+  
+  public GridmixJob(final Configuration conf, long submissionMillis,
+      final JobStory jobdesc, Path outRoot, UserGroupInformation ugi, 
+      final int seq) throws IOException {
+    this.ugi = ugi;
+    this.jobdesc = jobdesc;
+    this.seq = seq;
+
+    ((StringBuilder)nameFormat.get().out()).setLength(JOB_NAME_PREFIX.length());
+    try {
+      job = this.ugi.doAs(new PrivilegedExceptionAction<Job>() {
+        public Job run() throws IOException {
+
+          String jobId = null == jobdesc.getJobID() 
+                         ? "<unknown>" 
+                         : jobdesc.getJobID().toString();
+          Job ret = new Job(conf,
+                            nameFormat.get().format("%06d", seq).toString());
+          ret.getConfiguration().setInt(GRIDMIX_JOB_SEQ, seq);
+
+          ret.getConfiguration().set(Gridmix.ORIGINAL_JOB_ID, jobId);
+          ret.getConfiguration().set(Gridmix.ORIGINAL_JOB_NAME,
+                                     jobdesc.getName());
+          if (conf.getBoolean(GRIDMIX_USE_QUEUE_IN_TRACE, false)) {
+            setJobQueue(ret, jobdesc.getQueueName());
+          } else {
+            setJobQueue(ret, conf.get(GRIDMIX_DEFAULT_QUEUE));
+          }
+
+          // check if the job can emulate compression
+          if (canEmulateCompression()) {
+            // set the compression related configs if compression emulation is
+            // enabled
+            if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)) {
+              CompressionEmulationUtil.configureCompressionEmulation(
+                  jobdesc.getJobConf(), ret.getConfiguration());
+            }
+          }
+          
+          // configure high ram properties if enabled
+          if (conf.getBoolean(GRIDMIX_HIGHRAM_EMULATION_ENABLE, true)) {
+            configureHighRamProperties(jobdesc.getJobConf(), 
+                                       ret.getConfiguration());
+          }
+          
+          // configure task jvm options if enabled
+          // this knob can be turned off if there is a mismatch between the
+          // target (simulation) cluster and the original cluster. Such a 
+          // mismatch can result in job failures (due to memory issues) on the 
+          // target (simulated) cluster.
+          //
+          // TODO If configured, scale the original task's JVM (heap related)
+          //      options to suit the target (simulation) cluster
+          if (conf.getBoolean(GRIDMIX_TASK_JVM_OPTIONS_ENABLE, true)) {
+            configureTaskJVMOptions(jobdesc.getJobConf(), 
+                                    ret.getConfiguration());
+          }
+          
+          return ret;
+        }
+      });
+    } catch (InterruptedException e) {
+      throw new IOException(e);
+    }
+
+    submissionTimeNanos = TimeUnit.NANOSECONDS.convert(
+        submissionMillis, TimeUnit.MILLISECONDS);
+    outdir = new Path(outRoot, "" + seq);
+  }
+  
+  @SuppressWarnings("deprecation")
+  protected static void configureTaskJVMOptions(Configuration originalJobConf,
+                                                Configuration simulatedJobConf){
+    // Get the heap related java opts used for the original job and set the 
+    // same for the simulated job.
+    //    set task task heap options
+    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
+                                   JobConf.MAPRED_TASK_JAVA_OPTS);
+    //  set map task heap options
+    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
+                                   MRJobConfig.MAP_JAVA_OPTS);
+
+    //  set reduce task heap options
+    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
+                                   MRJobConfig.REDUCE_JAVA_OPTS);
+  }
+  
+  // Configures the task's max heap options using the specified key
+  private static void configureTaskJVMMaxHeapOptions(Configuration srcConf, 
+                                                     Configuration destConf,
+                                                     String key) {
+    String srcHeapOpts = srcConf.get(key);
+    if (srcHeapOpts != null) {
+      List<String> srcMaxOptsList = new ArrayList<String>();
+      // extract the max heap options and ignore the rest
+      extractMaxHeapOpts(srcHeapOpts, srcMaxOptsList, 
+                         new ArrayList<String>());
+      if (srcMaxOptsList.size() > 0) {
+        List<String> destOtherOptsList = new ArrayList<String>();
+        // extract the other heap options and ignore the max options in the 
+        // destination configuration
+        String destHeapOpts = destConf.get(key);
+        if (destHeapOpts != null) {
+          extractMaxHeapOpts(destHeapOpts, new ArrayList<String>(), 
+                             destOtherOptsList);
+        }
+        
+        // the source configuration might have some task level max heap opts set
+        // remove these opts from the destination configuration and replace
+        // with the options set in the original configuration
+        StringBuilder newHeapOpts = new StringBuilder();
+        
+        for (String otherOpt : destOtherOptsList) {
+          newHeapOpts.append(otherOpt).append(" ");
+        }
+        
+        for (String opts : srcMaxOptsList) {
+          newHeapOpts.append(opts).append(" ");
+        }
+        
+        // set the final heap opts 
+        destConf.set(key, newHeapOpts.toString().trim());
+      }
+    }
+  }
+
+  // Scales the desired job-level configuration parameter. This API makes sure 
+  // that the ratio of the job level configuration parameter to the cluster 
+  // level configuration parameter is maintained in the simulated run. Hence 
+  // the values are scaled from the original cluster's configuration to the 
+  // simulated cluster's configuration for higher emulation accuracy.
+  // This kind of scaling is useful for memory parameters.
+  private static void scaleConfigParameter(Configuration sourceConf, 
+                        Configuration destConf, String clusterValueKey, 
+                        String jobValueKey, long defaultValue) {
+    long simulatedClusterDefaultValue = 
+           destConf.getLong(clusterValueKey, defaultValue);
+    
+    long originalClusterDefaultValue = 
+           sourceConf.getLong(clusterValueKey, defaultValue);
+    
+    long originalJobValue = 
+           sourceConf.getLong(jobValueKey, defaultValue);
+    
+    double scaleFactor = (double)originalJobValue/originalClusterDefaultValue;
+    
+    long simulatedJobValue = (long)(scaleFactor * simulatedClusterDefaultValue);
+    
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("For the job configuration parameter '" + jobValueKey 
+                + "' and the cluster configuration parameter '" 
+                + clusterValueKey + "', the original job's configuration value"
+                + " is scaled from '" + originalJobValue + "' to '" 
+                + simulatedJobValue + "' using the default (unit) value of "
+                + "'" + originalClusterDefaultValue + "' for the original "
+                + " cluster and '" + simulatedClusterDefaultValue + "' for the"
+                + " simulated cluster.");
+    }
+    
+    destConf.setLong(jobValueKey, simulatedJobValue);
+  }
+  
+  // Checks if the scaling of original job's memory parameter value is 
+  // valid
+  @SuppressWarnings("deprecation")
+  private static boolean checkMemoryUpperLimits(String jobKey, String limitKey,  
+                                                Configuration conf, 
+                                                boolean convertLimitToMB) {
+    if (conf.get(limitKey) != null) {
+      long limit = conf.getLong(limitKey, JobConf.DISABLED_MEMORY_LIMIT);
+      // scale only if the max memory limit is set.
+      if (limit >= 0) {
+        if (convertLimitToMB) {
+          limit /= (1024 * 1024); //Converting to MB
+        }
+        
+        long scaledConfigValue = 
+               conf.getLong(jobKey, JobConf.DISABLED_MEMORY_LIMIT);
+        
+        // check now
+        if (scaledConfigValue > limit) {
+          throw new RuntimeException("Simulated job's configuration" 
+              + " parameter '" + jobKey + "' got scaled to a value '" 
+              + scaledConfigValue + "' which exceeds the upper limit of '" 
+              + limit + "' defined for the simulated cluster by the key '" 
+              + limitKey + "'. To disable High-Ram feature emulation, set '" 
+              + GRIDMIX_HIGHRAM_EMULATION_ENABLE + "' to 'false'.");
+        }
+        return true;
+      }
+    }
+    return false;
+  }
+  
+  // Check if the parameter scaling does not exceed the cluster limits.
+  @SuppressWarnings("deprecation")
+  private static void validateTaskMemoryLimits(Configuration conf, 
+                        String jobKey, String clusterMaxKey) {
+    if (!checkMemoryUpperLimits(jobKey, 
+        JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, conf, true)) {
+      checkMemoryUpperLimits(jobKey, clusterMaxKey, conf, false);
+    }
+  }
+
+  /**
+   * Sets the high ram job properties in the simulated job's configuration.
+   */
+  @SuppressWarnings("deprecation")
+  static void configureHighRamProperties(Configuration sourceConf, 
+                                         Configuration destConf) {
+    // set the memory per map task
+    scaleConfigParameter(sourceConf, destConf, 
+                         MRConfig.MAPMEMORY_MB, MRJobConfig.MAP_MEMORY_MB, 
+                         JobConf.DISABLED_MEMORY_LIMIT);
+    
+    // validate and fail early
+    validateTaskMemoryLimits(destConf, MRJobConfig.MAP_MEMORY_MB, 
+                             JTConfig.JT_MAX_MAPMEMORY_MB);
+    
+    // set the memory per reduce task
+    scaleConfigParameter(sourceConf, destConf, 
+                         MRConfig.REDUCEMEMORY_MB, MRJobConfig.REDUCE_MEMORY_MB,
+                         JobConf.DISABLED_MEMORY_LIMIT);
+    // validate and fail early
+    validateTaskMemoryLimits(destConf, MRJobConfig.REDUCE_MEMORY_MB, 
+                             JTConfig.JT_MAX_REDUCEMEMORY_MB);
+  }
+  
+  /**
+   * Indicates whether this {@link GridmixJob} supports compression emulation.
+   */
+  protected abstract boolean canEmulateCompression();
+  
+  protected GridmixJob(final Configuration conf, long submissionMillis, 
+                       final String name) throws IOException {
+    submissionTimeNanos = TimeUnit.NANOSECONDS.convert(
+        submissionMillis, TimeUnit.MILLISECONDS);
+    jobdesc = null;
+    outdir = null;
+    seq = -1;
+    ugi = UserGroupInformation.getCurrentUser();
+
+    try {
+      job = this.ugi.doAs(new PrivilegedExceptionAction<Job>() {
+        public Job run() throws IOException {
+          Job ret = new Job(conf, name);
+          ret.getConfiguration().setInt(GRIDMIX_JOB_SEQ, seq);
+          setJobQueue(ret, conf.get(GRIDMIX_DEFAULT_QUEUE));
+          return ret;
+        }
+      });
+    } catch (InterruptedException e) {
+      throw new IOException(e);
+    }
+  }
+
+  public UserGroupInformation getUgi() {
+    return ugi;
+  }
+
+  public String toString() {
+    return job.getJobName();
+  }
+
+  public long getDelay(TimeUnit unit) {
+    return unit.convert(submissionTimeNanos - System.nanoTime(),
+        TimeUnit.NANOSECONDS);
+  }
+
+  @Override
+  public int compareTo(Delayed other) {
+    if (this == other) {
+      return 0;
+    }
+    if (other instanceof GridmixJob) {
+      final long otherNanos = ((GridmixJob)other).submissionTimeNanos;
+      if (otherNanos < submissionTimeNanos) {
+        return 1;
+      }
+      if (otherNanos > submissionTimeNanos) {
+        return -1;
+      }
+      return id() - ((GridmixJob)other).id();
+    }
+    final long diff =
+      getDelay(TimeUnit.NANOSECONDS) - other.getDelay(TimeUnit.NANOSECONDS);
+    return 0 == diff ? 0 : (diff > 0 ? 1 : -1);
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (this == other) {
+      return true;
+    }
+    // not possible unless job is cloned; all jobs should be unique
+    return other instanceof GridmixJob && id() == ((GridmixJob)other).id();
+  }
+
+  @Override
+  public int hashCode() {
+    return id();
+  }
+
+  int id() {
+    return seq;
+  }
+
+  Job getJob() {
+    return job;
+  }
+
+  JobStory getJobDesc() {
+    return jobdesc;
+  }
+
+  static void pushDescription(int seq, List<InputSplit> splits) {
+    if (null != descCache.putIfAbsent(seq, splits)) {
+      throw new IllegalArgumentException("Description exists for id " + seq);
+    }
+  }
+
+  static List<InputSplit> pullDescription(JobContext jobCtxt) {
+    return pullDescription(GridmixJob.getJobSeqId(jobCtxt));
+  }
+  
+  static List<InputSplit> pullDescription(int seq) {
+    return descCache.remove(seq);
+  }
+
+  static void clearAll() {
+    descCache.clear();
+  }
+
+  void buildSplits(FilePool inputDir) throws IOException {
+
+  }
+  static int getJobSeqId(JobContext job) {
+    return job.getConfiguration().getInt(GRIDMIX_JOB_SEQ,-1);
+  }
+
+  public static class DraftPartitioner<V> extends Partitioner<GridmixKey,V> {
+    public int getPartition(GridmixKey key, V value, int numReduceTasks) {
+      return key.getPartition();
+    }
+  }
+
+  public static class SpecGroupingComparator
+      implements RawComparator<GridmixKey> {
+    private final DataInputBuffer di = new DataInputBuffer();
+    private final byte[] reset = di.getData();
+    @Override
+    public int compare(GridmixKey g1, GridmixKey g2) {
+      final byte t1 = g1.getType();
+      final byte t2 = g2.getType();
+      if (t1 == GridmixKey.REDUCE_SPEC ||
+          t2 == GridmixKey.REDUCE_SPEC) {
+        return t1 - t2;
+      }
+      assert t1 == GridmixKey.DATA;
+      assert t2 == GridmixKey.DATA;
+      return g1.compareTo(g2);
+    }
+    @Override
+    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+      try {
+        final int ret;
+        di.reset(b1, s1, l1);
+        final int x1 = WritableUtils.readVInt(di);
+        di.reset(b2, s2, l2);
+        final int x2 = WritableUtils.readVInt(di);
+        final int t1 = b1[s1 + x1];
+        final int t2 = b2[s2 + x2];
+        if (t1 == GridmixKey.REDUCE_SPEC ||
+            t2 == GridmixKey.REDUCE_SPEC) {
+          ret = t1 - t2;
+        } else {
+          assert t1 == GridmixKey.DATA;
+          assert t2 == GridmixKey.DATA;
+          ret =
+            WritableComparator.compareBytes(b1, s1, x1, b2, s2, x2);
+        }
+        di.reset(reset, 0, 0);
+        return ret;
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+
+  static class RawBytesOutputFormat<K>
+      extends FileOutputFormat<K,GridmixRecord> {
+
+    @Override
+    public RecordWriter<K,GridmixRecord> getRecordWriter(
+        TaskAttemptContext job) throws IOException {
+
+      Path file = getDefaultWorkFile(job, "");
+      final DataOutputStream fileOut;
+
+      fileOut = 
+        new DataOutputStream(CompressionEmulationUtil
+            .getPossiblyCompressedOutputStream(file, job.getConfiguration()));
+
+      return new RecordWriter<K,GridmixRecord>() {
+        @Override
+        public void write(K ignored, GridmixRecord value)
+            throws IOException {
+          // Let the Gridmix record fill itself.
+          value.write(fileOut);
+        }
+        @Override
+        public void close(TaskAttemptContext ctxt) throws IOException {
+          fileOut.close();
+        }
+      };
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
new file mode 100644
index 0000000..83eb947
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
+import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
+
+import java.util.concurrent.CountDownLatch;
+import java.io.IOException;
+
+enum GridmixJobSubmissionPolicy {
+
+  REPLAY("REPLAY", 320000) {
+    @Override
+    public JobFactory<ClusterStats> createJobFactory(
+      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
+      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
+      throws IOException {
+      return new ReplayJobFactory(
+        submitter, producer, scratchDir, conf, startFlag, userResolver);
+    }
+  },
+
+  STRESS("STRESS", 5000) {
+    @Override
+    public JobFactory<ClusterStats> createJobFactory(
+      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
+      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
+      throws IOException {
+      return new StressJobFactory(
+        submitter, producer, scratchDir, conf, startFlag, userResolver);
+    }
+  },
+
+  SERIAL("SERIAL", 0) {
+    @Override
+    public JobFactory<JobStats> createJobFactory(
+      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
+      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
+      throws IOException {
+      return new SerialJobFactory(
+        submitter, producer, scratchDir, conf, startFlag, userResolver);
+    }
+  };
+
+  public static final String JOB_SUBMISSION_POLICY =
+    "gridmix.job-submission.policy";
+
+  private final String name;
+  private final int pollingInterval;
+
+  GridmixJobSubmissionPolicy(String name, int pollingInterval) {
+    this.name = name;
+    this.pollingInterval = pollingInterval;
+  }
+
+  public abstract JobFactory createJobFactory(
+    JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
+    Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
+    throws IOException;
+
+  public int getPollingInterval() {
+    return pollingInterval;
+  }
+
+  public static GridmixJobSubmissionPolicy getPolicy(
+    Configuration conf, GridmixJobSubmissionPolicy defaultPolicy) {
+    String policy = conf.get(JOB_SUBMISSION_POLICY, defaultPolicy.name());
+    return valueOf(policy.toUpperCase());
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
new file mode 100644
index 0000000..e03e1b9
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
@@ -0,0 +1,301 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+
+class GridmixKey extends GridmixRecord {
+  static final byte REDUCE_SPEC = 0;
+  static final byte DATA = 1;
+
+  static final int META_BYTES = 1;
+
+  private byte type;
+  private int partition; // NOT serialized
+  private Spec spec = new Spec();
+
+  GridmixKey() {
+    this(DATA, 1, 0L);
+  }
+  GridmixKey(byte type, int size, long seed) {
+    super(size, seed);
+    this.type = type;
+    // setting type may change pcnt random bytes
+    setSize(size);
+  }
+
+  @Override
+  public int getSize() {
+    switch (type) {
+      case REDUCE_SPEC:
+        return super.getSize() + spec.getSize() + META_BYTES;
+      case DATA:
+        return super.getSize() + META_BYTES;
+      default:
+        throw new IllegalStateException("Invalid type: " + type);
+    }
+  }
+
+  @Override
+  public void setSize(int size) {
+    switch (type) {
+      case REDUCE_SPEC:
+        super.setSize(size - (META_BYTES + spec.getSize()));
+        break;
+      case DATA:
+        super.setSize(size - META_BYTES);
+        break;
+      default:
+        throw new IllegalStateException("Invalid type: " + type);
+    }
+  }
+
+  /**
+   * Partition is not serialized.
+   */
+  public int getPartition() {
+    return partition;
+  }
+  public void setPartition(int partition) {
+    this.partition = partition;
+  }
+
+  public long getReduceInputRecords() {
+    assert REDUCE_SPEC == getType();
+    return spec.rec_in;
+  }
+  public void setReduceInputRecords(long rec_in) {
+    assert REDUCE_SPEC == getType();
+    final int origSize = getSize();
+    spec.rec_in = rec_in;
+    setSize(origSize);
+  }
+
+  public long getReduceOutputRecords() {
+    assert REDUCE_SPEC == getType();
+    return spec.rec_out;
+  }
+  public void setReduceOutputRecords(long rec_out) {
+    assert REDUCE_SPEC == getType();
+    final int origSize = getSize();
+    spec.rec_out = rec_out;
+    setSize(origSize);
+  }
+
+  public long getReduceOutputBytes() {
+    assert REDUCE_SPEC == getType();
+    return spec.bytes_out;
+  };
+  public void setReduceOutputBytes(long b_out) {
+    assert REDUCE_SPEC == getType();
+    final int origSize = getSize();
+    spec.bytes_out = b_out;
+    setSize(origSize);
+  }
+
+  /**
+   * Get the {@link ResourceUsageMetrics} stored in the key.
+   */
+  public ResourceUsageMetrics getReduceResourceUsageMetrics() {
+    assert REDUCE_SPEC == getType();
+    return spec.metrics;
+  }
+  
+  /**
+   * Store the {@link ResourceUsageMetrics} in the key.
+   */
+  public void setReduceResourceUsageMetrics(ResourceUsageMetrics metrics) {
+    assert REDUCE_SPEC == getType();
+    spec.setResourceUsageSpecification(metrics);
+  }
+  
+  public byte getType() {
+    return type;
+  }
+  public void setType(byte type) throws IOException {
+    final int origSize = getSize();
+    switch (type) {
+      case REDUCE_SPEC:
+      case DATA:
+        this.type = type;
+        break;
+      default:
+        throw new IOException("Invalid type: " + type);
+    }
+    setSize(origSize);
+  }
+
+  public void setSpec(Spec spec) {
+    assert REDUCE_SPEC == getType();
+    final int origSize = getSize();
+    this.spec.set(spec);
+    setSize(origSize);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    setType(in.readByte());
+    if (REDUCE_SPEC == getType()) {
+      spec.readFields(in);
+    }
+  }
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    final byte t = getType();
+    out.writeByte(t);
+    if (REDUCE_SPEC == t) {
+      spec.write(out);
+    }
+  }
+  int fixedBytes() {
+    return super.fixedBytes() +
+      (REDUCE_SPEC == getType() ? spec.getSize() : 0) + META_BYTES;
+  }
+  @Override
+  public int compareTo(GridmixRecord other) {
+    final GridmixKey o = (GridmixKey) other;
+    final byte t1 = getType();
+    final byte t2 = o.getType();
+    if (t1 != t2) {
+      return t1 - t2;
+    }
+    return super.compareTo(other);
+  }
+
+  /**
+   * Note that while the spec is not explicitly included, changing the spec
+   * may change its size, which will affect equality.
+   */
+  @Override
+  public boolean equals(Object other) {
+    if (this == other) {
+      return true;
+    }
+    if (other != null && other.getClass() == getClass()) {
+      final GridmixKey o = ((GridmixKey)other);
+      return getType() == o.getType() && super.equals(o);
+    }
+    return false;
+  }
+
+  @Override
+  public int hashCode() {
+    return super.hashCode() ^ getType();
+  }
+
+  public static class Spec implements Writable {
+    long rec_in;
+    long rec_out;
+    long bytes_out;
+    private ResourceUsageMetrics metrics = null;
+    private int sizeOfResourceUsageMetrics = 0;
+    public Spec() { }
+
+    public void set(Spec other) {
+      rec_in = other.rec_in;
+      bytes_out = other.bytes_out;
+      rec_out = other.rec_out;
+      setResourceUsageSpecification(other.metrics);
+    }
+
+    /**
+     * Sets the {@link ResourceUsageMetrics} for this {@link Spec}.
+     */
+    public void setResourceUsageSpecification(ResourceUsageMetrics metrics) {
+      this.metrics = metrics;
+      if (metrics != null) {
+        this.sizeOfResourceUsageMetrics = metrics.size();
+      } else {
+        this.sizeOfResourceUsageMetrics = 0;
+      }
+    }
+    
+    public int getSize() {
+      return WritableUtils.getVIntSize(rec_in) +
+             WritableUtils.getVIntSize(rec_out) +
+             WritableUtils.getVIntSize(bytes_out) +
+             WritableUtils.getVIntSize(sizeOfResourceUsageMetrics) +
+             sizeOfResourceUsageMetrics;
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+      rec_in = WritableUtils.readVLong(in);
+      rec_out = WritableUtils.readVLong(in);
+      bytes_out = WritableUtils.readVLong(in);
+      sizeOfResourceUsageMetrics =  WritableUtils.readVInt(in);
+      if (sizeOfResourceUsageMetrics > 0) {
+        metrics = new ResourceUsageMetrics();
+        metrics.readFields(in);
+      }
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+      WritableUtils.writeVLong(out, rec_in);
+      WritableUtils.writeVLong(out, rec_out);
+      WritableUtils.writeVLong(out, bytes_out);
+      WritableUtils.writeVInt(out, sizeOfResourceUsageMetrics);
+      if (sizeOfResourceUsageMetrics > 0) {
+        metrics.write(out);
+      }
+    }
+  }
+
+  public static class Comparator extends GridmixRecord.Comparator {
+
+    private final DataInputBuffer di = new DataInputBuffer();
+    private final byte[] reset = di.getData();
+
+    public Comparator() {
+      super(GridmixKey.class);
+    }
+
+    @Override
+    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+      try {
+        di.reset(b1, s1, l1);
+        final int x1 = WritableUtils.readVInt(di);
+        di.reset(b2, s2, l2);
+        final int x2 = WritableUtils.readVInt(di);
+        final int ret = (b1[s1 + x1] != b2[s2 + x2])
+          ? b1[s1 + x1] - b2[s2 + x2]
+          : super.compare(b1, s1, x1, b2, s2, x2);
+        di.reset(reset, 0, 0);
+        return ret;
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+
+    static {
+      WritableComparator.define(GridmixKey.class, new Comparator());
+    }
+  }
+}
+
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
new file mode 100644
index 0000000..481799f
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
@@ -0,0 +1,271 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.EOFException;
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+
+class GridmixRecord implements WritableComparable<GridmixRecord> {
+
+  private static final int FIXED_BYTES = 1;
+  private int size = -1;
+  private long seed;
+  private final DataInputBuffer dib =
+    new DataInputBuffer();
+  private final DataOutputBuffer dob =
+    new DataOutputBuffer(Long.SIZE / Byte.SIZE);
+  private byte[] literal = dob.getData();
+  private boolean compressible = false;
+  private float compressionRatio = 
+    CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;
+  private RandomTextDataGenerator rtg = null;
+
+  GridmixRecord() {
+    this(1, 0L);
+  }
+
+  GridmixRecord(int size, long seed) {
+    this.seed = seed;
+    setSizeInternal(size);
+  }
+
+  public int getSize() {
+    return size;
+  }
+
+  public void setSize(int size) {
+    setSizeInternal(size);
+  }
+
+  void setCompressibility(boolean compressible, float ratio) {
+    this.compressible = compressible;
+    this.compressionRatio = ratio;
+    // Initialize the RandomTextDataGenerator once for every GridMix record
+    // Note that RandomTextDataGenerator is needed only when the GridMix record
+    // is configured to generate compressible text data.
+    if (compressible) {
+      rtg = 
+        CompressionEmulationUtil.getRandomTextDataGenerator(ratio, 
+                                   RandomTextDataGenerator.DEFAULT_SEED);
+    }
+  }
+  
+  private void setSizeInternal(int size) {
+    this.size = Math.max(1, size);
+    try {
+      seed = maskSeed(seed, this.size);
+      dob.reset();
+      dob.writeLong(seed);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public final void setSeed(long seed) {
+    this.seed = seed;
+  }
+
+  /** Marsaglia, 2003. */
+  long nextRand(long x) {
+    x ^= (x << 13);
+    x ^= (x >>> 7);
+    return (x ^= (x << 17));
+  }
+
+  /**
+   * Generate random text data that can be compressed. If the record is marked
+   * compressible (via {@link FileOutputFormat#COMPRESS}), only then the 
+   * random data will be text data else 
+   * {@link GridmixRecord#writeRandom(DataOutput, int)} will be invoked.
+   */
+  private void writeRandomText(DataOutput out, final int size) 
+  throws IOException {
+    long tmp = seed;
+    out.writeLong(tmp);
+    int i = size - (Long.SIZE / Byte.SIZE);
+    //TODO Should we use long for size. What if the data is more than 4G?
+    
+    String randomWord = rtg.getRandomWord();
+    byte[] bytes = randomWord.getBytes("UTF-8");
+    long randomWordSize = bytes.length;
+    while (i >= randomWordSize) {
+      out.write(bytes);
+      i -= randomWordSize;
+      
+      // get the next random word
+      randomWord = rtg.getRandomWord();
+      bytes = randomWord.getBytes("UTF-8");
+      // determine the random word size
+      randomWordSize = bytes.length;
+    }
+    
+    // pad the remaining bytes
+    if (i > 0) {
+      out.write(bytes, 0, i);
+    }
+  }
+  
+  public void writeRandom(DataOutput out, final int size) throws IOException {
+    long tmp = seed;
+    out.writeLong(tmp);
+    int i = size - (Long.SIZE / Byte.SIZE);
+    while (i > Long.SIZE / Byte.SIZE - 1) {
+      tmp = nextRand(tmp);
+      out.writeLong(tmp);
+      i -= Long.SIZE / Byte.SIZE;
+    }
+    for (tmp = nextRand(tmp); i > 0; --i) {
+      out.writeByte((int)(tmp & 0xFF));
+      tmp >>>= Byte.SIZE;
+    }
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    size = WritableUtils.readVInt(in);
+    int payload = size - WritableUtils.getVIntSize(size);
+    if (payload > Long.SIZE / Byte.SIZE) {
+      seed = in.readLong();
+      payload -= Long.SIZE / Byte.SIZE;
+    } else {
+      Arrays.fill(literal, (byte)0);
+      in.readFully(literal, 0, payload);
+      dib.reset(literal, 0, literal.length);
+      seed = dib.readLong();
+      payload = 0;
+    }
+    final int vBytes = in.skipBytes(payload);
+    if (vBytes != payload) {
+      throw new EOFException("Expected " + payload + ", read " + vBytes);
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    // data bytes including vint encoding
+    WritableUtils.writeVInt(out, size);
+    final int payload = size - WritableUtils.getVIntSize(size);
+    if (payload > Long.SIZE / Byte.SIZE) {
+      if (compressible) {
+        writeRandomText(out, payload);
+      } else {
+        writeRandom(out, payload);
+      }
+    } else if (payload > 0) {
+      //TODO What is compressible is turned on? LOG is a bad idea!
+      out.write(literal, 0, payload);
+    }
+  }
+
+  @Override
+  public int compareTo(GridmixRecord other) {
+    return compareSeed(other.seed,
+        Math.max(0, other.getSize() - other.fixedBytes()));
+  }
+
+  int fixedBytes() {
+    // min vint size
+    return FIXED_BYTES;
+  }
+
+  private static long maskSeed(long sd, int sz) {
+    // Don't use fixedBytes here; subclasses will set intended random len
+    if (sz <= FIXED_BYTES) {
+      sd = 0L;
+    } else if (sz < Long.SIZE / Byte.SIZE + FIXED_BYTES) {
+      final int tmp = sz - FIXED_BYTES;
+      final long mask = (1L << (Byte.SIZE * tmp)) - 1;
+      sd &= mask << (Byte.SIZE * (Long.SIZE / Byte.SIZE - tmp));
+    }
+    return sd;
+  }
+
+  int compareSeed(long jSeed, int jSize) {
+    final int iSize = Math.max(0, getSize() - fixedBytes());
+    final int seedLen = Math.min(iSize, jSize) + FIXED_BYTES;
+    jSeed = maskSeed(jSeed, seedLen);
+    long iSeed = maskSeed(seed, seedLen);
+    final int cmplen = Math.min(iSize, jSize);
+    for (int i = 0; i < cmplen; i += Byte.SIZE) {
+      final int k = cmplen - i;
+      for (long j = Long.SIZE - Byte.SIZE;
+          j >= Math.max(0, Long.SIZE / Byte.SIZE - k) * Byte.SIZE;
+          j -= Byte.SIZE) {
+        final int xi = (int)((iSeed >>> j) & 0xFFL);
+        final int xj = (int)((jSeed >>> j) & 0xFFL);
+        if (xi != xj) {
+          return xi - xj;
+        }
+      }
+      iSeed = nextRand(iSeed);
+      jSeed = nextRand(jSeed);
+    }
+    return iSize - jSize;
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (this == other) {
+      return true;
+    }
+    if (other != null && other.getClass() == getClass()) {
+      final GridmixRecord o = ((GridmixRecord)other);
+      return getSize() == o.getSize() && seed == o.seed;
+    }
+    return false;
+  }
+
+  @Override
+  public int hashCode() {
+    return (int)(seed * getSize());
+  }
+
+  public static class Comparator extends WritableComparator {
+
+    public Comparator() {
+      super(GridmixRecord.class);
+    }
+
+    public Comparator(Class<? extends WritableComparable<?>> sub) {
+      super(sub);
+    }
+
+    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+      int n1 = WritableUtils.decodeVIntSize(b1[s1]);
+      int n2 = WritableUtils.decodeVIntSize(b2[s2]);
+      n1 -= WritableUtils.getVIntSize(n1);
+      n2 -= WritableUtils.getVIntSize(n2);
+      return compareBytes(b1, s1+n1, l1-n1, b2, s2+n2, l2-n2);
+    }
+
+    static {
+      WritableComparator.define(GridmixRecord.class, new Comparator());
+    }
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
new file mode 100644
index 0000000..b611c9d
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
@@ -0,0 +1,148 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+
+class GridmixSplit extends CombineFileSplit {
+  private int id;
+  private int nSpec;
+  private int maps;
+  private int reduces;
+  private long inputRecords;
+  private long outputBytes;
+  private long outputRecords;
+  private long maxMemory;
+  private double[] reduceBytes = new double[0];
+  private double[] reduceRecords = new double[0];
+
+  // Spec for reduces id mod this
+  private long[] reduceOutputBytes = new long[0];
+  private long[] reduceOutputRecords = new long[0];
+
+  GridmixSplit() {
+    super();
+  }
+
+  public GridmixSplit(CombineFileSplit cfsplit, int maps, int id,
+      long inputBytes, long inputRecords, long outputBytes,
+      long outputRecords, double[] reduceBytes, double[] reduceRecords,
+      long[] reduceOutputBytes, long[] reduceOutputRecords)
+      throws IOException {
+    super(cfsplit);
+    this.id = id;
+    this.maps = maps;
+    reduces = reduceBytes.length;
+    this.inputRecords = inputRecords;
+    this.outputBytes = outputBytes;
+    this.outputRecords = outputRecords;
+    this.reduceBytes = reduceBytes;
+    this.reduceRecords = reduceRecords;
+    nSpec = reduceOutputBytes.length;
+    this.reduceOutputBytes = reduceOutputBytes;
+    this.reduceOutputRecords = reduceOutputRecords;
+  }
+  public int getId() {
+    return id;
+  }
+  public int getMapCount() {
+    return maps;
+  }
+  public long getInputRecords() {
+    return inputRecords;
+  }
+  public long[] getOutputBytes() {
+    if (0 == reduces) {
+      return new long[] { outputBytes };
+    }
+    final long[] ret = new long[reduces];
+    for (int i = 0; i < reduces; ++i) {
+      ret[i] = Math.round(outputBytes * reduceBytes[i]);
+    }
+    return ret;
+  }
+  public long[] getOutputRecords() {
+    if (0 == reduces) {
+      return new long[] { outputRecords };
+    }
+    final long[] ret = new long[reduces];
+    for (int i = 0; i < reduces; ++i) {
+      ret[i] = Math.round(outputRecords * reduceRecords[i]);
+    }
+    return ret;
+  }
+  public long getReduceBytes(int i) {
+    return reduceOutputBytes[i];
+  }
+  public long getReduceRecords(int i) {
+    return reduceOutputRecords[i];
+  }
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    WritableUtils.writeVInt(out, id);
+    WritableUtils.writeVInt(out, maps);
+    WritableUtils.writeVLong(out, inputRecords);
+    WritableUtils.writeVLong(out, outputBytes);
+    WritableUtils.writeVLong(out, outputRecords);
+    WritableUtils.writeVLong(out, maxMemory);
+    WritableUtils.writeVInt(out, reduces);
+    for (int i = 0; i < reduces; ++i) {
+      out.writeDouble(reduceBytes[i]);
+      out.writeDouble(reduceRecords[i]);
+    }
+    WritableUtils.writeVInt(out, nSpec);
+    for (int i = 0; i < nSpec; ++i) {
+      WritableUtils.writeVLong(out, reduceOutputBytes[i]);
+      WritableUtils.writeVLong(out, reduceOutputRecords[i]);
+    }
+  }
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    id = WritableUtils.readVInt(in);
+    maps = WritableUtils.readVInt(in);
+    inputRecords = WritableUtils.readVLong(in);
+    outputBytes = WritableUtils.readVLong(in);
+    outputRecords = WritableUtils.readVLong(in);
+    maxMemory = WritableUtils.readVLong(in);
+    reduces = WritableUtils.readVInt(in);
+    if (reduceBytes.length < reduces) {
+      reduceBytes = new double[reduces];
+      reduceRecords = new double[reduces];
+    }
+    for (int i = 0; i < reduces; ++i) {
+      reduceBytes[i] = in.readDouble();
+      reduceRecords[i] = in.readDouble();
+    }
+    nSpec = WritableUtils.readVInt(in);
+    if (reduceOutputBytes.length < nSpec) {
+      reduceOutputBytes = new long[nSpec];
+      reduceOutputRecords = new long[nSpec];
+    }
+    for (int i = 0; i < nSpec; ++i) {
+      reduceOutputBytes[i] = WritableUtils.readVLong(in);
+      reduceOutputRecords[i] = WritableUtils.readVLong(in);
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
new file mode 100644
index 0000000..a9d404d
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
@@ -0,0 +1,139 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map.Entry;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Given a {@link #FilePool}, obtain a set of files capable of satisfying
+ * a full set of splits, then iterate over each source to fill the request.
+ */
+class InputStriper {
+  public static final Log LOG = LogFactory.getLog(InputStriper.class);
+  int idx;
+  long currentStart;
+  FileStatus current;
+  final List<FileStatus> files = new ArrayList<FileStatus>();
+  final Configuration conf = new Configuration();
+
+  /**
+   * @param inputDir Pool from which files are requested.
+   * @param mapBytes Sum of all expected split requests.
+   */
+  InputStriper(FilePool inputDir, long mapBytes)
+      throws IOException {
+    final long inputBytes = inputDir.getInputFiles(mapBytes, files);
+    if (mapBytes > inputBytes) {
+      LOG.warn("Using " + inputBytes + "/" + mapBytes + " bytes");
+    }
+    if (files.isEmpty() && mapBytes > 0) {
+      throw new IOException("Failed to satisfy request for " + mapBytes);
+    }
+    current = files.isEmpty() ? null : files.get(0);
+  }
+
+  /**
+   * @param inputDir Pool used to resolve block locations.
+   * @param bytes Target byte count
+   * @param nLocs Number of block locations per split.
+   * @return A set of files satisfying the byte count, with locations weighted
+   *         to the dominating proportion of input bytes.
+   */
+  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)
+      throws IOException {
+    final ArrayList<Path> paths = new ArrayList<Path>();
+    final ArrayList<Long> start = new ArrayList<Long>();
+    final ArrayList<Long> length = new ArrayList<Long>();
+    final HashMap<String,Double> sb = new HashMap<String,Double>();
+    do {
+      paths.add(current.getPath());
+      start.add(currentStart);
+      final long fromFile = Math.min(bytes, current.getLen() - currentStart);
+      length.add(fromFile);
+      for (BlockLocation loc :
+          inputDir.locationsFor(current, currentStart, fromFile)) {
+        final double tedium = loc.getLength() / (1.0 * bytes);
+        for (String l : loc.getHosts()) {
+          Double j = sb.get(l);
+          if (null == j) {
+            sb.put(l, tedium);
+          } else {
+            sb.put(l, j.doubleValue() + tedium);
+          }
+        }
+      }
+      currentStart += fromFile;
+      bytes -= fromFile;
+      // Switch to a new file if
+      //  - the current file is uncompressed and completely used
+      //  - the current file is compressed
+      
+      CompressionCodecFactory compressionCodecs = 
+        new CompressionCodecFactory(conf);
+      CompressionCodec codec = compressionCodecs.getCodec(current.getPath());
+      if (current.getLen() - currentStart == 0
+          || codec != null) {
+        current = files.get(++idx % files.size());
+        currentStart = 0;
+      }
+    } while (bytes > 0);
+    final ArrayList<Entry<String,Double>> sort =
+      new ArrayList<Entry<String,Double>>(sb.entrySet());
+    Collections.sort(sort, hostRank);
+    final String[] hosts = new String[Math.min(nLocs, sort.size())];
+    for (int i = 0; i < nLocs && i < sort.size(); ++i) {
+      hosts[i] = sort.get(i).getKey();
+    }
+    return new CombineFileSplit(paths.toArray(new Path[0]),
+        toLongArray(start), toLongArray(length), hosts);
+  }
+
+  private long[] toLongArray(final ArrayList<Long> sigh) {
+    final long[] ret = new long[sigh.size()];
+    for (int i = 0; i < ret.length; ++i) {
+      ret[i] = sigh.get(i);
+    }
+    return ret;
+  }
+
+  static final Comparator<Entry<String,Double>> hostRank =
+    new Comparator<Entry<String,Double>>() {
+      public int compare(Entry<String,Double> a, Entry<String,Double> b) {
+          final double va = a.getValue();
+          final double vb = b.getValue();
+          return va > vb ? -1 : va < vb ? 1 : 0;
+        }
+    };
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
new file mode 100644
index 0000000..a6fc6c6
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
@@ -0,0 +1,110 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * Factory passing reduce specification as its last record.
+ */
+class IntermediateRecordFactory extends RecordFactory {
+
+  private final GridmixKey.Spec spec;
+  private final RecordFactory factory;
+  private final int partition;
+  private final long targetRecords;
+  private boolean done = false;
+  private long accRecords = 0L;
+
+  /**
+   * @param targetBytes Expected byte count.
+   * @param targetRecords Expected record count; will emit spec records after
+   *                      this boundary is passed.
+   * @param partition Reduce to which records are emitted.
+   * @param spec Specification to emit.
+   * @param conf Unused.
+   */
+  public IntermediateRecordFactory(long targetBytes, long targetRecords,
+      int partition, GridmixKey.Spec spec, Configuration conf) {
+    this(new AvgRecordFactory(targetBytes, targetRecords, conf), partition,
+        targetRecords, spec, conf);
+  }
+
+  /**
+   * @param factory Factory from which byte/record counts are obtained.
+   * @param partition Reduce to which records are emitted.
+   * @param targetRecords Expected record count; will emit spec records after
+   *                      this boundary is passed.
+   * @param spec Specification to emit.
+   * @param conf Unused.
+   */
+  public IntermediateRecordFactory(RecordFactory factory, int partition,
+      long targetRecords, GridmixKey.Spec spec, Configuration conf) {
+    this.spec = spec;
+    this.factory = factory;
+    this.partition = partition;
+    this.targetRecords = targetRecords;
+  }
+
+  @Override
+  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
+    assert key != null;
+    final boolean rslt = factory.next(key, val);
+    ++accRecords;
+    if (rslt) {
+      if (accRecords < targetRecords) {
+        key.setType(GridmixKey.DATA);
+      } else {
+        final int orig = key.getSize();
+        key.setType(GridmixKey.REDUCE_SPEC);
+        spec.rec_in = accRecords;
+        key.setSpec(spec);
+        val.setSize(val.getSize() - (key.getSize() - orig));
+        // reset counters
+        accRecords = 0L;
+        spec.bytes_out = 0L;
+        spec.rec_out = 0L;
+        done = true;
+      }
+    } else if (!done) {
+      // ensure spec emitted
+      key.setType(GridmixKey.REDUCE_SPEC);
+      key.setPartition(partition);
+      key.setSize(0);
+      val.setSize(0);
+      spec.rec_in = 0L;
+      key.setSpec(spec);
+      done = true;
+      return true;
+    }
+    key.setPartition(partition);
+    return rslt;
+  }
+
+  @Override
+  public float getProgress() throws IOException {
+    return factory.getProgress();
+  }
+
+  @Override
+  public void close() throws IOException {
+    factory.close();
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
new file mode 100644
index 0000000..f31e854
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
@@ -0,0 +1,135 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public enum JobCreator {
+
+  LOADJOB {
+    @Override
+    public GridmixJob createGridmixJob(
+      Configuration gridmixConf, long submissionMillis, JobStory jobdesc,
+      Path outRoot, UserGroupInformation ugi, int seq) throws IOException {
+
+      // Build configuration for this simulated job
+      Configuration conf = new Configuration(gridmixConf);
+      dce.configureDistCacheFiles(conf, jobdesc.getJobConf());
+      return new LoadJob(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
+    }
+
+    @Override
+    public boolean canEmulateDistCacheLoad() {
+      return true;
+    }
+  },
+
+  SLEEPJOB {
+    private String[] hosts;
+      
+    @Override
+    public GridmixJob createGridmixJob(
+      Configuration conf, long submissionMillis, JobStory jobdesc, Path outRoot,
+      UserGroupInformation ugi, int seq) throws IOException {
+      int numLocations = conf.getInt(SLEEPJOB_RANDOM_LOCATIONS, 0);
+      if (numLocations < 0) numLocations=0;
+      if ((numLocations > 0) && (hosts == null)) {
+        final JobClient client = new JobClient(new JobConf(conf));
+        ClusterStatus stat = client.getClusterStatus(true);
+        final int nTrackers = stat.getTaskTrackers();
+        final ArrayList<String> hostList = new ArrayList<String>(nTrackers);
+        final Pattern trackerPattern = Pattern.compile("tracker_([^:]*):.*");
+        final Matcher m = trackerPattern.matcher("");
+        for (String tracker : stat.getActiveTrackerNames()) {
+          m.reset(tracker);
+          if (!m.find()) {
+            continue;
+          }
+          final String name = m.group(1);
+          hostList.add(name);
+        }
+        hosts = hostList.toArray(new String[hostList.size()]);
+      }
+      return new SleepJob(conf, submissionMillis, jobdesc, outRoot, ugi, seq,
+          numLocations, hosts);
+    }
+
+    @Override
+    public boolean canEmulateDistCacheLoad() {
+      return false;
+    }
+  };
+
+  public static final String GRIDMIX_JOB_TYPE = "gridmix.job.type";
+  public static final String SLEEPJOB_RANDOM_LOCATIONS = 
+    "gridmix.sleep.fake-locations";
+
+  /**
+   * Create Gridmix simulated job.
+   * @param conf configuration of simulated job
+   * @param submissionMillis At what time submission of this simulated job be
+   *                         done
+   * @param jobdesc JobStory obtained from trace
+   * @param outRoot gridmix output directory
+   * @param ugi UGI of job submitter of this simulated job
+   * @param seq job sequence number
+   * @return the created simulated job
+   * @throws IOException
+   */
+  public abstract GridmixJob createGridmixJob(
+    final Configuration conf, long submissionMillis, final JobStory jobdesc,
+    Path outRoot, UserGroupInformation ugi, final int seq) throws IOException;
+
+  public static JobCreator getPolicy(
+    Configuration conf, JobCreator defaultPolicy) {
+    return conf.getEnum(GRIDMIX_JOB_TYPE, defaultPolicy);
+  }
+
+  /**
+   * @return true if gridmix simulated jobs of this job type can emulate
+   *         distributed cache load
+   */
+  abstract boolean canEmulateDistCacheLoad();
+
+  DistributedCacheEmulator dce;
+  /**
+   * This method is to be called before calling any other method in JobCreator
+   * except canEmulateDistCacheLoad(), especially if canEmulateDistCacheLoad()
+   * returns true for that job type.
+   * @param e Distributed Cache Emulator
+   */
+  void setDistCacheEmulator(DistributedCacheEmulator e) {
+    this.dce = e;
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
new file mode 100644
index 0000000..b4737cf
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
@@ -0,0 +1,253 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants.Values;
+import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
+import org.apache.hadoop.tools.rumen.TaskInfo;
+import org.apache.hadoop.tools.rumen.ZombieJobProducer;
+import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.concurrent.atomic.AtomicInteger;
+
+
+/**
+ * Component reading job traces generated by Rumen. Each job in the trace is
+ * assigned a sequence number and given a submission time relative to the
+ * job that preceded it. Jobs are enqueued in the JobSubmitter provided at
+ * construction.
+ * @see org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer
+ */
+abstract class JobFactory<T> implements Gridmix.Component<Void>,StatListener<T> {
+
+  public static final Log LOG = LogFactory.getLog(JobFactory.class);
+
+  protected final Path scratch;
+  protected final float rateFactor;
+  protected final Configuration conf;
+  protected final Thread rThread;
+  protected final AtomicInteger sequence;
+  protected final JobSubmitter submitter;
+  protected final CountDownLatch startFlag;
+  protected final UserResolver userResolver;
+  protected final JobCreator jobCreator;
+  protected volatile IOException error = null;
+  protected final JobStoryProducer jobProducer;
+  protected final ReentrantLock lock = new ReentrantLock(true);
+  protected int numJobsInTrace = 0;
+
+  /**
+   * Creating a new instance does not start the thread.
+   * @param submitter Component to which deserialized jobs are passed
+   * @param jobTrace Stream of job traces with which to construct a
+   *                 {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
+   * @param scratch Directory into which to write output from simulated jobs
+   * @param conf Config passed to all jobs to be submitted
+   * @param startFlag Latch released from main to start pipeline
+   * @throws java.io.IOException
+   */
+  public JobFactory(JobSubmitter submitter, InputStream jobTrace,
+      Path scratch, Configuration conf, CountDownLatch startFlag,
+      UserResolver userResolver) throws IOException {
+    this(submitter, new ZombieJobProducer(jobTrace, null), scratch, conf,
+        startFlag, userResolver);
+  }
+
+  /**
+   * Constructor permitting JobStoryProducer to be mocked.
+   * @param submitter Component to which deserialized jobs are passed
+   * @param jobProducer Producer generating JobStory objects.
+   * @param scratch Directory into which to write output from simulated jobs
+   * @param conf Config passed to all jobs to be submitted
+   * @param startFlag Latch released from main to start pipeline
+   */
+  protected JobFactory(JobSubmitter submitter, JobStoryProducer jobProducer,
+      Path scratch, Configuration conf, CountDownLatch startFlag,
+      UserResolver userResolver) {
+    sequence = new AtomicInteger(0);
+    this.scratch = scratch;
+    this.rateFactor = conf.getFloat(Gridmix.GRIDMIX_SUB_MUL, 1.0f);
+    this.jobProducer = jobProducer;
+    this.conf = new Configuration(conf);
+    this.submitter = submitter;
+    this.startFlag = startFlag;
+    this.rThread = createReaderThread();
+    if(LOG.isDebugEnabled()) {
+      LOG.debug(" The submission thread name is " + rThread.getName());
+    }
+    this.userResolver = userResolver;
+    this.jobCreator = JobCreator.getPolicy(conf, JobCreator.LOADJOB);
+  }
+
+  static class MinTaskInfo extends TaskInfo {
+    public MinTaskInfo(TaskInfo info) {
+      super(info.getInputBytes(), info.getInputRecords(),
+            info.getOutputBytes(), info.getOutputRecords(),
+            info.getTaskMemory(), info.getResourceUsageMetrics());
+    }
+    public long getInputBytes() {
+      return Math.max(0, super.getInputBytes());
+    }
+    public int getInputRecords() {
+      return Math.max(0, super.getInputRecords());
+    }
+    public long getOutputBytes() {
+      return Math.max(0, super.getOutputBytes());
+    }
+    public int getOutputRecords() {
+      return Math.max(0, super.getOutputRecords());
+    }
+    public long getTaskMemory() {
+      return Math.max(0, super.getTaskMemory());
+    }
+  }
+
+  protected static class FilterJobStory implements JobStory {
+
+    protected final JobStory job;
+
+    public FilterJobStory(JobStory job) {
+      this.job = job;
+    }
+    public JobConf getJobConf() { return job.getJobConf(); }
+    public String getName() { return job.getName(); }
+    public JobID getJobID() { return job.getJobID(); }
+    public String getUser() { return job.getUser(); }
+    public long getSubmissionTime() { return job.getSubmissionTime(); }
+    public InputSplit[] getInputSplits() { return job.getInputSplits(); }
+    public int getNumberMaps() { return job.getNumberMaps(); }
+    public int getNumberReduces() { return job.getNumberReduces(); }
+    public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
+      return job.getTaskInfo(taskType, taskNumber);
+    }
+    public TaskAttemptInfo getTaskAttemptInfo(TaskType taskType, int taskNumber,
+        int taskAttemptNumber) {
+      return job.getTaskAttemptInfo(taskType, taskNumber, taskAttemptNumber);
+    }
+    public TaskAttemptInfo getMapTaskAttemptInfoAdjusted(
+        int taskNumber, int taskAttemptNumber, int locality) {
+      return job.getMapTaskAttemptInfoAdjusted(
+          taskNumber, taskAttemptNumber, locality);
+    }
+    public Values getOutcome() {
+      return job.getOutcome();
+    }
+    public String getQueueName() {
+      return job.getQueueName();
+    }
+  }
+
+  protected abstract Thread createReaderThread() ; 
+
+  // gets the next job from the trace and does some bookkeeping for the same
+  private JobStory getNextJobFromTrace() throws IOException {
+    JobStory story = jobProducer.getNextJob();
+    if (story != null) {
+      ++numJobsInTrace;
+    }
+    return story;
+  }
+  
+  protected JobStory getNextJobFiltered() throws IOException {
+    JobStory job = getNextJobFromTrace();
+    while (job != null &&
+      (job.getOutcome() != Pre21JobHistoryConstants.Values.SUCCESS ||
+        job.getSubmissionTime() < 0)) {
+      if (LOG.isDebugEnabled()) {
+        String reason = null;
+        if (job.getOutcome() != Pre21JobHistoryConstants.Values.SUCCESS) {
+          reason = "STATE (" + job.getOutcome().name() + ") ";
+        }
+        if (job.getSubmissionTime() < 0) {
+          reason += "SUBMISSION-TIME (" + job.getSubmissionTime() + ")";
+        }
+        LOG.debug("Ignoring job " + job.getJobID() + " from the input trace."
+                  + " Reason: " + reason == null ? "N/A" : reason);
+      }
+      job = getNextJobFromTrace();
+    }
+    return null == job ? null : new FilterJobStory(job) {
+      @Override
+      public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
+        return new MinTaskInfo(this.job.getTaskInfo(taskType, taskNumber));
+      }
+    };
+  }
+
+
+  /**
+   * Obtain the error that caused the thread to exit unexpectedly.
+   */
+  public IOException error() {
+    return error;
+  }
+
+  /**
+   * Add is disabled.
+   * @throws UnsupportedOperationException
+   */
+  public void add(Void ignored) {
+    throw new UnsupportedOperationException(getClass().getName() +
+        " is at the start of the pipeline and accepts no events");
+  }
+
+  /**
+   * Start the reader thread, wait for latch if necessary.
+   */
+  public void start() {
+    rThread.start();
+  }
+
+  /**
+   * Wait for the reader thread to exhaust the job trace.
+   */
+  public void join(long millis) throws InterruptedException {
+    rThread.join(millis);
+  }
+
+  /**
+   * Interrupt the reader thread.
+   */
+  public void shutdown() {
+    rThread.interrupt();
+  }
+
+  /**
+   * Interrupt the reader thread. This requires no special consideration, as
+   * the thread has no pending work queue.
+   */
+  public void abort() {
+    // Currently no special work
+    rThread.interrupt();
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
new file mode 100644
index 0000000..af7331c
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
@@ -0,0 +1,255 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.nio.channels.ClosedByInterruptException;
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Queue;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.mapreduce.Job;
+
+/**
+ * Component accepting submitted, running jobs and responsible for
+ * monitoring jobs for success and failure. Once a job is submitted, it is
+ * polled for status until complete. If a job is complete, then the monitor
+ * thread returns immediately to the queue. If not, the monitor will sleep
+ * for some duration.
+ */
+class JobMonitor implements Gridmix.Component<Job> {
+
+  public static final Log LOG = LogFactory.getLog(JobMonitor.class);
+
+  private final Queue<Job> mJobs;
+  private final MonitorThread mThread;
+  private final BlockingQueue<Job> runningJobs;
+  private final long pollDelayMillis;
+  private Statistics statistics;
+  private boolean graceful = false;
+  private boolean shutdown = false;
+
+  public JobMonitor(Statistics statistics) {
+    this(5,TimeUnit.SECONDS, statistics);
+  }
+
+  /**
+   * Create a JobMonitor that sleeps for the specified duration after
+   * polling a still-running job.
+   * @param pollDelay Delay after polling a running job
+   * @param unit Time unit for pollDelaySec (rounded to milliseconds)
+   * @param statistics StatCollector , listener to job completion.
+   */
+  public JobMonitor(int pollDelay, TimeUnit unit, Statistics statistics) {
+    mThread = new MonitorThread();
+    runningJobs = new LinkedBlockingQueue<Job>();
+    mJobs = new LinkedList<Job>();
+    this.pollDelayMillis = TimeUnit.MILLISECONDS.convert(pollDelay, unit);
+    this.statistics = statistics;
+  }
+
+  /**
+   * Add a job to the polling queue.
+   */
+  public void add(Job job) throws InterruptedException {
+    runningJobs.put(job);
+  }
+
+  /**
+   * Add a submission failed job , such that it can be communicated
+   * back to serial.
+   * TODO: Cleaner solution for this problem
+   * @param job
+   */
+  public void submissionFailed(Job job) {
+    LOG.info("Job submission failed notification for job " + job.getJobID());
+    this.statistics.add(job);
+  }
+
+  /**
+   * Temporary hook for recording job success.
+   */
+  protected void onSuccess(Job job) {
+    LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " success");
+  }
+
+  /**
+   * Temporary hook for recording job failure.
+   */
+  protected void onFailure(Job job) {
+    LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " failure");
+  }
+
+  /**
+   * If shutdown before all jobs have completed, any still-running jobs
+   * may be extracted from the component.
+   * @throws IllegalStateException If monitoring thread is still running.
+   * @return Any jobs submitted and not known to have completed.
+   */
+  List<Job> getRemainingJobs() {
+    if (mThread.isAlive()) {
+      LOG.warn("Internal error: Polling running monitor for jobs");
+    }
+    synchronized (mJobs) {
+      return new ArrayList<Job>(mJobs);
+    }
+  }
+
+  /**
+   * Monitoring thread pulling running jobs from the component and into
+   * a queue to be polled for status.
+   */
+  private class MonitorThread extends Thread {
+
+    public MonitorThread() {
+      super("GridmixJobMonitor");
+    }
+
+    /**
+     * Check a job for success or failure.
+     */
+    public void process(Job job) throws IOException, InterruptedException {
+      if (job.isSuccessful()) {
+        onSuccess(job);
+      } else {
+        onFailure(job);
+      }
+    }
+
+    @Override
+    public void run() {
+      boolean graceful;
+      boolean shutdown;
+      while (true) {
+        try {
+          synchronized (mJobs) {
+            graceful = JobMonitor.this.graceful;
+            shutdown = JobMonitor.this.shutdown;
+            runningJobs.drainTo(mJobs);
+          }
+
+          // shutdown conditions; either shutdown requested and all jobs
+          // have completed or abort requested and there are recently
+          // submitted jobs not in the monitored set
+          if (shutdown) {
+            if (!graceful) {
+              while (!runningJobs.isEmpty()) {
+                synchronized (mJobs) {
+                  runningJobs.drainTo(mJobs);
+                }
+              }
+              break;
+            } else if (mJobs.isEmpty()) {
+              break;
+            }
+          }
+          while (!mJobs.isEmpty()) {
+            Job job;
+            synchronized (mJobs) {
+              job = mJobs.poll();
+            }
+            try {
+              if (job.isComplete()) {
+                process(job);
+                statistics.add(job);
+                continue;
+              }
+            } catch (IOException e) {
+              if (e.getCause() instanceof ClosedByInterruptException) {
+                // Job doesn't throw InterruptedException, but RPC socket layer
+                // is blocking and may throw a wrapped Exception if this thread
+                // is interrupted. Since the lower level cleared the flag,
+                // reset it here
+                Thread.currentThread().interrupt();
+              } else {
+                LOG.warn("Lost job " + (null == job.getJobName()
+                     ? "<unknown>" : job.getJobName()), e);
+                continue;
+              }
+            }
+            synchronized (mJobs) {
+              if (!mJobs.offer(job)) {
+                LOG.error("Lost job " + (null == job.getJobName()
+                     ? "<unknown>" : job.getJobName())); // should never
+                                                         // happen
+              }
+            }
+            break;
+          }
+          try {
+            TimeUnit.MILLISECONDS.sleep(pollDelayMillis);
+          } catch (InterruptedException e) {
+            shutdown = true;
+            continue;
+          }
+        } catch (Throwable e) {
+          LOG.warn("Unexpected exception: ", e);
+        }
+      }
+    }
+  }
+
+  /**
+   * Start the internal, monitoring thread.
+   */
+  public void start() {
+    mThread.start();
+  }
+
+  /**
+   * Wait for the monitor to halt, assuming shutdown or abort have been
+   * called. Note that, since submission may be sporatic, this will hang
+   * if no form of shutdown has been requested.
+   */
+  public void join(long millis) throws InterruptedException {
+    mThread.join(millis);
+  }
+
+  /**
+   * Drain all submitted jobs to a queue and stop the monitoring thread.
+   * Upstream submitter is assumed dead.
+   */
+  public void abort() {
+    synchronized (mJobs) {
+      graceful = false;
+      shutdown = true;
+    }
+    mThread.interrupt();
+  }
+
+  /**
+   * When all monitored jobs have completed, stop the monitoring thread.
+   * Upstream submitter is assumed dead.
+   */
+  public void shutdown() {
+    synchronized (mJobs) {
+      graceful = true;
+      shutdown = true;
+    }
+    mThread.interrupt();
+  }
+}
+
+
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
new file mode 100644
index 0000000..62dd9fa
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
@@ -0,0 +1,196 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.nio.channels.ClosedByInterruptException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.RejectedExecutionException;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Component accepting deserialized job traces, computing split data, and
+ * submitting to the cluster on deadline. Each job added from an upstream
+ * factory must be submitted to the cluster by the deadline recorded on it.
+ * Once submitted, jobs must be added to a downstream component for
+ * monitoring.
+ */
+class JobSubmitter implements Gridmix.Component<GridmixJob> {
+
+  public static final Log LOG = LogFactory.getLog(JobSubmitter.class);
+
+  private final Semaphore sem;
+  private final Statistics statistics;
+  private final FilePool inputDir;
+  private final JobMonitor monitor;
+  private final ExecutorService sched;
+  private volatile boolean shutdown = false;
+
+  /**
+   * Initialize the submission component with downstream monitor and pool of
+   * files from which split data may be read.
+   * @param monitor Monitor component to which jobs should be passed
+   * @param threads Number of submission threads
+   *   See {@link Gridmix#GRIDMIX_SUB_THR}.
+   * @param queueDepth Max depth of pending work queue
+   *   See {@link Gridmix#GRIDMIX_QUE_DEP}.
+   * @param inputDir Set of files from which split data may be mined for
+   *   synthetic jobs.
+   * @param statistics
+   */
+  public JobSubmitter(JobMonitor monitor, int threads, int queueDepth,
+      FilePool inputDir, Statistics statistics) {
+    sem = new Semaphore(queueDepth);
+    sched = new ThreadPoolExecutor(threads, threads, 0L,
+        TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>());
+    this.inputDir = inputDir;
+    this.monitor = monitor;
+    this.statistics = statistics;
+  }
+
+  /**
+   * Runnable wrapping a job to be submitted to the cluster.
+   */
+  private class SubmitTask implements Runnable {
+
+    final GridmixJob job;
+    public SubmitTask(GridmixJob job) {
+      this.job = job;
+    }
+    public void run() {
+      try {
+        // pre-compute split information
+        try {
+          job.buildSplits(inputDir);
+        } catch (IOException e) {
+          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
+                   + job.getUgi(), e);
+          monitor.submissionFailed(job.getJob());
+          return;
+        } catch (Exception e) {
+          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
+                   + job.getUgi(), e);
+          monitor.submissionFailed(job.getJob());
+          return;
+        }
+        // Sleep until deadline
+        long nsDelay = job.getDelay(TimeUnit.NANOSECONDS);
+        while (nsDelay > 0) {
+          TimeUnit.NANOSECONDS.sleep(nsDelay);
+          nsDelay = job.getDelay(TimeUnit.NANOSECONDS);
+        }
+        try {
+          // submit job
+          monitor.add(job.call());
+          statistics.addJobStats(job.getJob(), job.getJobDesc());
+          LOG.debug("SUBMIT " + job + "@" + System.currentTimeMillis() +
+              " (" + job.getJob().getJobID() + ")");
+        } catch (IOException e) {
+          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
+                   + job.getUgi(), e);
+          if (e.getCause() instanceof ClosedByInterruptException) {
+            throw new InterruptedException("Failed to submit " +
+                job.getJob().getJobName());
+          }
+          monitor.submissionFailed(job.getJob());
+        } catch (ClassNotFoundException e) {
+          LOG.warn("Failed to submit " + job.getJob().getJobName(), e);
+          monitor.submissionFailed(job.getJob());
+        }
+      } catch (InterruptedException e) {
+        // abort execution, remove splits if nesc
+        // TODO release ThdLoc
+        GridmixJob.pullDescription(job.id());
+        Thread.currentThread().interrupt();
+        monitor.submissionFailed(job.getJob());
+      } catch(Exception e) {
+        //Due to some exception job wasnt submitted.
+        LOG.info(" Job " + job.getJob().getJobID() + " submission failed " , e);
+        monitor.submissionFailed(job.getJob());
+      } finally {
+        sem.release();
+      }
+    }
+  }
+
+  /**
+   * Enqueue the job to be submitted per the deadline associated with it.
+   */
+  public void add(final GridmixJob job) throws InterruptedException {
+    final boolean addToQueue = !shutdown;
+    if (addToQueue) {
+      final SubmitTask task = new SubmitTask(job);
+      sem.acquire();
+      try {
+        sched.execute(task);
+      } catch (RejectedExecutionException e) {
+        sem.release();
+      }
+    }
+  }
+
+  /**
+   * (Re)scan the set of input files from which splits are derived.
+   * @throws java.io.IOException
+   */
+  public void refreshFilePool() throws IOException {
+    inputDir.refresh();
+  }
+
+  /**
+   * Does nothing, as the threadpool is already initialized and waiting for
+   * work from the upstream factory.
+   */
+  public void start() { }
+
+  /**
+   * Continue running until all queued jobs have been submitted to the
+   * cluster.
+   */
+  public void join(long millis) throws InterruptedException {
+    if (!shutdown) {
+      throw new IllegalStateException("Cannot wait for active submit thread");
+    }
+    sched.awaitTermination(millis, TimeUnit.MILLISECONDS);
+  }
+
+  /**
+   * Finish all jobs pending submission, but do not accept new work.
+   */
+  public void shutdown() {
+    // complete pending tasks, but accept no new tasks
+    shutdown = true;
+    sched.shutdown();
+  }
+
+  /**
+   * Discard pending work, including precomputed work waiting to be
+   * submitted.
+   */
+  public void abort() {
+    //pendingJobs.clear();
+    shutdown = true;
+    sched.shutdownNow();
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
new file mode 100644
index 0000000..74cd9ad
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
@@ -0,0 +1,549 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskInputOutputContext;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+import org.apache.hadoop.tools.rumen.TaskInfo;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Random;
+
+/**
+ * Synthetic job generated from a trace description.
+ */
+class LoadJob extends GridmixJob {
+
+  public static final Log LOG = LogFactory.getLog(LoadJob.class);
+
+  public LoadJob(final Configuration conf, long submissionMillis, 
+                 final JobStory jobdesc, Path outRoot, UserGroupInformation ugi,
+                 final int seq) throws IOException {
+    super(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
+  }
+
+  public Job call() throws IOException, InterruptedException,
+                           ClassNotFoundException {
+    ugi.doAs(
+      new PrivilegedExceptionAction<Job>() {
+        public Job run() throws IOException, ClassNotFoundException,
+                                InterruptedException {
+          job.setMapperClass(LoadMapper.class);
+          job.setReducerClass(LoadReducer.class);
+          job.setNumReduceTasks(jobdesc.getNumberReduces());
+          job.setMapOutputKeyClass(GridmixKey.class);
+          job.setMapOutputValueClass(GridmixRecord.class);
+          job.setSortComparatorClass(GridmixKey.Comparator.class);
+          job.setGroupingComparatorClass(SpecGroupingComparator.class);
+          job.setInputFormatClass(LoadInputFormat.class);
+          job.setOutputFormatClass(RawBytesOutputFormat.class);
+          job.setPartitionerClass(DraftPartitioner.class);
+          job.setJarByClass(LoadJob.class);
+          job.getConfiguration().setBoolean(Job.USED_GENERIC_PARSER, true);
+          FileOutputFormat.setOutputPath(job, outdir);
+          job.submit();
+          return job;
+        }
+      });
+
+    return job;
+  }
+
+  @Override
+  protected boolean canEmulateCompression() {
+    return true;
+  }
+  
+  /**
+   * This is a progress based resource usage matcher.
+   */
+  @SuppressWarnings("unchecked")
+  static class ResourceUsageMatcherRunner extends Thread {
+    private final ResourceUsageMatcher matcher;
+    private final Progressive progress;
+    private final long sleepTime;
+    private static final String SLEEP_CONFIG = 
+      "gridmix.emulators.resource-usage.sleep-duration";
+    private static final long DEFAULT_SLEEP_TIME = 100; // 100ms
+    
+    ResourceUsageMatcherRunner(final TaskInputOutputContext context, 
+                               ResourceUsageMetrics metrics) {
+      Configuration conf = context.getConfiguration();
+      
+      // set the resource calculator plugin
+      Class<? extends ResourceCalculatorPlugin> clazz =
+        conf.getClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN,
+                      null, ResourceCalculatorPlugin.class);
+      ResourceCalculatorPlugin plugin = 
+        ResourceCalculatorPlugin.getResourceCalculatorPlugin(clazz, conf);
+      
+      // set the other parameters
+      this.sleepTime = conf.getLong(SLEEP_CONFIG, DEFAULT_SLEEP_TIME);
+      progress = new Progressive() {
+        @Override
+        public float getProgress() {
+          return context.getProgress();
+        }
+      };
+      
+      // instantiate a resource-usage-matcher
+      matcher = new ResourceUsageMatcher();
+      matcher.configure(conf, plugin, metrics, progress);
+    }
+    
+    protected void match() throws Exception {
+      // match the resource usage
+      matcher.matchResourceUsage();
+    }
+    
+    @Override
+    public void run() {
+      LOG.info("Resource usage matcher thread started.");
+      try {
+        while (progress.getProgress() < 1) {
+          // match
+          match();
+          
+          // sleep for some time
+          try {
+            Thread.sleep(sleepTime);
+          } catch (Exception e) {}
+        }
+        
+        // match for progress = 1
+        match();
+        LOG.info("Resource usage emulation complete! Matcher exiting");
+      } catch (Exception e) {
+        LOG.info("Exception while running the resource-usage-emulation matcher"
+                 + " thread! Exiting.", e);
+      }
+    }
+  }
+  
+  // Makes sure that the TaskTracker doesn't kill the map/reduce tasks while
+  // they are emulating
+  private static class StatusReporter extends Thread {
+    private TaskAttemptContext context;
+    StatusReporter(TaskAttemptContext context) {
+      this.context = context;
+    }
+    
+    @Override
+    public void run() {
+      LOG.info("Status reporter thread started.");
+      try {
+        while (context.getProgress() < 1) {
+          // report progress
+          context.progress();
+
+          // sleep for some time
+          try {
+            Thread.sleep(100); // sleep for 100ms
+          } catch (Exception e) {}
+        }
+        
+        LOG.info("Status reporter thread exiting");
+      } catch (Exception e) {
+        LOG.info("Exception while running the status reporter thread!", e);
+      }
+    }
+  }
+  
+  public static class LoadMapper
+  extends Mapper<NullWritable, GridmixRecord, GridmixKey, GridmixRecord> {
+
+    private double acc;
+    private double ratio;
+    private final ArrayList<RecordFactory> reduces =
+      new ArrayList<RecordFactory>();
+    private final Random r = new Random();
+
+    private final GridmixKey key = new GridmixKey();
+    private final GridmixRecord val = new GridmixRecord();
+
+    private ResourceUsageMatcherRunner matcher = null;
+    private StatusReporter reporter = null;
+    
+    @Override
+    protected void setup(Context ctxt) 
+    throws IOException, InterruptedException {
+      final Configuration conf = ctxt.getConfiguration();
+      final LoadSplit split = (LoadSplit) ctxt.getInputSplit();
+      final int maps = split.getMapCount();
+      final long[] reduceBytes = split.getOutputBytes();
+      final long[] reduceRecords = split.getOutputRecords();
+
+      // enable gridmix map output record for compression
+      final boolean emulateMapOutputCompression = 
+        CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
+        && conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);
+      float compressionRatio = 1.0f;
+      if (emulateMapOutputCompression) {
+        compressionRatio = 
+          CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);
+        LOG.info("GridMix is configured to use a compression ratio of " 
+                 + compressionRatio + " for the map output data.");
+        key.setCompressibility(true, compressionRatio);
+        val.setCompressibility(true, compressionRatio);
+      }
+      
+      long totalRecords = 0L;
+      final int nReduces = ctxt.getNumReduceTasks();
+      if (nReduces > 0) {
+        int idx = 0;
+        int id = split.getId();
+        for (int i = 0; i < nReduces; ++i) {
+          final GridmixKey.Spec spec = new GridmixKey.Spec();
+          if (i == id) {
+            spec.bytes_out = split.getReduceBytes(idx);
+            spec.rec_out = split.getReduceRecords(idx);
+            spec.setResourceUsageSpecification(
+                   split.getReduceResourceUsageMetrics(idx));
+            ++idx;
+            id += maps;
+          }
+          
+          // set the map output bytes such that the final reduce input bytes 
+          // match the expected value obtained from the original job
+          long mapOutputBytes = reduceBytes[i];
+          if (emulateMapOutputCompression) {
+            mapOutputBytes /= compressionRatio;
+          }
+          reduces.add(new IntermediateRecordFactory(
+              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, 
+                                   5*1024),
+              i, reduceRecords[i], spec, conf));
+          totalRecords += reduceRecords[i];
+        }
+      } else {
+        long mapOutputBytes = reduceBytes[0];
+        if (emulateMapOutputCompression) {
+          mapOutputBytes /= compressionRatio;
+        }
+        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],
+                                         conf, 5*1024));
+        totalRecords = reduceRecords[0];
+      }
+      final long splitRecords = split.getInputRecords();
+      int missingRecSize = 
+        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);
+      final long inputRecords = 
+        (splitRecords <= 0 && split.getLength() >= 0)
+        ? Math.max(1, split.getLength() / missingRecSize)
+        : splitRecords;
+      ratio = totalRecords / (1.0 * inputRecords);
+      acc = 0.0;
+      
+      matcher = new ResourceUsageMatcherRunner(ctxt, 
+                      split.getMapResourceUsageMetrics());
+      matcher.setDaemon(true);
+      
+      // start the status reporter thread
+      reporter = new StatusReporter(ctxt);
+      reporter.setDaemon(true);
+      reporter.start();
+    }
+
+    @Override
+    public void map(NullWritable ignored, GridmixRecord rec,
+                    Context context) throws IOException, InterruptedException {
+      acc += ratio;
+      while (acc >= 1.0 && !reduces.isEmpty()) {
+        key.setSeed(r.nextLong());
+        val.setSeed(r.nextLong());
+        final int idx = r.nextInt(reduces.size());
+        final RecordFactory f = reduces.get(idx);
+        if (!f.next(key, val)) {
+          reduces.remove(idx);
+          continue;
+        }
+        context.write(key, val);
+        acc -= 1.0;
+        
+        // match inline
+        try {
+          matcher.match();
+        } catch (Exception e) {
+          LOG.debug("Error in resource usage emulation! Message: ", e);
+        }
+      }
+    }
+
+    @Override
+    public void cleanup(Context context) 
+    throws IOException, InterruptedException {
+      for (RecordFactory factory : reduces) {
+        key.setSeed(r.nextLong());
+        while (factory.next(key, val)) {
+          context.write(key, val);
+          key.setSeed(r.nextLong());
+          
+          // match inline
+          try {
+            matcher.match();
+          } catch (Exception e) {
+            LOG.debug("Error in resource usage emulation! Message: ", e);
+          }
+        }
+      }
+      
+      // start the matcher thread since the map phase ends here
+      matcher.start();
+    }
+  }
+
+  public static class LoadReducer
+  extends Reducer<GridmixKey,GridmixRecord,NullWritable,GridmixRecord> {
+
+    private final Random r = new Random();
+    private final GridmixRecord val = new GridmixRecord();
+
+    private double acc;
+    private double ratio;
+    private RecordFactory factory;
+
+    private ResourceUsageMatcherRunner matcher = null;
+    private StatusReporter reporter = null;
+    
+    @Override
+    protected void setup(Context context)
+    throws IOException, InterruptedException {
+      if (!context.nextKey() 
+          || context.getCurrentKey().getType() != GridmixKey.REDUCE_SPEC) {
+        throw new IOException("Missing reduce spec");
+      }
+      long outBytes = 0L;
+      long outRecords = 0L;
+      long inRecords = 0L;
+      ResourceUsageMetrics metrics = new ResourceUsageMetrics();
+      for (GridmixRecord ignored : context.getValues()) {
+        final GridmixKey spec = context.getCurrentKey();
+        inRecords += spec.getReduceInputRecords();
+        outBytes += spec.getReduceOutputBytes();
+        outRecords += spec.getReduceOutputRecords();
+        if (spec.getReduceResourceUsageMetrics() != null) {
+          metrics = spec.getReduceResourceUsageMetrics();
+        }
+      }
+      if (0 == outRecords && inRecords > 0) {
+        LOG.info("Spec output bytes w/o records. Using input record count");
+        outRecords = inRecords;
+      }
+      
+      // enable gridmix reduce output record for compression
+      Configuration conf = context.getConfiguration();
+      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
+          && FileOutputFormat.getCompressOutput(context)) {
+        float compressionRatio = 
+          CompressionEmulationUtil
+            .getReduceOutputCompressionEmulationRatio(conf);
+        LOG.info("GridMix is configured to use a compression ratio of " 
+                 + compressionRatio + " for the reduce output data.");
+        val.setCompressibility(true, compressionRatio);
+        
+        // Set the actual output data size to make sure that the actual output 
+        // data size is same after compression
+        outBytes /= compressionRatio;
+      }
+      
+      factory =
+        new AvgRecordFactory(outBytes, outRecords, 
+                             context.getConfiguration(), 5*1024);
+      ratio = outRecords / (1.0 * inRecords);
+      acc = 0.0;
+      
+      matcher = new ResourceUsageMatcherRunner(context, metrics);
+      
+      // start the status reporter thread
+      reporter = new StatusReporter(context);
+      reporter.start();
+    }
+    @Override
+    protected void reduce(GridmixKey key, Iterable<GridmixRecord> values,
+                          Context context) 
+    throws IOException, InterruptedException {
+      for (GridmixRecord ignored : values) {
+        acc += ratio;
+        while (acc >= 1.0 && factory.next(null, val)) {
+          context.write(NullWritable.get(), val);
+          acc -= 1.0;
+          
+          // match inline
+          try {
+            matcher.match();
+          } catch (Exception e) {
+            LOG.debug("Error in resource usage emulation! Message: ", e);
+          }
+        }
+      }
+    }
+    @Override
+    protected void cleanup(Context context)
+    throws IOException, InterruptedException {
+      val.setSeed(r.nextLong());
+      while (factory.next(null, val)) {
+        context.write(NullWritable.get(), val);
+        val.setSeed(r.nextLong());
+        
+        // match inline
+        try {
+          matcher.match();
+        } catch (Exception e) {
+          LOG.debug("Error in resource usage emulation! Message: ", e);
+        }
+      }
+    }
+  }
+
+  static class LoadRecordReader
+  extends RecordReader<NullWritable,GridmixRecord> {
+
+    private RecordFactory factory;
+    private final Random r = new Random();
+    private final GridmixRecord val = new GridmixRecord();
+
+    public LoadRecordReader() { }
+
+    @Override
+    public void initialize(InputSplit genericSplit, TaskAttemptContext ctxt)
+    throws IOException, InterruptedException {
+      final LoadSplit split = (LoadSplit)genericSplit;
+      final Configuration conf = ctxt.getConfiguration();
+      factory = 
+        new ReadRecordFactory(split.getLength(), split.getInputRecords(), 
+                              new FileQueue(split, conf), conf);
+    }
+
+    @Override
+    public boolean nextKeyValue() throws IOException {
+      val.setSeed(r.nextLong());
+      return factory.next(null, val);
+    }
+    @Override
+    public float getProgress() throws IOException {
+      return factory.getProgress();
+    }
+    @Override
+    public NullWritable getCurrentKey() {
+      return NullWritable.get();
+    }
+    @Override
+    public GridmixRecord getCurrentValue() {
+      return val;
+    }
+    @Override
+    public void close() throws IOException {
+      factory.close();
+    }
+  }
+
+  static class LoadInputFormat
+  extends InputFormat<NullWritable,GridmixRecord> {
+
+    @Override
+    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
+      return pullDescription(jobCtxt);
+    }
+    @Override
+    public RecordReader<NullWritable,GridmixRecord> createRecordReader(
+        InputSplit split, final TaskAttemptContext taskContext)
+        throws IOException {
+      return new LoadRecordReader();
+    }
+  }
+
+  @Override
+  void buildSplits(FilePool inputDir) throws IOException {
+    long mapInputBytesTotal = 0L;
+    long mapOutputBytesTotal = 0L;
+    long mapOutputRecordsTotal = 0L;
+    final JobStory jobdesc = getJobDesc();
+    if (null == jobdesc) {
+      return;
+    }
+    final int maps = jobdesc.getNumberMaps();
+    final int reds = jobdesc.getNumberReduces();
+    for (int i = 0; i < maps; ++i) {
+      final TaskInfo info = jobdesc.getTaskInfo(TaskType.MAP, i);
+      mapInputBytesTotal += info.getInputBytes();
+      mapOutputBytesTotal += info.getOutputBytes();
+      mapOutputRecordsTotal += info.getOutputRecords();
+    }
+    final double[] reduceRecordRatio = new double[reds];
+    final double[] reduceByteRatio = new double[reds];
+    for (int i = 0; i < reds; ++i) {
+      final TaskInfo info = jobdesc.getTaskInfo(TaskType.REDUCE, i);
+      reduceByteRatio[i] = info.getInputBytes() / (1.0 * mapOutputBytesTotal);
+      reduceRecordRatio[i] =
+        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);
+    }
+    final InputStriper striper = new InputStriper(inputDir, mapInputBytesTotal);
+    final List<InputSplit> splits = new ArrayList<InputSplit>();
+    for (int i = 0; i < maps; ++i) {
+      final int nSpec = reds / maps + ((reds % maps) > i ? 1 : 0);
+      final long[] specBytes = new long[nSpec];
+      final long[] specRecords = new long[nSpec];
+      final ResourceUsageMetrics[] metrics = new ResourceUsageMetrics[nSpec];
+      for (int j = 0; j < nSpec; ++j) {
+        final TaskInfo info =
+          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);
+        specBytes[j] = info.getOutputBytes();
+        specRecords[j] = info.getOutputRecords();
+        metrics[j] = info.getResourceUsageMetrics();
+        if (LOG.isDebugEnabled()) {
+          LOG.debug(String.format("SPEC(%d) %d -> %d %d %d", id(), i,
+                    i + j * maps, info.getOutputRecords(), 
+                    info.getOutputBytes()));
+        }
+      }
+      final TaskInfo info = jobdesc.getTaskInfo(TaskType.MAP, i);
+      splits.add(
+        new LoadSplit(striper.splitFor(inputDir, info.getInputBytes(), 3), 
+                      maps, i, info.getInputBytes(), info.getInputRecords(),
+                      info.getOutputBytes(), info.getOutputRecords(),
+                      reduceByteRatio, reduceRecordRatio, specBytes, 
+                      specRecords, info.getResourceUsageMetrics(),
+                      metrics));
+    }
+    pushDescription(id(), splits);
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
new file mode 100644
index 0000000..27e7547
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
@@ -0,0 +1,180 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+
+class LoadSplit extends CombineFileSplit {
+  private int id;
+  private int nSpec;
+  private int maps;
+  private int reduces;
+  private long inputRecords;
+  private long outputBytes;
+  private long outputRecords;
+  private long maxMemory;
+  private double[] reduceBytes = new double[0];
+  private double[] reduceRecords = new double[0];
+
+  // Spec for reduces id mod this
+  private long[] reduceOutputBytes = new long[0];
+  private long[] reduceOutputRecords = new long[0];
+
+  private ResourceUsageMetrics mapMetrics;
+  private ResourceUsageMetrics[] reduceMetrics;
+
+  LoadSplit() {
+    super();
+  }
+
+  public LoadSplit(CombineFileSplit cfsplit, int maps, int id, long inputBytes, 
+                   long inputRecords, long outputBytes, long outputRecords, 
+                   double[] reduceBytes, double[] reduceRecords, 
+                   long[] reduceOutputBytes, long[] reduceOutputRecords,
+                   ResourceUsageMetrics metrics,
+                   ResourceUsageMetrics[] rMetrics)
+  throws IOException {
+    super(cfsplit);
+    this.id = id;
+    this.maps = maps;
+    reduces = reduceBytes.length;
+    this.inputRecords = inputRecords;
+    this.outputBytes = outputBytes;
+    this.outputRecords = outputRecords;
+    this.reduceBytes = reduceBytes;
+    this.reduceRecords = reduceRecords;
+    nSpec = reduceOutputBytes.length;
+    this.reduceOutputBytes = reduceOutputBytes;
+    this.reduceOutputRecords = reduceOutputRecords;
+    this.mapMetrics = metrics;
+    this.reduceMetrics = rMetrics;
+  }
+
+  public int getId() {
+    return id;
+  }
+  public int getMapCount() {
+    return maps;
+  }
+  public long getInputRecords() {
+    return inputRecords;
+  }
+  public long[] getOutputBytes() {
+    if (0 == reduces) {
+      return new long[] { outputBytes };
+    }
+    final long[] ret = new long[reduces];
+    for (int i = 0; i < reduces; ++i) {
+      ret[i] = Math.round(outputBytes * reduceBytes[i]);
+    }
+    return ret;
+  }
+  public long[] getOutputRecords() {
+    if (0 == reduces) {
+      return new long[] { outputRecords };
+    }
+    final long[] ret = new long[reduces];
+    for (int i = 0; i < reduces; ++i) {
+      ret[i] = Math.round(outputRecords * reduceRecords[i]);
+    }
+    return ret;
+  }
+  public long getReduceBytes(int i) {
+    return reduceOutputBytes[i];
+  }
+  public long getReduceRecords(int i) {
+    return reduceOutputRecords[i];
+  }
+  
+  public ResourceUsageMetrics getMapResourceUsageMetrics() {
+    return mapMetrics;
+  }
+  
+  public ResourceUsageMetrics getReduceResourceUsageMetrics(int i) {
+    return reduceMetrics[i];
+  }
+  
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    WritableUtils.writeVInt(out, id);
+    WritableUtils.writeVInt(out, maps);
+    WritableUtils.writeVLong(out, inputRecords);
+    WritableUtils.writeVLong(out, outputBytes);
+    WritableUtils.writeVLong(out, outputRecords);
+    WritableUtils.writeVLong(out, maxMemory);
+    WritableUtils.writeVInt(out, reduces);
+    for (int i = 0; i < reduces; ++i) {
+      out.writeDouble(reduceBytes[i]);
+      out.writeDouble(reduceRecords[i]);
+    }
+    WritableUtils.writeVInt(out, nSpec);
+    for (int i = 0; i < nSpec; ++i) {
+      WritableUtils.writeVLong(out, reduceOutputBytes[i]);
+      WritableUtils.writeVLong(out, reduceOutputRecords[i]);
+    }
+    mapMetrics.write(out);
+    int numReduceMetrics = (reduceMetrics == null) ? 0 : reduceMetrics.length;
+    WritableUtils.writeVInt(out, numReduceMetrics);
+    for (int i = 0; i < numReduceMetrics; ++i) {
+      reduceMetrics[i].write(out);
+    }
+  }
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    id = WritableUtils.readVInt(in);
+    maps = WritableUtils.readVInt(in);
+    inputRecords = WritableUtils.readVLong(in);
+    outputBytes = WritableUtils.readVLong(in);
+    outputRecords = WritableUtils.readVLong(in);
+    maxMemory = WritableUtils.readVLong(in);
+    reduces = WritableUtils.readVInt(in);
+    if (reduceBytes.length < reduces) {
+      reduceBytes = new double[reduces];
+      reduceRecords = new double[reduces];
+    }
+    for (int i = 0; i < reduces; ++i) {
+      reduceBytes[i] = in.readDouble();
+      reduceRecords[i] = in.readDouble();
+    }
+    nSpec = WritableUtils.readVInt(in);
+    if (reduceOutputBytes.length < nSpec) {
+      reduceOutputBytes = new long[nSpec];
+      reduceOutputRecords = new long[nSpec];
+    }
+    for (int i = 0; i < nSpec; ++i) {
+      reduceOutputBytes[i] = WritableUtils.readVLong(in);
+      reduceOutputRecords[i] = WritableUtils.readVLong(in);
+    }
+    mapMetrics = new ResourceUsageMetrics();
+    mapMetrics.readFields(in);
+    int numReduceMetrics = WritableUtils.readVInt(in);
+    reduceMetrics = new ResourceUsageMetrics[numReduceMetrics];
+    for (int i = 0; i < numReduceMetrics; ++i) {
+      reduceMetrics[i] = new ResourceUsageMetrics();
+      reduceMetrics[i].readFields(in);
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java
new file mode 100644
index 0000000..4f1399e
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java
@@ -0,0 +1,25 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+/**
+ * Used to track progress of tasks.
+ */
+public interface Progressive {
+  public float getProgress();
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
new file mode 100644
index 0000000..497108a
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
@@ -0,0 +1,332 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.Random;
+import java.net.URI;
+
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PositionedReadable;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * Pseudo local file system that generates random data for any file on the fly
+ * instead of storing files on disk. So opening same file multiple times will
+ * not give same file content. There are no directories in this file system
+ * other than the root and all the files are under root i.e. "/". All file URIs
+ * on pseudo local file system should be of the format <code>
+ * pseudo:///&lt;name&gt;.&lt;fileSize&gt;</code> where name is a unique name
+ * and &lt;fileSize&gt; is a number representing the size of the file in bytes.
+ */
+class PseudoLocalFs extends FileSystem {
+  Path home;
+  /**
+   * The creation time and modification time of all files in
+   * {@link PseudoLocalFs} is same.
+   */
+  private static final long TIME = System.currentTimeMillis();
+  private static final String HOME_DIR = "/";
+  private static final long BLOCK_SIZE  = 4 * 1024 * 1024L; // 4 MB
+  private static final int DEFAULT_BUFFER_SIZE = 1024  * 1024; // 1MB
+
+  static final URI NAME = URI.create("pseudo:///");
+
+  PseudoLocalFs() {
+    this(new Path(HOME_DIR));
+  }
+
+  PseudoLocalFs(Path home) {
+    super();
+    this.home = home;
+  }
+
+  @Override
+  public URI getUri() {
+    return NAME;
+  }
+
+  @Override
+  public Path getHomeDirectory() {
+    return home;
+  }
+
+  @Override
+  public Path getWorkingDirectory() {
+    return getHomeDirectory();
+  }
+
+  /**
+   * Generates a valid pseudo local file path from the given <code>fileId</code>
+   * and <code>fileSize</code>.
+   * @param fileId unique file id string
+   * @param fileSize file size
+   * @return the generated relative path
+   */
+  static Path generateFilePath(String fileId, long fileSize) {
+    return new Path(fileId + "." + fileSize);
+  }
+
+  /**
+   * Creating a pseudo local file is nothing but validating the file path.
+   * Actual data of the file is generated on the fly when client tries to open
+   * the file for reading.
+   * @param path file path to be created
+   */
+  @Override
+  public FSDataOutputStream create(Path path) throws IOException {
+    try {
+      validateFileNameFormat(path);
+    } catch (FileNotFoundException e) {
+      throw new IOException("File creation failed for " + path);
+    }
+    return null;
+  }
+
+  /**
+   * Validate if the path provided is of expected format of Pseudo Local File
+   * System based files.
+   * @param path file path
+   * @return the file size
+   * @throws FileNotFoundException
+   */
+  long validateFileNameFormat(Path path) throws FileNotFoundException {
+    path = path.makeQualified(this);
+    boolean valid = true;
+    long fileSize = 0;
+    if (!path.toUri().getScheme().equals(getUri().getScheme())) {
+      valid = false;
+    } else {
+      String[] parts = path.toUri().getPath().split("\\.");
+      try {
+        fileSize = Long.valueOf(parts[parts.length - 1]);
+        valid = (fileSize >= 0);
+      } catch (NumberFormatException e) {
+        valid = false;
+      }
+    }
+    if (!valid) {
+      throw new FileNotFoundException("File " + path
+          + " does not exist in pseudo local file system");
+    }
+    return fileSize;
+  }
+
+  /**
+   * @See create(Path) for details
+   */
+  @Override
+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {
+    long fileSize = validateFileNameFormat(path);
+    InputStream in = new RandomInputStream(fileSize, bufferSize);
+    return new FSDataInputStream(in);
+  }
+
+  /**
+   * @See create(Path) for details
+   */
+  @Override
+  public FSDataInputStream open(Path path) throws IOException {
+    return open(path, DEFAULT_BUFFER_SIZE);
+  }
+
+  @Override
+  public FileStatus getFileStatus(Path path) throws IOException {
+    long fileSize = validateFileNameFormat(path);
+    return new FileStatus(fileSize, false, 1, BLOCK_SIZE, TIME, path);
+  }
+
+  @Override
+  public boolean exists(Path path) {
+    try{
+      validateFileNameFormat(path);
+    } catch (FileNotFoundException e) {
+      return false;
+    }
+    return true;
+  }
+
+  @Override
+  public FSDataOutputStream create(Path path, FsPermission permission,
+      boolean overwrite, int bufferSize, short replication, long blockSize,
+      Progressable progress) throws IOException {
+    return create(path);
+  }
+
+  @Override
+  public FileStatus[] listStatus(Path path) throws FileNotFoundException,
+      IOException {
+    return new FileStatus[] {getFileStatus(path)};
+  }
+
+  /**
+   * Input Stream that generates specified number of random bytes.
+   */
+  static class RandomInputStream extends InputStream
+      implements Seekable, PositionedReadable {
+
+    private final Random r = new Random();
+    private BytesWritable val = null;
+    private int positionInVal = 0;// current position in the buffer 'val'
+
+    private long totalSize = 0;// total number of random bytes to be generated
+    private long curPos = 0;// current position in this stream
+
+    /**
+     * @param size total number of random bytes to be generated in this stream
+     * @param bufferSize the buffer size. An internal buffer array of length
+     * <code>bufferSize</code> is created. If <code>bufferSize</code> is not a
+     * positive number, then a default value of 1MB is used.
+     */
+    RandomInputStream(long size, int bufferSize) {
+      totalSize = size;
+      if (bufferSize <= 0) {
+        bufferSize = DEFAULT_BUFFER_SIZE;
+      }
+      val = new BytesWritable(new byte[bufferSize]);
+    }
+
+    @Override
+    public int read() throws IOException {
+      byte[] b = new byte[1];
+      if (curPos < totalSize) {
+        if (positionInVal < val.getLength()) {// use buffered byte
+          b[0] = val.getBytes()[positionInVal++];
+          ++curPos;
+        } else {// generate data
+          int num = read(b);
+          if (num < 0) {
+            return num;
+          }
+        }
+      } else {
+        return -1;
+      }
+      return b[0];
+    }
+
+    @Override
+    public int read(byte[] bytes) throws IOException {
+      return read(bytes, 0, bytes.length);
+    }
+
+    @Override
+    public int read(byte[] bytes, int off, int len) throws IOException {
+      if (curPos == totalSize) {
+        return -1;// EOF
+      }
+      int numBytes = len;
+      if (numBytes > (totalSize - curPos)) {// position in file is close to EOF
+        numBytes = (int)(totalSize - curPos);
+      }
+      if (numBytes > (val.getLength() - positionInVal)) {
+        // need to generate data into val
+        r.nextBytes(val.getBytes());
+        positionInVal = 0;
+      }
+
+      System.arraycopy(val.getBytes(), positionInVal, bytes, off, numBytes);
+      curPos += numBytes;
+      positionInVal += numBytes;
+      return numBytes;
+    }
+
+    @Override
+    public int available() {
+      return (int)(val.getLength() - positionInVal);
+    }
+
+    @Override
+    public int read(long position, byte[] buffer, int offset, int length)
+        throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void readFully(long position, byte[] buffer) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void readFully(long position, byte[] buffer, int offset, int length)
+        throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    /**
+     * Get the current position in this stream/pseudo-file
+     * @return the position in this stream/pseudo-file
+     * @throws IOException
+     */
+    @Override
+    public long getPos() throws IOException {
+      return curPos;
+    }
+
+    @Override
+    public void seek(long pos) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public boolean seekToNewSource(long targetPos) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  @Override
+  public FSDataOutputStream append(Path path, int bufferSize,
+      Progressable progress) throws IOException {
+    throw new UnsupportedOperationException("Append is not supported"
+        + " in pseudo local file system.");
+  }
+
+  @Override
+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {
+    throw new UnsupportedOperationException("Mkdirs is not supported"
+        + " in pseudo local file system.");
+  }
+
+  @Override
+  public boolean rename(Path src, Path dst) throws IOException {
+    throw new UnsupportedOperationException("Rename is not supported"
+        + " in pseudo local file system.");
+  }
+
+  @Override
+  public boolean delete(Path path, boolean recursive) {
+    throw new UnsupportedOperationException("File deletion is not supported "
+        + "in pseudo local file system.");
+  }
+
+  @Override
+  public void setWorkingDirectory(Path newDir) {
+    throw new UnsupportedOperationException("SetWorkingDirectory "
+        + "is not supported in pseudo local file system.");
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
new file mode 100644
index 0000000..95e8f90
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
@@ -0,0 +1,209 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+
+/**
+ * Random algorithms.
+ */
+public class RandomAlgorithms {
+  
+  private interface IndexMapper {
+    int get(int pos);
+    void swap(int a, int b);
+    int getSize();
+    void reset();
+  }
+
+  /**
+   * A sparse index mapping table - useful when we want to
+   * non-destructively permute a small fraction of a large array.
+   */
+  private static class SparseIndexMapper implements IndexMapper {
+    Map<Integer, Integer> mapping = new HashMap<Integer, Integer>();
+    int size;
+    
+    SparseIndexMapper(int size) { 
+      this.size = size;
+    }
+    
+    public int get(int pos) {
+      Integer mapped = mapping.get(pos);
+      if (mapped == null) return pos;
+      return mapped;
+    }
+
+    public void swap(int a, int b) {
+      if (a == b) return;
+      int valA = get(a);
+      int valB = get(b);
+      if (b == valA) {
+        mapping.remove(b);
+      } else {
+        mapping.put(b, valA);
+      }
+      if (a == valB) {
+        mapping.remove(a);
+      } else {
+        mapping.put(a, valB);
+      }
+    }
+    
+    public int getSize() {
+      return size;
+    }
+    
+    public void reset() {
+      mapping.clear();
+    }
+  }
+
+  /**
+   * A dense index mapping table - useful when we want to
+   * non-destructively permute a large fraction of an array.
+   */
+  private static class DenseIndexMapper implements IndexMapper {
+    int[] mapping;
+
+    DenseIndexMapper(int size) {
+      mapping = new int[size];
+      for (int i=0; i<size; ++i) {
+        mapping[i] = i;
+      }
+    }
+
+    public int get(int pos) {
+      if ( (pos < 0) || (pos>=mapping.length) ) {
+        throw new IndexOutOfBoundsException();
+      }
+      return mapping[pos];
+    }
+
+    public void swap(int a, int b) {
+      if (a == b) return;
+      int valA = get(a);
+      int valB = get(b);
+      mapping[a]=valB;
+      mapping[b]=valA;
+    }
+    
+    public int getSize() {
+      return mapping.length;
+    }
+    
+    public void reset() {
+      return;
+    }
+  }
+
+  /**
+   * Iteratively pick random numbers from pool 0..n-1. Each number can only be
+   * picked once.
+   */
+  public static class Selector {
+    private IndexMapper mapping;
+    private int n;
+    private Random rand;
+
+    /**
+     * Constructor.
+     * 
+     * @param n
+     *          The pool of integers: 0..n-1.
+     * @param selPcnt
+     *          Percentage of selected numbers. This is just a hint for internal
+     *          memory optimization.
+     * @param rand
+     *          Random number generator.
+     */
+    public Selector(int n, double selPcnt, Random rand) {
+      if (n <= 0) {
+        throw new IllegalArgumentException("n should be positive");
+      }
+      
+      boolean sparse = (n > 200) && (selPcnt < 0.1);
+      
+      this.n = n;
+      mapping = (sparse) ? new SparseIndexMapper(n) : new DenseIndexMapper(n);
+      this.rand = rand;
+    }
+    
+    /**
+     * Select the next random number.
+     * @return Random number selected. Or -1 if the remaining pool is empty.
+     */
+    public int next() {
+      switch (n) {
+      case 0: return -1;
+      case 1: 
+      {
+        int index = mapping.get(0);
+        --n;
+        return index;
+      }
+      default:
+      {
+        int pos = rand.nextInt(n);
+        int index = mapping.get(pos);
+        mapping.swap(pos, --n);
+        return index;
+      }
+      }
+    }
+
+    /**
+     * Get the remaining random number pool size.
+     */
+    public int getPoolSize() {
+      return n;
+    }
+    
+    /**
+     * Reset the selector for reuse usage.
+     */
+    public void reset() {
+      mapping.reset();
+      n = mapping.getSize();
+    }
+  }
+  
+  
+  /**
+   * Selecting m random integers from 0..n-1.
+   * @return An array of selected integers.
+   */
+  public static int[] select(int m, int n, Random rand) {
+    if (m >= n) {
+      int[] ret = new int[n];
+      for (int i=0; i<n; ++i) {
+        ret[i] = i;
+      }
+      return ret;
+    }
+    
+    Selector selector = new Selector(n, (float)m/n, rand);
+    int[] selected = new int[m];
+    for (int i=0; i<m; ++i) {
+      selected[i] = selector.next();
+    }
+    return selected;
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
new file mode 100644
index 0000000..877d434
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
@@ -0,0 +1,147 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.commons.lang.RandomStringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * A random text generator. The words are simply sequences of alphabets.
+ */
+class RandomTextDataGenerator {
+  static final Log LOG = LogFactory.getLog(RandomTextDataGenerator.class);
+  
+  /**
+   * Configuration key for random text data generator's list size.
+   */
+  static final String GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE = 
+    "gridmix.datagenerator.randomtext.listsize";
+  
+  /**
+   * Configuration key for random text data generator's word size.
+   */
+  static final String GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE = 
+    "gridmix.datagenerator.randomtext.wordsize";
+  
+  /**
+   * Default random text data generator's list size.
+   */
+  static final int DEFAULT_LIST_SIZE = 200;
+  
+  /**
+   * Default random text data generator's word size.
+   */
+  static final int DEFAULT_WORD_SIZE = 10;
+  
+  /**
+   * Default random text data generator's seed.
+   */
+  static final long DEFAULT_SEED = 0L;
+  
+  /**
+   * A list of random words
+   */
+  private String[] words;
+  private Random random;
+  
+  /**
+   * Constructor for {@link RandomTextDataGenerator} with default seed.
+   * @param size the total number of words to consider.
+   * @param wordSize Size of each word
+   */
+  RandomTextDataGenerator(int size, int wordSize) {
+    this(size, DEFAULT_SEED , wordSize);
+  }
+  
+  /**
+   * Constructor for {@link RandomTextDataGenerator}.
+   * @param size the total number of words to consider.
+   * @param seed Random number generator seed for repeatability
+   * @param wordSize Size of each word
+   */
+  RandomTextDataGenerator(int size, Long seed, int wordSize) {
+    random = new Random(seed);
+    words = new String[size];
+    
+    //TODO change the default with the actual stats
+    //TODO do u need varied sized words?
+    for (int i = 0; i < size; ++i) {
+      words[i] = 
+        RandomStringUtils.random(wordSize, 0, 0, true, false, null, random);
+    }
+  }
+  
+  /**
+   * Get the configured random text data generator's list size.
+   */
+  static int getRandomTextDataGeneratorListSize(Configuration conf) {
+    return conf.getInt(GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, DEFAULT_LIST_SIZE);
+  }
+  
+  /**
+   * Set the random text data generator's list size.
+   */
+  static void setRandomTextDataGeneratorListSize(Configuration conf, 
+                                                 int listSize) {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Random text data generator is configured to use a dictionary " 
+                + " with " + listSize + " words");
+    }
+    conf.setInt(GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, listSize);
+  }
+  
+  /**
+   * Get the configured random text data generator word size.
+   */
+  static int getRandomTextDataGeneratorWordSize(Configuration conf) {
+    return conf.getInt(GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, DEFAULT_WORD_SIZE);
+  }
+  
+  /**
+   * Set the random text data generator word size.
+   */
+  static void setRandomTextDataGeneratorWordSize(Configuration conf, 
+                                                 int wordSize) {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Random text data generator is configured to use a dictionary " 
+                + " with words of length " + wordSize);
+    }
+    conf.setInt(GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, wordSize);
+  }
+  
+  /**
+   * Returns a randomly selected word from a list of random words.
+   */
+  String getRandomWord() {
+    int index = random.nextInt(words.length);
+    return words[index];
+  }
+  
+  /**
+   * This is mainly for testing.
+   */
+  List<String> getRandomWords() {
+    return Arrays.asList(words);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
new file mode 100644
index 0000000..2cb806e
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.IOUtils;
+
+/**
+ * For every record consumed, read key + val bytes from the stream provided.
+ */
+class ReadRecordFactory extends RecordFactory {
+
+  /**
+   * Size of internal, scratch buffer to read from internal stream.
+   */
+  public static final String GRIDMIX_READ_BUF_SIZE = "gridmix.read.buffer.size";
+
+  private final byte[] buf;
+  private final InputStream src;
+  private final RecordFactory factory;
+
+  /**
+   * @param targetBytes Expected byte count.
+   * @param targetRecords Expected record count.
+   * @param src Stream to read bytes.
+   * @param conf Used to establish read buffer size. @see #GRIDMIX_READ_BUF_SIZE
+   */
+  public ReadRecordFactory(long targetBytes, long targetRecords,
+      InputStream src, Configuration conf) {
+    this(new AvgRecordFactory(targetBytes, targetRecords, conf), src, conf);
+  }
+
+  /**
+   * @param factory Factory to draw record sizes.
+   * @param src Stream to read bytes.
+   * @param conf Used to establish read buffer size. @see #GRIDMIX_READ_BUF_SIZE
+   */
+  public ReadRecordFactory(RecordFactory factory, InputStream src,
+      Configuration conf) {
+    this.src = src;
+    this.factory = factory;
+    buf = new byte[conf.getInt(GRIDMIX_READ_BUF_SIZE, 64 * 1024)];
+  }
+
+  @Override
+  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
+    if (!factory.next(key, val)) {
+      return false;
+    }
+    for (int len = (null == key ? 0 : key.getSize()) + val.getSize();
+         len > 0; len -= buf.length) {
+      IOUtils.readFully(src, buf, 0, Math.min(buf.length, len));
+    }
+    return true;
+  }
+
+  @Override
+  public float getProgress() throws IOException {
+    return factory.getProgress();
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.cleanup(null, src);
+    factory.close();
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
new file mode 100644
index 0000000..7abcf78
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/**
+ * Interface for producing records as inputs and outputs to tasks.
+ */
+abstract class RecordFactory implements Closeable {
+
+  /**
+   * Transform the given record or perform some operation.
+   * @return true if the record should be emitted.
+   */
+  public abstract boolean next(GridmixKey key, GridmixRecord val)
+    throws IOException;
+
+  /**
+   * Estimate of exhausted record capacity.
+   */
+  public abstract float getProgress() throws IOException;
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
new file mode 100644
index 0000000..d1b1481
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
@@ -0,0 +1,128 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.io.IOException;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+
+ class ReplayJobFactory extends JobFactory<Statistics.ClusterStats> {
+  public static final Log LOG = LogFactory.getLog(ReplayJobFactory.class);
+
+  /**
+   * Creating a new instance does not start the thread.
+   *
+   * @param submitter   Component to which deserialized jobs are passed
+   * @param jobProducer Job story producer
+   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
+   * @param scratch     Directory into which to write output from simulated jobs
+   * @param conf        Config passed to all jobs to be submitted
+   * @param startFlag   Latch released from main to start pipeline
+   * @param resolver
+   * @throws java.io.IOException
+   */
+  public ReplayJobFactory(
+    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
+    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
+    throws IOException {
+    super(submitter, jobProducer, scratch, conf, startFlag, resolver);
+  }
+
+   
+    @Override
+  public Thread createReaderThread() {
+    return new ReplayReaderThread("ReplayJobFactory");
+  }
+
+   /**
+    * @param item
+    */
+   public void update(Statistics.ClusterStats item) {
+   }
+
+   private class ReplayReaderThread extends Thread {
+
+    public ReplayReaderThread(String threadName) {
+      super(threadName);
+    }
+
+
+    public void run() {
+      try {
+        startFlag.await();
+        if (Thread.currentThread().isInterrupted()) {
+          return;
+        }
+        final long initTime = TimeUnit.MILLISECONDS.convert(
+          System.nanoTime(), TimeUnit.NANOSECONDS);
+        LOG.info("START REPLAY @ " + initTime);
+        long first = -1;
+        long last = -1;
+        while (!Thread.currentThread().isInterrupted()) {
+          try {
+            final JobStory job = getNextJobFiltered();
+            if (null == job) {
+              return;
+            }
+            if (first < 0) {
+              first = job.getSubmissionTime();
+            }
+            final long current = job.getSubmissionTime();
+            if (current < last) {
+              LOG.warn("Job " + job.getJobID() + " out of order");
+              continue;
+            }
+            last = current;
+            submitter.add(
+              jobCreator.createGridmixJob(
+                conf, initTime + Math.round(rateFactor * (current - first)),
+                job, scratch,
+                userResolver.getTargetUgi(
+                  UserGroupInformation.createRemoteUser(job.getUser())), 
+                sequence.getAndIncrement()));
+          } catch (IOException e) {
+            error = e;
+            return;
+          }
+        }
+      } catch (InterruptedException e) {
+        // exit thread; ignore any jobs remaining in the trace
+      } finally {
+        IOUtils.cleanup(null, jobProducer);
+      }
+    }
+  }
+
+   /**
+    * Start the reader thread, wait for latch if necessary.
+    */
+   @Override
+   public void start() {
+     this.rThread.start();
+   }
+
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
new file mode 100644
index 0000000..db643de
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
@@ -0,0 +1,138 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.LineReader;
+
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+
+public class RoundRobinUserResolver implements UserResolver {
+  public static final Log LOG = LogFactory.getLog(RoundRobinUserResolver.class);
+
+  private int uidx = 0;
+  private List<UserGroupInformation> users = Collections.emptyList();
+
+  /**
+   *  Mapping between user names of original cluster and UGIs of proxy users of
+   *  simulated cluster
+   */
+  private final HashMap<String,UserGroupInformation> usercache =
+      new HashMap<String,UserGroupInformation>();
+  
+  /**
+   * Userlist assumes one user per line.
+   * Each line in users-list-file is of the form &lt;username&gt;[,group]* 
+   * <br> Group names are ignored(they are not parsed at all).
+   */
+  private List<UserGroupInformation> parseUserList(URI userUri, 
+                                                   Configuration conf) 
+  throws IOException {
+    if (null == userUri) {
+      return Collections.emptyList();
+    }
+    
+    final Path userloc = new Path(userUri.toString());
+    final Text rawUgi = new Text();
+    final FileSystem fs = userloc.getFileSystem(conf);
+    final ArrayList<UserGroupInformation> ugiList =
+        new ArrayList<UserGroupInformation>();
+
+    LineReader in = null;
+    try {
+      in = new LineReader(fs.open(userloc));
+      while (in.readLine(rawUgi) > 0) {//line is of the form username[,group]*
+        // e is end position of user name in this line
+        int e = rawUgi.find(",");
+        if (rawUgi.getLength() == 0 || e == 0) {
+          throw new IOException("Missing username: " + rawUgi);
+        }
+        if (e == -1) {
+          e = rawUgi.getLength();
+        }
+        final String username = Text.decode(rawUgi.getBytes(), 0, e);
+        UserGroupInformation ugi = null;
+        try {
+          ugi = UserGroupInformation.createProxyUser(username,
+                    UserGroupInformation.getLoginUser());
+        } catch (IOException ioe) {
+          LOG.error("Error while creating a proxy user " ,ioe);
+        }
+        if (ugi != null) {
+          ugiList.add(ugi);
+        }
+        // No need to parse groups, even if they exist. Go to next line
+      }
+    } finally {
+      if (in != null) {
+        in.close();
+      }
+    }
+    return ugiList;
+  }
+
+  @Override
+  public synchronized boolean setTargetUsers(URI userloc, Configuration conf)
+  throws IOException {
+    uidx = 0;
+    users = parseUserList(userloc, conf);
+    if (users.size() == 0) {
+      throw new IOException(buildEmptyUsersErrorMsg(userloc));
+    }
+    usercache.clear();
+    return true;
+  }
+
+  static String buildEmptyUsersErrorMsg(URI userloc) {
+    return "Empty user list is not allowed for RoundRobinUserResolver. Provided"
+    + " user resource URI '" + userloc + "' resulted in an empty user list.";
+  }
+
+  @Override
+  public synchronized UserGroupInformation getTargetUgi(
+    UserGroupInformation ugi) {
+    // UGI of proxy user
+    UserGroupInformation targetUGI = usercache.get(ugi.getUserName());
+    if (targetUGI == null) {
+      targetUGI = users.get(uidx++ % users.size());
+      usercache.put(ugi.getUserName(), targetUGI);
+    }
+    return targetUGI;
+  }
+
+  /**
+   * {@inheritDoc}
+   * <p>
+   * {@link RoundRobinUserResolver} needs to map the users in the
+   * trace to the provided list of target users. So user list is needed.
+   */
+  public boolean needsTargetUsersList() {
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
new file mode 100644
index 0000000..3301cbd
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
@@ -0,0 +1,178 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.io.IOException;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.locks.Condition;
+
+public class SerialJobFactory extends JobFactory<JobStats> {
+
+  public static final Log LOG = LogFactory.getLog(SerialJobFactory.class);
+  private final Condition jobCompleted = lock.newCondition();
+
+  /**
+   * Creating a new instance does not start the thread.
+   *
+   * @param submitter   Component to which deserialized jobs are passed
+   * @param jobProducer Job story producer
+   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
+   * @param scratch     Directory into which to write output from simulated jobs
+   * @param conf        Config passed to all jobs to be submitted
+   * @param startFlag   Latch released from main to start pipeline
+   * @throws java.io.IOException
+   */
+  public SerialJobFactory(
+    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
+    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
+    throws IOException {
+    super(submitter, jobProducer, scratch, conf, startFlag, resolver);
+  }
+
+  @Override
+  public Thread createReaderThread() {
+    return new SerialReaderThread("SerialJobFactory");
+  }
+
+  private class SerialReaderThread extends Thread {
+
+    public SerialReaderThread(String threadName) {
+      super(threadName);
+    }
+
+    /**
+     * SERIAL : In this scenario .  method waits on notification ,
+     * that a submitted job is actually completed. Logic is simple.
+     * ===
+     * while(true) {
+     * wait till previousjob is completed.
+     * break;
+     * }
+     * submit newJob.
+     * previousJob = newJob;
+     * ==
+     */
+    @Override
+    public void run() {
+      try {
+        startFlag.await();
+        if (Thread.currentThread().isInterrupted()) {
+          return;
+        }
+        LOG.info("START SERIAL @ " + System.currentTimeMillis());
+        GridmixJob prevJob;
+        while (!Thread.currentThread().isInterrupted()) {
+          final JobStory job;
+          try {
+            job = getNextJobFiltered();
+            if (null == job) {
+              return;
+            }
+            if (LOG.isDebugEnabled()) {
+              LOG.debug(
+                "Serial mode submitting job " + job.getName());
+            }
+            prevJob = jobCreator.createGridmixJob(
+              conf, 0L, job, scratch, 
+              userResolver.getTargetUgi(
+                UserGroupInformation.createRemoteUser(job.getUser())),
+              sequence.getAndIncrement());
+
+            lock.lock();
+            try {
+              LOG.info(" Submitted the job " + prevJob);
+              submitter.add(prevJob);
+            } finally {
+              lock.unlock();
+            }
+          } catch (IOException e) {
+            error = e;
+            //If submission of current job fails , try to submit the next job.
+            return;
+          }
+
+          if (prevJob != null) {
+            //Wait till previous job submitted is completed.
+            lock.lock();
+            try {
+              while (true) {
+                try {
+                  jobCompleted.await();
+                } catch (InterruptedException ie) {
+                  LOG.error(
+                    " Error in SerialJobFactory while waiting for job completion ",
+                    ie);
+                  return;
+                }
+                if (LOG.isDebugEnabled()) {
+                  LOG.info(" job " + job.getName() + " completed ");
+                }
+                break;
+              }
+            } finally {
+              lock.unlock();
+            }
+            prevJob = null;
+          }
+        }
+      } catch (InterruptedException e) {
+        return;
+      } finally {
+        IOUtils.cleanup(null, jobProducer);
+      }
+    }
+
+  }
+
+  /**
+   * SERIAL. Once you get notification from StatsCollector about the job
+   * completion ,simply notify the waiting thread.
+   *
+   * @param item
+   */
+  @Override
+  public void update(Statistics.JobStats item) {
+    //simply notify in case of serial submissions. We are just bothered
+    //if submitted job is completed or not.
+    lock.lock();
+    try {
+      jobCompleted.signalAll();
+    } finally {
+      lock.unlock();
+    }
+  }
+
+  /**
+   * Start the reader thread, wait for latch if necessary.
+   */
+  @Override
+  public void start() {
+    LOG.info(" Starting Serial submission ");
+    this.rThread.start();
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
new file mode 100644
index 0000000..a9f2999
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
@@ -0,0 +1,411 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapred.TaskStatus;
+import org.apache.hadoop.mapred.gridmix.RandomAlgorithms.Selector;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo;
+import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+public class SleepJob extends GridmixJob {
+  public static final Log LOG = LogFactory.getLog(SleepJob.class);
+  private static final ThreadLocal <Random> rand = 
+    new ThreadLocal <Random> () {
+        @Override protected Random initialValue() {
+            return new Random();
+    }
+  };
+  
+  public static final String SLEEPJOB_MAPTASK_ONLY="gridmix.sleep.maptask-only";
+  private final boolean mapTasksOnly;
+  private final int fakeLocations;
+  private final String[] hosts;
+  private final Selector selector;
+  
+  /**
+   * Interval at which to report progress, in seconds.
+   */
+  public static final String GRIDMIX_SLEEP_INTERVAL = "gridmix.sleep.interval";
+  public static final String GRIDMIX_SLEEP_MAX_MAP_TIME = 
+    "gridmix.sleep.max-map-time";
+  public static final String GRIDMIX_SLEEP_MAX_REDUCE_TIME = 
+    "gridmix.sleep.max-reduce-time";
+
+  private final long mapMaxSleepTime, reduceMaxSleepTime;
+
+  public SleepJob(Configuration conf, long submissionMillis, JobStory jobdesc,
+      Path outRoot, UserGroupInformation ugi, int seq, int numLocations,
+      String[] hosts) throws IOException {
+    super(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
+    this.fakeLocations = numLocations;
+    this.hosts = hosts;
+    this.selector = (fakeLocations > 0)? new Selector(hosts.length, (float) fakeLocations
+        / hosts.length, rand.get()) : null;
+    this.mapTasksOnly = conf.getBoolean(SLEEPJOB_MAPTASK_ONLY, false);
+    mapMaxSleepTime = conf.getLong(GRIDMIX_SLEEP_MAX_MAP_TIME, Long.MAX_VALUE);
+    reduceMaxSleepTime = conf.getLong(GRIDMIX_SLEEP_MAX_REDUCE_TIME,
+        Long.MAX_VALUE);
+  }
+
+  @Override
+  protected boolean canEmulateCompression() {
+    return false;
+  }
+  
+  @Override
+  public Job call()
+    throws IOException, InterruptedException, ClassNotFoundException {
+    ugi.doAs(
+      new PrivilegedExceptionAction<Job>() {
+        public Job run()
+          throws IOException, ClassNotFoundException, InterruptedException {
+          job.setMapperClass(SleepMapper.class);
+          job.setReducerClass(SleepReducer.class);
+          job.setNumReduceTasks((mapTasksOnly) ? 0 : jobdesc.getNumberReduces());
+          job.setMapOutputKeyClass(GridmixKey.class);
+          job.setMapOutputValueClass(NullWritable.class);
+          job.setSortComparatorClass(GridmixKey.Comparator.class);
+          job.setGroupingComparatorClass(SpecGroupingComparator.class);
+          job.setInputFormatClass(SleepInputFormat.class);
+          job.setOutputFormatClass(NullOutputFormat.class);
+          job.setPartitionerClass(DraftPartitioner.class);
+          job.setJarByClass(SleepJob.class);
+          job.getConfiguration().setBoolean(Job.USED_GENERIC_PARSER, true);
+          job.submit();
+          return job;
+
+        }
+      });
+
+    return job;
+  }
+
+  public static class SleepMapper
+  extends Mapper<LongWritable, LongWritable, GridmixKey, NullWritable> {
+
+    @Override
+    public void map(LongWritable key, LongWritable value, Context context)
+    throws IOException, InterruptedException {
+      context.setStatus("Sleeping... " + value.get() + " ms left");
+      long now = System.currentTimeMillis();
+      if (now < key.get()) {
+        TimeUnit.MILLISECONDS.sleep(key.get() - now);
+      }
+    }
+
+    @Override
+    public void cleanup(Context context)
+    throws IOException, InterruptedException {
+      final int nReds = context.getNumReduceTasks();
+      if (nReds > 0) {
+        final SleepSplit split = (SleepSplit) context.getInputSplit();
+        int id = split.getId();
+        final int nMaps = split.getNumMaps();
+        //This is a hack to pass the sleep duration via Gridmix key
+        //TODO: We need to come up with better solution for this.
+        
+        final GridmixKey key = new GridmixKey(GridmixKey.REDUCE_SPEC, 0, 0L);
+        for (int i = id, idx = 0; i < nReds; i += nMaps) {
+          key.setPartition(i);
+          key.setReduceOutputBytes(split.getReduceDurations(idx++));
+          id += nReds;
+          context.write(key, NullWritable.get());
+        }
+      }
+    }
+
+  }
+
+  public static class SleepReducer
+  extends Reducer<GridmixKey, NullWritable, NullWritable, NullWritable> {
+
+    private long duration = 0L;
+
+    @Override
+    protected void setup(Context context)
+    throws IOException, InterruptedException {
+      if (!context.nextKey() ||
+        context.getCurrentKey().getType() != GridmixKey.REDUCE_SPEC) {
+        throw new IOException("Missing reduce spec");
+      }
+      for (NullWritable ignored : context.getValues()) {
+        final GridmixKey spec = context.getCurrentKey();
+        duration += spec.getReduceOutputBytes();
+      }
+      long sleepInterval = 
+        context.getConfiguration().getLong(GRIDMIX_SLEEP_INTERVAL, 5);
+      final long RINTERVAL = 
+        TimeUnit.MILLISECONDS.convert(sleepInterval, TimeUnit.SECONDS);
+      //This is to stop accumulating deviation from expected sleep time
+      //over a period of time.
+      long start = System.currentTimeMillis();
+      long slept = 0L;
+      long sleep = 0L;
+      while (slept < duration) {
+        final long rem = duration - slept;
+        sleep = Math.min(rem, RINTERVAL);
+        context.setStatus("Sleeping... " + rem + " ms left");
+        TimeUnit.MILLISECONDS.sleep(sleep);
+        slept = System.currentTimeMillis() - start;
+      }
+    }
+
+    @Override
+    protected void cleanup(Context context)
+    throws IOException, InterruptedException {
+      final String msg = "Slept for " + duration;
+      LOG.info(msg);
+      context.setStatus(msg);
+    }
+  }
+
+  public static class SleepInputFormat
+  extends InputFormat<LongWritable, LongWritable> {
+
+    @Override
+    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
+      return pullDescription(jobCtxt);
+    }
+
+    @Override
+    public RecordReader<LongWritable, LongWritable> createRecordReader(
+      InputSplit split, final TaskAttemptContext context)
+      throws IOException, InterruptedException {
+      final long duration = split.getLength();
+      long sleepInterval = 
+    	  context.getConfiguration().getLong(GRIDMIX_SLEEP_INTERVAL, 5);
+      final long RINTERVAL = 
+        TimeUnit.MILLISECONDS.convert(sleepInterval, TimeUnit.SECONDS);
+      if (RINTERVAL <= 0) {
+        throw new IOException(
+          "Invalid " + GRIDMIX_SLEEP_INTERVAL + ": " + RINTERVAL);
+      }
+      return new RecordReader<LongWritable, LongWritable>() {
+        long start = -1;
+        long slept = 0L;
+        long sleep = 0L;
+        final LongWritable key = new LongWritable();
+        final LongWritable val = new LongWritable();
+
+        @Override
+        public boolean nextKeyValue() throws IOException {
+          if (start == -1) {
+            start = System.currentTimeMillis();
+          }
+          slept += sleep;
+          sleep = Math.min(duration - slept, RINTERVAL);
+          key.set(slept + sleep + start);
+          val.set(duration - slept);
+          return slept < duration;
+        }
+
+        @Override
+        public float getProgress() throws IOException {
+          return slept / ((float) duration);
+        }
+
+        @Override
+        public LongWritable getCurrentKey() {
+          return key;
+        }
+
+        @Override
+        public LongWritable getCurrentValue() {
+          return val;
+        }
+
+        @Override
+        public void close() throws IOException {
+          final String msg = "Slept for " + duration;
+          LOG.info(msg);
+        }
+
+        public void initialize(InputSplit split, TaskAttemptContext ctxt) {
+        }
+      };
+    }
+  }
+
+  public static class SleepSplit extends InputSplit implements Writable {
+    private int id;
+    private int nSpec;
+    private int nMaps;
+    private long sleepDuration;
+    private long[] reduceDurations = new long[0];
+    private String[] locations = new String[0];
+
+    public SleepSplit() {
+    }
+
+    public SleepSplit(
+      int id, long sleepDuration, long[] reduceDurations, int nMaps,
+      String[] locations) {
+      this.id = id;
+      this.sleepDuration = sleepDuration;
+      nSpec = reduceDurations.length;
+      this.reduceDurations = reduceDurations;
+      this.nMaps = nMaps;
+      this.locations = locations;
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+      WritableUtils.writeVInt(out, id);
+      WritableUtils.writeVLong(out, sleepDuration);
+      WritableUtils.writeVInt(out, nMaps);
+      WritableUtils.writeVInt(out, nSpec);
+      for (int i = 0; i < nSpec; ++i) {
+        WritableUtils.writeVLong(out, reduceDurations[i]);
+      }
+      WritableUtils.writeVInt(out, locations.length);
+      for (int i = 0; i < locations.length; ++i) {
+        Text.writeString(out, locations[i]);
+      }
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+      id = WritableUtils.readVInt(in);
+      sleepDuration = WritableUtils.readVLong(in);
+      nMaps = WritableUtils.readVInt(in);
+      nSpec = WritableUtils.readVInt(in);
+      if (reduceDurations.length < nSpec) {
+        reduceDurations = new long[nSpec];
+      }
+      for (int i = 0; i < nSpec; ++i) {
+        reduceDurations[i] = WritableUtils.readVLong(in);
+      }
+      final int nLoc = WritableUtils.readVInt(in);
+      if (nLoc != locations.length) {
+        locations = new String[nLoc];
+      }
+      for (int i = 0; i < nLoc; ++i) {
+        locations[i] = Text.readString(in);
+      }
+    }
+
+    @Override
+    public long getLength() {
+      return sleepDuration;
+    }
+
+    public int getId() {
+      return id;
+    }
+
+    public int getNumMaps() {
+      return nMaps;
+    }
+
+    public long getReduceDurations(int i) {
+      return reduceDurations[i];
+    }
+
+    @Override
+    public String[] getLocations() {
+      return locations;
+    }
+  }
+
+  private TaskAttemptInfo getSuccessfulAttemptInfo(TaskType type, int task) {
+    TaskAttemptInfo ret;
+    for (int i = 0; true; ++i) {
+      // Rumen should make up an attempt if it's missing. Or this won't work
+      // at all. It's hard to discern what is happening in there.
+      ret = jobdesc.getTaskAttemptInfo(type, task, i);
+      if (ret.getRunState() == TaskStatus.State.SUCCEEDED) {
+        break;
+      }
+    }
+    if(ret.getRunState() != TaskStatus.State.SUCCEEDED) {
+      LOG.warn("No sucessful attempts tasktype " + type +" task "+ task);
+    }
+
+    return ret;
+  }
+
+  @Override
+  void buildSplits(FilePool inputDir) throws IOException {
+    final List<InputSplit> splits = new ArrayList<InputSplit>();
+    final int reds = (mapTasksOnly) ? 0 : jobdesc.getNumberReduces();
+    final int maps = jobdesc.getNumberMaps();
+    for (int i = 0; i < maps; ++i) {
+      final int nSpec = reds / maps + ((reds % maps) > i ? 1 : 0);
+      final long[] redDurations = new long[nSpec];
+      for (int j = 0; j < nSpec; ++j) {
+        final ReduceTaskAttemptInfo info =
+          (ReduceTaskAttemptInfo) getSuccessfulAttemptInfo(TaskType.REDUCE, 
+                                                           i + j * maps);
+        // Include only merge/reduce time
+        redDurations[j] = Math.min(reduceMaxSleepTime, info.getMergeRuntime()
+            + info.getReduceRuntime());
+        if (LOG.isDebugEnabled()) {
+          LOG.debug(
+            String.format(
+              "SPEC(%d) %d -> %d %d/%d", id(), i, i + j * maps, redDurations[j],
+              info.getRuntime()));
+        }
+      }
+      final TaskAttemptInfo info = getSuccessfulAttemptInfo(TaskType.MAP, i);
+      ArrayList<String> locations = new ArrayList<String>(fakeLocations);
+      if (fakeLocations > 0) {
+        selector.reset();
+      }
+      for (int k=0; k<fakeLocations; ++k) {
+        int index = selector.next();
+        if (index < 0) break;
+        locations.add(hosts[index]);
+      }
+
+      splits.add(new SleepSplit(i,
+          Math.min(info.getRuntime(), mapMaxSleepTime), redDurations, maps,
+          locations.toArray(new String[locations.size()])));    }
+    pushDescription(id(), splits);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java
new file mode 100644
index 0000000..2a0f74f
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java
@@ -0,0 +1,32 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.gridmix;
+
+/**
+ * Stat listener.
+ * @param <T>
+ */
+interface StatListener<T>{
+
+  /**
+   * 
+   * @param item
+   */
+  void update(T item);
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java
new file mode 100644
index 0000000..54f1730
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java
@@ -0,0 +1,330 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.gridmix.Gridmix.Component;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.ReentrantLock;
+
+/**
+ * Component collecting the stats required by other components
+ * to make decisions.
+ * Single thread Collector tries to collec the stats.
+ * Each of thread poll updates certain datastructure(Currently ClusterStats).
+ * Components interested in these datastructure, need to register.
+ * StatsCollector notifies each of the listeners.
+ */
+public class Statistics implements Component<Job> {
+  public static final Log LOG = LogFactory.getLog(Statistics.class);
+
+  private final StatCollector statistics = new StatCollector();
+  private JobClient cluster;
+
+  //List of cluster status listeners.
+  private final List<StatListener<ClusterStats>> clusterStatlisteners =
+    new CopyOnWriteArrayList<StatListener<ClusterStats>>();
+
+  //List of job status listeners.
+  private final List<StatListener<JobStats>> jobStatListeners =
+    new CopyOnWriteArrayList<StatListener<JobStats>>();
+
+  //List of jobids and noofMaps for each job
+  private static final Map<Integer, JobStats> jobMaps =
+    new ConcurrentHashMap<Integer,JobStats>();
+
+  private int completedJobsInCurrentInterval = 0;
+  private final int jtPollingInterval;
+  private volatile boolean shutdown = false;
+  private final int maxJobCompletedInInterval;
+  private static final String MAX_JOBS_COMPLETED_IN_POLL_INTERVAL_KEY =
+    "gridmix.max-jobs-completed-in-poll-interval";
+  private final ReentrantLock lock = new ReentrantLock();
+  private final Condition jobCompleted = lock.newCondition();
+  private final CountDownLatch startFlag;
+
+  public Statistics(
+    final Configuration conf, int pollingInterval, CountDownLatch startFlag)
+    throws IOException, InterruptedException {
+      UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+      this.cluster = ugi.doAs(new PrivilegedExceptionAction<JobClient>() {
+        public JobClient run() throws IOException {
+          return new JobClient(new JobConf(conf));
+        }
+      });
+
+    this.jtPollingInterval = pollingInterval;
+    maxJobCompletedInInterval = conf.getInt(
+      MAX_JOBS_COMPLETED_IN_POLL_INTERVAL_KEY, 1);
+    this.startFlag = startFlag;
+  }
+
+  public void addJobStats(Job job, JobStory jobdesc) {
+    int seq = GridmixJob.getJobSeqId(job);
+    if (seq < 0) {
+      LOG.info("Not tracking job " + job.getJobName()
+               + " as seq id is less than zero: " + seq);
+      return;
+    }
+    
+    int maps = 0;
+    int reds = 0;
+    if (jobdesc == null) {
+      throw new IllegalArgumentException(
+        " JobStory not available for job " + job.getJobName());
+    } else {
+      maps = jobdesc.getNumberMaps();
+      reds = jobdesc.getNumberReduces();
+    }
+    JobStats stats = new JobStats(maps, reds, job);
+    jobMaps.put(seq,stats);
+  }
+
+  /**
+   * Used by JobMonitor to add the completed job.
+   */
+  @Override
+  public void add(Job job) {
+    //This thread will be notified initially by jobmonitor incase of
+    //data generation. Ignore that as we are getting once the input is
+    //generated.
+    if (!statistics.isAlive()) {
+      return;
+    }
+    JobStats stat = jobMaps.remove(GridmixJob.getJobSeqId(job));
+
+    if (stat == null) return;
+    
+    completedJobsInCurrentInterval++;
+    //check if we have reached the maximum level of job completions.
+    if (completedJobsInCurrentInterval >= maxJobCompletedInInterval) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(
+          " Reached maximum limit of jobs in a polling interval " +
+            completedJobsInCurrentInterval);
+      }
+      completedJobsInCurrentInterval = 0;
+      lock.lock();
+      try {
+        //Job is completed notify all the listeners.
+        for (StatListener<JobStats> l : jobStatListeners) {
+          l.update(stat);
+        }
+        this.jobCompleted.signalAll();
+      } finally {
+        lock.unlock();
+      }
+    }
+  }
+
+  //TODO: We have just 2 types of listeners as of now . If no of listeners
+  //increase then we should move to map kind of model.
+
+  public void addClusterStatsObservers(StatListener<ClusterStats> listener) {
+    clusterStatlisteners.add(listener);
+  }
+
+  public void addJobStatsListeners(StatListener<JobStats> listener) {
+    this.jobStatListeners.add(listener);
+  }
+
+  /**
+   * Attempt to start the service.
+   */
+  @Override
+  public void start() {
+    statistics.start();
+  }
+
+  private class StatCollector extends Thread {
+
+    StatCollector() {
+      super("StatsCollectorThread");
+    }
+
+    public void run() {
+      try {
+        startFlag.await();
+        if (Thread.currentThread().isInterrupted()) {
+          return;
+        }
+      } catch (InterruptedException ie) {
+        LOG.error(
+          "Statistics Error while waiting for other threads to get ready ", ie);
+        return;
+      }
+      while (!shutdown) {
+        lock.lock();
+        try {
+          jobCompleted.await(jtPollingInterval, TimeUnit.MILLISECONDS);
+        } catch (InterruptedException ie) {
+          if (!shutdown) {
+            LOG.error("Statistics interrupt while waiting for completion of "
+                + "a job.", ie);
+          }
+          return;
+        } finally {
+          lock.unlock();
+        }
+
+        //Fetch cluster data only if required.i.e .
+        // only if there are clusterStats listener.
+        if (clusterStatlisteners.size() > 0) {
+          try {
+            ClusterStatus clusterStatus = cluster.getClusterStatus();
+            updateAndNotifyClusterStatsListeners(clusterStatus);
+          } catch (IOException e) {
+            LOG.error(
+              "Statistics io exception while polling JT ", e);
+            return;
+          }
+        }
+      }
+    }
+
+    private void updateAndNotifyClusterStatsListeners(
+      ClusterStatus clusterStatus) {
+      ClusterStats stats = ClusterStats.getClusterStats();
+      stats.setClusterMetric(clusterStatus);
+      for (StatListener<ClusterStats> listener : clusterStatlisteners) {
+        listener.update(stats);
+      }
+    }
+
+  }
+
+  /**
+   * Wait until the service completes. It is assumed that either a
+   * {@link #shutdown} or {@link #abort} has been requested.
+   */
+  @Override
+  public void join(long millis) throws InterruptedException {
+    statistics.join(millis);
+  }
+
+  @Override
+  public void shutdown() {
+    shutdown = true;
+    jobMaps.clear();
+    clusterStatlisteners.clear();
+    jobStatListeners.clear();
+    statistics.interrupt();
+  }
+
+  @Override
+  public void abort() {
+    shutdown = true;
+    jobMaps.clear();
+    clusterStatlisteners.clear();
+    jobStatListeners.clear();
+    statistics.interrupt();
+  }
+
+  /**
+   * Class to encapsulate the JobStats information.
+   * Current we just need information about completedJob.
+   * TODO: In future we need to extend this to send more information.
+   */
+  static class JobStats {
+    private int noOfMaps;
+    private int noOfReds;
+    private Job job;
+
+    public JobStats(int noOfMaps,int numOfReds, Job job){
+      this.job = job;
+      this.noOfMaps = noOfMaps;
+      this.noOfReds = numOfReds;
+    }
+    public int getNoOfMaps() {
+      return noOfMaps;
+    }
+    public int getNoOfReds() {
+      return noOfReds;
+    }
+
+    /**
+     * Returns the job ,
+     * We should not use job.getJobID it returns null in 20.1xx.
+     * Use (GridmixJob.getJobSeqId(job)) instead
+     * @return job
+     */
+    public Job getJob() {
+      return job;
+    }
+  }
+
+  static class ClusterStats {
+    private ClusterStatus status = null;
+    private static ClusterStats stats = new ClusterStats();
+
+    private ClusterStats() {
+
+    }
+
+    /**
+     * @return stats
+     */
+    static ClusterStats getClusterStats() {
+      return stats;
+    }
+
+    /**
+     * @param metrics
+     */
+    void setClusterMetric(ClusterStatus metrics) {
+      this.status = metrics;
+    }
+
+    /**
+     * @return metrics
+     */
+    public ClusterStatus getStatus() {
+      return status;
+    }
+
+    int getNumRunningJob() {
+      return jobMaps.size();
+    }
+
+    /**
+     * @return runningWatitingJobs
+     */
+    static Collection<JobStats> getRunningJobStats() {
+      return jobMaps.values();
+    }
+
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
new file mode 100644
index 0000000..d78d631
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
@@ -0,0 +1,472 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
+import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+
+import java.io.IOException;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class StressJobFactory extends JobFactory<Statistics.ClusterStats> {
+  public static final Log LOG = LogFactory.getLog(StressJobFactory.class);
+
+  private final LoadStatus loadStatus = new LoadStatus();
+  /**
+   * The minimum ratio between pending+running map tasks (aka. incomplete map
+   * tasks) and cluster map slot capacity for us to consider the cluster is
+   * overloaded. For running maps, we only count them partially. Namely, a 40%
+   * completed map is counted as 0.6 map tasks in our calculation.
+   */
+  private static final float OVERLOAD_MAPTASK_MAPSLOT_RATIO = 2.0f;
+  public static final String CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO=
+      "gridmix.throttle.maps.task-to-slot-ratio";
+  final float overloadMapTaskMapSlotRatio;
+
+  /**
+   * The minimum ratio between pending+running reduce tasks (aka. incomplete
+   * reduce tasks) and cluster reduce slot capacity for us to consider the
+   * cluster is overloaded. For running reduces, we only count them partially.
+   * Namely, a 40% completed reduce is counted as 0.6 reduce tasks in our
+   * calculation.
+   */
+  private static final float OVERLOAD_REDUCETASK_REDUCESLOT_RATIO = 2.5f;
+  public static final String CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO=
+    "gridmix.throttle.reduces.task-to-slot-ratio";
+  final float overloadReduceTaskReduceSlotRatio;
+
+  /**
+   * The maximum share of the cluster's mapslot capacity that can be counted
+   * toward a job's incomplete map tasks in overload calculation.
+   */
+  private static final float MAX_MAPSLOT_SHARE_PER_JOB=0.1f;
+  public static final String CONF_MAX_MAPSLOT_SHARE_PER_JOB=
+    "gridmix.throttle.maps.max-slot-share-per-job";  
+  final float maxMapSlotSharePerJob;
+  
+  /**
+   * The maximum share of the cluster's reduceslot capacity that can be counted
+   * toward a job's incomplete reduce tasks in overload calculation.
+   */
+  private static final float MAX_REDUCESLOT_SHARE_PER_JOB=0.1f;
+  public static final String CONF_MAX_REDUCESLOT_SHARE_PER_JOB=
+    "gridmix.throttle.reducess.max-slot-share-per-job";  
+  final float maxReduceSlotSharePerJob;
+
+  /**
+   * The ratio of the maximum number of pending+running jobs over the number of
+   * task trackers.
+   */
+  private static final float MAX_JOB_TRACKER_RATIO=1.0f;
+  public static final String CONF_MAX_JOB_TRACKER_RATIO=
+    "gridmix.throttle.jobs-to-tracker-ratio";  
+  final float maxJobTrackerRatio;
+
+  /**
+   * Creating a new instance does not start the thread.
+   *
+   * @param submitter   Component to which deserialized jobs are passed
+   * @param jobProducer Stream of job traces with which to construct a
+   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
+   * @param scratch     Directory into which to write output from simulated jobs
+   * @param conf        Config passed to all jobs to be submitted
+   * @param startFlag   Latch released from main to start pipeline
+   * @throws java.io.IOException
+   */
+  public StressJobFactory(
+    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
+    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
+    throws IOException {
+    super(
+      submitter, jobProducer, scratch, conf, startFlag, resolver);
+    overloadMapTaskMapSlotRatio = conf.getFloat(
+        CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO, OVERLOAD_MAPTASK_MAPSLOT_RATIO);
+    overloadReduceTaskReduceSlotRatio = conf.getFloat(
+        CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO, 
+        OVERLOAD_REDUCETASK_REDUCESLOT_RATIO);
+    maxMapSlotSharePerJob = conf.getFloat(
+        CONF_MAX_MAPSLOT_SHARE_PER_JOB, MAX_MAPSLOT_SHARE_PER_JOB);
+    maxReduceSlotSharePerJob = conf.getFloat(
+        CONF_MAX_REDUCESLOT_SHARE_PER_JOB, MAX_REDUCESLOT_SHARE_PER_JOB);
+    maxJobTrackerRatio = conf.getFloat(
+        CONF_MAX_JOB_TRACKER_RATIO, MAX_JOB_TRACKER_RATIO);
+  }
+
+  public Thread createReaderThread() {
+    return new StressReaderThread("StressJobFactory");
+  }
+
+  /*
+  * Worker thread responsible for reading descriptions, assigning sequence
+  * numbers, and normalizing time.
+  */
+  private class StressReaderThread extends Thread {
+
+    public StressReaderThread(String name) {
+      super(name);
+    }
+
+    /**
+     * STRESS: Submits the job in STRESS mode.
+     * while(JT is overloaded) {
+     * wait();
+     * }
+     * If not overloaded , get number of slots available.
+     * Keep submitting the jobs till ,total jobs  is sufficient to
+     * load the JT.
+     * That is submit  (Sigma(no of maps/Job)) > (2 * no of slots available)
+     */
+    public void run() {
+      try {
+        startFlag.await();
+        if (Thread.currentThread().isInterrupted()) {
+          return;
+        }
+        LOG.info("START STRESS @ " + System.currentTimeMillis());
+        while (!Thread.currentThread().isInterrupted()) {
+          try {
+            while (loadStatus.overloaded()) {
+              if (LOG.isDebugEnabled()) {
+                LOG.debug("Cluster overloaded in run! Sleeping...");
+              }
+              // sleep 
+              try {
+                Thread.sleep(1000);
+              } catch (InterruptedException ie) {
+                return;
+              }
+            }
+
+            while (!loadStatus.overloaded()) {
+              if (LOG.isDebugEnabled()) {
+                LOG.debug("Cluster underloaded in run! Stressing...");
+              }
+              try {
+                //TODO This in-line read can block submission for large jobs.
+                final JobStory job = getNextJobFiltered();
+                if (null == job) {
+                  return;
+                }
+                if (LOG.isDebugEnabled()) {
+                  LOG.debug("Job Selected: " + job.getJobID());
+                }
+                submitter.add(
+                  jobCreator.createGridmixJob(
+                    conf, 0L, job, scratch, 
+                    userResolver.getTargetUgi(
+                      UserGroupInformation.createRemoteUser(job.getUser())), 
+                    sequence.getAndIncrement()));
+                // TODO: We need to take care of scenario when one map/reduce
+                // takes more than 1 slot.
+                
+                // Lock the loadjob as we are making updates
+                int incompleteMapTasks = (int) calcEffectiveIncompleteMapTasks(
+                                                 loadStatus.getMapCapacity(), 
+                                                 job.getNumberMaps(), 0.0f);
+                loadStatus.decrementMapLoad(incompleteMapTasks);
+                
+                int incompleteReduceTasks = 
+                  (int) calcEffectiveIncompleteReduceTasks(
+                          loadStatus.getReduceCapacity(), 
+                          job.getNumberReduces(), 0.0f);
+                loadStatus.decrementReduceLoad(incompleteReduceTasks);
+                  
+                loadStatus.decrementJobLoad(1);
+              } catch (IOException e) {
+                LOG.error("Error while submitting the job ", e);
+                error = e;
+                return;
+              }
+
+            }
+          } finally {
+            // do nothing
+          }
+        }
+      } catch (InterruptedException e) {
+        return;
+      } finally {
+        IOUtils.cleanup(null, jobProducer);
+      }
+    }
+  }
+
+  /**
+   * STRESS Once you get the notification from StatsCollector.Collect the
+   * clustermetrics. Update current loadStatus with new load status of JT.
+   *
+   * @param item
+   */
+  @Override
+  public void update(Statistics.ClusterStats item) {
+    ClusterStatus clusterMetrics = item.getStatus();
+    try {
+      checkLoadAndGetSlotsToBackfill(item, clusterMetrics);
+    } catch (Exception e) {
+      LOG.error("Couldn't get the new Status",e);
+    }
+  }
+
+  float calcEffectiveIncompleteMapTasks(int mapSlotCapacity,
+      int numMaps, float mapProgress) {
+    float maxEffIncompleteMapTasks = Math.max(1.0f, mapSlotCapacity
+        * maxMapSlotSharePerJob);
+    float mapProgressAdjusted = Math.max(Math.min(mapProgress, 1.0f), 0.0f);
+    return Math.min(maxEffIncompleteMapTasks, 
+                    numMaps * (1.0f - mapProgressAdjusted));
+  }
+
+  float calcEffectiveIncompleteReduceTasks(int reduceSlotCapacity,
+      int numReduces, float reduceProgress) {
+    float maxEffIncompleteReduceTasks = Math.max(1.0f, reduceSlotCapacity
+        * maxReduceSlotSharePerJob);
+    float reduceProgressAdjusted = 
+      Math.max(Math.min(reduceProgress, 1.0f), 0.0f);
+    return Math.min(maxEffIncompleteReduceTasks, 
+                    numReduces * (1.0f - reduceProgressAdjusted));
+  }
+
+  /**
+   * We try to use some light-weight mechanism to determine cluster load.
+   *
+   * @param stats
+   * @param clusterStatus Cluster status
+   * @throws java.io.IOException
+   */
+  private void checkLoadAndGetSlotsToBackfill(
+    ClusterStats stats, ClusterStatus clusterStatus) throws IOException, InterruptedException {
+    
+    // update the max cluster capacity incase its updated
+    int mapCapacity = clusterStatus.getMaxMapTasks();
+    loadStatus.updateMapCapacity(mapCapacity);
+    
+    int reduceCapacity = clusterStatus.getMaxReduceTasks();
+    
+    loadStatus.updateReduceCapacity(reduceCapacity);
+    
+    int numTrackers = clusterStatus.getTaskTrackers();
+    
+    int jobLoad = 
+      (int) (maxJobTrackerRatio * numTrackers) - stats.getNumRunningJob();
+    loadStatus.updateJobLoad(jobLoad);
+    if (loadStatus.getJobLoad() <= 0) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is "
+                  + Boolean.TRUE.toString() + " NumJobsBackfill is "
+                  + loadStatus.getJobLoad());
+      }
+      return; // stop calculation because we know it is overloaded.
+    }
+
+    float incompleteMapTasks = 0; // include pending & running map tasks.
+    for (JobStats job : ClusterStats.getRunningJobStats()) {
+      float mapProgress = job.getJob().mapProgress();
+      int noOfMaps = job.getNoOfMaps();
+      incompleteMapTasks += 
+        calcEffectiveIncompleteMapTasks(mapCapacity, noOfMaps, mapProgress);
+    }
+    
+    int mapSlotsBackFill = 
+      (int) ((overloadMapTaskMapSlotRatio * mapCapacity) - incompleteMapTasks);
+    loadStatus.updateMapLoad(mapSlotsBackFill);
+    
+    if (loadStatus.getMapLoad() <= 0) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(System.currentTimeMillis() + " [MAP-LOAD] Overloaded is "
+                  + Boolean.TRUE.toString() + " MapSlotsBackfill is "
+                  + loadStatus.getMapLoad());
+      }
+      return; // stop calculation because we know it is overloaded.
+    }
+
+    float incompleteReduceTasks = 0; // include pending & running reduce tasks.
+    for (JobStats job : ClusterStats.getRunningJobStats()) {
+      // Cached the num-reds value in JobStats
+      int noOfReduces = job.getNoOfReds();
+      if (noOfReduces > 0) {
+        float reduceProgress = job.getJob().reduceProgress();
+        incompleteReduceTasks += 
+          calcEffectiveIncompleteReduceTasks(reduceCapacity, noOfReduces, 
+                                             reduceProgress);
+      }
+    }
+    
+    int reduceSlotsBackFill = 
+      (int)((overloadReduceTaskReduceSlotRatio * reduceCapacity) 
+             - incompleteReduceTasks);
+    loadStatus.updateReduceLoad(reduceSlotsBackFill);
+    if (loadStatus.getReduceLoad() <= 0) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(System.currentTimeMillis() + " [REDUCE-LOAD] Overloaded is "
+                  + Boolean.TRUE.toString() + " ReduceSlotsBackfill is "
+                  + loadStatus.getReduceLoad());
+      }
+      return; // stop calculation because we know it is overloaded.
+    }
+
+    if (LOG.isDebugEnabled()) {
+      LOG.debug(System.currentTimeMillis() + " [OVERALL] Overloaded is "
+                + Boolean.FALSE.toString() + "Current load Status is " 
+                + loadStatus);
+    }
+  }
+
+  static class LoadStatus {
+    /**
+     * Additional number of map slots that can be requested before
+     * declaring (by Gridmix STRESS mode) the cluster as overloaded. 
+     */
+    private volatile int mapSlotsBackfill;
+    
+    /**
+     * Determines the total map slot capacity of the cluster.
+     */
+    private volatile int mapSlotCapacity;
+    
+    /**
+     * Additional number of reduce slots that can be requested before
+     * declaring (by Gridmix STRESS mode) the cluster as overloaded.
+     */
+    private volatile int reduceSlotsBackfill;
+    
+    /**
+     * Determines the total reduce slot capacity of the cluster.
+     */
+    private volatile int reduceSlotCapacity;
+
+    /**
+     * Determines the max count of running jobs in the cluster.
+     */
+    private volatile int numJobsBackfill;
+    
+    // set the default to true
+    private AtomicBoolean overloaded = new AtomicBoolean(true);
+
+    /**
+     * Construct the LoadStatus in an unknown state - assuming the cluster is
+     * overloaded by setting numSlotsBackfill=0.
+     */
+    LoadStatus() {
+      mapSlotsBackfill = 0;
+      reduceSlotsBackfill = 0;
+      numJobsBackfill = 0;
+      
+      mapSlotCapacity = -1;
+      reduceSlotCapacity = -1;
+    }
+    
+    public synchronized int getMapLoad() {
+      return mapSlotsBackfill;
+    }
+    
+    public synchronized int getMapCapacity() {
+      return mapSlotCapacity;
+    }
+    
+    public synchronized int getReduceLoad() {
+      return reduceSlotsBackfill;
+    }
+    
+    public synchronized int getReduceCapacity() {
+      return reduceSlotCapacity;
+    }
+    
+    public synchronized int getJobLoad() {
+      return numJobsBackfill;
+    }
+    
+    public synchronized void decrementMapLoad(int mapSlotsConsumed) {
+      this.mapSlotsBackfill -= mapSlotsConsumed;
+      updateOverloadStatus();
+    }
+    
+    public synchronized void decrementReduceLoad(int reduceSlotsConsumed) {
+      this.reduceSlotsBackfill -= reduceSlotsConsumed;
+      updateOverloadStatus();
+    }
+
+    public synchronized void decrementJobLoad(int numJobsConsumed) {
+      this.numJobsBackfill -= numJobsConsumed;
+      updateOverloadStatus();
+    }
+    
+    public synchronized void updateMapCapacity(int mapSlotsCapacity) {
+      this.mapSlotCapacity = mapSlotsCapacity;
+      updateOverloadStatus();
+    }
+    
+    public synchronized void updateReduceCapacity(int reduceSlotsCapacity) {
+      this.reduceSlotCapacity = reduceSlotsCapacity;
+      updateOverloadStatus();
+    }
+    
+    public synchronized void updateMapLoad(int mapSlotsBackfill) {
+      this.mapSlotsBackfill = mapSlotsBackfill;
+      updateOverloadStatus();
+    }
+    
+    public synchronized void updateReduceLoad(int reduceSlotsBackfill) {
+      this.reduceSlotsBackfill = reduceSlotsBackfill;
+      updateOverloadStatus();
+    }
+    
+    public synchronized void updateJobLoad(int numJobsBackfill) {
+      this.numJobsBackfill = numJobsBackfill;
+      updateOverloadStatus();
+    }
+    
+    private synchronized void updateOverloadStatus() {
+      overloaded.set((mapSlotsBackfill <= 0) || (reduceSlotsBackfill <= 0)
+                     || (numJobsBackfill <= 0));
+    }
+    
+    public synchronized boolean overloaded() {
+      return overloaded.get();
+    }
+    
+    public synchronized String toString() {
+    // TODO Use StringBuilder instead
+      return " Overloaded = " + overloaded()
+             + ", MapSlotBackfill = " + mapSlotsBackfill 
+             + ", MapSlotCapacity = " + mapSlotCapacity
+             + ", ReduceSlotBackfill = " + reduceSlotsBackfill 
+             + ", ReduceSlotCapacity = " + reduceSlotCapacity
+             + ", NumJobsBackfill = " + numJobsBackfill;
+    }
+  }
+
+  /**
+   * Start the reader thread, wait for latch if necessary.
+   */
+  @Override
+  public void start() {
+    LOG.info(" Starting Stress submission ");
+    this.rThread.start();
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
new file mode 100644
index 0000000..d0d552a
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.net.URI;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Resolves all UGIs to the submitting user.
+ */
+public class SubmitterUserResolver implements UserResolver {
+  public static final Log LOG = LogFactory.getLog(SubmitterUserResolver.class);
+  
+  private UserGroupInformation ugi = null;
+
+  public SubmitterUserResolver() throws IOException {
+    LOG.info(" Current user resolver is SubmitterUserResolver ");
+    ugi = UserGroupInformation.getLoginUser();
+  }
+
+  public synchronized boolean setTargetUsers(URI userdesc, Configuration conf)
+      throws IOException {
+    return false;
+  }
+
+  public synchronized UserGroupInformation getTargetUgi(
+      UserGroupInformation ugi) {
+    return this.ugi;
+  }
+
+  /**
+   * {@inheritDoc}
+   * <p>
+   * Since {@link SubmitterUserResolver} returns the user name who is running
+   * gridmix, it doesn't need a target list of users.
+   */
+  public boolean needsTargetUsersList() {
+    return false;
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
new file mode 100644
index 0000000..16026f2
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
+
+/**
+ * Summarizes various aspects of a {@link Gridmix} run.
+ */
+class Summarizer {
+  private ExecutionSummarizer executionSummarizer;
+  private ClusterSummarizer clusterSummarizer;
+  protected static final String NA = "N/A";
+  
+  Summarizer() {
+    this(new String[]{NA});
+  }
+  
+  Summarizer(String[] args) {
+    executionSummarizer = new ExecutionSummarizer(args);
+    clusterSummarizer = new ClusterSummarizer();
+  }
+  
+  ExecutionSummarizer getExecutionSummarizer() {
+    return executionSummarizer;
+  }
+  
+  ClusterSummarizer getClusterSummarizer() {
+    return clusterSummarizer;
+  }
+  
+  void start(Configuration conf) {
+    executionSummarizer.start(conf);
+    clusterSummarizer.start(conf);
+  }
+  
+  /**
+   * This finalizes the summarizer.
+   */
+  @SuppressWarnings("unchecked")
+  void finalize(JobFactory factory, String path, long size, 
+                UserResolver resolver, DataStatistics stats, Configuration conf)
+  throws IOException {
+    executionSummarizer.finalize(factory, path, size, resolver, stats, conf);
+  }
+  
+  /**
+   * Summarizes the current {@link Gridmix} run and the cluster used. 
+   */
+  @Override
+  public String toString() {
+    StringBuilder builder = new StringBuilder();
+    builder.append(executionSummarizer.toString());
+    builder.append(clusterSummarizer.toString());
+    return builder.toString();
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
new file mode 100644
index 0000000..ca8c98b
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.net.URI;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * Maps users in the trace to a set of valid target users on the test cluster.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public interface UserResolver {
+
+  /**
+   * Configure the user map given the URI and configuration. The resolver's
+   * contract will define how the resource will be interpreted, but the default
+   * will typically interpret the URI as a {@link org.apache.hadoop.fs.Path}
+   * listing target users.
+   * This method should be called only if {@link #needsTargetUsersList()}
+   * returns true.
+   * @param userdesc URI from which user information may be loaded per the
+   * subclass contract.
+   * @param conf The tool configuration.
+   * @return true if the resource provided was used in building the list of
+   * target users
+   */
+  public boolean setTargetUsers(URI userdesc, Configuration conf)
+    throws IOException;
+
+  /**
+   * Map the given UGI to another per the subclass contract.
+   * @param ugi User information from the trace.
+   */
+  public UserGroupInformation getTargetUgi(UserGroupInformation ugi);
+
+  /**
+   * Indicates whether this user resolver needs a list of target users to be
+   * provided.
+   *
+   * @return true if a list of target users is to be provided for this
+   * user resolver
+   */
+  public boolean needsTargetUsersList();
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
new file mode 100644
index 0000000..8f4af1a
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
@@ -0,0 +1,315 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.gridmix.Progressive;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+
+/**
+ * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the cumulative CPU 
+ * usage by performing certain CPU intensive operations. Performing such CPU 
+ * intensive operations essentially uses up some CPU. Every 
+ * {@link ResourceUsageEmulatorPlugin} is configured with a feedback module i.e 
+ * a {@link ResourceCalculatorPlugin}, to monitor the resource usage.</p>
+ * 
+ * <p>{@link CumulativeCpuUsageEmulatorPlugin} emulates the CPU usage in steps. 
+ * The frequency of emulation can be configured via 
+ * {@link #CPU_EMULATION_PROGRESS_INTERVAL}.
+ * CPU usage values are matched via emulation only on the interval boundaries.
+ * </p>
+ *  
+ * {@link CumulativeCpuUsageEmulatorPlugin} is a wrapper program for managing 
+ * the CPU usage emulation feature. It internally uses an emulation algorithm 
+ * (called as core and described using {@link CpuUsageEmulatorCore}) for 
+ * performing the actual emulation. Multiple calls to this core engine should 
+ * use up some amount of CPU.<br>
+ * 
+ * <p>{@link CumulativeCpuUsageEmulatorPlugin} provides a calibration feature 
+ * via {@link #initialize(Configuration, ResourceUsageMetrics, 
+ *                        ResourceCalculatorPlugin, Progressive)} to calibrate 
+ *  the plugin and its core for the underlying hardware. As a result of 
+ *  calibration, every call to the emulation engine's core should roughly use up
+ *  1% of the total usage value to be emulated. This makes sure that the 
+ *  underlying hardware is profiled before use and that the plugin doesn't 
+ *  accidently overuse the CPU. With 1% as the unit emulation target value for 
+ *  the core engine, there will be roughly 100 calls to the engine resulting in 
+ *  roughly 100 calls to the feedback (resource usage monitor) module. 
+ *  Excessive usage of the feedback module is discouraged as 
+ *  it might result into excess CPU usage resulting into no real CPU emulation.
+ *  </p>
+ */
+public class CumulativeCpuUsageEmulatorPlugin 
+implements ResourceUsageEmulatorPlugin {
+  protected CpuUsageEmulatorCore emulatorCore;
+  private ResourceCalculatorPlugin monitor;
+  private Progressive progress;
+  private boolean enabled = true;
+  private float emulationInterval; // emulation interval
+  private long targetCpuUsage = 0;
+  private float lastSeenProgress = 0;
+  private long lastSeenCpuUsageCpuUsage = 0;
+  
+  // Configuration parameters
+  public static final String CPU_EMULATION_PROGRESS_INTERVAL = 
+    "gridmix.emulators.resource-usage.cpu.emulation-interval";
+  private static final float DEFAULT_EMULATION_FREQUENCY = 0.1F; // 10 times
+
+  /**
+   * This is the core CPU usage emulation algorithm. This is the core engine
+   * which actually performs some CPU intensive operations to consume some
+   * amount of CPU. Multiple calls of {@link #compute()} should help the 
+   * plugin emulate the desired level of CPU usage. This core engine can be
+   * calibrated using the {@link #calibrate(ResourceCalculatorPlugin, long)}
+   * API to suit the underlying hardware better. It also can be used to optimize
+   * the emulation cycle.
+   */
+  public interface CpuUsageEmulatorCore {
+    /**
+     * Performs some computation to use up some CPU.
+     */
+    public void compute();
+    
+    /**
+     * Allows the core to calibrate itself.
+     */
+    public void calibrate(ResourceCalculatorPlugin monitor, 
+                          long totalCpuUsage);
+  }
+  
+  /**
+   * This is the core engine to emulate the CPU usage. The only responsibility 
+   * of this class is to perform certain math intensive operations to make sure 
+   * that some desired value of CPU is used.
+   */
+  public static class DefaultCpuUsageEmulator implements CpuUsageEmulatorCore {
+    // number of times to loop for performing the basic unit computation
+    private int numIterations;
+    private final Random random;
+    
+    /**
+     * This is to fool the JVM and make it think that we need the value 
+     * stored in the unit computation i.e {@link #compute()}. This will prevent
+     * the JVM from optimizing the code.
+     */
+    protected double returnValue;
+    
+    /**
+     * Initialized the {@link DefaultCpuUsageEmulator} with default values. 
+     * Note that the {@link DefaultCpuUsageEmulator} should be calibrated 
+     * (see {@link #calibrate(ResourceCalculatorPlugin, long)}) when initialized
+     * using this constructor.
+     */
+    public DefaultCpuUsageEmulator() {
+      this(-1);
+    }
+    
+    DefaultCpuUsageEmulator(int numIterations) {
+      this.numIterations = numIterations;
+      random = new Random();
+    }
+    
+    /**
+     * This will consume some desired level of CPU. This API will try to use up
+     * 'X' percent of the target cumulative CPU usage. Currently X is set to 
+     * 10%.
+     */
+    public void compute() {
+      for (int i = 0; i < numIterations; ++i) {
+        performUnitComputation();
+      }
+    }
+    
+    // Perform unit computation. The complete CPU emulation will be based on 
+    // multiple invocations to this unit computation module.
+    protected void performUnitComputation() {
+      //TODO can this be configurable too. Users/emulators should be able to 
+      // pick and choose what MATH operations to run.
+      // Example :
+      //           BASIC : ADD, SUB, MUL, DIV
+      //           ADV   : SQRT, SIN, COSIN..
+      //           COMPO : (BASIC/ADV)*
+      // Also define input generator. For now we can use the random number 
+      // generator. Later this can be changed to accept multiple sources.
+      
+      int randomData = random.nextInt();
+      int randomDataCube = randomData * randomData * randomData;
+      double randomDataCubeRoot = Math.cbrt(randomData);
+      returnValue = Math.log(Math.tan(randomDataCubeRoot 
+                                      * Math.exp(randomDataCube)) 
+                             * Math.sqrt(randomData));
+    }
+    
+    /**
+     * This will calibrate the algorithm such that a single invocation of
+     * {@link #compute()} emulates roughly 1% of the total desired resource 
+     * usage value.
+     */
+    public void calibrate(ResourceCalculatorPlugin monitor, 
+                          long totalCpuUsage) {
+      long initTime = monitor.getProcResourceValues().getCumulativeCpuTime();
+      
+      long defaultLoopSize = 0;
+      long finalTime = initTime;
+      
+      //TODO Make this configurable
+      while (finalTime - initTime < 100) { // 100 ms
+        ++defaultLoopSize;
+        performUnitComputation(); //perform unit computation
+        finalTime = monitor.getProcResourceValues().getCumulativeCpuTime();
+      }
+      
+      long referenceRuntime = finalTime - initTime;
+      
+      // time for one loop = (final-time - init-time) / total-loops
+      float timePerLoop = ((float)referenceRuntime) / defaultLoopSize;
+      
+      // compute the 1% of the total CPU usage desired
+      //TODO Make this configurable
+      long onePercent = totalCpuUsage / 100;
+      
+      // num-iterations for 1% = (total-desired-usage / 100) / time-for-one-loop
+      numIterations = Math.max(1, (int)((float)onePercent/timePerLoop));
+      
+      System.out.println("Calibration done. Basic computation runtime : " 
+          + timePerLoop + " milliseconds. Optimal number of iterations (1%): " 
+          + numIterations);
+    }
+  }
+  
+  public CumulativeCpuUsageEmulatorPlugin() {
+    this(new DefaultCpuUsageEmulator());
+  }
+  
+  /**
+   * For testing.
+   */
+  public CumulativeCpuUsageEmulatorPlugin(CpuUsageEmulatorCore core) {
+    emulatorCore = core;
+  }
+  
+  // Note that this weighing function uses only the current progress. In future,
+  // this might depend on progress, emulation-interval and expected target.
+  private float getWeightForProgressInterval(float progress) {
+    // we want some kind of exponential growth function that gives less weight
+    // on lower progress boundaries but high (exact emulation) near progress 
+    // value of 1.
+    // so here is how the current growth function looks like
+    //    progress    weight
+    //      0.1       0.0001
+    //      0.2       0.0016
+    //      0.3       0.0081
+    //      0.4       0.0256
+    //      0.5       0.0625
+    //      0.6       0.1296
+    //      0.7       0.2401
+    //      0.8       0.4096
+    //      0.9       0.6561
+    //      1.0       1.000
+    
+    return progress * progress * progress * progress;
+  }
+  
+  @Override
+  //TODO Multi-threading for speedup?
+  public void emulate() throws IOException, InterruptedException {
+    if (enabled) {
+      float currentProgress = progress.getProgress();
+      if (lastSeenProgress < currentProgress 
+          && ((currentProgress - lastSeenProgress) >= emulationInterval
+              || currentProgress == 1)) {
+        // Estimate the final cpu usage
+        //
+        //   Consider the following
+        //     Cl/Cc/Cp : Last/Current/Projected Cpu usage
+        //     Pl/Pc/Pp : Last/Current/Projected progress
+        //   Then
+        //     (Cp-Cc)/(Pp-Pc) = (Cc-Cl)/(Pc-Pl)
+        //   Solving this for Cp, we get
+        //     Cp = Cc + (1-Pc)*(Cc-Cl)/Pc-Pl)
+        //   Note that (Cc-Cl)/(Pc-Pl) is termed as 'rate' in the following 
+        //   section
+        
+        long currentCpuUsage = 
+          monitor.getProcResourceValues().getCumulativeCpuTime();
+        // estimate the cpu usage rate
+        float rate = (currentCpuUsage - lastSeenCpuUsageCpuUsage)
+                     / (currentProgress - lastSeenProgress);
+        long projectedUsage = 
+          currentCpuUsage + (long)((1 - currentProgress) * rate);
+        
+        if (projectedUsage < targetCpuUsage) {
+          // determine the correction factor between the current usage and the
+          // expected usage and add some weight to the target
+          long currentWeighedTarget = 
+            (long)(targetCpuUsage 
+                   * getWeightForProgressInterval(currentProgress));
+          
+          while (monitor.getProcResourceValues().getCumulativeCpuTime() 
+                 < currentWeighedTarget) {
+            emulatorCore.compute();
+            // sleep for 100ms
+            try {
+              Thread.sleep(100);
+            } catch (InterruptedException ie) {
+              String message = 
+                "CumulativeCpuUsageEmulatorPlugin got interrupted. Exiting.";
+              throw new RuntimeException(message);
+            }
+          }
+        }
+        
+        // set the last seen progress
+        lastSeenProgress = progress.getProgress();
+        // set the last seen usage
+        lastSeenCpuUsageCpuUsage = 
+          monitor.getProcResourceValues().getCumulativeCpuTime();
+      }
+    }
+  }
+
+  @Override
+  public void initialize(Configuration conf, ResourceUsageMetrics metrics,
+                         ResourceCalculatorPlugin monitor,
+                         Progressive progress) {
+    // get the target CPU usage
+    targetCpuUsage = metrics.getCumulativeCpuUsage();
+    if (targetCpuUsage <= 0 ) {
+      enabled = false;
+      return;
+    } else {
+      enabled = true;
+    }
+    
+    this.monitor = monitor;
+    this.progress = progress;
+    emulationInterval =  conf.getFloat(CPU_EMULATION_PROGRESS_INTERVAL, 
+                                       DEFAULT_EMULATION_FREQUENCY);
+    
+    // calibrate the core cpu-usage utility
+    emulatorCore.calibrate(monitor, targetCpuUsage);
+    
+    // initialize the states
+    lastSeenProgress = 0;
+    lastSeenCpuUsageCpuUsage = 0;
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
new file mode 100644
index 0000000..7d40cfd
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
+
+import java.io.IOException;
+
+import org.apache.hadoop.mapred.gridmix.Progressive;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * <p>Each resource to be emulated should have a corresponding implementation 
+ * class that implements {@link ResourceUsageEmulatorPlugin}.</p>
+ * <br><br>
+ * {@link ResourceUsageEmulatorPlugin} will be configured using the 
+ * {@link #initialize(Configuration, ResourceUsageMetrics, 
+ *                    ResourceCalculatorPlugin, Progressive)} call.
+ * Every 
+ * {@link ResourceUsageEmulatorPlugin} is also configured with a feedback module
+ * i.e a {@link ResourceCalculatorPlugin}, to monitor the current resource 
+ * usage. {@link ResourceUsageMetrics} decides the final resource usage value to
+ * emulate. {@link Progressive} keeps track of the task's progress.</p>
+ * 
+ * <br><br>
+ * 
+ * For configuring GridMix to load and and use a resource usage emulator, 
+ * see {@link ResourceUsageMatcher}. 
+ */
+public interface ResourceUsageEmulatorPlugin {
+  /**
+   * Initialize the plugin. This might involve
+   *   - initializing the variables
+   *   - calibrating the plugin
+   */
+  void initialize(Configuration conf, ResourceUsageMetrics metrics, 
+                  ResourceCalculatorPlugin monitor,
+                  Progressive progress);
+
+  /**
+   * Emulate the resource usage to match the usage target. The plugin can use
+   * the given {@link ResourceCalculatorPlugin} to query for the current 
+   * resource usage.
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  void emulate() throws IOException, InterruptedException;
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
new file mode 100644
index 0000000..917cd09
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.gridmix.Progressive;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * <p>This is the driver class for managing all the resource usage emulators.
+ * {@link ResourceUsageMatcher} expects a comma separated list of 
+ * {@link ResourceUsageEmulatorPlugin} implementations specified using 
+ * {@link #RESOURCE_USAGE_EMULATION_PLUGINS} as the configuration parameter.</p>
+ * 
+ * <p>Note that the order in which the emulators are invoked is same as the 
+ * order in which they are configured.
+ */
+public class ResourceUsageMatcher {
+  /**
+   * Configuration key to set resource usage emulators.
+   */
+  public static final String RESOURCE_USAGE_EMULATION_PLUGINS =
+    "gridmix.emulators.resource-usage.plugins";
+  
+  private List<ResourceUsageEmulatorPlugin> emulationPlugins = 
+    new ArrayList<ResourceUsageEmulatorPlugin>();
+  
+  /**
+   * Configure the {@link ResourceUsageMatcher} to load the configured plugins
+   * and initialize them.
+   */
+  @SuppressWarnings("unchecked")
+  public void configure(Configuration conf, ResourceCalculatorPlugin monitor, 
+                        ResourceUsageMetrics metrics, Progressive progress) {
+    Class[] plugins = conf.getClasses(RESOURCE_USAGE_EMULATION_PLUGINS);
+    if (plugins == null) {
+      System.out.println("No resource usage emulator plugins configured.");
+    } else {
+      for (Class clazz : plugins) {
+        if (clazz != null) {
+          if (ResourceUsageEmulatorPlugin.class.isAssignableFrom(clazz)) {
+            ResourceUsageEmulatorPlugin plugin = 
+              (ResourceUsageEmulatorPlugin) ReflectionUtils.newInstance(clazz, 
+                                                                        conf);
+            emulationPlugins.add(plugin);
+          } else {
+            throw new RuntimeException("Misconfigured resource usage plugins. " 
+                + "Class " + clazz.getClass().getName() + " is not a resource "
+                + "usage plugin as it does not extend "
+                + ResourceUsageEmulatorPlugin.class.getName());
+          }
+        }
+      }
+    }
+
+    // initialize the emulators once all the configured emulator plugins are
+    // loaded
+    for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
+      emulator.initialize(conf, metrics, monitor, progress);
+    }
+  }
+  
+  public void matchResourceUsage() throws Exception {
+    for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
+      // match the resource usage
+      emulator.emulate();
+    }
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
new file mode 100644
index 0000000..a50358a
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
@@ -0,0 +1,258 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.gridmix.Progressive;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+
+/**
+ * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the total heap 
+ * usage by loading the JVM heap memory. Adding smaller chunks of data to the 
+ * heap will essentially use up some heap space thus forcing the JVM to expand 
+ * its heap and thus resulting into increase in the heap usage.</p>
+ * 
+ * <p>{@link TotalHeapUsageEmulatorPlugin} emulates the heap usage in steps. 
+ * The frequency of emulation can be configured via 
+ * {@link #HEAP_EMULATION_PROGRESS_INTERVAL}.
+ * Heap usage values are matched via emulation only at specific interval 
+ * boundaries.
+ * </p>
+ *  
+ * {@link TotalHeapUsageEmulatorPlugin} is a wrapper program for managing 
+ * the heap usage emulation feature. It internally uses an emulation algorithm 
+ * (called as core and described using {@link HeapUsageEmulatorCore}) for 
+ * performing the actual emulation. Multiple calls to this core engine should 
+ * use up some amount of heap.
+ */
+public class TotalHeapUsageEmulatorPlugin 
+implements ResourceUsageEmulatorPlugin {
+  // Configuration parameters
+  //  the core engine to emulate heap usage
+  protected HeapUsageEmulatorCore emulatorCore;
+  //  the progress bar
+  private Progressive progress;
+  //  decides if this plugin can emulate heap usage or not
+  private boolean enabled = true;
+  //  the progress boundaries/interval where emulation should be done
+  private float emulationInterval;
+  //  target heap usage to emulate
+  private long targetHeapUsageInMB = 0;
+  
+  /**
+   * The frequency (based on task progress) with which memory-emulation code is
+   * run. If the value is set to 0.1 then the emulation will happen at 10% of 
+   * the task's progress. The default value of this parameter is 
+   * {@link #DEFAULT_EMULATION_PROGRESS_INTERVAL}.
+   */
+  public static final String HEAP_EMULATION_PROGRESS_INTERVAL = 
+    "gridmix.emulators.resource-usage.heap.emulation-interval";
+  
+  // Default value for emulation interval
+  private static final float DEFAULT_EMULATION_PROGRESS_INTERVAL = 0.1F; // 10 %
+
+  private float prevEmulationProgress = 0F;
+  
+  /**
+   * The minimum buffer reserved for other non-emulation activities.
+   */
+  public static final String MIN_HEAP_FREE_RATIO = 
+    "gridmix.emulators.resource-usage.heap.min-free-ratio";
+  
+  private float minFreeHeapRatio;
+  
+  private static final float DEFAULT_MIN_FREE_HEAP_RATIO = 0.3F;
+  
+  /**
+   * Determines the unit increase per call to the core engine's load API. This
+   * is expressed as a percentage of the difference between the expected total 
+   * heap usage and the current usage. 
+   */
+  public static final String HEAP_LOAD_RATIO = 
+    "gridmix.emulators.resource-usage.heap.load-ratio";
+  
+  private float heapLoadRatio;
+  
+  private static final float DEFAULT_HEAP_LOAD_RATIO = 0.1F;
+  
+  public static int ONE_MB = 1024 * 1024;
+  
+  /**
+   * Defines the core heap usage emulation algorithm. This engine is expected
+   * to perform certain memory intensive operations to consume some
+   * amount of heap. {@link #load(long)} should load the current heap and 
+   * increase the heap usage by the specified value. This core engine can be 
+   * initialized using the {@link #initialize(ResourceCalculatorPlugin, long)} 
+   * API to suit the underlying hardware better.
+   */
+  public interface HeapUsageEmulatorCore {
+    /**
+     * Performs some memory intensive operations to use up some heap.
+     */
+    public void load(long sizeInMB);
+    
+    /**
+     * Initialize the core.
+     */
+    public void initialize(ResourceCalculatorPlugin monitor, 
+                           long totalHeapUsageInMB);
+    
+    /**
+     * Reset the resource usage
+     */
+    public void reset();
+  }
+  
+  /**
+   * This is the core engine to emulate the heap usage. The only responsibility 
+   * of this class is to perform certain memory intensive operations to make 
+   * sure that some desired value of heap is used.
+   */
+  public static class DefaultHeapUsageEmulator 
+  implements HeapUsageEmulatorCore {
+    // store the unit loads in a list
+    protected static ArrayList<Object> heapSpace = new ArrayList<Object>();
+    
+    /**
+     * Increase heap usage by current process by the given amount.
+     * This is done by creating objects each of size 1MB.
+     */
+    public void load(long sizeInMB) {
+      for (long i = 0; i < sizeInMB; ++i) {
+        // Create another String object of size 1MB
+        heapSpace.add((Object)new byte[ONE_MB]);
+      }
+    }
+    
+    /**
+     * This will initialize the core and check if the core can emulate the 
+     * desired target on the underlying hardware.
+     */
+    public void initialize(ResourceCalculatorPlugin monitor, 
+                           long totalHeapUsageInMB) {
+      long maxPhysicalMemoryInMB = monitor.getPhysicalMemorySize() / ONE_MB ;
+      if(maxPhysicalMemoryInMB < totalHeapUsageInMB) {
+        throw new RuntimeException("Total heap the can be used is " 
+            + maxPhysicalMemoryInMB 
+            + " bytes while the emulator is configured to emulate a total of " 
+            + totalHeapUsageInMB + " bytes");
+      }
+    }
+    
+    /**
+     * Clear references to all the GridMix-allocated special objects so that 
+     * heap usage is reduced.
+     */
+    @Override
+    public void reset() {
+      heapSpace.clear();
+    }
+  }
+  
+  public TotalHeapUsageEmulatorPlugin() {
+    this(new DefaultHeapUsageEmulator());
+  }
+  
+  /**
+   * For testing.
+   */
+  public TotalHeapUsageEmulatorPlugin(HeapUsageEmulatorCore core) {
+    emulatorCore = core;
+  }
+  
+  protected long getTotalHeapUsageInMB() {
+    return Runtime.getRuntime().totalMemory() / ONE_MB;
+  }
+  
+  protected long getMaxHeapUsageInMB() {
+    return Runtime.getRuntime().maxMemory() / ONE_MB;
+  }
+  
+  @Override
+  public void emulate() throws IOException, InterruptedException {
+    if (enabled) {
+      float currentProgress = progress.getProgress();
+      if (prevEmulationProgress < currentProgress 
+          && ((currentProgress - prevEmulationProgress) >= emulationInterval
+              || currentProgress == 1)) {
+
+        long maxHeapSizeInMB = getMaxHeapUsageInMB();
+        long committedHeapSizeInMB = getTotalHeapUsageInMB();
+        
+        // Increase committed heap usage, if needed
+        // Using a linear weighing function for computing the expected usage
+        long expectedHeapUsageInMB = 
+          Math.min(maxHeapSizeInMB,
+                   (long) (targetHeapUsageInMB * currentProgress));
+        if (expectedHeapUsageInMB < maxHeapSizeInMB
+            && committedHeapSizeInMB < expectedHeapUsageInMB) {
+          long bufferInMB = (long)(minFreeHeapRatio * expectedHeapUsageInMB);
+          long currentDifferenceInMB = 
+            expectedHeapUsageInMB - committedHeapSizeInMB;
+          long currentIncrementLoadSizeInMB = 
+                (long)(currentDifferenceInMB * heapLoadRatio);
+          // Make sure that at least 1 MB is incremented.
+          currentIncrementLoadSizeInMB = 
+            Math.max(1, currentIncrementLoadSizeInMB);
+          while (committedHeapSizeInMB + bufferInMB < expectedHeapUsageInMB) {
+            // add blocks in order of X% of the difference, X = 10% by default
+            emulatorCore.load(currentIncrementLoadSizeInMB);
+            committedHeapSizeInMB = getTotalHeapUsageInMB();
+          }
+        }
+        
+        // store the emulation progress boundary
+        prevEmulationProgress = currentProgress;
+      }
+      
+      // reset the core so that the garbage is reclaimed
+      emulatorCore.reset();
+    }
+  }
+
+  @Override
+  public void initialize(Configuration conf, ResourceUsageMetrics metrics,
+                         ResourceCalculatorPlugin monitor,
+                         Progressive progress) {
+    // get the target heap usage
+    targetHeapUsageInMB = metrics.getHeapUsage() / ONE_MB;
+    if (targetHeapUsageInMB <= 0 ) {
+      enabled = false;
+      return;
+    } else {
+      // calibrate the core heap-usage utility
+      emulatorCore.initialize(monitor, targetHeapUsageInMB);
+      enabled = true;
+    }
+    
+    this.progress = progress;
+    emulationInterval = 
+      conf.getFloat(HEAP_EMULATION_PROGRESS_INTERVAL, 
+                    DEFAULT_EMULATION_PROGRESS_INTERVAL);
+    
+    minFreeHeapRatio = conf.getFloat(MIN_HEAP_FREE_RATIO, 
+                                     DEFAULT_MIN_FREE_HEAP_RATIO);
+    
+    heapLoadRatio = conf.getFloat(HEAP_LOAD_RATIO, DEFAULT_HEAP_LOAD_RATIO);
+    
+    prevEmulationProgress = 0;
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
new file mode 100644
index 0000000..413dfd9
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
@@ -0,0 +1,106 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.tools.rumen.JobStory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.concurrent.CountDownLatch;
+
+
+/**
+ * Component generating random job traces for testing on a single node.
+ */
+class DebugJobFactory {
+
+  interface Debuggable {
+    ArrayList<JobStory> getSubmitted();
+  }
+
+  public static JobFactory getFactory(
+    JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
+    CountDownLatch startFlag, UserResolver resolver) throws IOException {
+    GridmixJobSubmissionPolicy policy = GridmixJobSubmissionPolicy.getPolicy(
+      conf, GridmixJobSubmissionPolicy.STRESS);
+    if (policy == GridmixJobSubmissionPolicy.REPLAY) {
+      return new DebugReplayJobFactory(
+        submitter, scratch, numJobs, conf, startFlag, resolver);
+    } else if (policy == GridmixJobSubmissionPolicy.STRESS) {
+      return new DebugStressJobFactory(
+        submitter, scratch, numJobs, conf, startFlag, resolver);
+    } else if (policy == GridmixJobSubmissionPolicy.SERIAL) {
+      return new DebugSerialJobFactory(
+        submitter, scratch, numJobs, conf, startFlag, resolver);
+
+    }
+    return null;
+  }
+
+  static class DebugReplayJobFactory extends ReplayJobFactory
+    implements Debuggable {
+    public DebugReplayJobFactory(
+      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
+      CountDownLatch startFlag, UserResolver resolver) throws IOException {
+      super(
+        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
+        startFlag, resolver);
+    }
+
+    @Override
+    public ArrayList<JobStory> getSubmitted() {
+      return ((DebugJobProducer) jobProducer).submitted;
+    }
+
+  }
+
+  static class DebugSerialJobFactory extends SerialJobFactory
+    implements Debuggable {
+    public DebugSerialJobFactory(
+      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
+      CountDownLatch startFlag, UserResolver resolver) throws IOException {
+      super(
+        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
+        startFlag, resolver);
+    }
+
+    @Override
+    public ArrayList<JobStory> getSubmitted() {
+      return ((DebugJobProducer) jobProducer).submitted;
+    }
+  }
+
+  static class DebugStressJobFactory extends StressJobFactory
+    implements Debuggable {
+    public DebugStressJobFactory(
+      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
+      CountDownLatch startFlag, UserResolver resolver) throws IOException {
+      super(
+        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
+        startFlag, resolver);
+    }
+
+    @Override
+    public ArrayList<JobStory> getSubmitted() {
+      return ((DebugJobProducer) jobProducer).submitted;
+    }
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
new file mode 100644
index 0000000..fca29af
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
@@ -0,0 +1,313 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.hadoop.mapred.TaskStatus.State;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.MapTaskAttemptInfo;
+import org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo;
+import org.apache.hadoop.tools.rumen.TaskInfo;
+import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
+import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants.Values;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.InputSplit;
+
+import java.util.ArrayList;
+import java.util.Random;
+import java.util.Arrays;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.TimeUnit;
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+
+public class DebugJobProducer implements JobStoryProducer {
+  public static final Log LOG = LogFactory.getLog(DebugJobProducer.class);
+  final ArrayList<JobStory> submitted;
+  private final Configuration conf;
+  private final AtomicInteger numJobs;
+
+  public DebugJobProducer(int numJobs, Configuration conf) {
+    super();
+    MockJob.reset();
+    this.conf = conf;
+    this.numJobs = new AtomicInteger(numJobs);
+    this.submitted = new ArrayList<JobStory>();
+  }
+
+  @Override
+  public JobStory getNextJob() throws IOException {
+    if (numJobs.getAndDecrement() > 0) {
+      final MockJob ret = new MockJob(conf);
+      submitted.add(ret);
+      return ret;
+    }
+    return null;
+  }
+
+  @Override
+  public void close() {
+  }
+
+
+  static double[] getDistr(Random r, double mindist, int size) {
+    assert 0.0 <= mindist && mindist <= 1.0;
+    final double min = mindist / size;
+    final double rem = 1.0 - min * size;
+    final double[] tmp = new double[size];
+    for (int i = 0; i < tmp.length - 1; ++i) {
+      tmp[i] = r.nextDouble() * rem;
+    }
+    tmp[tmp.length - 1] = rem;
+    Arrays.sort(tmp);
+
+    final double[] ret = new double[size];
+    ret[0] = tmp[0] + min;
+    for (int i = 1; i < size; ++i) {
+      ret[i] = tmp[i] - tmp[i - 1] + min;
+    }
+    return ret;
+  }
+
+
+  /**
+   * Generate random task data for a synthetic job.
+   */
+  static class MockJob implements JobStory {
+
+    static final int MIN_REC = 1 << 14;
+    static final int MIN_BYTES = 1 << 20;
+    static final int VAR_REC = 1 << 14;
+    static final int VAR_BYTES = 4 << 20;
+    static final int MAX_MAP = 5;
+    static final int MAX_RED = 3;
+    final Configuration conf;
+
+    static void initDist(
+      Random r, double min, int[] recs, long[] bytes, long tot_recs,
+      long tot_bytes) {
+      final double[] recs_dist = getDistr(r, min, recs.length);
+      final double[] bytes_dist = getDistr(r, min, recs.length);
+      long totalbytes = 0L;
+      int totalrecs = 0;
+      for (int i = 0; i < recs.length; ++i) {
+        recs[i] = (int) Math.round(tot_recs * recs_dist[i]);
+        bytes[i] = Math.round(tot_bytes * bytes_dist[i]);
+        totalrecs += recs[i];
+        totalbytes += bytes[i];
+      }
+      // Add/remove excess
+      recs[0] += totalrecs - tot_recs;
+      bytes[0] += totalbytes - tot_bytes;
+      if (LOG.isInfoEnabled()) {
+        LOG.info(
+          "DIST: " + Arrays.toString(recs) + " " + tot_recs + "/" + totalrecs +
+            " " + Arrays.toString(bytes) + " " + tot_bytes + "/" + totalbytes);
+      }
+    }
+
+    private static final AtomicInteger seq = new AtomicInteger(0);
+    // set timestamp in the past
+    private static final AtomicLong timestamp = new AtomicLong(
+      System.currentTimeMillis() - TimeUnit.MILLISECONDS.convert(
+        60, TimeUnit.DAYS));
+
+    private final int id;
+    private final String name;
+    private final int[] m_recsIn, m_recsOut, r_recsIn, r_recsOut;
+    private final long[] m_bytesIn, m_bytesOut, r_bytesIn, r_bytesOut;
+    private final long submitTime;
+
+    public MockJob(Configuration conf) {
+      final Random r = new Random();
+      final long seed = r.nextLong();
+      r.setSeed(seed);
+      id = seq.getAndIncrement();
+      name = String.format("MOCKJOB%06d", id);
+
+      this.conf = conf;
+      LOG.info(name + " (" + seed + ")");
+      submitTime = timestamp.addAndGet(
+        TimeUnit.MILLISECONDS.convert(
+          r.nextInt(10), TimeUnit.SECONDS));
+
+      m_recsIn = new int[r.nextInt(MAX_MAP) + 1];
+      m_bytesIn = new long[m_recsIn.length];
+      m_recsOut = new int[m_recsIn.length];
+      m_bytesOut = new long[m_recsIn.length];
+
+      r_recsIn = new int[r.nextInt(MAX_RED) + 1];
+      r_bytesIn = new long[r_recsIn.length];
+      r_recsOut = new int[r_recsIn.length];
+      r_bytesOut = new long[r_recsIn.length];
+
+      // map input
+      final long map_recs = r.nextInt(VAR_REC) + MIN_REC;
+      final long map_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
+      initDist(r, 0.5, m_recsIn, m_bytesIn, map_recs, map_bytes);
+
+      // shuffle
+      final long shuffle_recs = r.nextInt(VAR_REC) + MIN_REC;
+      final long shuffle_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
+      initDist(r, 0.5, m_recsOut, m_bytesOut, shuffle_recs, shuffle_bytes);
+      initDist(r, 0.8, r_recsIn, r_bytesIn, shuffle_recs, shuffle_bytes);
+
+      // reduce output
+      final long red_recs = r.nextInt(VAR_REC) + MIN_REC;
+      final long red_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
+      initDist(r, 0.5, r_recsOut, r_bytesOut, red_recs, red_bytes);
+
+      if (LOG.isDebugEnabled()) {
+        int iMapBTotal = 0, oMapBTotal = 0, iRedBTotal = 0, oRedBTotal = 0;
+        int iMapRTotal = 0, oMapRTotal = 0, iRedRTotal = 0, oRedRTotal = 0;
+        for (int i = 0; i < m_recsIn.length; ++i) {
+          iMapRTotal += m_recsIn[i];
+          iMapBTotal += m_bytesIn[i];
+          oMapRTotal += m_recsOut[i];
+          oMapBTotal += m_bytesOut[i];
+        }
+        for (int i = 0; i < r_recsIn.length; ++i) {
+          iRedRTotal += r_recsIn[i];
+          iRedBTotal += r_bytesIn[i];
+          oRedRTotal += r_recsOut[i];
+          oRedBTotal += r_bytesOut[i];
+        }
+        LOG.debug(
+          String.format(
+            "%s: M (%03d) %6d/%10d -> %6d/%10d" +
+              " R (%03d) %6d/%10d -> %6d/%10d @%d", name, m_bytesIn.length,
+            iMapRTotal, iMapBTotal, oMapRTotal, oMapBTotal, r_bytesIn.length,
+            iRedRTotal, iRedBTotal, oRedRTotal, oRedBTotal, submitTime));
+      }
+    }
+    @Override
+   public String getName() {
+     return name;
+    }
+
+   @Override
+   public String getUser() {
+     // Obtain user name from job configuration, if available.
+     // Otherwise use dummy user names.
+     String user = conf.get(MRJobConfig.USER_NAME);
+     if (user == null) {
+       user = String.format("foobar%d", id);
+     }
+     GridmixTestUtils.createHomeAndStagingDirectory(user, (JobConf)conf);
+     return user;
+   }
+
+   @Override
+   public JobID getJobID() {
+     return new JobID("job_mock_" + name, id);
+    }
+
+    @Override
+   public Values getOutcome() {
+     return Values.SUCCESS;
+    }
+
+   @Override
+   public long getSubmissionTime() {
+     return submitTime;
+   }
+
+   @Override
+   public int getNumberMaps() {
+     return m_bytesIn.length;
+   }
+
+   @Override
+   public int getNumberReduces() {
+     return r_bytesIn.length;
+   }
+    
+    @Override
+    public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
+      switch (taskType) {
+        case MAP:
+          return new TaskInfo(m_bytesIn[taskNumber], m_recsIn[taskNumber],
+              m_bytesOut[taskNumber], m_recsOut[taskNumber], -1);
+        case REDUCE:
+          return new TaskInfo(r_bytesIn[taskNumber], r_recsIn[taskNumber],
+              r_bytesOut[taskNumber], r_recsOut[taskNumber], -1);
+        default:
+          throw new IllegalArgumentException("Not interested");
+      }
+    }
+
+    @Override
+    public InputSplit[] getInputSplits() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public TaskAttemptInfo getTaskAttemptInfo(
+      TaskType taskType, int taskNumber, int taskAttemptNumber) {
+      switch (taskType) {
+        case MAP:
+          return new MapTaskAttemptInfo(
+            State.SUCCEEDED, 
+            new TaskInfo(
+              m_bytesIn[taskNumber], m_recsIn[taskNumber],
+              m_bytesOut[taskNumber], m_recsOut[taskNumber], -1),
+            100);
+
+        case REDUCE:
+          return new ReduceTaskAttemptInfo(
+            State.SUCCEEDED, 
+            new TaskInfo(
+              r_bytesIn[taskNumber], r_recsIn[taskNumber],
+              r_bytesOut[taskNumber], r_recsOut[taskNumber], -1),
+            100, 100, 100);
+      }
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public TaskAttemptInfo getMapTaskAttemptInfoAdjusted(
+      int taskNumber, int taskAttemptNumber, int locality) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public org.apache.hadoop.mapred.JobConf getJobConf() {
+      return new JobConf(conf);
+    }
+
+    @Override
+    public String getQueueName() {
+      String qName = "q" + ((id % 2) + 1);
+      return qName;
+    }
+    
+    public static void reset() {
+      seq.set(0);
+      timestamp.set(System.currentTimeMillis() - TimeUnit.MILLISECONDS.convert(
+        60, TimeUnit.DAYS));
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
new file mode 100644
index 0000000..49f1709
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
@@ -0,0 +1,93 @@
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;
+import org.apache.hadoop.security.Groups;
+
+import java.io.IOException;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+public class GridmixTestUtils {
+  private static final Log LOG = LogFactory.getLog(GridmixTestUtils.class);
+  static final Path DEST = new Path("/gridmix");
+  static FileSystem dfs = null;
+  static MiniDFSCluster dfsCluster = null;
+  static MiniMRCluster mrCluster = null;
+
+  public static void initCluster() throws IOException {
+    Configuration conf = new Configuration();
+    conf.set("mapred.queue.names", "default,q1,q2");
+    dfsCluster = new MiniDFSCluster(conf, 3, true, null);
+    dfs = dfsCluster.getFileSystem();
+    conf.set(JTConfig.JT_RETIREJOBS, "false");
+    mrCluster = new MiniMRCluster(3, dfs.getUri().toString(), 1, null, null, 
+                                  new JobConf(conf));
+  }
+
+  public static void shutdownCluster() throws IOException {
+    if (mrCluster != null) {
+      mrCluster.shutdown();
+    }
+    if (dfsCluster != null) {
+      dfsCluster.shutdown();
+    }
+  }
+
+  /**
+   * Methods to generate the home directory for dummy users.
+   *
+   * @param conf
+   */
+  public static void createHomeAndStagingDirectory(String user, JobConf conf) {
+    try {
+      FileSystem fs = dfsCluster.getFileSystem();
+      String path = "/user/" + user;
+      Path homeDirectory = new Path(path);
+      if(fs.exists(homeDirectory)) {
+        fs.delete(homeDirectory,true);
+      }
+      LOG.info("Creating Home directory : " + homeDirectory);
+      fs.mkdirs(homeDirectory);
+      changePermission(user,homeDirectory, fs);
+      Path stagingArea = 
+        new Path(conf.get("mapreduce.jobtracker.staging.root.dir",
+                          "/tmp/hadoop/mapred/staging"));
+      LOG.info("Creating Staging root directory : " + stagingArea);
+      fs.mkdirs(stagingArea);
+      fs.setPermission(stagingArea, new FsPermission((short) 0777));
+    } catch (IOException ioe) {
+      ioe.printStackTrace();
+    }
+  }
+
+  static void changePermission(String user, Path homeDirectory, FileSystem fs)
+    throws IOException {
+    fs.setOwner(homeDirectory, user, "");
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
new file mode 100644
index 0000000..51071a0
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
@@ -0,0 +1,564 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.DataInput;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.GzipCodec;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Utils;
+import org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil.RandomTextDataMapper;
+import org.apache.hadoop.mapred.gridmix.GenerateData.GenSplit;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+
+import static org.junit.Assert.*;
+import org.junit.Test;
+
+/**
+ * Test {@link CompressionEmulationUtil}
+ */
+public class TestCompressionEmulationUtils {
+  //TODO Remove this once LocalJobRunner can run Gridmix.
+  static class CustomInputFormat extends GenerateData.GenDataFormat {
+    @Override
+    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
+      // get the total data to be generated
+      long toGen =
+        jobCtxt.getConfiguration().getLong(GenerateData.GRIDMIX_GEN_BYTES, -1);
+      if (toGen < 0) {
+        throw new IOException("Invalid/missing generation bytes: " + toGen);
+      }
+      // get the total number of mappers configured
+      int totalMappersConfigured =
+        jobCtxt.getConfiguration().getInt(MRJobConfig.NUM_MAPS, -1);
+      if (totalMappersConfigured < 0) {
+        throw new IOException("Invalid/missing num mappers: " 
+                              + totalMappersConfigured);
+      }
+      
+      final long bytesPerTracker = toGen / totalMappersConfigured;
+      final ArrayList<InputSplit> splits = 
+        new ArrayList<InputSplit>(totalMappersConfigured);
+      for (int i = 0; i < totalMappersConfigured; ++i) {
+        splits.add(new GenSplit(bytesPerTracker, 
+                   new String[] { "tracker_local" }));
+      }
+      return splits;
+    }
+  }
+  
+  /**
+   * Test {@link RandomTextDataMapper} via {@link CompressionEmulationUtil}.
+   */
+  @Test
+  public void testRandomCompressedTextDataGenerator() throws Exception {
+    int wordSize = 10;
+    int listSize = 20;
+    long dataSize = 10*1024*1024;
+    
+    Configuration conf = new Configuration();
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
+    
+    // configure the RandomTextDataGenerator to generate desired sized data
+    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, 
+                listSize);
+    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, 
+                wordSize);
+    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);
+    conf.set("mapreduce.job.hdfs-servers", "");
+    
+    FileSystem lfs = FileSystem.getLocal(conf);
+    
+    // define the test's root temp directory
+    Path rootTempDir =
+        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
+            lfs.getUri(), lfs.getWorkingDirectory());
+
+    Path tempDir = new Path(rootTempDir, "TestRandomCompressedTextDataGenr");
+    lfs.delete(tempDir, true);
+    
+    runDataGenJob(conf, tempDir);
+    
+    // validate the output data
+    FileStatus[] files = 
+      lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());
+    long size = 0;
+    long maxLineSize = 0;
+    
+    for (FileStatus status : files) {
+      InputStream in = 
+        CompressionEmulationUtil
+          .getPossiblyDecompressedInputStream(status.getPath(), conf, 0);
+      BufferedReader reader = new BufferedReader(new InputStreamReader(in));
+      String line = reader.readLine();
+      if (line != null) {
+        long lineSize = line.getBytes().length;
+        if (lineSize > maxLineSize) {
+          maxLineSize = lineSize;
+        }
+        while (line != null) {
+          for (String word : line.split("\\s")) {
+            size += word.getBytes().length;
+          }
+          line = reader.readLine();
+        }
+      }
+      reader.close();
+    }
+
+    assertTrue(size >= dataSize);
+    assertTrue(size <= dataSize + maxLineSize);
+  }
+  
+  /**
+   * Runs a GridMix data-generation job.
+   */
+  private static void runDataGenJob(Configuration conf, Path tempDir) 
+  throws IOException, ClassNotFoundException, InterruptedException {
+    JobClient client = new JobClient(conf);
+    
+    // get the local job runner
+    conf.setInt(MRJobConfig.NUM_MAPS, 1);
+    
+    Job job = new Job(conf);
+    
+    CompressionEmulationUtil.configure(job);
+    job.setInputFormatClass(CustomInputFormat.class);
+    
+    // set the output path
+    FileOutputFormat.setOutputPath(job, tempDir);
+    
+    // submit and wait for completion
+    job.submit();
+    int ret = job.waitForCompletion(true) ? 0 : 1;
+
+    assertEquals("Job Failed", 0, ret);
+  }
+  
+  /**
+   * Test if {@link RandomTextDataGenerator} can generate random text data 
+   * with the desired compression ratio. This involves
+   *   - using {@link CompressionEmulationUtil} to configure the MR job for 
+   *     generating the random text data with the desired compression ratio
+   *   - running the MR job
+   *   - test {@link RandomTextDataGenerator}'s output and match the output size
+   *     (compressed) with the expected compression ratio.
+   */
+  private void testCompressionRatioConfigure(float ratio)
+  throws Exception {
+    long dataSize = 10*1024*1024;
+    
+    Configuration conf = new Configuration();
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
+    
+    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);
+    conf.set("mapreduce.job.hdfs-servers", "");
+    
+    float expectedRatio = CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;
+    if (ratio > 0) {
+      // set the compression ratio in the conf
+      CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);
+      expectedRatio = 
+        CompressionEmulationUtil.standardizeCompressionRatio(ratio);
+    }
+    
+    // invoke the utility to map from ratio to word-size
+    CompressionEmulationUtil.setupDataGeneratorConfig(conf);
+    
+    FileSystem lfs = FileSystem.getLocal(conf);
+    
+    // define the test's root temp directory
+    Path rootTempDir =
+        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
+            lfs.getUri(), lfs.getWorkingDirectory());
+
+    Path tempDir = 
+      new Path(rootTempDir, "TestCustomRandomCompressedTextDataGenr");
+    lfs.delete(tempDir, true);
+    
+    runDataGenJob(conf, tempDir);
+    
+    // validate the output data
+    FileStatus[] files = 
+      lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());
+    long size = 0;
+    
+    for (FileStatus status : files) {
+      size += status.getLen();
+    }
+
+    float compressionRatio = ((float)size)/dataSize;
+    float stdRatio = 
+      CompressionEmulationUtil.standardizeCompressionRatio(compressionRatio);
+    
+    assertEquals(expectedRatio, stdRatio, 0.0D);
+  }
+  
+  /**
+   * Test compression ratio with multiple compression ratios.
+   */
+  @Test
+  public void testCompressionRatios() throws Exception {
+    // test default compression ratio i.e 0.5
+    testCompressionRatioConfigure(0F);
+    // test for a sample compression ratio of 0.2
+    testCompressionRatioConfigure(0.2F);
+    // test for a sample compression ratio of 0.4
+    testCompressionRatioConfigure(0.4F);
+    // test for a sample compression ratio of 0.65
+    testCompressionRatioConfigure(0.65F);
+    // test for a compression ratio of 0.682 which should be standardized
+    // to round(0.682) i.e 0.68
+    testCompressionRatioConfigure(0.682F);
+    // test for a compression ratio of 0.567 which should be standardized
+    // to round(0.567) i.e 0.57
+    testCompressionRatioConfigure(0.567F);
+    
+    // test with a compression ratio of 0.01 which less than the min supported
+    // value of 0.07
+    boolean failed = false;
+    try {
+      testCompressionRatioConfigure(0.01F);
+    } catch (RuntimeException re) {
+      failed = true;
+    }
+    assertTrue("Compression ratio min value (0.07) check failed!", failed);
+    
+    // test with a compression ratio of 0.01 which less than the max supported
+    // value of 0.68
+    failed = false;
+    try {
+      testCompressionRatioConfigure(0.7F);
+    } catch (RuntimeException re) {
+      failed = true;
+    }
+    assertTrue("Compression ratio max value (0.68) check failed!", failed);
+  }
+  
+  /**
+   * Test compression ratio standardization.
+   */
+  @Test
+  public void testCompressionRatioStandardization() throws Exception {
+    assertEquals(0.55F, 
+        CompressionEmulationUtil.standardizeCompressionRatio(0.55F), 0.0D);
+    assertEquals(0.65F, 
+        CompressionEmulationUtil.standardizeCompressionRatio(0.652F), 0.0D);
+    assertEquals(0.78F, 
+        CompressionEmulationUtil.standardizeCompressionRatio(0.777F), 0.0D);
+    assertEquals(0.86F, 
+        CompressionEmulationUtil.standardizeCompressionRatio(0.855F), 0.0D);
+  }
+  
+  /**
+   * Test map input compression ratio configuration utilities.
+   */
+  @Test
+  public void testInputCompressionRatioConfiguration() throws Exception {
+    Configuration conf = new Configuration();
+    float ratio = 0.567F;
+    CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);
+    assertEquals(ratio, 
+        CompressionEmulationUtil.getMapInputCompressionEmulationRatio(conf), 
+        0.0D);
+  }
+  
+  /**
+   * Test map output compression ratio configuration utilities.
+   */
+  @Test
+  public void testIntermediateCompressionRatioConfiguration() 
+  throws Exception {
+    Configuration conf = new Configuration();
+    float ratio = 0.567F;
+    CompressionEmulationUtil.setMapOutputCompressionEmulationRatio(conf, ratio);
+    assertEquals(ratio, 
+        CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf), 
+        0.0D);
+  }
+  
+  /**
+   * Test reduce output compression ratio configuration utilities.
+   */
+  @Test
+  public void testOutputCompressionRatioConfiguration() throws Exception {
+    Configuration conf = new Configuration();
+    float ratio = 0.567F;
+    CompressionEmulationUtil.setReduceOutputCompressionEmulationRatio(conf, 
+                                                                      ratio);
+    assertEquals(ratio, 
+        CompressionEmulationUtil.getReduceOutputCompressionEmulationRatio(conf),
+        0.0D);
+  }
+  
+  /**
+   * Test compressible {@link GridmixRecord}.
+   */
+  @Test
+  public void testCompressibleGridmixRecord() throws IOException {
+    JobConf conf = new JobConf();
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
+    
+    FileSystem lfs = FileSystem.getLocal(conf);
+    int dataSize = 1024 * 1024 * 10; // 10 MB
+    float ratio = 0.357F;
+    
+    // define the test's root temp directory
+    Path rootTempDir =
+        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
+            lfs.getUri(), lfs.getWorkingDirectory());
+
+    Path tempDir = new Path(rootTempDir, 
+                            "TestPossiblyCompressibleGridmixRecord");
+    lfs.delete(tempDir, true);
+    
+    // define a compressible GridmixRecord
+    GridmixRecord record = new GridmixRecord(dataSize, 0);
+    record.setCompressibility(true, ratio); // enable compression
+    
+    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, 
+                  CompressionCodec.class);
+    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);
+    
+    // write the record to a file
+    Path recordFile = new Path(tempDir, "record");
+    OutputStream outStream = CompressionEmulationUtil
+                               .getPossiblyCompressedOutputStream(recordFile, 
+                                                                  conf);    
+    DataOutputStream out = new DataOutputStream(outStream);
+    record.write(out);
+    out.close();
+    outStream.close();
+    
+    // open the compressed stream for reading
+    Path actualRecordFile = recordFile.suffix(".gz");
+    InputStream in = 
+      CompressionEmulationUtil
+        .getPossiblyDecompressedInputStream(actualRecordFile, conf, 0);
+    
+    // get the compressed file size
+    long compressedFileSize = lfs.listStatus(actualRecordFile)[0].getLen();
+    
+    GridmixRecord recordRead = new GridmixRecord();
+    recordRead.readFields(new DataInputStream(in));
+    
+    assertEquals("Record size mismatch in a compressible GridmixRecord",
+                 dataSize, recordRead.getSize());
+    assertTrue("Failed to generate a compressible GridmixRecord",
+               recordRead.getSize() > compressedFileSize);
+    
+    // check if the record can generate data with the desired compression ratio
+    float seenRatio = ((float)compressedFileSize)/dataSize;
+    assertEquals(CompressionEmulationUtil.standardizeCompressionRatio(ratio), 
+        CompressionEmulationUtil.standardizeCompressionRatio(seenRatio), 1.0D);
+  }
+  
+  /**
+   * Test 
+   * {@link CompressionEmulationUtil#isCompressionEmulationEnabled(
+   *          org.apache.hadoop.conf.Configuration)}.
+   */
+  @Test
+  public void testIsCompressionEmulationEnabled() {
+    Configuration conf = new Configuration();
+    // Check default values
+    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
+    
+    // Check disabled
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
+    assertFalse(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
+    
+    // Check enabled
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
+  }
+  
+  /**
+   * Test 
+   * {@link CompressionEmulationUtil#getPossiblyDecompressedInputStream(Path, 
+   *                                   Configuration, long)}
+   *  and
+   *  {@link CompressionEmulationUtil#getPossiblyCompressedOutputStream(Path, 
+   *                                    Configuration)}.
+   */
+  @Test
+  public void testPossiblyCompressedDecompressedStreams() throws IOException {
+    JobConf conf = new JobConf();
+    FileSystem lfs = FileSystem.getLocal(conf);
+    String inputLine = "Hi Hello!";
+
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
+    conf.setBoolean(FileOutputFormat.COMPRESS, true);
+    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, 
+                  CompressionCodec.class);
+
+    // define the test's root temp directory
+    Path rootTempDir =
+        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
+            lfs.getUri(), lfs.getWorkingDirectory());
+
+    Path tempDir =
+      new Path(rootTempDir, "TestPossiblyCompressedDecompressedStreams");
+    lfs.delete(tempDir, true);
+
+    // create a compressed file
+    Path compressedFile = new Path(tempDir, "test");
+    OutputStream out = 
+      CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, 
+                                                                 conf);
+    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));
+    writer.write(inputLine);
+    writer.close();
+    
+    // now read back the data from the compressed stream
+    compressedFile = compressedFile.suffix(".gz");
+    InputStream in = 
+      CompressionEmulationUtil
+        .getPossiblyDecompressedInputStream(compressedFile, conf, 0);
+    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
+    String readLine = reader.readLine();
+    assertEquals("Compression/Decompression error", inputLine, readLine);
+    reader.close();
+  }
+  
+  /**
+   * Test if 
+   * {@link CompressionEmulationUtil#configureCompressionEmulation(
+   *        org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.JobConf)}
+   *  can extract compression related configuration parameters.
+   */
+  @Test
+  public void testExtractCompressionConfigs() {
+    JobConf source = new JobConf();
+    JobConf target = new JobConf();
+    
+    // set the default values
+    source.setBoolean(FileOutputFormat.COMPRESS, false);
+    source.set(FileOutputFormat.COMPRESS_CODEC, "MyDefaultCodec");
+    source.set(FileOutputFormat.COMPRESS_TYPE, "MyDefaultType");
+    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false); 
+    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, "MyDefaultCodec2");
+    
+    CompressionEmulationUtil.configureCompressionEmulation(source, target);
+    
+    // check default values
+    assertFalse(target.getBoolean(FileOutputFormat.COMPRESS, true));
+    assertEquals("MyDefaultCodec", target.get(FileOutputFormat.COMPRESS_CODEC));
+    assertEquals("MyDefaultType", target.get(FileOutputFormat.COMPRESS_TYPE));
+    assertFalse(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true));
+    assertEquals("MyDefaultCodec2", 
+                 target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));
+    assertFalse(CompressionEmulationUtil
+                .isInputCompressionEmulationEnabled(target));
+    
+    // set new values
+    source.setBoolean(FileOutputFormat.COMPRESS, true);
+    source.set(FileOutputFormat.COMPRESS_CODEC, "MyCodec");
+    source.set(FileOutputFormat.COMPRESS_TYPE, "MyType");
+    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true); 
+    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, "MyCodec2");
+    org.apache.hadoop.mapred.FileInputFormat.setInputPaths(source, "file.gz");
+    
+    target = new JobConf(); // reset
+    CompressionEmulationUtil.configureCompressionEmulation(source, target);
+    
+    // check new values
+    assertTrue(target.getBoolean(FileOutputFormat.COMPRESS, false));
+    assertEquals("MyCodec", target.get(FileOutputFormat.COMPRESS_CODEC));
+    assertEquals("MyType", target.get(FileOutputFormat.COMPRESS_TYPE));
+    assertTrue(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));
+    assertEquals("MyCodec2", 
+                 target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));
+    assertTrue(CompressionEmulationUtil
+               .isInputCompressionEmulationEnabled(target));
+  }
+  
+  /**
+   * Test of {@link FileQueue} can identify compressed file and provide
+   * readers to extract uncompressed data only if input-compression is enabled.
+   */
+  @Test
+  public void testFileQueueDecompression() throws IOException {
+    JobConf conf = new JobConf();
+    FileSystem lfs = FileSystem.getLocal(conf);
+    String inputLine = "Hi Hello!";
+    
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
+    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);
+    org.apache.hadoop.mapred.FileOutputFormat.setOutputCompressorClass(conf, 
+                                                GzipCodec.class);
+
+    // define the test's root temp directory
+    Path rootTempDir =
+        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
+            lfs.getUri(), lfs.getWorkingDirectory());
+
+    Path tempDir = new Path(rootTempDir, "TestFileQueueDecompression");
+    lfs.delete(tempDir, true);
+
+    // create a compressed file
+    Path compressedFile = new Path(tempDir, "test");
+    OutputStream out = 
+      CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, 
+                                                                 conf);
+    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));
+    writer.write(inputLine);
+    writer.close();
+    
+    compressedFile = compressedFile.suffix(".gz");
+    // now read back the data from the compressed stream using FileQueue
+    long fileSize = lfs.listStatus(compressedFile)[0].getLen();
+    CombineFileSplit split = 
+      new CombineFileSplit(new Path[] {compressedFile}, new long[] {fileSize});
+    FileQueue queue = new FileQueue(split, conf);
+    byte[] bytes = new byte[inputLine.getBytes().length];
+    queue.read(bytes);
+    queue.close();
+    String readLine = new String(bytes);
+    assertEquals("Compression/Decompression error", inputLine, readLine);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java
new file mode 100644
index 0000000..4be90c6
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java
@@ -0,0 +1,189 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Random;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import static org.junit.Assert.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+
+public class TestFilePool {
+
+  static final Log LOG = LogFactory.getLog(TestFileQueue.class);
+  static final int NFILES = 26;
+  static final Path base = getBaseDir();
+
+  static Path getBaseDir() {
+    try {
+      final Configuration conf = new Configuration();
+      final FileSystem fs = FileSystem.getLocal(conf).getRaw();
+      return new Path(System.getProperty("test.build.data", "/tmp"),
+          "testFilePool").makeQualified(fs);
+    } catch (IOException e) {
+      fail();
+    }
+    return null;
+  }
+
+  @BeforeClass
+  public static void setup() throws IOException {
+    final Configuration conf = new Configuration();
+    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
+    fs.delete(base, true);
+    final Random r = new Random();
+    final long seed = r.nextLong();
+    r.setSeed(seed);
+    LOG.info("seed: " + seed);
+    fs.mkdirs(base);
+    for (int i = 0; i < NFILES; ++i) {
+      Path file = base;
+      for (double d = 0.6; d > 0.0; d *= 0.8) {
+        if (r.nextDouble() < d) {
+          file = new Path(base, Integer.toString(r.nextInt(3)));
+          continue;
+        }
+        break;
+      }
+      OutputStream out = null;
+      try {
+        out = fs.create(new Path(file, "" + (char)('A' + i)));
+        final byte[] b = new byte[1024];
+        Arrays.fill(b, (byte)('A' + i));
+        for (int len = ((i % 13) + 1) * 1024; len > 0; len -= 1024) {
+          out.write(b);
+        }
+      } finally {
+        if (out != null) {
+          out.close();
+        }
+      }
+    }
+  }
+
+  @AfterClass
+  public static void cleanup() throws IOException {
+    final Configuration conf = new Configuration();
+    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
+    fs.delete(base, true);
+  }
+
+  @Test
+  public void testUnsuitable() throws Exception {
+    try {
+      final Configuration conf = new Configuration();
+      // all files 13k or less
+      conf.setLong(FilePool.GRIDMIX_MIN_FILE, 14 * 1024);
+      final FilePool pool = new FilePool(conf, base);
+      pool.refresh();
+    } catch (IOException e) {
+      return;
+    }
+    fail();
+  }
+
+  @Test
+  public void testPool() throws Exception {
+    final Random r = new Random();
+    final Configuration conf = new Configuration();
+    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
+    final FilePool pool = new FilePool(conf, base);
+    pool.refresh();
+    final ArrayList<FileStatus> files = new ArrayList<FileStatus>();
+
+    // ensure 1k, 2k files excluded
+    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
+    assertEquals(expectedPoolSize, pool.getInputFiles(Long.MAX_VALUE, files));
+    assertEquals(NFILES - 4, files.size());
+
+    // exact match
+    files.clear();
+    assertEquals(expectedPoolSize, pool.getInputFiles(expectedPoolSize, files));
+
+    // match random within 12k
+    files.clear();
+    final long rand = r.nextInt(expectedPoolSize);
+    assertTrue("Missed: " + rand,
+        (NFILES / 2) * 1024 > rand - pool.getInputFiles(rand, files));
+
+    // all files
+    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 0);
+    pool.refresh();
+    files.clear();
+    assertEquals((NFILES / 2 * (NFILES / 2 + 1)) * 1024,
+        pool.getInputFiles(Long.MAX_VALUE, files));
+  }
+
+  void checkSplitEq(FileSystem fs, CombineFileSplit split, long bytes)
+      throws Exception {
+    long splitBytes = 0L;
+    HashSet<Path> uniq = new HashSet<Path>();
+    for (int i = 0; i < split.getNumPaths(); ++i) {
+      splitBytes += split.getLength(i);
+      assertTrue(
+          split.getLength(i) <= fs.getFileStatus(split.getPath(i)).getLen());
+      assertFalse(uniq.contains(split.getPath(i)));
+      uniq.add(split.getPath(i));
+    }
+    assertEquals(bytes, splitBytes);
+  }
+
+  @Test
+  public void testStriper() throws Exception {
+    final Random r = new Random();
+    final Configuration conf = new Configuration();
+    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
+    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
+    final FilePool pool = new FilePool(conf, base) {
+      @Override
+      public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
+          throws IOException {
+        return new BlockLocation[] { new BlockLocation() };
+      }
+    };
+    pool.refresh();
+
+    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
+    final InputStriper striper = new InputStriper(pool, expectedPoolSize);
+    int last = 0;
+    for (int i = 0; i < expectedPoolSize;
+        last = Math.min(expectedPoolSize - i, r.nextInt(expectedPoolSize))) {
+      checkSplitEq(fs, striper.splitFor(pool, last, 0), last);
+      i += last;
+    }
+    final InputStriper striper2 = new InputStriper(pool, expectedPoolSize);
+    checkSplitEq(fs, striper2.splitFor(pool, expectedPoolSize, 0),
+        expectedPoolSize);
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
new file mode 100644
index 0000000..a4668ee
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
@@ -0,0 +1,143 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.util.Arrays;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import static org.junit.Assert.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
+
+public class TestFileQueue {
+
+  static final Log LOG = LogFactory.getLog(TestFileQueue.class);
+  static final int NFILES = 4;
+  static final int BLOCK = 256;
+  static final Path[] paths = new Path[NFILES];
+  static final String[] loc = new String[NFILES];
+  static final long[] start = new long[NFILES];
+  static final long[] len = new long[NFILES];
+
+  @BeforeClass
+  public static void setup() throws IOException {
+    final Configuration conf = new Configuration();
+    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
+    final Path p = new Path(System.getProperty("test.build.data", "/tmp"),
+        "testFileQueue").makeQualified(fs);
+    fs.delete(p, true);
+    final byte[] b = new byte[BLOCK];
+    for (int i = 0; i < NFILES; ++i) {
+      Arrays.fill(b, (byte)('A' + i));
+      paths[i] = new Path(p, "" + (char)('A' + i));
+      OutputStream f = null;
+      try {
+        f = fs.create(paths[i]);
+        f.write(b);
+      } finally {
+        if (f != null) {
+          f.close();
+        }
+      }
+    }
+  }
+
+  @AfterClass
+  public static void cleanup() throws IOException {
+    final Configuration conf = new Configuration();
+    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
+    final Path p = new Path(System.getProperty("test.build.data", "/tmp"),
+        "testFileQueue").makeQualified(fs);
+    fs.delete(p, true);
+  }
+
+  static ByteArrayOutputStream fillVerif() throws IOException {
+    final byte[] b = new byte[BLOCK];
+    final ByteArrayOutputStream out = new ByteArrayOutputStream();
+    for (int i = 0; i < NFILES; ++i) {
+      Arrays.fill(b, (byte)('A' + i));
+      out.write(b, 0, (int)len[i]);
+    }
+    return out;
+  }
+
+  @Test
+  public void testRepeat() throws Exception {
+    final Configuration conf = new Configuration();
+    Arrays.fill(loc, "");
+    Arrays.fill(start, 0L);
+    Arrays.fill(len, BLOCK);
+
+    final ByteArrayOutputStream out = fillVerif();
+    final FileQueue q =
+      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
+    final byte[] verif = out.toByteArray();
+    final byte[] check = new byte[2 * NFILES * BLOCK];
+    q.read(check, 0, NFILES * BLOCK);
+    assertArrayEquals(verif, Arrays.copyOf(check, NFILES * BLOCK));
+
+    final byte[] verif2 = new byte[2 * NFILES * BLOCK];
+    System.arraycopy(verif, 0, verif2, 0, verif.length);
+    System.arraycopy(verif, 0, verif2, verif.length, verif.length);
+    q.read(check, 0, 2 * NFILES * BLOCK);
+    assertArrayEquals(verif2, check);
+
+  }
+
+  @Test
+  public void testUneven() throws Exception {
+    final Configuration conf = new Configuration();
+    Arrays.fill(loc, "");
+    Arrays.fill(start, 0L);
+    Arrays.fill(len, BLOCK);
+
+    final int B2 = BLOCK / 2;
+    for (int i = 0; i < NFILES; i += 2) {
+      start[i] += B2;
+      len[i] -= B2;
+    }
+    final FileQueue q =
+      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
+    final ByteArrayOutputStream out = fillVerif();
+    final byte[] verif = out.toByteArray();
+    final byte[] check = new byte[NFILES / 2 * BLOCK + NFILES / 2 * B2];
+    q.read(check, 0, verif.length);
+    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));
+    q.read(check, 0, verif.length);
+    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));
+  }
+
+  @Test
+  public void testEmpty() throws Exception {
+    final Configuration conf = new Configuration();
+    // verify OK if unused
+    final FileQueue q = new FileQueue(new CombineFileSplit(
+          new Path[0], new long[0], new long[0], new String[0]), conf);
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
new file mode 100644
index 0000000..422ec12
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
@@ -0,0 +1,453 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.DummyResourceCalculatorPlugin;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.gridmix.DebugJobProducer.MockJob;
+import org.apache.hadoop.mapred.gridmix.TestHighRamJob.DummyGridmixJob;
+import org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators.FakeProgressive;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin.DefaultHeapUsageEmulator;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+
+/**
+ * Test Gridmix memory emulation.
+ */
+public class TestGridmixMemoryEmulation {
+  /**
+   * This is a dummy class that fakes heap usage.
+   */
+  private static class FakeHeapUsageEmulatorCore 
+  extends DefaultHeapUsageEmulator {
+    private int numCalls = 0;
+    
+    @Override
+    public void load(long sizeInMB) {
+      ++numCalls;
+      super.load(sizeInMB);
+    }
+    
+    // Get the total number of times load() was invoked
+    int getNumCalls() {
+      return numCalls;
+    }
+    
+    // Get the total number of 1mb objects stored within
+    long getHeapUsageInMB() {
+      return heapSpace.size();
+    }
+    
+    @Override
+    public void reset() {
+      // no op to stop emulate() from resetting
+    }
+    
+    /**
+     * For re-testing purpose.
+     */
+    void resetFake() {
+      numCalls = 0;
+      super.reset();
+    }
+  }
+
+  /**
+   * This is a dummy class that fakes the heap usage emulator plugin.
+   */
+  private static class FakeHeapUsageEmulatorPlugin 
+  extends TotalHeapUsageEmulatorPlugin {
+    private FakeHeapUsageEmulatorCore core;
+    
+    public FakeHeapUsageEmulatorPlugin(FakeHeapUsageEmulatorCore core) {
+      super(core);
+      this.core = core;
+    }
+    
+    @Override
+    protected long getMaxHeapUsageInMB() {
+      return Long.MAX_VALUE / ONE_MB;
+    }
+    
+    @Override
+    protected long getTotalHeapUsageInMB() {
+      return core.getHeapUsageInMB();
+    }
+  }
+  
+  /**
+   * Test {@link TotalHeapUsageEmulatorPlugin}'s core heap usage emulation 
+   * engine.
+   */
+  @Test
+  public void testHeapUsageEmulator() throws IOException {
+    FakeHeapUsageEmulatorCore heapEmulator = new FakeHeapUsageEmulatorCore();
+    
+    long testSizeInMB = 10; // 10 mb
+    long previousHeap = heapEmulator.getHeapUsageInMB();
+    heapEmulator.load(testSizeInMB);
+    long currentHeap = heapEmulator.getHeapUsageInMB();
+    
+    // check if the heap has increased by expected value
+    assertEquals("Default heap emulator failed to load 10mb", 
+                 previousHeap + testSizeInMB, currentHeap);
+    
+    // test reset
+    heapEmulator.resetFake();
+    assertEquals("Default heap emulator failed to reset", 
+                 0, heapEmulator.getHeapUsageInMB());
+  }
+
+  /**
+   * Test {@link TotalHeapUsageEmulatorPlugin}.
+   */
+  @Test
+  public void testTotalHeapUsageEmulatorPlugin() throws Exception {
+    Configuration conf = new Configuration();
+    // set the dummy resource calculator for testing
+    ResourceCalculatorPlugin monitor = new DummyResourceCalculatorPlugin();
+    long maxHeapUsage = 1024 * TotalHeapUsageEmulatorPlugin.ONE_MB; // 1GB
+    conf.setLong(DummyResourceCalculatorPlugin.MAXPMEM_TESTING_PROPERTY, 
+                 maxHeapUsage);
+    monitor.setConf(conf);
+    
+    // no buffer to be reserved
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
+    // only 1 call to be made per cycle
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
+    long targetHeapUsageInMB = 200; // 200mb
+    
+    // fake progress indicator
+    FakeProgressive fakeProgress = new FakeProgressive();
+    
+    // fake heap usage generator
+    FakeHeapUsageEmulatorCore fakeCore = new FakeHeapUsageEmulatorCore();
+    
+    // a heap usage emulator with fake core
+    FakeHeapUsageEmulatorPlugin heapPlugin = 
+      new FakeHeapUsageEmulatorPlugin(fakeCore);
+    
+    // test with invalid or missing resource usage value
+    ResourceUsageMetrics invalidUsage = 
+      TestResourceUsageEmulators.createMetrics(0);
+    heapPlugin.initialize(conf, invalidUsage, null, null);
+    
+    // test if disabled heap emulation plugin's emulate() call is a no-operation
+    // this will test if the emulation plugin is disabled or not
+    int numCallsPre = fakeCore.getNumCalls();
+    long heapUsagePre = fakeCore.getHeapUsageInMB();
+    heapPlugin.emulate();
+    int numCallsPost = fakeCore.getNumCalls();
+    long heapUsagePost = fakeCore.getHeapUsageInMB();
+    
+    //  test if no calls are made heap usage emulator core
+    assertEquals("Disabled heap usage emulation plugin works!", 
+                 numCallsPre, numCallsPost);
+    //  test if no calls are made heap usage emulator core
+    assertEquals("Disabled heap usage emulation plugin works!", 
+                 heapUsagePre, heapUsagePost);
+    
+    // test with wrong/invalid configuration
+    Boolean failed = null;
+    invalidUsage = 
+      TestResourceUsageEmulators.createMetrics(maxHeapUsage 
+                                   + TotalHeapUsageEmulatorPlugin.ONE_MB);
+    try {
+      heapPlugin.initialize(conf, invalidUsage, monitor, null);
+      failed = false;
+    } catch (Exception e) {
+      failed = true;
+    }
+    assertNotNull("Fail case failure!", failed);
+    assertTrue("Expected failure!", failed); 
+    
+    // test with valid resource usage value
+    ResourceUsageMetrics metrics = 
+      TestResourceUsageEmulators.createMetrics(targetHeapUsageInMB 
+                                   * TotalHeapUsageEmulatorPlugin.ONE_MB);
+    
+    // test with default emulation interval
+    // in every interval, the emulator will add 100% of the expected usage 
+    // (since gridmix.emulators.resource-usage.heap.load-ratio=1)
+    // so at 10%, emulator will add 10% (difference), at 20% it will add 10% ...
+    // So to emulate 200MB, it will add
+    //   20mb + 20mb + 20mb + 20mb + .. = 200mb 
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 
+                          10);
+    
+    // test with custom value for emulation interval of 20%
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL,
+                  0.2F);
+    //  40mb + 40mb + 40mb + 40mb + 40mb = 200mb
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 5);
+    
+    // test with custom value of free heap ratio and load ratio = 1
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.5F);
+    //  40mb + 0mb + 80mb + 0mb + 0mb = 120mb
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 120, 2);
+    
+    // test with custom value of heap load ratio and min free heap ratio = 0
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
+    // 20mb (call#1) + 20mb (call#1) + 20mb (call#2) + 20mb (call#2) +.. = 200mb
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 
+                          10);
+    
+    // test with custom value of free heap ratio = 0.3 and load ratio = 0.5
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.25F);
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);
+    // 20mb (call#1) + 20mb (call#1) + 30mb (call#2) + 0mb (call#2) 
+    // + 30mb (call#3) + 0mb (call#3) + 35mb (call#4) + 0mb (call#4)
+    // + 37mb (call#5) + 0mb (call#5) = 162mb
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 162, 6);
+    
+    // test if emulation interval boundary is respected
+    fakeProgress = new FakeProgressive(); // initialize
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
+    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL,
+                  0.25F);
+    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);
+    fakeCore.resetFake();
+    // take a snapshot after the initialization
+    long initHeapUsage = fakeCore.getHeapUsageInMB();
+    long initNumCallsUsage = fakeCore.getNumCalls();
+    // test with 0 progress
+    testEmulationBoundary(0F, fakeCore, fakeProgress, heapPlugin, initHeapUsage, 
+                          initNumCallsUsage, "[no-op, 0 progress]");
+    // test with 24% progress
+    testEmulationBoundary(0.24F, fakeCore, fakeProgress, heapPlugin, 
+                          initHeapUsage, initNumCallsUsage, 
+                          "[no-op, 24% progress]");
+    // test with 25% progress
+    testEmulationBoundary(0.25F, fakeCore, fakeProgress, heapPlugin, 
+        targetHeapUsageInMB / 4, 1, "[op, 25% progress]");
+    // test with 80% progress
+    testEmulationBoundary(0.80F, fakeCore, fakeProgress, heapPlugin, 
+        (targetHeapUsageInMB * 4) / 5, 2, "[op, 80% progress]");
+    
+    // now test if the final call with 100% progress ramps up the heap usage
+    testEmulationBoundary(1F, fakeCore, fakeProgress, heapPlugin, 
+        targetHeapUsageInMB, 3, "[op, 100% progress]");
+  }
+
+  // test whether the heap usage emulator achieves the desired target using
+  // desired calls to the underling core engine.
+  private static void testEmulationAccuracy(Configuration conf, 
+                        FakeHeapUsageEmulatorCore fakeCore,
+                        ResourceCalculatorPlugin monitor,
+                        ResourceUsageMetrics metrics,
+                        TotalHeapUsageEmulatorPlugin heapPlugin,
+                        long expectedTotalHeapUsageInMB,
+                        long expectedTotalNumCalls)
+  throws Exception {
+    FakeProgressive fakeProgress = new FakeProgressive();
+    fakeCore.resetFake();
+    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);
+    int numLoops = 0;
+    while (fakeProgress.getProgress() < 1) {
+      ++numLoops;
+      float progress = numLoops / 100.0F;
+      fakeProgress.setProgress(progress);
+      heapPlugin.emulate();
+    }
+    
+    // test if the resource plugin shows the expected usage
+    assertEquals("Cumulative heap usage emulator plugin failed (total usage)!", 
+                 expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 1L);
+    // test if the resource plugin shows the expected num calls
+    assertEquals("Cumulative heap usage emulator plugin failed (num calls)!", 
+                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
+  }
+
+  // tests if the heap usage emulation plugin emulates only at the expected
+  // progress gaps
+  private static void testEmulationBoundary(float progress, 
+      FakeHeapUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, 
+      TotalHeapUsageEmulatorPlugin heapPlugin, long expectedTotalHeapUsageInMB, 
+      long expectedTotalNumCalls, String info) throws Exception {
+    fakeProgress.setProgress(progress);
+    heapPlugin.emulate();
+    // test heap usage
+    assertEquals("Emulation interval test for heap usage failed " + info + "!", 
+                 expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 0L);
+    // test num calls
+    assertEquals("Emulation interval test for heap usage failed " + info + "!", 
+                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
+  }
+  
+  /**
+   * Test the specified task java heap options.
+   */
+  @SuppressWarnings("deprecation")
+  private void testJavaHeapOptions(String mapOptions, 
+      String reduceOptions, String taskOptions, String defaultMapOptions, 
+      String defaultReduceOptions, String defaultTaskOptions, 
+      String expectedMapOptions, String expectedReduceOptions, 
+      String expectedTaskOptions) throws Exception {
+    Configuration simulatedConf = new Configuration();
+    // reset the configuration parameters
+    simulatedConf.unset(MRJobConfig.MAP_JAVA_OPTS);
+    simulatedConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);
+    simulatedConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);
+    
+    // set the default map task options
+    if (defaultMapOptions != null) {
+      simulatedConf.set(MRJobConfig.MAP_JAVA_OPTS, defaultMapOptions);
+    }
+    // set the default reduce task options
+    if (defaultReduceOptions != null) {
+      simulatedConf.set(MRJobConfig.REDUCE_JAVA_OPTS, defaultReduceOptions);
+    }
+    // set the default task options
+    if (defaultTaskOptions != null) {
+      simulatedConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, defaultTaskOptions);
+    }
+    
+    Configuration originalConf = new Configuration();
+    // reset the configuration parameters
+    originalConf.unset(MRJobConfig.MAP_JAVA_OPTS);
+    originalConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);
+    originalConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);
+    
+    // set the map task options
+    if (mapOptions != null) {
+      originalConf.set(MRJobConfig.MAP_JAVA_OPTS, mapOptions);
+    }
+    // set the reduce task options
+    if (reduceOptions != null) {
+      originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, reduceOptions);
+    }
+    // set the task options
+    if (taskOptions != null) {
+      originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, taskOptions);
+    }
+    
+    // configure the task jvm's heap options
+    GridmixJob.configureTaskJVMOptions(originalConf, simulatedConf);
+    
+    assertEquals("Map heap options mismatch!", expectedMapOptions, 
+                 simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));
+    assertEquals("Reduce heap options mismatch!", expectedReduceOptions, 
+                 simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));
+    assertEquals("Task heap options mismatch!", expectedTaskOptions, 
+                 simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));
+  }
+  
+  /**
+   * Test task-level java heap options configuration in {@link GridmixJob}.
+   */
+  @Test
+  public void testJavaHeapOptions() throws Exception {
+    // test missing opts
+    testJavaHeapOptions(null, null, null, null, null, null, null, null, 
+                        null);
+    
+    // test original heap opts and missing default opts
+    testJavaHeapOptions("-Xms10m", "-Xms20m", "-Xms30m", null, null, null,
+                        null, null, null);
+    
+    // test missing opts with default opts
+    testJavaHeapOptions(null, null, null, "-Xms10m", "-Xms20m", "-Xms30m",
+                        "-Xms10m", "-Xms20m", "-Xms30m");
+    
+    // test empty option
+    testJavaHeapOptions("", "", "", null, null, null, null, null, null);
+    
+    // test empty default option and no original heap options
+    testJavaHeapOptions(null, null, null, "", "", "", "", "", "");
+    
+    // test empty opts and default opts
+    testJavaHeapOptions("", "", "", "-Xmx10m -Xms1m", "-Xmx50m -Xms2m", 
+                        "-Xms2m -Xmx100m", "-Xmx10m -Xms1m", "-Xmx50m -Xms2m", 
+                        "-Xms2m -Xmx100m");
+    
+    // test custom heap opts with no default opts
+    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx30m", null, null, null,
+                        "-Xmx10m", "-Xmx20m", "-Xmx30m");
+    
+    // test heap opts with default opts (multiple value)
+    testJavaHeapOptions("-Xms5m -Xmx200m", "-Xms15m -Xmx300m", 
+                        "-Xms25m -Xmx50m", "-XXabc", "-XXxyz", "-XXdef", 
+                        "-XXabc -Xmx200m", "-XXxyz -Xmx300m", "-XXdef -Xmx50m");
+    
+    // test heap opts with default opts (duplication of -Xmx)
+    testJavaHeapOptions("-Xms5m -Xmx200m", "-Xms15m -Xmx300m", 
+                        "-Xms25m -Xmx50m", "-XXabc -Xmx500m", "-XXxyz -Xmx600m",
+                        "-XXdef -Xmx700m", "-XXabc -Xmx200m", "-XXxyz -Xmx300m",
+                        "-XXdef -Xmx50m");
+    
+    // test heap opts with default opts (single value)
+    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx50m", "-Xms2m", 
+                        "-Xms3m", "-Xms5m", "-Xms2m -Xmx10m", "-Xms3m -Xmx20m",
+                        "-Xms5m -Xmx50m");
+    
+    // test heap opts with default opts (duplication of -Xmx)
+    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx50m", "-Xmx2m", 
+                        "-Xmx3m", "-Xmx5m", "-Xmx10m", "-Xmx20m", "-Xmx50m");
+  }
+  
+  /**
+   * Test disabled task heap options configuration in {@link GridmixJob}.
+   */
+  @Test
+  @SuppressWarnings("deprecation")
+  public void testJavaHeapOptionsDisabled() throws Exception {
+    Configuration gridmixConf = new Configuration();
+    gridmixConf.setBoolean(GridmixJob.GRIDMIX_TASK_JVM_OPTIONS_ENABLE, false);
+    
+    // set the default values of simulated job
+    gridmixConf.set(MRJobConfig.MAP_JAVA_OPTS, "-Xmx1m");
+    gridmixConf.set(MRJobConfig.REDUCE_JAVA_OPTS, "-Xmx2m");
+    gridmixConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, "-Xmx3m");
+    
+    // set the default map and reduce task options for original job
+    final JobConf originalConf = new JobConf();
+    originalConf.set(MRJobConfig.MAP_JAVA_OPTS, "-Xmx10m");
+    originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, "-Xmx20m");
+    originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, "-Xmx30m");
+    
+    // define a mock job
+    MockJob story = new MockJob(originalConf) {
+      public JobConf getJobConf() {
+        return originalConf;
+      }
+    };
+    
+    GridmixJob job = new DummyGridmixJob(gridmixConf, story);
+    Job simulatedJob = job.getJob();
+    Configuration simulatedConf = simulatedJob.getConfiguration();
+    
+    assertEquals("Map heap options works when disabled!", "-Xmx1m", 
+                 simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));
+    assertEquals("Reduce heap options works when disabled!", "-Xmx2m", 
+                 simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));
+    assertEquals("Task heap options works when disabled!", "-Xmx3m", 
+                 simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
new file mode 100644
index 0000000..2f3ce70
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
@@ -0,0 +1,278 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Random;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.io.WritableUtils;
+
+public class TestGridmixRecord {
+  private static final Log LOG = LogFactory.getLog(TestGridmixRecord.class);
+
+  static void lengthTest(GridmixRecord x, GridmixRecord y, int min,
+      int max) throws Exception {
+    final Random r = new Random();
+    final long seed = r.nextLong();
+    r.setSeed(seed);
+    LOG.info("length: " + seed);
+    final DataInputBuffer in = new DataInputBuffer();
+    final DataOutputBuffer out1 = new DataOutputBuffer();
+    final DataOutputBuffer out2 = new DataOutputBuffer();
+    for (int i = min; i < max; ++i) {
+      setSerialize(x, r.nextLong(), i, out1);
+      // check write
+      assertEquals(i, out1.getLength());
+      // write to stream
+      x.write(out2);
+      // check read
+      in.reset(out1.getData(), 0, out1.getLength());
+      y.readFields(in);
+      assertEquals(i, x.getSize());
+      assertEquals(i, y.getSize());
+    }
+    // check stream read
+    in.reset(out2.getData(), 0, out2.getLength());
+    for (int i = min; i < max; ++i) {
+      y.readFields(in);
+      assertEquals(i, y.getSize());
+    }
+  }
+
+  static void randomReplayTest(GridmixRecord x, GridmixRecord y, int min,
+      int max) throws Exception {
+    final Random r = new Random();
+    final long seed = r.nextLong();
+    r.setSeed(seed);
+    LOG.info("randReplay: " + seed);
+    final DataOutputBuffer out1 = new DataOutputBuffer();
+    for (int i = min; i < max; ++i) {
+      final int s = out1.getLength();
+      x.setSeed(r.nextLong());
+      x.setSize(i);
+      x.write(out1);
+      assertEquals(i, out1.getLength() - s);
+    }
+    final DataInputBuffer in = new DataInputBuffer();
+    in.reset(out1.getData(), 0, out1.getLength());
+    final DataOutputBuffer out2 = new DataOutputBuffer();
+    // deserialize written records, write to separate buffer
+    for (int i = min; i < max; ++i) {
+      final int s = in.getPosition();
+      y.readFields(in);
+      assertEquals(i, in.getPosition() - s);
+      y.write(out2);
+    }
+    // verify written contents match
+    assertEquals(out1.getLength(), out2.getLength());
+    // assumes that writes will grow buffer deterministically
+    assertEquals("Bad test", out1.getData().length, out2.getData().length);
+    assertArrayEquals(out1.getData(), out2.getData());
+  }
+
+  static void eqSeedTest(GridmixRecord x, GridmixRecord y, int max)
+      throws Exception {
+    final Random r = new Random();
+    final long s = r.nextLong();
+    r.setSeed(s);
+    LOG.info("eqSeed: " + s);
+    assertEquals(x.fixedBytes(), y.fixedBytes());
+    final int min = x.fixedBytes() + 1;
+    final DataOutputBuffer out1 = new DataOutputBuffer();
+    final DataOutputBuffer out2 = new DataOutputBuffer();
+    for (int i = min; i < max; ++i) {
+      final long seed = r.nextLong();
+      setSerialize(x, seed, i, out1);
+      setSerialize(y, seed, i, out2);
+      assertEquals(x, y);
+      assertEquals(x.hashCode(), y.hashCode());
+
+      // verify written contents match
+      assertEquals(out1.getLength(), out2.getLength());
+      // assumes that writes will grow buffer deterministically
+      assertEquals("Bad test", out1.getData().length, out2.getData().length);
+      assertArrayEquals(out1.getData(), out2.getData());
+    }
+  }
+
+  static void binSortTest(GridmixRecord x, GridmixRecord y, int min,
+      int max, WritableComparator cmp) throws Exception {
+    final Random r = new Random();
+    final long s = r.nextLong();
+    r.setSeed(s);
+    LOG.info("sort: " + s);
+    final DataOutputBuffer out1 = new DataOutputBuffer();
+    final DataOutputBuffer out2 = new DataOutputBuffer();
+    for (int i = min; i < max; ++i) {
+      final long seed1 = r.nextLong();
+      setSerialize(x, seed1, i, out1);
+      assertEquals(0, x.compareSeed(seed1, Math.max(0, i - x.fixedBytes())));
+
+      final long seed2 = r.nextLong();
+      setSerialize(y, seed2, i, out2);
+      assertEquals(0, y.compareSeed(seed2, Math.max(0, i - x.fixedBytes())));
+
+      // for eq sized records, ensure byte cmp where req
+      final int chk = WritableComparator.compareBytes(
+          out1.getData(), 0, out1.getLength(),
+          out2.getData(), 0, out2.getLength());
+      assertEquals(chk, x.compareTo(y));
+      assertEquals(chk, cmp.compare(
+            out1.getData(), 0, out1.getLength(),
+            out2.getData(), 0, out2.getLength()));
+      // write second copy, compare eq
+      final int s1 = out1.getLength();
+      x.write(out1);
+      assertEquals(0, cmp.compare(out1.getData(), 0, s1,
+            out1.getData(), s1, out1.getLength() - s1));
+      final int s2 = out2.getLength();
+      y.write(out2);
+      assertEquals(0, cmp.compare(out2.getData(), 0, s2,
+            out2.getData(), s2, out2.getLength() - s2));
+      assertEquals(chk, cmp.compare(out1.getData(), 0, s1,
+            out2.getData(), s2, out2.getLength() - s2));
+    }
+  }
+
+  static void checkSpec(GridmixKey a, GridmixKey b) throws Exception {
+    final Random r = new Random();
+    final long s = r.nextLong();
+    r.setSeed(s);
+    LOG.info("spec: " + s);
+    final DataInputBuffer in = new DataInputBuffer();
+    final DataOutputBuffer out = new DataOutputBuffer();
+    a.setType(GridmixKey.REDUCE_SPEC);
+    b.setType(GridmixKey.REDUCE_SPEC);
+    for (int i = 0; i < 100; ++i) {
+      final int in_rec = r.nextInt(Integer.MAX_VALUE);
+      a.setReduceInputRecords(in_rec);
+      final int out_rec = r.nextInt(Integer.MAX_VALUE);
+      a.setReduceOutputRecords(out_rec);
+      final int out_bytes = r.nextInt(Integer.MAX_VALUE);
+      a.setReduceOutputBytes(out_bytes);
+      final int min = WritableUtils.getVIntSize(in_rec)
+                    + WritableUtils.getVIntSize(out_rec)
+                    + WritableUtils.getVIntSize(out_bytes)
+                    + WritableUtils.getVIntSize(0);
+      assertEquals(min + 2, a.fixedBytes()); // meta + vint min
+      final int size = r.nextInt(1024) + a.fixedBytes() + 1;
+      setSerialize(a, r.nextLong(), size, out);
+      assertEquals(size, out.getLength());
+      assertTrue(a.equals(a));
+      assertEquals(0, a.compareTo(a));
+
+      in.reset(out.getData(), 0, out.getLength());
+
+      b.readFields(in);
+      assertEquals(size, b.getSize());
+      assertEquals(in_rec, b.getReduceInputRecords());
+      assertEquals(out_rec, b.getReduceOutputRecords());
+      assertEquals(out_bytes, b.getReduceOutputBytes());
+      assertTrue(a.equals(b));
+      assertEquals(0, a.compareTo(b));
+      assertEquals(a.hashCode(), b.hashCode());
+    }
+  }
+
+  static void setSerialize(GridmixRecord x, long seed, int size,
+      DataOutputBuffer out) throws IOException {
+    x.setSeed(seed);
+    x.setSize(size);
+    out.reset();
+    x.write(out);
+  }
+
+  @Test
+  public void testKeySpec() throws Exception {
+    final int min = 6;
+    final int max = 300;
+    final GridmixKey a = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);
+    final GridmixKey b = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);
+    lengthTest(a, b, min, max);
+    randomReplayTest(a, b, min, max);
+    binSortTest(a, b, min, max, new GridmixKey.Comparator());
+    // 2 fixed GR bytes, 1 type, 3 spec
+    eqSeedTest(a, b, max);
+    checkSpec(a, b);
+  }
+
+  @Test
+  public void testKeyData() throws Exception {
+    final int min = 2;
+    final int max = 300;
+    final GridmixKey a = new GridmixKey(GridmixKey.DATA, 1, 0L);
+    final GridmixKey b = new GridmixKey(GridmixKey.DATA, 1, 0L);
+    lengthTest(a, b, min, max);
+    randomReplayTest(a, b, min, max);
+    binSortTest(a, b, min, max, new GridmixKey.Comparator());
+    // 2 fixed GR bytes, 1 type
+    eqSeedTest(a, b, 300);
+  }
+
+  @Test
+  public void testBaseRecord() throws Exception {
+    final int min = 1;
+    final int max = 300;
+    final GridmixRecord a = new GridmixRecord();
+    final GridmixRecord b = new GridmixRecord();
+    lengthTest(a, b, min, max);
+    randomReplayTest(a, b, min, max);
+    binSortTest(a, b, min, max, new GridmixRecord.Comparator());
+    // 2 fixed GR bytes
+    eqSeedTest(a, b, 300);
+  }
+
+  public static void main(String[] argv) throws Exception {
+    boolean fail = false;
+    final TestGridmixRecord test = new TestGridmixRecord();
+    try { test.testKeySpec(); } catch (Exception e) {
+      fail = true;
+      e.printStackTrace();
+    }
+    try {test.testKeyData(); } catch (Exception e) {
+      fail = true;
+      e.printStackTrace();
+    }
+    try {test.testBaseRecord(); } catch (Exception e) {
+      fail = true;
+      e.printStackTrace();
+    }
+    System.exit(fail ? -1 : 0);
+  }
+
+  static void printDebug(GridmixRecord a, GridmixRecord b) throws IOException {
+    DataOutputBuffer out = new DataOutputBuffer();
+    a.write(out);
+    System.out.println("A " +
+        Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));
+    out.reset();
+    b.write(out);
+    System.out.println("B " +
+        Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));
+  }
+
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
new file mode 100644
index 0000000..64af603
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
@@ -0,0 +1,377 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import static org.junit.Assert.*;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.UtilsForTests;
+import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
+import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
+import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.apache.hadoop.tools.rumen.JobStoryProducer;
+import org.junit.Test;
+
+/**
+ * Test {@link ExecutionSummarizer} and {@link ClusterSummarizer}.
+ */
+public class TestGridmixSummary {
+  
+  /**
+   * Test {@link DataStatistics}.
+   */
+  @Test
+  public void testDataStatistics() throws Exception {
+    // test data-statistics getters with compression enabled
+    DataStatistics stats = new DataStatistics(10, 2, true);
+    assertEquals("Data size mismatch", 10, stats.getDataSize());
+    assertEquals("Num files mismatch", 2, stats.getNumFiles());
+    assertTrue("Compression configuration mismatch", stats.isDataCompressed());
+    
+    // test data-statistics getters with compression disabled
+    stats = new DataStatistics(100, 5, false);
+    assertEquals("Data size mismatch", 100, stats.getDataSize());
+    assertEquals("Num files mismatch", 5, stats.getNumFiles());
+    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
+    
+    // test publish data stats
+    Configuration conf = new Configuration();
+    Path rootTempDir = new Path(System.getProperty("test.build.data", "/tmp"));
+    Path testDir = new Path(rootTempDir, "testDataStatistics");
+    FileSystem fs = testDir.getFileSystem(conf);
+    fs.delete(testDir, true);
+    Path testInputDir = new Path(testDir, "test");
+    fs.mkdirs(testInputDir);
+    
+    // test empty folder (compression = true)
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    Boolean failed = null;
+    try {
+      GenerateData.publishDataStatistics(testInputDir, 1024L, conf);
+      failed = false;
+    } catch (RuntimeException e) {
+      failed = true;
+    }
+    assertNotNull("Expected failure!", failed);
+    assertTrue("Compression data publishing error", failed);
+    
+    // test with empty folder (compression = off)
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
+    stats = GenerateData.publishDataStatistics(testInputDir, 1024L, conf);
+    assertEquals("Data size mismatch", 0, stats.getDataSize());
+    assertEquals("Num files mismatch", 0, stats.getNumFiles());
+    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
+    
+    // test with some plain input data (compression = off)
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
+    Path inputDataFile = new Path(testInputDir, "test");
+    long size = 
+      UtilsForTests.createTmpFileDFS(fs, inputDataFile, 
+          FsPermission.createImmutable((short)777), "hi hello bye").size();
+    stats = GenerateData.publishDataStatistics(testInputDir, -1, conf);
+    assertEquals("Data size mismatch", size, stats.getDataSize());
+    assertEquals("Num files mismatch", 1, stats.getNumFiles());
+    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
+    
+    // test with some plain input data (compression = on)
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    failed = null;
+    try {
+      GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
+      failed = false;
+    } catch (RuntimeException e) {
+      failed = true;
+    }
+    assertNotNull("Expected failure!", failed);
+    assertTrue("Compression data publishing error", failed);
+    
+    // test with some compressed input data (compression = off)
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
+    fs.delete(inputDataFile, false);
+    inputDataFile = new Path(testInputDir, "test.gz");
+    size = 
+      UtilsForTests.createTmpFileDFS(fs, inputDataFile, 
+          FsPermission.createImmutable((short)777), "hi hello").size();
+    stats =  GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
+    assertEquals("Data size mismatch", size, stats.getDataSize());
+    assertEquals("Num files mismatch", 1, stats.getNumFiles());
+    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
+    
+    // test with some compressed input data (compression = on)
+    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
+    stats = GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
+    assertEquals("Data size mismatch", size, stats.getDataSize());
+    assertEquals("Num files mismatch", 1, stats.getNumFiles());
+    assertTrue("Compression configuration mismatch", stats.isDataCompressed());
+  }
+  
+  /**
+   * A fake {@link JobFactory}.
+   */
+  @SuppressWarnings("unchecked")
+  private static class FakeJobFactory extends JobFactory {
+    /**
+     * A fake {@link JobStoryProducer} for {@link FakeJobFactory}.
+     */
+    private static class FakeJobStoryProducer implements JobStoryProducer {
+      @Override
+      public void close() throws IOException {
+      }
+
+      @Override
+      public JobStory getNextJob() throws IOException {
+        return null;
+      }
+    }
+    
+    FakeJobFactory(Configuration conf) {
+      super(null, new FakeJobStoryProducer(), null, conf, null, null);
+    }
+    
+    @Override
+    public void update(Object item) {
+    }
+    
+    @Override
+    protected Thread createReaderThread() {
+      return new Thread();
+    }
+  }
+  
+  /**
+   * Test {@link ExecutionSummarizer}.
+   */
+  @Test
+  @SuppressWarnings("unchecked")
+  public void testExecutionSummarizer() throws IOException {
+    Configuration conf = new Configuration();
+    
+    ExecutionSummarizer es = new ExecutionSummarizer();
+    assertEquals("ExecutionSummarizer init failed", 
+                 Summarizer.NA, es.getCommandLineArgsString());
+    
+    long startTime = System.currentTimeMillis();
+    // test configuration parameters
+    String[] initArgs = new String[] {"-Xmx20m", "-Dtest.args='test'"};
+    es = new ExecutionSummarizer(initArgs);
+    
+    assertEquals("ExecutionSummarizer init failed", 
+                 "-Xmx20m -Dtest.args='test'", 
+                 es.getCommandLineArgsString());
+    
+    // test start time
+    assertTrue("Start time mismatch", es.getStartTime() >= startTime);
+    assertTrue("Start time mismatch", 
+               es.getStartTime() <= System.currentTimeMillis());
+    
+    // test start() of ExecutionSummarizer
+    es.update(null);
+    assertEquals("ExecutionSummarizer init failed", 0, 
+                 es.getSimulationStartTime());
+    testExecutionSummarizer(0, 0, 0, 0, 0, 0, es);
+    
+    long simStartTime = System.currentTimeMillis();
+    es.start(null);
+    assertTrue("Simulation start time mismatch", 
+               es.getSimulationStartTime() >= simStartTime);
+    assertTrue("Simulation start time mismatch", 
+               es.getSimulationStartTime() <= System.currentTimeMillis());
+    
+    // test with job stats
+    JobStats stats = generateFakeJobStats(1, 10, true);
+    es.update(stats);
+    testExecutionSummarizer(1, 10, 0, 1, 1, 0, es);
+    
+    // test with failed job 
+    stats = generateFakeJobStats(5, 1, false);
+    es.update(stats);
+    testExecutionSummarizer(6, 11, 0, 2, 1, 1, es);
+    
+    // test finalize
+    //  define a fake job factory
+    JobFactory factory = new FakeJobFactory(conf);
+    
+    // fake the num jobs in trace
+    factory.numJobsInTrace = 3;
+    
+    Path rootTempDir = new Path(System.getProperty("test.build.data", "/tmp"));
+    Path testDir = new Path(rootTempDir, "testGridmixSummary");
+    Path testTraceFile = new Path(testDir, "test-trace.json");
+    FileSystem fs = FileSystem.getLocal(conf);
+    fs.create(testTraceFile).close();
+    
+    // finalize the summarizer
+    UserResolver resolver = new RoundRobinUserResolver();
+    DataStatistics dataStats = new DataStatistics(100, 2, true);
+    String policy = GridmixJobSubmissionPolicy.REPLAY.name();
+    conf.set(GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, policy);
+    es.finalize(factory, testTraceFile.toString(), 1024L, resolver, dataStats, 
+                conf);
+    
+    // test num jobs in trace
+    assertEquals("Mismtach in num jobs in trace", 3, es.getNumJobsInTrace());
+    
+    // test trace signature
+    String tid = 
+      ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
+    assertEquals("Mismatch in trace signature", 
+                 tid, es.getInputTraceSignature());
+    // test trace location
+    Path qPath = fs.makeQualified(testTraceFile);
+    assertEquals("Mismatch in trace filename", 
+                 qPath.toString(), es.getInputTraceLocation());
+    // test expected data size
+    assertEquals("Mismatch in expected data size", 
+                 "1.0k", es.getExpectedDataSize());
+    // test input data statistics
+    assertEquals("Mismatch in input data statistics", 
+                 ExecutionSummarizer.stringifyDataStatistics(dataStats), 
+                 es.getInputDataStatistics());
+    // test user resolver
+    assertEquals("Mismatch in user resolver", 
+                 resolver.getClass().getName(), es.getUserResolver());
+    // test policy
+    assertEquals("Mismatch in policy", policy, es.getJobSubmissionPolicy());
+    
+    // test data stringification using large data
+    es.finalize(factory, testTraceFile.toString(), 1024*1024*1024*10L, resolver,
+                dataStats, conf);
+    assertEquals("Mismatch in expected data size", 
+                 "10.0g", es.getExpectedDataSize());
+    
+    // test trace signature uniqueness
+    //  touch the trace file
+    fs.delete(testTraceFile, false);
+    //  sleep for 1 sec
+    try {
+      Thread.sleep(1000);
+    } catch (InterruptedException ie) {}
+    fs.create(testTraceFile).close();
+    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, 
+                conf);
+    // test missing expected data size
+    assertEquals("Mismatch in trace data size", 
+                 Summarizer.NA, es.getExpectedDataSize());
+    assertFalse("Mismatch in trace signature", 
+                tid.equals(es.getInputTraceSignature()));
+    // get the new identifier
+    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
+    assertEquals("Mismatch in trace signature", 
+                 tid, es.getInputTraceSignature());
+    
+    testTraceFile = new Path(testDir, "test-trace2.json");
+    fs.create(testTraceFile).close();
+    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, 
+                conf);
+    assertFalse("Mismatch in trace signature", 
+                tid.equals(es.getInputTraceSignature()));
+    // get the new identifier
+    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
+    assertEquals("Mismatch in trace signature", 
+                 tid, es.getInputTraceSignature());
+    
+    // finalize trace identifier '-' input
+    es.finalize(factory, "-", 0L, resolver, dataStats, conf);
+    assertEquals("Mismatch in trace signature",
+                 Summarizer.NA, es.getInputTraceSignature());
+    assertEquals("Mismatch in trace file location", 
+                 Summarizer.NA, es.getInputTraceLocation());
+  }
+  
+  // test the ExecutionSummarizer
+  private static void testExecutionSummarizer(int numMaps, int numReds,
+      int totalJobsInTrace, int totalJobSubmitted, int numSuccessfulJob, 
+      int numFailedJobs, ExecutionSummarizer es) {
+    assertEquals("ExecutionSummarizer test failed [num-maps]", 
+                 numMaps, es.getNumMapTasksLaunched());
+    assertEquals("ExecutionSummarizer test failed [num-reducers]", 
+                 numReds, es.getNumReduceTasksLaunched());
+    assertEquals("ExecutionSummarizer test failed [num-jobs-in-trace]", 
+                 totalJobsInTrace, es.getNumJobsInTrace());
+    assertEquals("ExecutionSummarizer test failed [num-submitted jobs]", 
+                 totalJobSubmitted, es.getNumSubmittedJobs());
+    assertEquals("ExecutionSummarizer test failed [num-successful-jobs]", 
+                 numSuccessfulJob, es.getNumSuccessfulJobs());
+    assertEquals("ExecutionSummarizer test failed [num-failed jobs]", 
+                 numFailedJobs, es.getNumFailedJobs());
+  }
+  
+  // generate fake job stats
+  @SuppressWarnings("deprecation")
+  private static JobStats generateFakeJobStats(final int numMaps, 
+      final int numReds, final boolean isSuccessful) 
+  throws IOException {
+    // A fake job 
+    Job fakeJob = new Job() {
+      @Override
+      public int getNumReduceTasks() {
+        return numReds;
+      };
+      
+      @Override
+      public boolean isSuccessful() throws IOException, InterruptedException {
+        return isSuccessful;
+      };
+    };
+    return new JobStats(numMaps, numReds, fakeJob);
+  }
+  
+  /**
+   * Test {@link ClusterSummarizer}.
+   */
+  @Test
+  @SuppressWarnings("deprecation")
+  public void testClusterSummarizer() throws IOException {
+    ClusterSummarizer cs = new ClusterSummarizer();
+    Configuration conf = new Configuration();
+    
+    String jt = "test-jt:1234";
+    String nn = "test-nn:5678";
+    conf.set(JTConfig.JT_IPC_ADDRESS, jt);
+    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, nn);
+    cs.start(conf);
+    
+    assertEquals("JT name mismatch", jt, cs.getJobTrackerInfo());
+    assertEquals("NN name mismatch", nn, cs.getNamenodeInfo());
+    
+    ClusterStats cstats = ClusterStats.getClusterStats();
+    conf.set(JTConfig.JT_IPC_ADDRESS, "local");
+    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "local");
+    JobClient jc = new JobClient(conf);
+    cstats.setClusterMetric(jc.getClusterStatus());
+    
+    cs.update(cstats);
+    
+    // test
+    assertEquals("Cluster summary test failed!", 1, cs.getMaxMapTasks());
+    assertEquals("Cluster summary test failed!", 1, cs.getMaxReduceTasks());
+    assertEquals("Cluster summary test failed!", 1, cs.getNumActiveTrackers());
+    assertEquals("Cluster summary test failed!", 0, 
+                 cs.getNumBlacklistedTrackers());
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
new file mode 100644
index 0000000..5523d73
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
@@ -0,0 +1,195 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import static org.junit.Assert.*;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.gridmix.DebugJobProducer.MockJob;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.MRConfig;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.rumen.JobStory;
+import org.junit.Test;
+
+/**
+ * Test if Gridmix correctly configures the simulated job's configuration for
+ * high ram job properties.
+ */
+public class TestHighRamJob {
+  /**
+   * A dummy {@link GridmixJob} that opens up the simulated job for testing.
+   */
+  protected static class DummyGridmixJob extends GridmixJob {
+    public DummyGridmixJob(Configuration conf, JobStory desc) 
+    throws IOException {
+      super(conf, System.currentTimeMillis(), desc, new Path("test"), 
+            UserGroupInformation.getCurrentUser(), -1);
+    }
+    
+    /**
+     * Do nothing since this is a dummy gridmix job.
+     */
+    @Override
+    public Job call() throws Exception {
+      return null;
+    }
+    
+    @Override
+    protected boolean canEmulateCompression() {
+      // return false as we don't need compression
+      return false;
+    }
+    
+    protected Job getJob() {
+      // open the simulated job for testing
+      return job;
+    }
+  }
+  
+  private static void testHighRamConfig(long jobMapMB, long jobReduceMB, 
+      long clusterMapMB, long clusterReduceMB, long simulatedClusterMapMB, 
+      long simulatedClusterReduceMB, long expectedMapMB, long expectedReduceMB, 
+      Configuration gConf) 
+  throws IOException {
+    Configuration simulatedJobConf = new Configuration(gConf);
+    simulatedJobConf.setLong(MRConfig.MAPMEMORY_MB, simulatedClusterMapMB);
+    simulatedJobConf.setLong(MRConfig.REDUCEMEMORY_MB, 
+                             simulatedClusterReduceMB);
+    
+    // define a source conf
+    Configuration sourceConf = new Configuration();
+    
+    // configure the original job
+    sourceConf.setLong(MRJobConfig.MAP_MEMORY_MB, jobMapMB);
+    sourceConf.setLong(MRConfig.MAPMEMORY_MB, clusterMapMB);
+    sourceConf.setLong(MRJobConfig.REDUCE_MEMORY_MB, jobReduceMB);
+    sourceConf.setLong(MRConfig.REDUCEMEMORY_MB, clusterReduceMB);
+    
+    // define a mock job
+    MockJob story = new MockJob(sourceConf);
+    
+    GridmixJob job = new DummyGridmixJob(simulatedJobConf, story);
+    Job simulatedJob = job.getJob();
+    Configuration simulatedConf = simulatedJob.getConfiguration();
+    
+    // check if the high ram properties are not set
+    assertEquals(expectedMapMB, 
+                 simulatedConf.getLong(MRJobConfig.MAP_MEMORY_MB,
+                                       JobConf.DISABLED_MEMORY_LIMIT));
+    assertEquals(expectedReduceMB, 
+                 simulatedConf.getLong(MRJobConfig.REDUCE_MEMORY_MB, 
+                                       JobConf.DISABLED_MEMORY_LIMIT));
+  }
+  
+  /**
+   * Tests high ram job properties configuration.
+   */
+  @SuppressWarnings("deprecation")
+  @Test
+  public void testHighRamFeatureEmulation() throws IOException {
+    // define the gridmix conf
+    Configuration gridmixConf = new Configuration();
+    
+    // test : check high ram emulation disabled
+    gridmixConf.setBoolean(GridmixJob.GRIDMIX_HIGHRAM_EMULATION_ENABLE, false);
+    testHighRamConfig(10, 20, 5, 10, JobConf.DISABLED_MEMORY_LIMIT, 
+                      JobConf.DISABLED_MEMORY_LIMIT, 
+                      JobConf.DISABLED_MEMORY_LIMIT, 
+                      JobConf.DISABLED_MEMORY_LIMIT, gridmixConf);
+    
+    // test : check with high ram enabled (default) and no scaling
+    gridmixConf = new Configuration();
+    // set the deprecated max memory limit
+    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
+                        20*1024*1024);
+    testHighRamConfig(10, 20, 5, 10, 5, 10, 10, 20, gridmixConf);
+    
+    // test : check with high ram enabled and scaling
+    gridmixConf = new Configuration();
+    // set the new max map/reduce memory limits
+    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 100);
+    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 300);
+    testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
+    
+    // test : check with high ram enabled and map memory scaling mismatch 
+    //        (deprecated)
+    gridmixConf = new Configuration();
+    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
+                        70*1024*1024);
+    Boolean failed = null;
+    try {
+      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
+      failed = false;
+    } catch (Exception e) {
+      failed = true;
+    }
+    assertNotNull(failed);
+    assertTrue("Exception expected for exceeding map memory limit "
+               + "(deprecation)!", failed);
+    
+    // test : check with high ram enabled and reduce memory scaling mismatch 
+    //        (deprecated)
+    gridmixConf = new Configuration();
+    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
+                        150*1024*1024);
+    failed = null;
+    try {
+      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
+      failed = false;
+    } catch (Exception e) {
+      failed = true;
+    }
+    assertNotNull(failed);
+    assertTrue("Exception expected for exceeding reduce memory limit "
+               + "(deprecation)!", failed);
+    
+    // test : check with high ram enabled and scaling mismatch on map limits
+    gridmixConf = new Configuration();
+    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 70);
+    failed = null;
+    try {
+      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
+      failed = false;
+    } catch (Exception e) {
+      failed = true;
+    }
+    assertNotNull(failed);
+    assertTrue("Exception expected for exceeding map memory limit!", failed);
+    
+    // test : check with high ram enabled and scaling mismatch on reduce 
+    //        limits
+    gridmixConf = new Configuration();
+    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 200);
+    failed = null;
+    try {
+      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
+      failed = false;
+    } catch (Exception e) {
+      failed = true;
+    }
+    assertNotNull(failed);
+    assertTrue("Exception expected for exceeding reduce memory limit!", failed);
+  }
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
new file mode 100644
index 0000000..b9c2728
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
@@ -0,0 +1,233 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import static org.junit.Assert.*;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStream;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.junit.Test;
+
+/**
+ * Test the basic functionality of PseudoLocalFs
+ */
+public class TestPseudoLocalFs {
+
+  /**
+   * Test if a file on PseudoLocalFs of a specific size can be opened and read.
+   * Validate the size of the data read.
+   * Test the read methods of {@link PseudoLocalFs.RandomInputStream}.
+   * @throws Exception
+   */
+  @Test
+  public void testPseudoLocalFsFileSize() throws Exception {
+    long fileSize = 10000;
+    Path path = PseudoLocalFs.generateFilePath("myPsedoFile", fileSize);
+    PseudoLocalFs pfs = new PseudoLocalFs();
+    pfs.create(path);
+
+    // Read 1 byte at a time and validate file size.
+    InputStream in = pfs.open(path, 0);
+    long totalSize = 0;
+
+    while (in.read() >= 0) {
+      ++totalSize;
+    }
+    in.close();
+    assertEquals("File size mismatch with read().", fileSize, totalSize);
+
+    // Read data from PseudoLocalFs-based file into buffer to
+    // validate read(byte[]) and file size.
+    in = pfs.open(path, 0);
+    totalSize = 0;
+    byte[] b = new byte[1024];
+    int bytesRead = in.read(b);
+    while (bytesRead >= 0) {
+      totalSize += bytesRead;
+      bytesRead = in.read(b);
+    }
+    assertEquals("File size mismatch with read(byte[]).", fileSize, totalSize);
+  }
+
+  /**
+   * Validate if file status is obtained for correctly formed file paths on
+   * PseudoLocalFs and also verify if appropriate exception is thrown for
+   * invalid file paths.
+   * @param pfs Pseudo Local File System
+   * @param path file path for which getFileStatus() is to be called
+   * @param shouldSucceed <code>true</code> if getFileStatus() should succeed
+   * @throws IOException
+   */
+  private void validateGetFileStatus(FileSystem pfs, Path path,
+      boolean shouldSucceed) throws IOException {
+    boolean expectedExceptionSeen = false;
+    FileStatus stat = null;
+    try {
+      stat = pfs.getFileStatus(path);
+    } catch(FileNotFoundException e) {
+      expectedExceptionSeen = true;
+    }
+    if (shouldSucceed) {
+      assertFalse("getFileStatus() has thrown Exception for valid file name "
+                  + path, expectedExceptionSeen);
+      assertNotNull("Missing file status for a valid file.", stat);
+
+      // validate fileSize
+      String[] parts = path.toUri().getPath().split("\\.");
+      long expectedFileSize = Long.valueOf(parts[parts.length - 1]);
+      assertEquals("Invalid file size.", expectedFileSize, stat.getLen());
+    } else {
+      assertTrue("getFileStatus() did not throw Exception for invalid file "
+                 + " name " + path, expectedExceptionSeen);
+    }
+  }
+
+  /**
+   * Validate if file creation succeeds for correctly formed file paths on
+   * PseudoLocalFs and also verify if appropriate exception is thrown for
+   * invalid file paths.
+   * @param pfs Pseudo Local File System
+   * @param path file path for which create() is to be called
+   * @param shouldSucceed <code>true</code> if create() should succeed
+   * @throws IOException
+   */
+  private void validateCreate(FileSystem pfs, Path path,
+      boolean shouldSucceed) throws IOException {
+    boolean expectedExceptionSeen = false;
+    try {
+      pfs.create(path);
+    } catch(IOException e) {
+      expectedExceptionSeen = true;
+    }
+    if (shouldSucceed) {
+      assertFalse("create() has thrown Exception for valid file name "
+                  + path, expectedExceptionSeen);
+    } else {
+      assertTrue("create() did not throw Exception for invalid file name "
+                 + path, expectedExceptionSeen);
+    }
+  }
+
+  /**
+   * Validate if opening of file succeeds for correctly formed file paths on
+   * PseudoLocalFs and also verify if appropriate exception is thrown for
+   * invalid file paths.
+   * @param pfs Pseudo Local File System
+   * @param path file path for which open() is to be called
+   * @param shouldSucceed <code>true</code> if open() should succeed
+   * @throws IOException
+   */
+  private void validateOpen(FileSystem pfs, Path path,
+      boolean shouldSucceed) throws IOException {
+    boolean expectedExceptionSeen = false;
+    try {
+      pfs.open(path);
+    } catch(IOException e) {
+      expectedExceptionSeen = true;
+    }
+    if (shouldSucceed) {
+      assertFalse("open() has thrown Exception for valid file name "
+                  + path, expectedExceptionSeen);
+    } else {
+      assertTrue("open() did not throw Exception for invalid file name "
+                 + path, expectedExceptionSeen);
+    }
+  }
+
+  /**
+   * Validate if exists() returns <code>true</code> for correctly formed file
+   * paths on PseudoLocalFs and returns <code>false</code> for improperly
+   * formed file paths.
+   * @param pfs Pseudo Local File System
+   * @param path file path for which exists() is to be called
+   * @param shouldSucceed expected return value of exists(&lt;path&gt;)
+   * @throws IOException
+   */
+  private void validateExists(FileSystem pfs, Path path,
+      boolean shouldSucceed) throws IOException {
+    boolean ret = pfs.exists(path);
+    if (shouldSucceed) {
+      assertTrue("exists() returned false for valid file name " + path, ret);
+    } else {
+      assertFalse("exists() returned true for invalid file name " + path, ret);
+    }
+  }
+
+  /**
+   *  Test Pseudo Local File System methods like getFileStatus(), create(),
+   *  open(), exists() for <li> valid file paths and <li> invalid file paths.
+   * @throws IOException
+   */
+  @Test
+  public void testPseudoLocalFsFileNames() throws IOException {
+    PseudoLocalFs pfs = new PseudoLocalFs();
+    Configuration conf = new Configuration();
+    conf.setClass("fs.pseudo.impl", PseudoLocalFs.class, FileSystem.class);
+
+    Path path = new Path("pseudo:///myPsedoFile.1234");
+    FileSystem testFs = path.getFileSystem(conf);
+    assertEquals("Failed to obtain a pseudo local file system object from path",
+                 pfs.getUri().getScheme(), testFs.getUri().getScheme());
+
+    // Validate PseudoLocalFS operations on URI of some other file system
+    path = new Path("file:///myPsedoFile.12345");
+    validateGetFileStatus(pfs, path, false);
+    validateCreate(pfs, path, false);
+    validateOpen(pfs, path, false);
+    validateExists(pfs, path, false);
+
+    path = new Path("pseudo:///myPsedoFile");//.<fileSize> missing
+    validateGetFileStatus(pfs, path, false);
+    validateCreate(pfs, path, false);
+    validateOpen(pfs, path, false);
+    validateExists(pfs, path, false);
+
+    // thing after final '.' is not a number
+    path = new Path("pseudo:///myPsedoFile.txt");
+    validateGetFileStatus(pfs, path, false);
+    validateCreate(pfs, path, false);
+    validateOpen(pfs, path, false);
+    validateExists(pfs, path, false);
+
+    // Generate valid file name(relative path) and validate operations on it
+    long fileSize = 231456;
+    path = PseudoLocalFs.generateFilePath("my.Psedo.File", fileSize);
+    // Validate the above generateFilePath()
+    assertEquals("generateFilePath() failed.", fileSize,
+                 pfs.validateFileNameFormat(path));
+
+    validateGetFileStatus(pfs, path, true);
+    validateCreate(pfs, path, true);
+    validateOpen(pfs, path, true);
+    validateExists(pfs, path, true);
+
+    // Validate operations on valid qualified path
+    path = new Path("myPsedoFile.1237");
+    path = path.makeQualified(pfs);
+    validateGetFileStatus(pfs, path, true);
+    validateCreate(pfs, path, true);
+    validateOpen(pfs, path, true);
+    validateExists(pfs, path, true);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
new file mode 100644
index 0000000..cd55483
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
@@ -0,0 +1,134 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import static org.junit.Assert.*;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.junit.Test;
+
+import com.sun.tools.javac.code.Attribute.Array;
+
+public class TestRandomAlgorithm {
+  private static final int[][] parameters = new int[][] {
+    {5, 1, 1}, 
+    {10, 1, 2},
+    {10, 2, 2},
+    {20, 1, 3},
+    {20, 2, 3},
+    {20, 3, 3},
+    {100, 3, 10},
+    {100, 3, 100},
+    {100, 3, 1000},
+    {100, 3, 10000},
+    {100, 3, 100000},
+    {100, 3, 1000000}
+  };
+  
+  private List<Integer> convertIntArray(int[] from) {
+    List<Integer> ret = new ArrayList<Integer>(from.length);
+    for (int v : from) {
+      ret.add(v);
+    }
+    return ret;
+  }
+  
+  private void testRandomSelectSelector(int niter, int m, int n) {
+    RandomAlgorithms.Selector selector = new RandomAlgorithms.Selector(n,
+        (double) m / n, new Random());
+    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(
+        niter);
+    for (int i = 0; i < niter; ++i, selector.reset()) {
+      int[] result = new int[m];
+      for (int j = 0; j < m; ++j) {
+        int v = selector.next();
+        if (v < 0)
+          break;
+        result[j]=v;
+      }
+      Arrays.sort(result);
+      List<Integer> resultAsList = convertIntArray(result);
+      Integer count = results.get(resultAsList);
+      if (count == null) {
+        results.put(resultAsList, 1);
+      } else {
+        results.put(resultAsList, ++count);
+      }
+    }
+
+    verifyResults(results, m, n);
+  }
+
+  private void testRandomSelect(int niter, int m, int n) {
+    Random random = new Random();
+    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(
+        niter);
+    for (int i = 0; i < niter; ++i) {
+      int[] result = RandomAlgorithms.select(m, n, random);
+      Arrays.sort(result);
+      List<Integer> resultAsList = convertIntArray(result);
+      Integer count = results.get(resultAsList);
+      if (count == null) {
+        results.put(resultAsList, 1);
+      } else {
+        results.put(resultAsList, ++count);
+      }
+    }
+
+    verifyResults(results, m, n);
+  }
+
+  private void verifyResults(Map<List<Integer>, Integer> results, int m, int n) {
+    if (n>=10) {
+      assertTrue(results.size() >= Math.min(m, 2));
+    }
+    for (List<Integer> result : results.keySet()) {
+      assertEquals(m, result.size());
+      Set<Integer> seen = new HashSet<Integer>();
+      for (int v : result) {
+        System.out.printf("%d ", v);
+        assertTrue((v >= 0) && (v < n));
+        assertTrue(seen.add(v));
+      }
+      System.out.printf(" ==> %d\n", results.get(result));
+    }
+    System.out.println("====");
+  }
+  
+  @Test
+  public void testRandomSelect() {
+    for (int[] param : parameters) {
+    testRandomSelect(param[0], param[1], param[2]);
+    }
+  }
+  
+  @Test
+  public void testRandomSelectSelector() {
+    for (int[] param : parameters) {
+      testRandomSelectSelector(param[0], param[1], param[2]);
+      }
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
new file mode 100644
index 0000000..e302db5
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator;
+
+import static org.junit.Assert.*;
+import org.junit.Test;
+
+/**
+ * Test {@link RandomTextDataGenerator}.
+ */
+public class TestRandomTextDataGenerator {
+  /**
+   * Test if {@link RandomTextDataGenerator} can generate random words of 
+   * desired size.
+   */
+  @Test
+  public void testRandomTextDataGenerator() {
+    RandomTextDataGenerator rtdg = new RandomTextDataGenerator(10, 0L, 5);
+    List<String> words = rtdg.getRandomWords();
+
+    // check the size
+    assertEquals("List size mismatch", 10, words.size());
+
+    // check the words
+    Set<String> wordsSet = new HashSet<String>(words);
+    assertEquals("List size mismatch due to duplicates", 10, wordsSet.size());
+
+    // check the word lengths
+    for (String word : wordsSet) {
+      assertEquals("Word size mismatch", 5, word.length());
+    }
+  }
+  
+  /**
+   * Test if {@link RandomTextDataGenerator} can generate same words given the
+   * same list-size, word-length and seed.
+   */
+  @Test
+  public void testRandomTextDataGeneratorRepeatability() {
+    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 0L, 5);
+    List<String> words1 = rtdg1.getRandomWords();
+
+    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);
+    List<String> words2 = rtdg2.getRandomWords();
+    
+    assertTrue("List mismatch", words1.equals(words2));
+  }
+  
+  /**
+   * Test if {@link RandomTextDataGenerator} can generate different words given 
+   * different seeds.
+   */
+  @Test
+  public void testRandomTextDataGeneratorUniqueness() {
+    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 1L, 5);
+    Set<String> words1 = new HashSet(rtdg1.getRandomWords());
+
+    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);
+    Set<String> words2 = new HashSet(rtdg2.getRandomWords());
+    
+    assertFalse("List size mismatch across lists", words1.equals(words2));
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
new file mode 100644
index 0000000..2ab2444
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
@@ -0,0 +1,81 @@
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.util.Random;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.DataOutputBuffer;
+
+public class TestRecordFactory {
+  private static final Log LOG = LogFactory.getLog(TestRecordFactory.class);
+
+  public static void testFactory(long targetBytes, long targetRecs)
+      throws Exception {
+    final Configuration conf = new Configuration();
+    final GridmixKey key = new GridmixKey();
+    final GridmixRecord val = new GridmixRecord();
+    LOG.info("Target bytes/records: " + targetBytes + "/" + targetRecs);
+    final RecordFactory f = new AvgRecordFactory(targetBytes, targetRecs, conf);
+    targetRecs = targetRecs <= 0 && targetBytes >= 0
+      ? Math.max(1,
+                 targetBytes 
+                 / conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 
+                               64 * 1024))
+      : targetRecs;
+
+    long records = 0L;
+    final DataOutputBuffer out = new DataOutputBuffer();
+    while (f.next(key, val)) {
+      ++records;
+      key.write(out);
+      val.write(out);
+    }
+    assertEquals(targetRecs, records);
+    assertEquals(targetBytes, out.getLength());
+  }
+
+  @Test
+  public void testRandom() throws Exception {
+    final Random r = new Random();
+    final long targetBytes = r.nextInt(1 << 20) + 3 * (1 << 14);
+    final long targetRecs = r.nextInt(1 << 14);
+    testFactory(targetBytes, targetRecs);
+  }
+
+  @Test
+  public void testAvg() throws Exception {
+    final Random r = new Random();
+    final long avgsize = r.nextInt(1 << 10) + 1;
+    final long targetRecs = r.nextInt(1 << 14);
+    testFactory(targetRecs * avgsize, targetRecs);
+  }
+
+  @Test
+  public void testZero() throws Exception {
+    final Random r = new Random();
+    final long targetBytes = r.nextInt(1 << 20);
+    testFactory(targetBytes, 0);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
new file mode 100644
index 0000000..35db026
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
@@ -0,0 +1,613 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.StatusReporter;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskInputOutputContext;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
+import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.ProcResourceValues;
+import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
+import org.apache.hadoop.mapred.DummyResourceCalculatorPlugin;
+import org.apache.hadoop.mapred.gridmix.LoadJob.ResourceUsageMatcherRunner;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageEmulatorPlugin;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher;
+import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin.DefaultCpuUsageEmulator;
+
+/**
+ * Test Gridmix's resource emulator framework and supported plugins.
+ */
+public class TestResourceUsageEmulators {
+  /**
+   * A {@link ResourceUsageEmulatorPlugin} implementation for testing purpose.
+   * It essentially creates a file named 'test' in the test directory.
+   */
+  static class TestResourceUsageEmulatorPlugin 
+  implements ResourceUsageEmulatorPlugin {
+    static final Path rootTempDir =
+        new Path(System.getProperty("test.build.data", "/tmp"));
+    static final Path tempDir = 
+      new Path(rootTempDir, "TestResourceUsageEmulatorPlugin");
+    static final String DEFAULT_IDENTIFIER = "test";
+    
+    private Path touchPath = null;
+    private FileSystem fs = null;
+    
+    @Override
+    public void emulate() throws IOException, InterruptedException {
+      // add some time between 2 calls to emulate()
+      try {
+        Thread.sleep(1000); // sleep for 1s
+      } catch (Exception e){}
+      
+      try {
+        fs.delete(touchPath, false); // delete the touch file
+        //TODO Search for a better touch utility
+        fs.create(touchPath).close(); // recreate it
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    protected String getIdentifier() {
+      return DEFAULT_IDENTIFIER;
+    }
+    
+    private static Path getFilePath(String id) {
+      return new Path(tempDir, id);
+    }
+    
+    private static Path getInitFilePath(String id) {
+      return new Path(tempDir, id + ".init");
+    }
+    
+    @Override
+    public void initialize(Configuration conf, ResourceUsageMetrics metrics,
+        ResourceCalculatorPlugin monitor, Progressive progress) {
+      // add some time between 2 calls to initialize()
+      try {
+        Thread.sleep(1000); // sleep for 1s
+      } catch (Exception e){}
+      
+      try {
+        fs = FileSystem.getLocal(conf);
+        
+        Path initPath = getInitFilePath(getIdentifier());
+        fs.delete(initPath, false); // delete the old file
+        fs.create(initPath).close(); // create a new one
+        
+        touchPath = getFilePath(getIdentifier());
+        fs.delete(touchPath, false);
+      } catch (Exception e) {
+        
+      } finally {
+        if (fs != null) {
+          try {
+            fs.deleteOnExit(tempDir);
+          } catch (IOException ioe){}
+        }
+      }
+    }
+    
+    // test if the emulation framework successfully loaded this plugin
+    static long testInitialization(String id, Configuration conf) 
+    throws IOException {
+      Path testPath = getInitFilePath(id);
+      FileSystem fs = FileSystem.getLocal(conf);
+      return fs.exists(testPath) 
+             ? fs.getFileStatus(testPath).getModificationTime() 
+             : 0;
+    }
+    
+    // test if the emulation framework successfully loaded this plugin
+    static long testEmulation(String id, Configuration conf) 
+    throws IOException {
+      Path testPath = getFilePath(id);
+      FileSystem fs = FileSystem.getLocal(conf);
+      return fs.exists(testPath) 
+             ? fs.getFileStatus(testPath).getModificationTime() 
+             : 0;
+    }
+  }
+  
+  /**
+   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
+   * a file named 'others' in the test directory.
+   */
+  static class TestOthers extends TestResourceUsageEmulatorPlugin {
+    static final String ID = "others";
+    
+    @Override
+    protected String getIdentifier() {
+      return ID;
+    }
+  }
+  
+  /**
+   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
+   * a file named 'cpu' in the test directory.
+   */
+  static class TestCpu extends TestResourceUsageEmulatorPlugin {
+    static final String ID = "cpu";
+    
+    @Override
+    protected String getIdentifier() {
+      return ID;
+    }
+  }
+  
+  /**
+   * Test {@link ResourceUsageMatcher}.
+   */
+  @Test
+  public void testResourceUsageMatcher() throws Exception {
+    ResourceUsageMatcher matcher = new ResourceUsageMatcher();
+    Configuration conf = new Configuration();
+    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
+                  TestResourceUsageEmulatorPlugin.class, 
+                  ResourceUsageEmulatorPlugin.class);
+    long currentTime = System.currentTimeMillis();
+    
+    matcher.configure(conf, null, null, null);
+    
+    matcher.matchResourceUsage();
+    
+    String id = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;
+    long result = 
+      TestResourceUsageEmulatorPlugin.testInitialization(id, conf);
+    assertTrue("Resource usage matcher failed to initialize the configured"
+               + " plugin", result > currentTime);
+    result = TestResourceUsageEmulatorPlugin.testEmulation(id, conf);
+    assertTrue("Resource usage matcher failed to load and emulate the"
+               + " configured plugin", result > currentTime);
+    
+    // test plugin order to first emulate cpu and then others
+    conf.setStrings(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
+                    TestCpu.class.getName() + "," + TestOthers.class.getName());
+    
+    matcher.configure(conf, null, null, null);
+
+    // test the initialization order
+    long time1 = 
+           TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);
+    long time2 = 
+           TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, 
+                                                              conf);
+    assertTrue("Resource usage matcher failed to initialize the configured"
+               + " plugins in order", time1 < time2);
+    
+    matcher.matchResourceUsage();
+
+    // Note that the cpu usage emulator plugin is configured 1st and then the
+    // others plugin.
+    time1 = 
+      TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);
+    time2 = 
+      TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, 
+                                                         conf);
+    assertTrue("Resource usage matcher failed to load the configured plugins", 
+               time1 < time2);
+  }
+  
+  /**
+   * Fakes the cumulative usage using {@link FakeCpuUsageEmulatorCore}.
+   */
+  static class FakeResourceUsageMonitor extends DummyResourceCalculatorPlugin {
+    private FakeCpuUsageEmulatorCore core;
+    
+    public FakeResourceUsageMonitor(FakeCpuUsageEmulatorCore core) {
+      this.core = core;
+    }
+    
+    /**
+     * A dummy CPU usage monitor. Every call to 
+     * {@link ResourceCalculatorPlugin#getCumulativeCpuTime()} will return the 
+     * value of {@link FakeCpuUsageEmulatorCore#getNumCalls()}.
+     */
+    @Override
+    public long getCumulativeCpuTime() {
+      return core.getCpuUsage();
+    }
+
+    /**
+     * Returns a {@link ProcResourceValues} with cumulative cpu usage  
+     * computed using {@link #getCumulativeCpuTime()}.
+     */
+    @Override
+    public ProcResourceValues getProcResourceValues() {
+      long usageValue = getCumulativeCpuTime();
+      return new ProcResourceValues(usageValue, -1, -1);
+    }
+  }
+  
+  /**
+   * A dummy {@link Progressive} implementation that allows users to set the
+   * progress for testing. The {@link Progressive#getProgress()} call will 
+   * return the last progress value set using 
+   * {@link FakeProgressive#setProgress(float)}.
+   */
+  static class FakeProgressive implements Progressive {
+    private float progress = 0F;
+    @Override
+    public float getProgress() {
+      return progress;
+    }
+    
+    void setProgress(float progress) {
+      this.progress = progress;
+    }
+  }
+  
+  /**
+   * A dummy reporter for {@link LoadJob.ResourceUsageMatcherRunner}.
+   */
+  private static class DummyReporter extends StatusReporter {
+    private Progressive progress;
+    
+    DummyReporter(Progressive progress) {
+      this.progress = progress;
+    }
+    
+    @Override
+    public org.apache.hadoop.mapreduce.Counter getCounter(Enum<?> name) {
+      return null;
+    }
+    
+    @Override
+    public org.apache.hadoop.mapreduce.Counter getCounter(String group,
+                                                          String name) {
+      return null;
+    }
+    
+    @Override
+    public void progress() {
+    }
+    
+    @Override
+    public float getProgress() {
+      return progress.getProgress();
+    }
+    
+    @Override
+    public void setStatus(String status) {
+    }
+  }
+  
+  // Extends ResourceUsageMatcherRunner for testing.
+  @SuppressWarnings("unchecked")
+  private static class FakeResourceUsageMatcherRunner 
+  extends ResourceUsageMatcherRunner {
+    FakeResourceUsageMatcherRunner(TaskInputOutputContext context, 
+                                   ResourceUsageMetrics metrics) {
+      super(context, metrics);
+    }
+    
+    // test ResourceUsageMatcherRunner
+    void test() throws Exception {
+      super.match();
+    }
+  }
+  
+  /**
+   * Test {@link LoadJob.ResourceUsageMatcherRunner}.
+   */
+  @Test
+  @SuppressWarnings("unchecked")
+  public void testResourceUsageMatcherRunner() throws Exception {
+    Configuration conf = new Configuration();
+    FakeProgressive progress = new FakeProgressive();
+    
+    // set the resource calculator plugin
+    conf.setClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN,
+                  DummyResourceCalculatorPlugin.class, 
+                  ResourceCalculatorPlugin.class);
+    // set the resources
+    // set the resource implementation class
+    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
+                  TestResourceUsageEmulatorPlugin.class, 
+                  ResourceUsageEmulatorPlugin.class);
+    
+    long currentTime = System.currentTimeMillis();
+    
+    // initialize the matcher class
+    TaskAttemptID id = new TaskAttemptID("test", 1, TaskType.MAP, 1, 1);
+    StatusReporter reporter = new DummyReporter(progress);
+    TaskInputOutputContext context = 
+      new MapContextImpl(conf, id, null, null, null, reporter, null);
+    FakeResourceUsageMatcherRunner matcher = 
+      new FakeResourceUsageMatcherRunner(context, null);
+    
+    // check if the matcher initialized the plugin
+    String identifier = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;
+    long initTime = 
+      TestResourceUsageEmulatorPlugin.testInitialization(identifier, conf);
+    assertTrue("ResourceUsageMatcherRunner failed to initialize the"
+               + " configured plugin", initTime > currentTime);
+    
+    // check the progress
+    assertEquals("Progress mismatch in ResourceUsageMatcherRunner", 
+                 0, progress.getProgress(), 0D);
+    
+    // call match() and check progress
+    progress.setProgress(0.01f);
+    currentTime = System.currentTimeMillis();
+    matcher.test();
+    long emulateTime = 
+      TestResourceUsageEmulatorPlugin.testEmulation(identifier, conf);
+    assertTrue("ProgressBasedResourceUsageMatcher failed to load and emulate"
+               + " the configured plugin", emulateTime > currentTime);
+  }
+  
+  /**
+   * Test {@link CumulativeCpuUsageEmulatorPlugin}'s core CPU usage emulation 
+   * engine.
+   */
+  @Test
+  public void testCpuUsageEmulator() throws IOException {
+    // test CpuUsageEmulator calibration with fake resource calculator plugin
+    long target = 100000L; // 100 secs
+    int unitUsage = 50;
+    FakeCpuUsageEmulatorCore fakeCpuEmulator = new FakeCpuUsageEmulatorCore();
+    fakeCpuEmulator.setUnitUsage(unitUsage);
+    FakeResourceUsageMonitor fakeMonitor = 
+      new FakeResourceUsageMonitor(fakeCpuEmulator);
+    
+    // calibrate for 100ms
+    fakeCpuEmulator.calibrate(fakeMonitor, target);
+    
+    // by default, CpuUsageEmulator.calibrate() will consume 100ms of CPU usage
+    assertEquals("Fake calibration failed", 
+                 100, fakeMonitor.getCumulativeCpuTime());
+    assertEquals("Fake calibration failed", 
+                 100, fakeCpuEmulator.getCpuUsage());
+    // by default, CpuUsageEmulator.performUnitComputation() will be called 
+    // twice
+    assertEquals("Fake calibration failed", 
+                 2, fakeCpuEmulator.getNumCalls());
+  }
+  
+  /**
+   * This is a dummy class that fakes CPU usage.
+   */
+  private static class FakeCpuUsageEmulatorCore 
+  extends DefaultCpuUsageEmulator {
+    private int numCalls = 0;
+    private int unitUsage = 1;
+    private int cpuUsage = 0;
+    
+    @Override
+    protected void performUnitComputation() {
+      ++numCalls;
+      cpuUsage += unitUsage;
+    }
+    
+    int getNumCalls() {
+      return numCalls;
+    }
+    
+    int getCpuUsage() {
+      return cpuUsage;
+    }
+    
+    void reset() {
+      numCalls = 0;
+      cpuUsage = 0;
+    }
+    
+    void setUnitUsage(int unitUsage) {
+      this.unitUsage = unitUsage;
+    }
+  }
+  
+  // Creates a ResourceUsageMetrics object from the target usage
+  static ResourceUsageMetrics createMetrics(long target) {
+    ResourceUsageMetrics metrics = new ResourceUsageMetrics();
+    metrics.setCumulativeCpuUsage(target);
+    metrics.setVirtualMemoryUsage(target);
+    metrics.setPhysicalMemoryUsage(target);
+    metrics.setHeapUsage(target);
+    return metrics;
+  }
+  
+  /**
+   * Test {@link CumulativeCpuUsageEmulatorPlugin}.
+   */
+  @Test
+  public void testCumulativeCpuUsageEmulatorPlugin() throws Exception {
+    Configuration conf = new Configuration();
+    long targetCpuUsage = 1000L;
+    int unitCpuUsage = 50;
+    
+    // fake progress indicator
+    FakeProgressive fakeProgress = new FakeProgressive();
+    
+    // fake cpu usage generator
+    FakeCpuUsageEmulatorCore fakeCore = new FakeCpuUsageEmulatorCore();
+    fakeCore.setUnitUsage(unitCpuUsage);
+    
+    // a cumulative cpu usage emulator with fake core
+    CumulativeCpuUsageEmulatorPlugin cpuPlugin = 
+      new CumulativeCpuUsageEmulatorPlugin(fakeCore);
+    
+    // test with invalid or missing resource usage value
+    ResourceUsageMetrics invalidUsage = createMetrics(0);
+    cpuPlugin.initialize(conf, invalidUsage, null, null);
+    
+    // test if disabled cpu emulation plugin's emulate() call is a no-operation
+    // this will test if the emulation plugin is disabled or not
+    int numCallsPre = fakeCore.getNumCalls();
+    long cpuUsagePre = fakeCore.getCpuUsage();
+    cpuPlugin.emulate();
+    int numCallsPost = fakeCore.getNumCalls();
+    long cpuUsagePost = fakeCore.getCpuUsage();
+    
+    //  test if no calls are made cpu usage emulator core
+    assertEquals("Disabled cumulative CPU usage emulation plugin works!", 
+                 numCallsPre, numCallsPost);
+    
+    //  test if no calls are made cpu usage emulator core
+    assertEquals("Disabled cumulative CPU usage emulation plugin works!", 
+                 cpuUsagePre, cpuUsagePost);
+    
+    // test with valid resource usage value
+    ResourceUsageMetrics metrics = createMetrics(targetCpuUsage);
+    
+    // fake monitor
+    ResourceCalculatorPlugin monitor = new FakeResourceUsageMonitor(fakeCore);
+    
+    // test with default emulation interval
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, 
+                          targetCpuUsage, targetCpuUsage / unitCpuUsage);
+    
+    // test with custom value for emulation interval of 20%
+    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
+                  0.2F);
+    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, 
+                          targetCpuUsage, targetCpuUsage / unitCpuUsage);
+    
+    // test if emulation interval boundary is respected (unit usage = 1)
+    //  test the case where the current progress is less than threshold
+    fakeProgress = new FakeProgressive(); // initialize
+    fakeCore.reset();
+    fakeCore.setUnitUsage(1);
+    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
+                  0.25F);
+    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
+    // take a snapshot after the initialization
+    long initCpuUsage = monitor.getCumulativeCpuTime();
+    long initNumCalls = fakeCore.getNumCalls();
+    // test with 0 progress
+    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, 
+                          initNumCalls, "[no-op, 0 progress]");
+    // test with 24% progress
+    testEmulationBoundary(0.24F, fakeCore, fakeProgress, cpuPlugin, 
+                          initCpuUsage, initNumCalls, "[no-op, 24% progress]");
+    // test with 25% progress
+    //  target = 1000ms, target emulation at 25% = 250ms, 
+    //  weighed target = 1000 * 0.25^4 (we are using progress^4 as the weight)
+    //                 ~ 4
+    //  but current usage = init-usage = 100, hence expected = 100
+    testEmulationBoundary(0.25F, fakeCore, fakeProgress, cpuPlugin, 
+                          initCpuUsage, initNumCalls, "[op, 25% progress]");
+    
+    // test with 80% progress
+    //  target = 1000ms, target emulation at 80% = 800ms, 
+    //  weighed target = 1000 * 0.25^4 (we are using progress^4 as the weight)
+    //                 ~ 410
+    //  current-usage = init-usage = 100, hence expected-usage = 410
+    testEmulationBoundary(0.80F, fakeCore, fakeProgress, cpuPlugin, 410, 410, 
+                          "[op, 80% progress]");
+    
+    // now test if the final call with 100% progress ramps up the CPU usage
+    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage,
+                          targetCpuUsage, "[op, 100% progress]");
+    
+    // test if emulation interval boundary is respected (unit usage = 50)
+    //  test the case where the current progress is less than threshold
+    fakeProgress = new FakeProgressive(); // initialize
+    fakeCore.reset();
+    fakeCore.setUnitUsage(unitCpuUsage);
+    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
+                  0.40F);
+    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
+    // take a snapshot after the initialization
+    initCpuUsage = monitor.getCumulativeCpuTime();
+    initNumCalls = fakeCore.getNumCalls();
+    // test with 0 progress
+    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, 
+                          initNumCalls, "[no-op, 0 progress]");
+    // test with 39% progress
+    testEmulationBoundary(0.39F, fakeCore, fakeProgress, cpuPlugin, 
+                          initCpuUsage, initNumCalls, "[no-op, 39% progress]");
+    // test with 40% progress
+    //  target = 1000ms, target emulation at 40% = 4000ms, 
+    //  weighed target = 1000 * 0.40^4 (we are using progress^4 as the weight)
+    //                 ~ 26
+    // current-usage = init-usage = 100, hence expected-usage = 100
+    testEmulationBoundary(0.40F, fakeCore, fakeProgress, cpuPlugin, 
+                          initCpuUsage, initNumCalls, "[op, 40% progress]");
+    
+    // test with 90% progress
+    //  target = 1000ms, target emulation at 90% = 900ms, 
+    //  weighed target = 1000 * 0.90^4 (we are using progress^4 as the weight)
+    //                 ~ 657
+    //  current-usage = init-usage = 100, hence expected-usage = 657 but 
+    //  the fake-core increases in steps of 50, hence final target = 700
+    testEmulationBoundary(0.90F, fakeCore, fakeProgress, cpuPlugin, 700, 
+                          700 / unitCpuUsage, "[op, 90% progress]");
+    
+    // now test if the final call with 100% progress ramps up the CPU usage
+    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage,
+                          targetCpuUsage / unitCpuUsage, "[op, 100% progress]");
+  }
+  
+  // test whether the CPU usage emulator achieves the desired target using
+  // desired calls to the underling core engine.
+  private static void testEmulationAccuracy(Configuration conf, 
+                        FakeCpuUsageEmulatorCore fakeCore,
+                        ResourceCalculatorPlugin monitor,
+                        ResourceUsageMetrics metrics,
+                        CumulativeCpuUsageEmulatorPlugin cpuPlugin,
+                        long expectedTotalCpuUsage, long expectedTotalNumCalls) 
+  throws Exception {
+    FakeProgressive fakeProgress = new FakeProgressive();
+    fakeCore.reset();
+    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
+    int numLoops = 0;
+    while (fakeProgress.getProgress() < 1) {
+      ++numLoops;
+      float progress = (float)numLoops / 100;
+      fakeProgress.setProgress(progress);
+      cpuPlugin.emulate();
+    }
+    
+    // test if the resource plugin shows the expected invocations
+    assertEquals("Cumulative cpu usage emulator plugin failed (num calls)!", 
+                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
+    // test if the resource plugin shows the expected usage
+    assertEquals("Cumulative cpu usage emulator plugin failed (total usage)!", 
+                 expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);
+  }
+  
+  // tests if the CPU usage emulation plugin emulates only at the expected
+  // progress gaps
+  private static void testEmulationBoundary(float progress, 
+      FakeCpuUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, 
+      CumulativeCpuUsageEmulatorPlugin cpuPlugin, long expectedTotalCpuUsage, 
+      long expectedTotalNumCalls, String info) throws Exception {
+    fakeProgress.setProgress(progress);
+    cpuPlugin.emulate();
+    
+    assertEquals("Emulation interval test for cpu usage failed " + info + "!", 
+                 expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);
+    assertEquals("Emulation interval test for num calls failed " + info + "!", 
+                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
new file mode 100644
index 0000000..8050f33
--- /dev/null
+++ b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
@@ -0,0 +1,172 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.security.UserGroupInformation;
+
+public class TestUserResolve {
+
+  private static Path rootDir = null;
+  private static Configuration conf = null;
+  private static FileSystem fs = null;
+
+  @BeforeClass
+  public static void createRootDir() throws IOException {
+    conf = new Configuration();
+    fs = FileSystem.getLocal(conf);
+    rootDir = new Path(new Path(System.getProperty("test.build.data", "/tmp"))
+                 .makeQualified(fs), "gridmixUserResolve");
+  }
+
+  /**
+   * Creates users file with the content as the String usersFileContent.
+   * @param usersFilePath    the path to the file that is to be created
+   * @param usersFileContent Content of users file
+   * @throws IOException
+   */
+  private static void writeUserList(Path usersFilePath, String usersFileContent)
+      throws IOException {
+
+    FSDataOutputStream out = null;
+    try {
+      out = fs.create(usersFilePath, true);
+      out.writeBytes(usersFileContent);
+    } finally {
+      if (out != null) {
+        out.close();
+      }
+    }
+  }
+
+  /**
+   * Validate RoundRobinUserResolver's behavior for bad user resource file.
+   * RoundRobinUserResolver.setTargetUsers() should throw proper Exception for
+   * the cases like
+   * <li> non existent user resource file and
+   * <li> empty user resource file
+   * 
+   * @param rslv              The RoundRobinUserResolver object
+   * @param userRsrc          users file
+   * @param expectedErrorMsg  expected error message
+   */
+  private void validateBadUsersFile(UserResolver rslv, URI userRsrc,
+      String expectedErrorMsg) {
+    boolean fail = false;
+    try {
+      rslv.setTargetUsers(userRsrc, conf);
+    } catch (IOException e) {
+      assertTrue("Exception message from RoundRobinUserResolver is wrong",
+          e.getMessage().equals(expectedErrorMsg));
+      fail = true;
+    }
+    assertTrue("User list required for RoundRobinUserResolver", fail);
+  }
+
+  /**
+   * Validate the behavior of {@link RoundRobinUserResolver} for different
+   * user resource files like
+   * <li> Empty user resource file
+   * <li> Non existent user resource file
+   * <li> User resource file with valid content
+   * @throws Exception
+   */
+  @Test
+  public void testRoundRobinResolver() throws Exception {
+
+    final UserResolver rslv = new RoundRobinUserResolver();
+    Path usersFilePath = new Path(rootDir, "users");
+    URI userRsrc = new URI(usersFilePath.toString());
+
+    // Check if the error message is as expected for non existent
+    // user resource file.
+    fs.delete(usersFilePath, false);
+    String expectedErrorMsg = "File " + userRsrc + " does not exist";
+    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);
+
+    // Check if the error message is as expected for empty user resource file
+    writeUserList(usersFilePath, "");// creates empty users file
+    expectedErrorMsg =
+        RoundRobinUserResolver.buildEmptyUsersErrorMsg(userRsrc);
+    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);
+
+    // Create user resource file with valid content like older users list file
+    // with usernames and groups
+    writeUserList(usersFilePath,
+        "user0,groupA,groupB,groupC\nuser1,groupA,groupC\n");
+    validateValidUsersFile(rslv, userRsrc);
+
+    // Create user resource file with valid content with
+    // usernames with groups and without groups
+    writeUserList(usersFilePath, "user0,groupA,groupB\nuser1,");
+    validateValidUsersFile(rslv, userRsrc);
+
+    // Create user resource file with valid content with
+    // usernames without groups
+    writeUserList(usersFilePath, "user0\nuser1");
+    validateValidUsersFile(rslv, userRsrc);
+  }
+
+  // Validate RoundRobinUserResolver for the case of
+  // user resource file with valid content.
+  private void validateValidUsersFile(UserResolver rslv, URI userRsrc)
+      throws IOException {
+    assertTrue(rslv.setTargetUsers(userRsrc, conf));
+    UserGroupInformation ugi1 = UserGroupInformation.createRemoteUser("hfre0");
+    assertEquals("user0", rslv.getTargetUgi(ugi1).getUserName());
+    assertEquals("user1", 
+        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre1"))
+            .getUserName());
+    assertEquals("user0",
+        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre2"))
+            .getUserName());
+    assertEquals("user0", rslv.getTargetUgi(ugi1).getUserName());
+    assertEquals("user1",
+        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre3"))
+            .getUserName());
+
+    // Verify if same user comes again, its mapped user name should be
+    // correct even though UGI is constructed again.
+    assertEquals("user0", rslv.getTargetUgi(
+        UserGroupInformation.createRemoteUser("hfre0")).getUserName());
+    assertEquals("user0",
+        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre5"))
+            .getUserName());
+    assertEquals("user0",
+        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre0"))
+            .getUserName());
+  }
+
+  @Test
+  public void testSubmitterResolver() throws Exception {
+    final UserResolver rslv = new SubmitterUserResolver();
+    assertFalse(rslv.needsTargetUsersList());
+    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
+    assertEquals(ugi, rslv.getTargetUgi((UserGroupInformation)null));
+  }
+}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json.gz b/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json.gz
new file mode 100644
index 0000000000000000000000000000000000000000..c5cc40e7618a5ec775ccd0a827e1d46ea120e0b2
GIT binary patch
literal 1452
zcmV;d1ylMTiwFq8l%-4p19xw7WMgl2Zgehcb8l_{?V3w(+c*$_@A(ym&rZsRL`i#d
zon+A>NekO)4@D8kHeG9@*Gkl)L6QGnN|Yr^l&vJJyDpGi;&5g-Lvr|WNax4NU@(jm
zFG@W2FdY0f7~XuCUXQPaqg?P$w0L)3wDM1P2tWWFWIz+6I{*L_N?A&iQfCvcB3hZ=
zxB)E$pQB_oiL#IhQQ*dNH@#1D*_Xi&-ozVI<|b_J2@8m9?gJ4MVgg1&fpL>EnEay}
z&0UcH;y**lV*U_|7_P@3x9u)tG>BQlKX3M<rOUi1%$vi^_eWd7CQ4Z~mNzT9b~B%y
zChqe7%{q$C=yx}WeR{Txf?;K|b2y|@S$m<EZWJ9G5mf0UH08bu$FtkX<YG3fU6jKp
z=cN2=&$QeDDZEX#Ui<X22!XEh9!_B`jS{v)v{|jww-1b_Q|cBn3mC`(s{AugC`QAU
z`P%bo!@Nr-nHqdH>KWsS^Dc}tHl<6RQN{W}%-zchA2P-@ihc??v=PQw)sXu@1dv*V
zuj9fCsp6@rmC#kT6iR<5Q5L7I(MWQs6GdSr7NUw*QBb0l*D3aVpROb!ArL9kC3Rki
z9tU5`ozR_v1Xz|`YQza$(}ag|RqhUD*SHTfDI16K-WZN}|E=vY_}U5}-=Z_-vMlY$
z@uAm<Ud}Wj>du2<)Jy>>hf0yMs$BRu>Ln3;E{WhVlW4odyGdj@fE<EE@GvAYEek>8
zP$V*KXyE-M0?^z;BF8pOZ0rXSunhouLDbYbP7vAhQfPq4hQ|jYIng>mRHZ>1L{bWM
zfT&p2eh}qz;yf2b=x9MifDNo35D~*B<{?0Y4g(@cup<rX7lQ~v6M}sp!WJfbID{+?
zcCwd47?{ZJ!%$P{I4OjI0rV+EY+xQAh2$(#C{*V_8$we0s1PdF^rVVt$Anm>jSI)+
znmq9`$I0b9{W6#AyUa~5&Tl6de+0ZNV42P@g;(;l`&<<y-|c88$0{v;Hj&*(>%yJH
zB#4mZ;N3`r5CTN@Vy&gp!8Iucm1<qWZrdF=>nW%pv>@)G8MJIddVmIz#V@3NP-}Wq
zxz_abQm|QwS7|+9dl8wWF{<@Z9eNv0au#(INco&dY)Z35;3-?uFriVLN}R-5xZJ36
zB1*R*i<s+QyJC59-83zhM7S(kBetQ*6B)<vnvU_@>dfe5-0Wh0`~TNxhRui6gE+%6
zZTk@QnQ@r<tOsTqwONxwzZYQL#nurOWBQet39)$yU~R)zU{wjC?!{6mRg9$P3YT;D
z*;hE+35m(o#rWn3Fu{j}``(i*>{##+n82@?c*R8Peq#O!m?$e5zEWJuL;FaRYiZ1N
z7j2a~DR$^g>!s_}>g;So-T0b*a~D-{29T(m_*i%!RGF^)8$Y(?();nJv1na+30<;t
z8q;v)hU|T`kXL|LaZ6rorRyv&#l~W%PtGr!rB>j6_kt`aUQrRk*g^mk+cYe`($y6>
z;JHvLovK>f?=uoNy!XOytt<$$U_p~luAhm)Hs9f1-IqNRd;4%C`!j&q-G6odtoZ(F
zsTOeX`HuXLPK8n7`3#t1i;rGp4YiE5Tq7-GE!SAfSj)AcWvt~|&sfWvXc_NuO`1=t
z9oCkXv6gFF%UH{`qh)Nis)jc2<X45Y%j>n(>sj>bzILyaDxz+xGbGpSh|n>S8j{r7
zcUsn}FF7syt)Z5&mTRPCtmPVO8Ed&Vw2ZY}>ltfV6D?yc*Or#CmTOzfSj)AeW!!gK
ziU+Rxtb8I2f8%LU{*P%)lX>FNbRNxea*GF_F9Uc6+Gjo2yiDBX_KA#hwfuHd%+L0H
zLA3sx(M()&%l9&I&t3Xi%8JYR^;`M6Et<t7H6;AHTq#$VYu+fU=ItksCqDrWrmn)J
GD*ynGGs_|X

literal 0
HcmV?d00001

diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index 2fb23c7..1b56c49 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -64,6 +64,11 @@
       <type>pom</type>
       <version>${project.version}</version>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-gridmix</artifactId>
+      <scope>compile</scope>
+    </dependency>
   </dependencies>
 
   <build>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index 1a92ef6..6ff170a 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -32,6 +32,7 @@
     <module>hadoop-distcp</module>
     <module>hadoop-archives</module>
     <module>hadoop-rumen</module>
+    <module>hadoop-gridmix</module>
     <module>hadoop-tools-dist</module>
     <module>hadoop-extras</module>
     <module>hadoop-pipes</module>
-- 
1.7.0.4

