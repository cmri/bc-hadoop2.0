From cf025f948f09075d31afbfe38b7608928d239f62 Mon Sep 17 00:00:00 2001
From: Aaron T. Myers <atm@cloudera.com>
Date: Wed, 30 Jan 2013 18:06:56 -0800
Subject: [PATCH 0904/1357] Revert "svn merge -c 1373683 from trunk for HADOOP-8700.  Use enum to define the checksum constants in DataChecksum."

This reverts commit 747fff4fb533e42045933b0df0a1bf7d52b8ade8.
---
 .../main/java/org/apache/hadoop/fs/FileSystem.java |    2 +-
 .../org/apache/hadoop/fs/FsServerDefaults.java     |   10 +-
 .../fs/MD5MD5CRC32CastagnoliFileChecksum.java      |    4 +-
 .../apache/hadoop/fs/MD5MD5CRC32FileChecksum.java  |   28 +++---
 .../hadoop/fs/MD5MD5CRC32GzipFileChecksum.java     |    4 +-
 .../main/java/org/apache/hadoop/fs/Options.java    |   14 +-
 .../org/apache/hadoop/fs/ftp/FtpConfigKeys.java    |    4 +-
 .../apache/hadoop/fs/local/LocalConfigKeys.java    |    3 +-
 .../java/org/apache/hadoop/util/DataChecksum.java  |  123 ++++++++++---------
 .../java/org/apache/hadoop/fs/TestFsOptions.java   |   12 +-
 .../org/apache/hadoop/util/TestDataChecksum.java   |   21 ++--
 .../java/org/apache/hadoop/hdfs/DFSClient.java     |   40 +++---
 .../apache/hadoop/hdfs/protocol/HdfsProtoUtil.java |    8 +-
 .../datatransfer/DataTransferProtoUtil.java        |   20 +++-
 .../hadoop/hdfs/server/datanode/BlockSender.java   |    3 +-
 .../hadoop/hdfs/server/namenode/FSNamesystem.java  |    4 +-
 .../java/org/apache/hadoop/hdfs/web/JsonUtil.java  |    6 +-
 .../hadoop/hdfs/TestDataTransferProtocol.java      |    2 +-
 .../hadoop/hdfs/TestDistributedFileSystem.java     |    8 +-
 .../hadoop/hdfs/protocol/TestHdfsProtoUtil.java    |   12 +-
 .../hdfs/server/datanode/SimulatedFSDataset.java   |    4 +-
 .../hdfs/server/datanode/TestBlockRecovery.java    |    2 +-
 .../hadoop/hdfs/server/datanode/TestDiskError.java |    2 +-
 .../server/datanode/TestSimulatedFSDataset.java    |    4 +-
 .../org/apache/hadoop/mapred/IFileInputStream.java |    2 +-
 .../apache/hadoop/mapred/IFileOutputStream.java    |    2 +-
 26 files changed, 184 insertions(+), 160 deletions(-)

diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
index 7d0c02c..d3c5cae 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
@@ -669,7 +669,7 @@ public abstract class FileSystem extends Configured implements Closeable {
         conf.getInt("io.file.buffer.size", 4096),
         false,
         CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_DEFAULT,
-        DataChecksum.Type.CRC32);
+        DataChecksum.CHECKSUM_CRC32);
   }
 
   /**
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java
index 637697b..5b0c8b9 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java
@@ -52,7 +52,7 @@ public class FsServerDefaults implements Writable {
   private int fileBufferSize;
   private boolean encryptDataTransfer;
   private long trashInterval;
-  private DataChecksum.Type checksumType;
+  private int checksumType;
 
   public FsServerDefaults() {
   }
@@ -60,7 +60,7 @@ public class FsServerDefaults implements Writable {
   public FsServerDefaults(long blockSize, int bytesPerChecksum,
       int writePacketSize, short replication, int fileBufferSize,
       boolean encryptDataTransfer, long trashInterval,
-      DataChecksum.Type checksumType) {
+      int checksumType) {
     this.blockSize = blockSize;
     this.bytesPerChecksum = bytesPerChecksum;
     this.writePacketSize = writePacketSize;
@@ -99,7 +99,7 @@ public class FsServerDefaults implements Writable {
     return trashInterval;
   }
 
-  public DataChecksum.Type getChecksumType() {
+  public int getChecksumType() {
     return checksumType;
   }
 
@@ -113,7 +113,7 @@ public class FsServerDefaults implements Writable {
     out.writeInt(writePacketSize);
     out.writeShort(replication);
     out.writeInt(fileBufferSize);
-    WritableUtils.writeEnum(out, checksumType);
+    out.writeInt(checksumType);
   }
 
   @InterfaceAudience.Private
@@ -123,6 +123,6 @@ public class FsServerDefaults implements Writable {
     writePacketSize = in.readInt();
     replication = in.readShort();
     fileBufferSize = in.readInt();
-    checksumType = WritableUtils.readEnum(in, DataChecksum.Type.class);
+    checksumType = in.readInt();
   }
 }
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java
index 5a4a6a9..3538787 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java
@@ -34,8 +34,8 @@ public class MD5MD5CRC32CastagnoliFileChecksum extends MD5MD5CRC32FileChecksum {
   }
 
   @Override
-  public DataChecksum.Type getCrcType() {
+  public int getCrcType() {
     // default to the one that is understood by all releases.
-    return DataChecksum.Type.CRC32C;
+    return DataChecksum.CHECKSUM_CRC32C;
   }
 }
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java
index 1c697b7..2ba0ee5 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java
@@ -60,15 +60,15 @@ public class MD5MD5CRC32FileChecksum extends FileChecksum {
   /** {@inheritDoc} */ 
   public String getAlgorithmName() {
     return "MD5-of-" + crcPerBlock + "MD5-of-" + bytesPerCRC +
-        getCrcType().name();
+        DataChecksum.getNameOfType(getCrcType());
   }
 
-  public static DataChecksum.Type getCrcTypeFromAlgorithmName(String algorithm)
+  public static int getCrcTypeFromAlgorithmName(String algorithm)
       throws IOException {
-    if (algorithm.endsWith(DataChecksum.Type.CRC32.name())) {
-      return DataChecksum.Type.CRC32;
-    } else if (algorithm.endsWith(DataChecksum.Type.CRC32C.name())) {
-      return DataChecksum.Type.CRC32C;
+    if (algorithm.endsWith(DataChecksum.getNameOfType(DataChecksum.CHECKSUM_CRC32))) {
+      return DataChecksum.CHECKSUM_CRC32;
+    } else if (algorithm.endsWith(DataChecksum.getNameOfType(DataChecksum.CHECKSUM_CRC32C))) {
+      return DataChecksum.CHECKSUM_CRC32C;
     }
 
     throw new IOException("Unknown checksum type in " + algorithm);
@@ -83,9 +83,9 @@ public class MD5MD5CRC32FileChecksum extends FileChecksum {
   }
 
   /** returns the CRC type */
-  public DataChecksum.Type getCrcType() {
+  public int getCrcType() {
     // default to the one that is understood by all releases.
-    return DataChecksum.Type.CRC32;
+    return DataChecksum.CHECKSUM_CRC32;
   }
 
   public ChecksumOpt getChecksumOpt() {
@@ -113,7 +113,7 @@ public class MD5MD5CRC32FileChecksum extends FileChecksum {
     if (that != null) {
       xml.attribute("bytesPerCRC", "" + that.bytesPerCRC);
       xml.attribute("crcPerBlock", "" + that.crcPerBlock);
-      xml.attribute("crcType", ""+ that.getCrcType().name());
+      xml.attribute("crcType", ""+ DataChecksum.getNameOfType(that.getCrcType()));
       xml.attribute("md5", "" + that.md5);
     }
     xml.endTag();
@@ -126,7 +126,7 @@ public class MD5MD5CRC32FileChecksum extends FileChecksum {
     final String crcPerBlock = attrs.getValue("crcPerBlock");
     final String md5 = attrs.getValue("md5");
     String crcType = attrs.getValue("crcType");
-    DataChecksum.Type finalCrcType;
+    int finalCrcType;
     if (bytesPerCRC == null || crcPerBlock == null || md5 == null) {
       return null;
     }
@@ -134,18 +134,18 @@ public class MD5MD5CRC32FileChecksum extends FileChecksum {
     try {
       // old versions don't support crcType.
       if (crcType == null || crcType == "") {
-        finalCrcType = DataChecksum.Type.CRC32;
+        finalCrcType = DataChecksum.CHECKSUM_CRC32;
       } else {
-        finalCrcType = DataChecksum.Type.valueOf(crcType);
+        finalCrcType = DataChecksum.getTypeFromName(crcType);
       }
 
       switch (finalCrcType) {
-        case CRC32:
+        case DataChecksum.CHECKSUM_CRC32:
           return new MD5MD5CRC32GzipFileChecksum(
               Integer.valueOf(bytesPerCRC),
               Integer.valueOf(crcPerBlock),
               new MD5Hash(md5));
-        case CRC32C:
+        case DataChecksum.CHECKSUM_CRC32C:
           return new MD5MD5CRC32CastagnoliFileChecksum(
               Integer.valueOf(bytesPerCRC),
               Integer.valueOf(crcPerBlock),
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java
index 5164d02..7b96ede 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java
@@ -33,8 +33,8 @@ public class MD5MD5CRC32GzipFileChecksum extends MD5MD5CRC32FileChecksum {
     super(bytesPerCRC, crcPerBlock, md5);
   }
   @Override
-  public DataChecksum.Type getCrcType() {
+  public int getCrcType() {
     // default to the one that is understood by all releases.
-    return DataChecksum.Type.CRC32;
+    return DataChecksum.CHECKSUM_CRC32;
   }
 }
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java
index 173e16e..6495554 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java
@@ -227,14 +227,14 @@ public final class Options {
    */
   public static class ChecksumOpt {
     private final int crcBlockSize;
-    private final DataChecksum.Type crcType;
+    private final int crcType;
 
     /**
      * Create a uninitialized one
      */
     public ChecksumOpt() {
       crcBlockSize = -1;
-      crcType = DataChecksum.Type.DEFAULT;
+      crcType = DataChecksum.CHECKSUM_DEFAULT;
     }
 
     /**
@@ -242,7 +242,7 @@ public final class Options {
      * @param type checksum type
      * @param size bytes per checksum
      */
-    public ChecksumOpt(DataChecksum.Type type, int size) {
+    public ChecksumOpt(int type, int size) {
       crcBlockSize = size;
       crcType = type;
     }
@@ -251,7 +251,7 @@ public final class Options {
       return crcBlockSize;
     }
 
-    public DataChecksum.Type getChecksumType() {
+    public int getChecksumType() {
       return crcType;
     }
 
@@ -259,7 +259,7 @@ public final class Options {
      * Create a ChecksumOpts that disables checksum
      */
     public static ChecksumOpt createDisabled() {
-      return new ChecksumOpt(DataChecksum.Type.NULL, -1);
+      return new ChecksumOpt(DataChecksum.CHECKSUM_NULL, -1);
     }
 
     /**
@@ -296,7 +296,7 @@ public final class Options {
       //   user specified value in checksumOpt
       //   default.
       if (userOpt != null &&
-            userOpt.getChecksumType() != DataChecksum.Type.DEFAULT) {
+            userOpt.getChecksumType() != DataChecksum.CHECKSUM_DEFAULT) {
         useDefaultType = false;
       } else {
         useDefaultType = true;
@@ -310,7 +310,7 @@ public final class Options {
       }
 
       // Take care of the rest of combinations
-      DataChecksum.Type type = useDefaultType ? defaultOpt.getChecksumType() :
+      int type = useDefaultType ? defaultOpt.getChecksumType() :
           userOpt.getChecksumType();
       if (whichSize == 0) {
         return new ChecksumOpt(type, defaultOpt.getBytesPerChecksum());
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java
index 2313a14..91bf54f 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java
@@ -52,8 +52,8 @@ public class FtpConfigKeys extends CommonConfigurationKeys {
   public static final int     CLIENT_WRITE_PACKET_SIZE_DEFAULT = 64*1024;
   public static final boolean ENCRYPT_DATA_TRANSFER_DEFAULT = false;
   public static final long    FS_TRASH_INTERVAL_DEFAULT = 0;
-  public static final DataChecksum.Type CHECKSUM_TYPE_DEFAULT =
-      DataChecksum.Type.CRC32;
+  public static final int     CHECKSUM_TYPE_DEFAULT =
+      DataChecksum.CHECKSUM_CRC32;
   
   protected static FsServerDefaults getServerDefaults() throws IOException {
     return new FsServerDefaults(
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalConfigKeys.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalConfigKeys.java
index d1ebca2..bd5c1b7 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalConfigKeys.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalConfigKeys.java
@@ -52,8 +52,7 @@ public class LocalConfigKeys extends CommonConfigurationKeys {
   public static final int CLIENT_WRITE_PACKET_SIZE_DEFAULT = 64*1024;
   public static final boolean ENCRYPT_DATA_TRANSFER_DEFAULT = false;
   public static final long FS_TRASH_INTERVAL_DEFAULT = 0;
-  public static final DataChecksum.Type CHECKSUM_TYPE_DEFAULT =
-      DataChecksum.Type.CRC32;
+  public static final int CHECKSUM_TYPE_DEFAULT = DataChecksum.CHECKSUM_CRC32;
   public static FsServerDefaults getServerDefaults() throws IOException {
     return new FsServerDefaults(
         BLOCK_SIZE_DEFAULT,
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java
index a8d1872..0220303 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java
@@ -45,46 +45,31 @@ public class DataChecksum implements Checksum {
   public static final int CHECKSUM_CRC32C  = 2;
   public static final int CHECKSUM_DEFAULT = 3; 
   public static final int CHECKSUM_MIXED   = 4;
- 
-  /** The checksum types */
-  public static enum Type {
-    NULL  (CHECKSUM_NULL, 0),
-    CRC32 (CHECKSUM_CRC32, 4),
-    CRC32C(CHECKSUM_CRC32C, 4),
-    DEFAULT(CHECKSUM_DEFAULT, 0), // This cannot be used to create DataChecksum
-    MIXED (CHECKSUM_MIXED, 0); // This cannot be used to create DataChecksum
-
-    public final int id;
-    public final int size;
-    
-    private Type(int id, int size) {
-      this.id = id;
-      this.size = size;
-    }
-
-    /** @return the type corresponding to the id. */
-    public static Type valueOf(int id) {
-      if (id < 0 || id >= values().length) {
-        throw new IllegalArgumentException("id=" + id
-            + " out of range [0, " + values().length + ")");
-      }
-      return values()[id];
-    }
-  }
-
-
-  public static DataChecksum newDataChecksum(Type type, int bytesPerChecksum ) {
+  
+  private static String[] NAMES = new String[] {
+    "NULL", "CRC32", "CRC32C"
+  };
+  
+  private static final int CHECKSUM_NULL_SIZE  = 0;
+  private static final int CHECKSUM_CRC32_SIZE = 4;
+  private static final int CHECKSUM_CRC32C_SIZE = 4;
+  
+  
+  public static DataChecksum newDataChecksum( int type, int bytesPerChecksum ) {
     if ( bytesPerChecksum <= 0 ) {
       return null;
     }
     
     switch ( type ) {
-    case NULL :
-      return new DataChecksum(type, new ChecksumNull(), bytesPerChecksum );
-    case CRC32 :
-      return new DataChecksum(type, new PureJavaCrc32(), bytesPerChecksum );
-    case CRC32C:
-      return new DataChecksum(type, new PureJavaCrc32C(), bytesPerChecksum);
+    case CHECKSUM_NULL :
+      return new DataChecksum( CHECKSUM_NULL, new ChecksumNull(), 
+                               CHECKSUM_NULL_SIZE, bytesPerChecksum );
+    case CHECKSUM_CRC32 :
+      return new DataChecksum( CHECKSUM_CRC32, new PureJavaCrc32(), 
+                               CHECKSUM_CRC32_SIZE, bytesPerChecksum );
+    case CHECKSUM_CRC32C:
+      return new DataChecksum( CHECKSUM_CRC32C, new PureJavaCrc32C(),
+                               CHECKSUM_CRC32C_SIZE, bytesPerChecksum);
     default:
       return null;  
     }
@@ -104,7 +89,7 @@ public class DataChecksum implements Checksum {
                            ( (bytes[offset+2] & 0xff) << 16 ) |
                            ( (bytes[offset+3] & 0xff) << 8 )  |
                            ( (bytes[offset+4] & 0xff) );
-    return newDataChecksum( Type.valueOf(bytes[offset]), bytesPerChecksum );
+    return newDataChecksum( bytes[offset], bytesPerChecksum );
   }
   
   /**
@@ -115,7 +100,7 @@ public class DataChecksum implements Checksum {
                                  throws IOException {
     int type = in.readByte();
     int bpc = in.readInt();
-    DataChecksum summer = newDataChecksum(Type.valueOf(type), bpc );
+    DataChecksum summer = newDataChecksum( type, bpc );
     if ( summer == null ) {
       throw new IOException( "Could not create DataChecksum of type " +
                              type + " with bytesPerChecksum " + bpc );
@@ -128,13 +113,13 @@ public class DataChecksum implements Checksum {
    */
   public void writeHeader( DataOutputStream out ) 
                            throws IOException { 
-    out.writeByte( type.id );
+    out.writeByte( type );
     out.writeInt( bytesPerChecksum );
   }
 
   public byte[] getHeader() {
     byte[] header = new byte[DataChecksum.HEADER_LEN];
-    header[0] = (byte) (type.id & 0xff);
+    header[0] = (byte) (type & 0xff);
     // Writing in buffer just like DataOutput.WriteInt()
     header[1+0] = (byte) ((bytesPerChecksum >>> 24) & 0xff);
     header[1+1] = (byte) ((bytesPerChecksum >>> 16) & 0xff);
@@ -150,11 +135,11 @@ public class DataChecksum implements Checksum {
    */
    public int writeValue( DataOutputStream out, boolean reset )
                           throws IOException {
-     if ( type.size <= 0 ) {
+     if ( size <= 0 ) {
        return 0;
      }
 
-     if ( type.size == 4 ) {
+     if ( size == 4 ) {
        out.writeInt( (int) summer.getValue() );
      } else {
        throw new IOException( "Unknown Checksum " + type );
@@ -164,7 +149,7 @@ public class DataChecksum implements Checksum {
        reset();
      }
      
-     return type.size;
+     return size;
    }
    
    /**
@@ -174,11 +159,11 @@ public class DataChecksum implements Checksum {
     */
     public int writeValue( byte[] buf, int offset, boolean reset )
                            throws IOException {
-      if ( type.size <= 0 ) {
+      if ( size <= 0 ) {
         return 0;
       }
 
-      if ( type.size == 4 ) {
+      if ( size == 4 ) {
         int checksum = (int) summer.getValue();
         buf[offset+0] = (byte) ((checksum >>> 24) & 0xff);
         buf[offset+1] = (byte) ((checksum >>> 16) & 0xff);
@@ -192,7 +177,7 @@ public class DataChecksum implements Checksum {
         reset();
       }
       
-      return type.size;
+      return size;
     }
    
    /**
@@ -200,33 +185,36 @@ public class DataChecksum implements Checksum {
     * @return true if the checksum matches and false otherwise.
     */
    public boolean compare( byte buf[], int offset ) {
-     if ( type.size == 4 ) {
+     if ( size == 4 ) {
        int checksum = ( (buf[offset+0] & 0xff) << 24 ) | 
                       ( (buf[offset+1] & 0xff) << 16 ) |
                       ( (buf[offset+2] & 0xff) << 8 )  |
                       ( (buf[offset+3] & 0xff) );
        return checksum == (int) summer.getValue();
      }
-     return type.size == 0;
+     return size == 0;
    }
    
-  private final Type type;
+  private final int type;
+  private final int size;
   private final Checksum summer;
   private final int bytesPerChecksum;
   private int inSum = 0;
   
-  private DataChecksum( Type type, Checksum checksum, int chunkSize ) {
-    this.type = type;
+  private DataChecksum( int checksumType, Checksum checksum,
+                        int sumSize, int chunkSize ) {
+    type = checksumType;
     summer = checksum;
+    size = sumSize;
     bytesPerChecksum = chunkSize;
   }
   
   // Accessors
-  public Type getChecksumType() {
+  public int getChecksumType() {
     return type;
   }
   public int getChecksumSize() {
-    return type.size;
+    return size;
   }
   public int getBytesPerChecksum() {
     return bytesPerChecksum;
@@ -274,7 +262,7 @@ public class DataChecksum implements Checksum {
   public void verifyChunkedSums(ByteBuffer data, ByteBuffer checksums,
       String fileName, long basePos)
   throws ChecksumException {
-    if (type.size == 0) return;
+    if (size == 0) return;
     
     if (data.hasArray() && checksums.hasArray()) {
       verifyChunkedSums(
@@ -284,7 +272,7 @@ public class DataChecksum implements Checksum {
       return;
     }
     if (NativeCrc32.isAvailable()) {
-      NativeCrc32.verifyChunkedSums(bytesPerChecksum, type.id, checksums, data,
+      NativeCrc32.verifyChunkedSums(bytesPerChecksum, type, checksums, data,
           fileName, basePos);
       return;
     }
@@ -294,7 +282,7 @@ public class DataChecksum implements Checksum {
     checksums.mark();
     try {
       byte[] buf = new byte[bytesPerChecksum];
-      byte[] sum = new byte[type.size];
+      byte[] sum = new byte[size];
       while (data.remaining() > 0) {
         int n = Math.min(data.remaining(), bytesPerChecksum);
         checksums.get(sum);
@@ -365,7 +353,7 @@ public class DataChecksum implements Checksum {
    *                  buffer to put the checksums.
    */
   public void calculateChunkedSums(ByteBuffer data, ByteBuffer checksums) {
-    if (type.size == 0) return;
+    if (size == 0) return;
     
     if (data.hasArray() && checksums.hasArray()) {
       calculateChunkedSums(data.array(), data.arrayOffset() + data.position(), data.remaining(),
@@ -425,14 +413,33 @@ public class DataChecksum implements Checksum {
   
   @Override
   public int hashCode() {
-    return (this.type.id + 31) * this.bytesPerChecksum;
+    return (this.type + 31) * this.bytesPerChecksum;
   }
   
   @Override
   public String toString() {
-    return "DataChecksum(type=" + type +
+    String strType = getNameOfType(type);
+    return "DataChecksum(type=" + strType +
       ", chunkSize=" + bytesPerChecksum + ")";
   }
+
+  public static String getNameOfType(int checksumType) {
+    if (checksumType < NAMES.length && checksumType > 0) {
+      return NAMES[checksumType];
+    } else {
+      return String.valueOf(checksumType);
+    }
+  }
+
+  public static int getTypeFromName(String checksumName) {
+    for (int i = 0; i < NAMES.length; i++) {
+      if (NAMES[i].equals(checksumName)) {
+        return i;
+      }
+    }
+    throw new RuntimeException("Invalid checksum name: '" +
+        checksumName + "'");
+  }
   
   /**
    * This just provides a dummy implimentation for Checksum class
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsOptions.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsOptions.java
index c66b4fa..673b0a6 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsOptions.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsOptions.java
@@ -30,7 +30,7 @@ public class TestFsOptions {
 
   @Test
   public void testProcessChecksumOpt() {
-    ChecksumOpt defaultOpt = new ChecksumOpt(DataChecksum.Type.CRC32, 512);
+    ChecksumOpt defaultOpt = new ChecksumOpt(DataChecksum.CHECKSUM_CRC32, 512);
     ChecksumOpt finalOpt;
 
     // Give a null 
@@ -39,7 +39,7 @@ public class TestFsOptions {
 
     // null with bpc
     finalOpt = ChecksumOpt.processChecksumOpt(defaultOpt, null, 1024);
-    checkParams(DataChecksum.Type.CRC32, 1024, finalOpt);
+    checkParams(DataChecksum.CHECKSUM_CRC32, 1024, finalOpt);
 
     ChecksumOpt myOpt = new ChecksumOpt();
 
@@ -47,15 +47,15 @@ public class TestFsOptions {
     finalOpt = ChecksumOpt.processChecksumOpt(defaultOpt, myOpt);
     checkParams(defaultOpt, finalOpt);
 
-    myOpt = new ChecksumOpt(DataChecksum.Type.CRC32C, 2048);
+    myOpt = new ChecksumOpt(DataChecksum.CHECKSUM_CRC32C, 2048);
 
     // custom config
     finalOpt = ChecksumOpt.processChecksumOpt(defaultOpt, myOpt);
-    checkParams(DataChecksum.Type.CRC32C, 2048, finalOpt);
+    checkParams(DataChecksum.CHECKSUM_CRC32C, 2048, finalOpt);
 
     // custom config + bpc
     finalOpt = ChecksumOpt.processChecksumOpt(defaultOpt, myOpt, 4096);
-    checkParams(DataChecksum.Type.CRC32C, 4096, finalOpt);
+    checkParams(DataChecksum.CHECKSUM_CRC32C, 4096, finalOpt);
   }
 
   private void checkParams(ChecksumOpt expected, ChecksumOpt obtained) {
@@ -63,7 +63,7 @@ public class TestFsOptions {
     assertEquals(expected.getBytesPerChecksum(), obtained.getBytesPerChecksum());
   }
 
-  private void checkParams(DataChecksum.Type type, int bpc, ChecksumOpt obtained) {
+  private void checkParams(int type, int bpc, ChecksumOpt obtained) {
     assertEquals(type, obtained.getChecksumType());
     assertEquals(bpc, obtained.getBytesPerChecksum());
   }
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDataChecksum.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDataChecksum.java
index 1e523da..e8d1670 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDataChecksum.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDataChecksum.java
@@ -35,13 +35,13 @@ public class TestDataChecksum {
   private static final int DATA_TRAILER_IN_BUFFER = 3;
   
   private static final int BYTES_PER_CHUNK = 512;
-  private static final DataChecksum.Type CHECKSUM_TYPES[] = {
-    DataChecksum.Type.CRC32, DataChecksum.Type.CRC32C
+  private static final int CHECKSUM_TYPES[] = new int[] {
+    DataChecksum.CHECKSUM_CRC32, DataChecksum.CHECKSUM_CRC32C
   };
   
   @Test
   public void testBulkOps() throws Exception {
-    for (DataChecksum.Type type : CHECKSUM_TYPES) {
+    for (int type : CHECKSUM_TYPES) {
       System.err.println(
           "---- beginning tests with checksum type " + type  + "----");
       DataChecksum checksum = DataChecksum.newDataChecksum(
@@ -118,20 +118,21 @@ public class TestDataChecksum {
   @Test
   public void testEquality() {
     assertEquals(
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512),
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512));
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512),
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512));
     assertFalse(
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512).equals(
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 1024)));
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512).equals(
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 1024)));
     assertFalse(
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512).equals(
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32C, 512)));        
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512).equals(
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32C, 512)));        
   }
   
   @Test
   public void testToString() {
     assertEquals("DataChecksum(type=CRC32, chunkSize=512)",
-        DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512).toString());
+        DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512)
+          .toString());
   }
 
   private static void corruptBufferOffset(ByteBuffer buf, int offset) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
index d8d9f5c..27d5874 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
@@ -154,7 +154,6 @@ import org.apache.hadoop.security.token.SecretManager.InvalidToken;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.security.token.TokenRenewer;
 import org.apache.hadoop.util.DataChecksum;
-import org.apache.hadoop.util.DataChecksum.Type;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.Time;
 
@@ -313,23 +312,24 @@ public class DFSClient implements java.io.Closeable {
         DFSConfigKeys.DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC_DEFAULT);
     }
 
-    private DataChecksum.Type getChecksumType(Configuration conf) {
-      final String checksum = conf.get(
-          DFSConfigKeys.DFS_CHECKSUM_TYPE_KEY,
+    private int getChecksumType(Configuration conf) {
+      String checksum = conf.get(DFSConfigKeys.DFS_CHECKSUM_TYPE_KEY,
           DFSConfigKeys.DFS_CHECKSUM_TYPE_DEFAULT);
-      try {
-        return DataChecksum.Type.valueOf(checksum);
-      } catch(IllegalArgumentException iae) {
-        LOG.warn("Bad checksum type: " + checksum + ". Using default "
-            + DFSConfigKeys.DFS_CHECKSUM_TYPE_DEFAULT);
-        return DataChecksum.Type.valueOf(
-            DFSConfigKeys.DFS_CHECKSUM_TYPE_DEFAULT); 
+      if ("CRC32".equals(checksum)) {
+        return DataChecksum.CHECKSUM_CRC32;
+      } else if ("CRC32C".equals(checksum)) {
+        return DataChecksum.CHECKSUM_CRC32C;
+      } else if ("NULL".equals(checksum)) {
+        return DataChecksum.CHECKSUM_NULL;
+      } else {
+        LOG.warn("Bad checksum type: " + checksum + ". Using default.");
+        return DataChecksum.CHECKSUM_CRC32C;
       }
     }
 
     // Construct a checksum option from conf
     private ChecksumOpt getChecksumOptFromConf(Configuration conf) {
-      DataChecksum.Type type = getChecksumType(conf);
+      int type = getChecksumType(conf);
       int bytesPerChecksum = conf.getInt(DFS_BYTES_PER_CHECKSUM_KEY,
           DFS_BYTES_PER_CHECKSUM_DEFAULT);
       return new ChecksumOpt(type, bytesPerChecksum);
@@ -350,7 +350,7 @@ public class DFSClient implements java.io.Closeable {
           myOpt.getBytesPerChecksum());
       if (dataChecksum == null) {
         throw new IOException("Invalid checksum type specified: "
-            + myOpt.getChecksumType().name());
+            + DataChecksum.getNameOfType(myOpt.getChecksumType()));
       }
       return dataChecksum;
     }
@@ -1628,7 +1628,7 @@ public class DFSClient implements java.io.Closeable {
     List<LocatedBlock> locatedblocks = blockLocations.getLocatedBlocks();
     final DataOutputBuffer md5out = new DataOutputBuffer();
     int bytesPerCRC = -1;
-    DataChecksum.Type crcType = DataChecksum.Type.DEFAULT;
+    int crcType = DataChecksum.CHECKSUM_DEFAULT;
     long crcPerBlock = 0;
     boolean refetchBlocks = false;
     int lastRetriedIndex = -1;
@@ -1706,7 +1706,7 @@ public class DFSClient implements java.io.Closeable {
           md5.write(md5out);
           
           // read crc-type
-          final DataChecksum.Type ct;
+          final int ct;
           if (checksumData.hasCrcType()) {
             ct = HdfsProtoUtil.fromProto(checksumData.getCrcType());
           } else {
@@ -1719,10 +1719,10 @@ public class DFSClient implements java.io.Closeable {
 
           if (i == 0) { // first block
             crcType = ct;
-          } else if (crcType != DataChecksum.Type.MIXED
+          } else if (crcType != DataChecksum.CHECKSUM_MIXED
               && crcType != ct) {
             // if crc types are mixed in a file
-            crcType = DataChecksum.Type.MIXED;
+            crcType = DataChecksum.CHECKSUM_MIXED;
           }
 
           done = true;
@@ -1764,10 +1764,10 @@ public class DFSClient implements java.io.Closeable {
     //compute file MD5
     final MD5Hash fileMD5 = MD5Hash.digest(md5out.getData()); 
     switch (crcType) {
-      case CRC32:
+      case DataChecksum.CHECKSUM_CRC32:
         return new MD5MD5CRC32GzipFileChecksum(bytesPerCRC,
             crcPerBlock, fileMD5);
-      case CRC32C:
+      case DataChecksum.CHECKSUM_CRC32C:
         return new MD5MD5CRC32CastagnoliFileChecksum(bytesPerCRC,
             crcPerBlock, fileMD5);
       default:
@@ -1836,7 +1836,7 @@ public class DFSClient implements java.io.Closeable {
    * @return the inferred checksum type
    * @throws IOException if an error occurs
    */
-  private static Type inferChecksumTypeByReading(
+  private static int inferChecksumTypeByReading(
       String clientName, SocketFactory socketFactory, int socketTimeout,
       LocatedBlock lb, DatanodeInfo dn,
       DataEncryptionKey encryptionKey, boolean connectToDnViaHostname)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsProtoUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsProtoUtil.java
index fe7446f..c1753d2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsProtoUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsProtoUtil.java
@@ -156,12 +156,12 @@ public abstract class HdfsProtoUtil {
     return ret;
   }
 
-  public static DataChecksum.Type fromProto(HdfsProtos.ChecksumTypeProto type) {
-    return DataChecksum.Type.valueOf(type.getNumber());
+  public static int fromProto(HdfsProtos.ChecksumTypeProto type) {
+    return type.getNumber();
   }
 
-  public static HdfsProtos.ChecksumTypeProto toProto(DataChecksum.Type type) {
-    return HdfsProtos.ChecksumTypeProto.valueOf(type.id);
+  public static HdfsProtos.ChecksumTypeProto toProto(int type) {
+    return HdfsProtos.ChecksumTypeProto.valueOf(type);
   }
 
   public static InputStream vintPrefixed(final InputStream input)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtoUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtoUtil.java
index d2b04ed..6e975e9 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtoUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtoUtil.java
@@ -31,6 +31,9 @@ import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.util.DataChecksum;
 
+import com.google.common.collect.BiMap;
+import com.google.common.collect.ImmutableBiMap;
+
 
 /**
  * Static utilities for dealing with the protocol buffers used by the
@@ -39,6 +42,19 @@ import org.apache.hadoop.util.DataChecksum;
 @InterfaceAudience.Private
 @InterfaceStability.Evolving
 public abstract class DataTransferProtoUtil {
+
+  /**
+   * Map between the internal DataChecksum identifiers and the protobuf-
+   * generated identifiers on the wire.
+   */
+  static BiMap<Integer, ChecksumTypeProto> checksumTypeMap =
+    ImmutableBiMap.<Integer, ChecksumTypeProto>builder()
+      .put(DataChecksum.CHECKSUM_CRC32, ChecksumTypeProto.CHECKSUM_CRC32)
+      .put(DataChecksum.CHECKSUM_CRC32C, ChecksumTypeProto.CHECKSUM_CRC32C)
+      .put(DataChecksum.CHECKSUM_NULL, ChecksumTypeProto.CHECKSUM_NULL)
+      .build();
+
+  
   static BlockConstructionStage fromProto(
       OpWriteBlockProto.BlockConstructionStage stage) {
     return BlockConstructionStage.valueOf(BlockConstructionStage.class,
@@ -52,7 +68,7 @@ public abstract class DataTransferProtoUtil {
   }
 
   public static ChecksumProto toProto(DataChecksum checksum) {
-    ChecksumTypeProto type = HdfsProtoUtil.toProto(checksum.getChecksumType());
+    ChecksumTypeProto type = checksumTypeMap.get(checksum.getChecksumType());
     if (type == null) {
       throw new IllegalArgumentException(
           "Can't convert checksum to protobuf: " + checksum);
@@ -68,7 +84,7 @@ public abstract class DataTransferProtoUtil {
     if (proto == null) return null;
 
     int bytesPerChecksum = proto.getBytesPerChecksum();
-    DataChecksum.Type type = HdfsProtoUtil.fromProto(proto.getType());
+    int type = checksumTypeMap.inverse().get(proto.getType());
     
     return DataChecksum.newDataChecksum(type, bytesPerChecksum);
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
index cbe3bd0..fc3be20 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader;
 import org.apache.hadoop.hdfs.util.DataTransferThrottler;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.ReadaheadPool;
 import org.apache.hadoop.io.ReadaheadPool.ReadaheadRequest;
 import org.apache.hadoop.io.nativeio.NativeIO;
 import org.apache.hadoop.net.SocketOutputStream;
@@ -254,7 +255,7 @@ class BlockSender implements java.io.Closeable {
         // even if the checksum type is NULL. So, choosing too big of a value
         // would risk sending too much unnecessary data. 512 (1 disk sector)
         // is likely to result in minimal extra IO.
-        csum = DataChecksum.newDataChecksum(DataChecksum.Type.NULL, 512);
+        csum = DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_NULL, 512);
       }
 
       /*
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index cc03a34..8efd850 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -492,9 +492,9 @@ public class FSNamesystem implements Namesystem, FSClusterStats,
 
       // Get the checksum type from config
       String checksumTypeStr = conf.get(DFS_CHECKSUM_TYPE_KEY, DFS_CHECKSUM_TYPE_DEFAULT);
-      DataChecksum.Type checksumType;
+      int checksumType;
       try {
-         checksumType = DataChecksum.Type.valueOf(checksumTypeStr);
+         checksumType = DataChecksum.getTypeFromName(checksumTypeStr);
       } catch (IllegalArgumentException iae) {
          throw new IOException("Invalid checksum type in "
             + DFS_CHECKSUM_TYPE_KEY + ": " + checksumTypeStr);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
index f251e34..42db3d3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
@@ -515,16 +515,16 @@ public class JsonUtil {
     final byte[] bytes = StringUtils.hexStringToByte((String)m.get("bytes"));
 
     final DataInputStream in = new DataInputStream(new ByteArrayInputStream(bytes));
-    final DataChecksum.Type crcType = 
+    final int crcType = 
         MD5MD5CRC32FileChecksum.getCrcTypeFromAlgorithmName(algorithm);
     final MD5MD5CRC32FileChecksum checksum;
 
     // Recreate what DFSClient would have returned.
     switch(crcType) {
-      case CRC32:
+      case DataChecksum.CHECKSUM_CRC32:
         checksum = new MD5MD5CRC32GzipFileChecksum();
         break;
-      case CRC32C:
+      case DataChecksum.CHECKSUM_CRC32C:
         checksum = new MD5MD5CRC32CastagnoliFileChecksum();
         break;
       default:
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
index b97f7bc..01d8eca 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
@@ -75,7 +75,7 @@ public class TestDataTransferProtocol {
                     "org.apache.hadoop.hdfs.TestDataTransferProtocol");
 
   private static final DataChecksum DEFAULT_CHECKSUM =
-    DataChecksum.newDataChecksum(DataChecksum.Type.CRC32C, 512);
+    DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32C, 512);
   
   DatanodeID datanode;
   InetSocketAddress dnAddr;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
index a970c5d..57d2e6f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
@@ -691,8 +691,8 @@ public class TestDistributedFileSystem {
     // create args 
     Path path1 = new Path(testBasePath, "file_wtih_crc1");
     Path path2 = new Path(testBasePath, "file_with_crc2");
-    ChecksumOpt opt1 = new ChecksumOpt(DataChecksum.Type.CRC32C, 512);
-    ChecksumOpt opt2 = new ChecksumOpt(DataChecksum.Type.CRC32, 512);
+    ChecksumOpt opt1 = new ChecksumOpt(DataChecksum.CHECKSUM_CRC32C, 512);
+    ChecksumOpt opt2 = new ChecksumOpt(DataChecksum.CHECKSUM_CRC32, 512);
 
     // common args
     FsPermission perm = FsPermission.getDefault().applyUMask(
@@ -728,8 +728,8 @@ public class TestDistributedFileSystem {
       assertFalse(sum1.equals(sum2));
 
       // check the individual params
-      assertEquals(DataChecksum.Type.CRC32C, sum1.getCrcType());
-      assertEquals(DataChecksum.Type.CRC32,  sum2.getCrcType());
+      assertEquals(DataChecksum.CHECKSUM_CRC32C, sum1.getCrcType());
+      assertEquals(DataChecksum.CHECKSUM_CRC32,  sum2.getCrcType());
 
     } finally {
       if (cluster != null) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestHdfsProtoUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestHdfsProtoUtil.java
index 0a04e3c..4dd6f3a 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestHdfsProtoUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestHdfsProtoUtil.java
@@ -26,17 +26,17 @@ import static org.junit.Assert.assertEquals;
 public class TestHdfsProtoUtil {
   @Test
   public void testChecksumTypeProto() {
-    assertEquals(DataChecksum.Type.NULL,
+    assertEquals(DataChecksum.CHECKSUM_NULL,
         HdfsProtoUtil.fromProto(HdfsProtos.ChecksumTypeProto.CHECKSUM_NULL));
-    assertEquals(DataChecksum.Type.CRC32,
+    assertEquals(DataChecksum.CHECKSUM_CRC32,
         HdfsProtoUtil.fromProto(HdfsProtos.ChecksumTypeProto.CHECKSUM_CRC32));
-    assertEquals(DataChecksum.Type.CRC32C,
+    assertEquals(DataChecksum.CHECKSUM_CRC32C,
         HdfsProtoUtil.fromProto(HdfsProtos.ChecksumTypeProto.CHECKSUM_CRC32C));
-    assertEquals(HdfsProtoUtil.toProto(DataChecksum.Type.NULL),
+    assertEquals(HdfsProtoUtil.toProto(DataChecksum.CHECKSUM_NULL),
         HdfsProtos.ChecksumTypeProto.CHECKSUM_NULL);
-    assertEquals(HdfsProtoUtil.toProto(DataChecksum.Type.CRC32),
+    assertEquals(HdfsProtoUtil.toProto(DataChecksum.CHECKSUM_CRC32),
         HdfsProtos.ChecksumTypeProto.CHECKSUM_CRC32);
-    assertEquals(HdfsProtoUtil.toProto(DataChecksum.Type.CRC32C),
+    assertEquals(HdfsProtoUtil.toProto(DataChecksum.CHECKSUM_CRC32C),
         HdfsProtos.ChecksumTypeProto.CHECKSUM_CRC32C);
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
index ef73d86..5e6e2cd 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
@@ -96,8 +96,8 @@ public class SimulatedFSDataset implements FsDatasetSpi<FsVolumeSpi> {
   
   static final byte[] nullCrcFileData;
   static {
-    DataChecksum checksum = DataChecksum.newDataChecksum(
-        DataChecksum.Type.NULL, 16*1024 );
+    DataChecksum checksum = DataChecksum.newDataChecksum( DataChecksum.
+                              CHECKSUM_NULL, 16*1024 );
     byte[] nullCrcHeader = checksum.getHeader();
     nullCrcFileData =  new byte[2 + nullCrcHeader.length];
     nullCrcFileData[0] = (byte) ((BlockMetadataHeader.VERSION >>> 8) & 0xff);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
index c62206f..1a6c9ec 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
@@ -550,7 +550,7 @@ public class TestBlockRecovery {
     ReplicaOutputStreams streams = null;
     try {
       streams = replicaInfo.createStreams(true,
-          DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512));
+          DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512));
       streams.getChecksumOut().write('a');
       dn.data.initReplicaRecovery(new RecoveringBlock(block, null, RECOVERY_ID+1));
       try {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java
index 3c10a7a..7a83bf3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java
@@ -142,7 +142,7 @@ public class TestDiskError {
     DataOutputStream out = new DataOutputStream(s.getOutputStream());
 
     DataChecksum checksum = DataChecksum.newDataChecksum(
-        DataChecksum.Type.CRC32, 512);
+        DataChecksum.CHECKSUM_CRC32, 512);
     new Sender(out).writeBlock(block.getBlock(),
         BlockTokenSecretManager.DUMMY_TOKEN, "",
         new DatanodeInfo[0], null,
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java
index e863009..1277f21 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java
@@ -67,7 +67,7 @@ public class TestSimulatedFSDataset {
       // data written
       ReplicaInPipelineInterface bInfo = fsdataset.createRbw(b);
       ReplicaOutputStreams out = bInfo.createStreams(true,
-          DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512));
+          DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 512));
       try {
         OutputStream dataOut  = out.getDataOut();
         assertEquals(0, fsdataset.getLength(b));
@@ -119,7 +119,7 @@ public class TestSimulatedFSDataset {
     short version = metaDataInput.readShort();
     assertEquals(BlockMetadataHeader.VERSION, version);
     DataChecksum checksum = DataChecksum.newDataChecksum(metaDataInput);
-    assertEquals(DataChecksum.Type.NULL, checksum.getChecksumType());
+    assertEquals(DataChecksum.CHECKSUM_NULL, checksum.getChecksumType());
     assertEquals(0, checksum.getChecksumSize());  
   }
 
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileInputStream.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileInputStream.java
index 02cbce3..b171fb0 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileInputStream.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileInputStream.java
@@ -71,7 +71,7 @@ public class IFileInputStream extends InputStream {
   public IFileInputStream(InputStream in, long len, Configuration conf) {
     this.in = in;
     this.inFd = getFileDescriptorIfAvail(in);
-    sum = DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 
+    sum = DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 
         Integer.MAX_VALUE);
     checksumSize = sum.getChecksumSize();
     length = len;
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileOutputStream.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileOutputStream.java
index 8f25ba7..c352ffd 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileOutputStream.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileOutputStream.java
@@ -49,7 +49,7 @@ public class IFileOutputStream extends FilterOutputStream {
    */
   public IFileOutputStream(OutputStream out) {
     super(out);
-    sum = DataChecksum.newDataChecksum(DataChecksum.Type.CRC32,
+    sum = DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32,
         Integer.MAX_VALUE);
     barray = new byte[sum.getChecksumSize()];
   }
-- 
1.7.0.4

