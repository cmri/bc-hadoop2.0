From da50472a99a27d47d161d66c7c2b4bbffcd778c0 Mon Sep 17 00:00:00 2001
From: Eli Collins <eli@apache.org>
Date: Mon, 16 Jul 2012 16:54:49 +0000
Subject: [PATCH 0233/1357] HDFS-3537. Move libhdfs and fuse-dfs source to native subdirectories. Contributed by Colin Patrick McCabe

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-2@1362127 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit fb154f83fa52d4f6707aec186196d037e4d61cff)
---
 hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt |   36 +-
 .../hadoop-hdfs/src/contrib/fuse-dfs/README        |  131 -
 .../src/contrib/fuse-dfs/build-contrib.xml         |  312 ---
 .../hadoop-hdfs/src/contrib/fuse-dfs/build.xml     |   87 -
 .../src/contrib/fuse-dfs/global_footer.mk          |   18 -
 .../src/contrib/fuse-dfs/global_header.mk          |   51 -
 .../hadoop-hdfs/src/contrib/fuse-dfs/ivy.xml       |   71 -
 .../src/contrib/fuse-dfs/ivy/libraries.properties  |    5 -
 .../src/contrib/fuse-dfs/src/CMakeLists.txt        |   73 -
 .../src/contrib/fuse-dfs/src/fuse_connect.c        |  239 --
 .../src/contrib/fuse-dfs/src/fuse_connect.h        |   28 -
 .../src/contrib/fuse-dfs/src/fuse_context_handle.h |   43 -
 .../src/contrib/fuse-dfs/src/fuse_dfs.c            |  129 -
 .../src/contrib/fuse-dfs/src/fuse_dfs.h            |   81 -
 .../src/contrib/fuse-dfs/src/fuse_dfs_wrapper.sh   |   46 -
 .../src/contrib/fuse-dfs/src/fuse_file_handle.h    |   44 -
 .../src/contrib/fuse-dfs/src/fuse_impls.h          |   63 -
 .../src/contrib/fuse-dfs/src/fuse_impls_access.c   |   29 -
 .../src/contrib/fuse-dfs/src/fuse_impls_chmod.c    |   53 -
 .../src/contrib/fuse-dfs/src/fuse_impls_chown.c    |   83 -
 .../src/contrib/fuse-dfs/src/fuse_impls_create.c   |   27 -
 .../src/contrib/fuse-dfs/src/fuse_impls_flush.c    |   55 -
 .../src/contrib/fuse-dfs/src/fuse_impls_getattr.c  |   71 -
 .../src/contrib/fuse-dfs/src/fuse_impls_mkdir.c    |   69 -
 .../src/contrib/fuse-dfs/src/fuse_impls_mknod.c    |   27 -
 .../src/contrib/fuse-dfs/src/fuse_impls_open.c     |   94 -
 .../src/contrib/fuse-dfs/src/fuse_impls_read.c     |  162 --
 .../src/contrib/fuse-dfs/src/fuse_impls_readdir.c  |  116 -
 .../src/contrib/fuse-dfs/src/fuse_impls_release.c  |   68 -
 .../src/contrib/fuse-dfs/src/fuse_impls_rename.c   |   68 -
 .../src/contrib/fuse-dfs/src/fuse_impls_rmdir.c    |   76 -
 .../src/contrib/fuse-dfs/src/fuse_impls_statfs.c   |   63 -
 .../src/contrib/fuse-dfs/src/fuse_impls_symlink.c  |   30 -
 .../src/contrib/fuse-dfs/src/fuse_impls_truncate.c |   75 -
 .../src/contrib/fuse-dfs/src/fuse_impls_unlink.c   |   64 -
 .../src/contrib/fuse-dfs/src/fuse_impls_utimens.c  |   63 -
 .../src/contrib/fuse-dfs/src/fuse_impls_write.c    |   82 -
 .../src/contrib/fuse-dfs/src/fuse_init.c           |  125 -
 .../src/contrib/fuse-dfs/src/fuse_init.h           |   31 -
 .../src/contrib/fuse-dfs/src/fuse_options.c        |  188 --
 .../src/contrib/fuse-dfs/src/fuse_options.h        |   44 -
 .../src/contrib/fuse-dfs/src/fuse_stat_struct.c    |  112 -
 .../src/contrib/fuse-dfs/src/fuse_stat_struct.h    |   36 -
 .../src/contrib/fuse-dfs/src/fuse_trash.c          |  126 -
 .../src/contrib/fuse-dfs/src/fuse_trash.h          |   26 -
 .../src/contrib/fuse-dfs/src/fuse_users.c          |  213 --
 .../src/contrib/fuse-dfs/src/fuse_users.h          |   70 -
 .../src/contrib/fuse-dfs/src/test/TestFuseDFS.java |  369 ---
 .../hadoop-hdfs/src/main/native/expect.h           |  101 -
 .../src/main/native/fuse-dfs/CMakeLists.txt        |   73 +
 .../src/main/native/fuse-dfs/doc/README            |  131 +
 .../src/main/native/fuse-dfs/fuse_connect.c        |  239 ++
 .../src/main/native/fuse-dfs/fuse_connect.h        |   28 +
 .../src/main/native/fuse-dfs/fuse_context_handle.h |   43 +
 .../src/main/native/fuse-dfs/fuse_dfs.c            |  129 +
 .../src/main/native/fuse-dfs/fuse_dfs.h            |   81 +
 .../src/main/native/fuse-dfs/fuse_dfs_wrapper.sh   |   46 +
 .../src/main/native/fuse-dfs/fuse_file_handle.h    |   44 +
 .../src/main/native/fuse-dfs/fuse_impls.h          |   63 +
 .../src/main/native/fuse-dfs/fuse_impls_access.c   |   29 +
 .../src/main/native/fuse-dfs/fuse_impls_chmod.c    |   53 +
 .../src/main/native/fuse-dfs/fuse_impls_chown.c    |   83 +
 .../src/main/native/fuse-dfs/fuse_impls_create.c   |   27 +
 .../src/main/native/fuse-dfs/fuse_impls_flush.c    |   55 +
 .../src/main/native/fuse-dfs/fuse_impls_getattr.c  |   71 +
 .../src/main/native/fuse-dfs/fuse_impls_mkdir.c    |   69 +
 .../src/main/native/fuse-dfs/fuse_impls_mknod.c    |   27 +
 .../src/main/native/fuse-dfs/fuse_impls_open.c     |   94 +
 .../src/main/native/fuse-dfs/fuse_impls_read.c     |  162 ++
 .../src/main/native/fuse-dfs/fuse_impls_readdir.c  |  116 +
 .../src/main/native/fuse-dfs/fuse_impls_release.c  |   68 +
 .../src/main/native/fuse-dfs/fuse_impls_rename.c   |   68 +
 .../src/main/native/fuse-dfs/fuse_impls_rmdir.c    |   76 +
 .../src/main/native/fuse-dfs/fuse_impls_statfs.c   |   63 +
 .../src/main/native/fuse-dfs/fuse_impls_symlink.c  |   30 +
 .../src/main/native/fuse-dfs/fuse_impls_truncate.c |   75 +
 .../src/main/native/fuse-dfs/fuse_impls_unlink.c   |   64 +
 .../src/main/native/fuse-dfs/fuse_impls_utimens.c  |   63 +
 .../src/main/native/fuse-dfs/fuse_impls_write.c    |   82 +
 .../src/main/native/fuse-dfs/fuse_init.c           |  125 +
 .../src/main/native/fuse-dfs/fuse_init.h           |   31 +
 .../src/main/native/fuse-dfs/fuse_options.c        |  188 ++
 .../src/main/native/fuse-dfs/fuse_options.h        |   44 +
 .../src/main/native/fuse-dfs/fuse_stat_struct.c    |  112 +
 .../src/main/native/fuse-dfs/fuse_stat_struct.h    |   36 +
 .../src/main/native/fuse-dfs/fuse_trash.c          |  126 +
 .../src/main/native/fuse-dfs/fuse_trash.h          |   26 +
 .../src/main/native/fuse-dfs/fuse_users.c          |  213 ++
 .../src/main/native/fuse-dfs/fuse_users.h          |   70 +
 .../src/main/native/fuse-dfs/test/TestFuseDFS.java |  369 +++
 .../hadoop-hdfs/src/main/native/hdfs.c             | 2519 --------------------
 .../hadoop-hdfs/src/main/native/hdfs.h             |  605 -----
 .../hadoop-hdfs/src/main/native/hdfsJniHelper.c    |  589 -----
 .../hadoop-hdfs/src/main/native/hdfsJniHelper.h    |  109 -
 .../hadoop-hdfs/src/main/native/hdfs_read.c        |   70 -
 .../hadoop-hdfs/src/main/native/hdfs_test.c        |  525 ----
 .../hadoop-hdfs/src/main/native/hdfs_test.h        |   46 -
 .../hadoop-hdfs/src/main/native/hdfs_write.c       |   94 -
 .../hadoop-hdfs/src/main/native/libhdfs/expect.h   |  101 +
 .../hadoop-hdfs/src/main/native/libhdfs/hdfs.c     | 2519 ++++++++++++++++++++
 .../hadoop-hdfs/src/main/native/libhdfs/hdfs.h     |  605 +++++
 .../src/main/native/libhdfs/hdfs_test.h            |   46 +
 .../src/main/native/libhdfs/jni_helper.c           |  589 +++++
 .../src/main/native/libhdfs/jni_helper.h           |  109 +
 .../src/main/native/libhdfs/native_mini_dfs.c      |  165 ++
 .../src/main/native/libhdfs/native_mini_dfs.h      |   81 +
 .../main/native/libhdfs/test/test_libhdfs_ops.c    |  525 ++++
 .../main/native/libhdfs/test/test_libhdfs_read.c   |   70 +
 .../main/native/libhdfs/test/test_libhdfs_write.c  |   94 +
 .../main/native/libhdfs/test_libhdfs_threaded.c    |  221 ++
 .../src/main/native/libhdfs/test_native_mini_dfs.c |   41 +
 .../hadoop-hdfs/src/main/native/native_mini_dfs.c  |  165 --
 .../hadoop-hdfs/src/main/native/native_mini_dfs.h  |   81 -
 .../src/main/native/test_libhdfs_threaded.c        |  221 --
 .../src/main/native/test_native_mini_dfs.c         |   41 -
 115 files changed, 8774 insertions(+), 9322 deletions(-)
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/README
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build-contrib.xml
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build.xml
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_footer.mk
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_header.mk
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy.xml
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy/libraries.properties
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/CMakeLists.txt
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_context_handle.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.h
 delete mode 100755 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs_wrapper.sh
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_file_handle.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_access.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chmod.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chown.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_create.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_flush.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_getattr.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mkdir.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mknod.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_open.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_read.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_readdir.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_release.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rename.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rmdir.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_statfs.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_symlink.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_truncate.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_unlink.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_utimens.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_write.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/test/TestFuseDFS.java
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/expect.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/doc/README
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_context_handle.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.h
 create mode 100755 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_file_handle.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_access.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chmod.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chown.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_create.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_flush.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_getattr.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mkdir.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mknod.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_read.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_readdir.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_release.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rename.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rmdir.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_statfs.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_symlink.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_truncate.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_unlink.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_utimens.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_write.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/TestFuseDFS.java
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_read.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_write.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/expect.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs_test.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.h
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_ops.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_read.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_write.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_native_mini_dfs.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.h
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_libhdfs_threaded.c
 delete mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_native_mini_dfs.c

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt b/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt
index 741c129..30ecf43 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt
@@ -84,15 +84,15 @@ include_directories(
     ${CMAKE_CURRENT_SOURCE_DIR}
     ${CMAKE_BINARY_DIR}
     ${JNI_INCLUDE_DIRS}
-    main/native/
+    main/native/libhdfs
 )
 
 set(_FUSE_DFS_VERSION 0.1.0)
 CONFIGURE_FILE(${CMAKE_SOURCE_DIR}/config.h.cmake ${CMAKE_BINARY_DIR}/config.h)
 
 add_dual_library(hdfs
-    main/native/hdfs.c
-    main/native/hdfsJniHelper.c
+    main/native/libhdfs/hdfs.c
+    main/native/libhdfs/jni_helper.c
 )
 target_link_dual_libraries(hdfs
     ${JAVA_JVM_LIBRARY}
@@ -102,49 +102,46 @@ set(LIBHDFS_VERSION "0.0.0")
 set_target_properties(hdfs PROPERTIES
     SOVERSION ${LIBHDFS_VERSION})
 
-add_executable(hdfs_test
-    main/native/hdfs_test.c
+add_executable(test_libhdfs_ops
+    main/native/libhdfs/test/test_libhdfs_ops.c
 )
-target_link_libraries(hdfs_test
+target_link_libraries(test_libhdfs_ops
     hdfs
     ${JAVA_JVM_LIBRARY}
 )
-output_directory(hdfs_test target/usr/local/bin)
 
-add_executable(hdfs_read
-    main/native/hdfs_read.c
+add_executable(test_libhdfs_read
+    main/native/libhdfs/test/test_libhdfs_read.c
 )
-target_link_libraries(hdfs_read
+target_link_libraries(test_libhdfs_read
     hdfs
     ${JAVA_JVM_LIBRARY}
 )
-output_directory(hdfs_read target/usr/local/bin)
 
-add_executable(hdfs_write
-    main/native/hdfs_write.c
+add_executable(test_libhdfs_write
+    main/native/libhdfs/test/test_libhdfs_write.c
 )
-target_link_libraries(hdfs_write
+target_link_libraries(test_libhdfs_write
     hdfs
     ${JAVA_JVM_LIBRARY}
 )
-output_directory(hdfs_write target/usr/local/bin)
 
 add_library(native_mini_dfs
-    main/native/native_mini_dfs.c
+    main/native/libhdfs/native_mini_dfs.c
 )
 target_link_libraries(native_mini_dfs
     hdfs
 )
 
 add_executable(test_native_mini_dfs
-    main/native/test_native_mini_dfs.c
+    main/native/libhdfs/test_native_mini_dfs.c
 )
 target_link_libraries(test_native_mini_dfs
     native_mini_dfs
 )
 
 add_executable(test_libhdfs_threaded
-    main/native/test_libhdfs_threaded.c
+    main/native/libhdfs/test_libhdfs_threaded.c
 )
 target_link_libraries(test_libhdfs_threaded
     hdfs
@@ -152,5 +149,4 @@ target_link_libraries(test_libhdfs_threaded
     pthread
 )
 
-add_subdirectory(contrib/fuse-dfs/src)
-
+add_subdirectory(main/native/fuse-dfs)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/README b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/README
deleted file mode 100644
index 1744892..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/README
+++ /dev/null
@@ -1,131 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-Fuse-DFS
-
-Supports reads, writes, and directory operations (e.g., cp, ls, more, cat, find, less, rm, mkdir, mv, rmdir).  Things like touch, chmod, chown, and permissions are in the works. Fuse-dfs currently shows all files as owned by nobody.
-
-Contributing
-
-It's pretty straightforward to add functionality to fuse-dfs as fuse makes things relatively simple. Some other tasks require also augmenting libhdfs to expose more hdfs functionality to C. See [http://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&mode=hide&pid=12310240&sorter/order=DESC&sorter/field=priority&resolution=-1&component=12312376  contrib/fuse-dfs JIRAs]
-
-Requirements
-
- * Hadoop with compiled libhdfs.so
- * Linux kernel > 2.6.9 with fuse, which is the default or Fuse 2.7.x, 2.8.x installed. See: [http://fuse.sourceforge.net/]
- * modprobe fuse to load it
- * fuse-dfs executable (see below)
- * fuse_dfs_wrapper.sh installed in /bin or other appropriate location (see below)
-
-
-BUILDING
-
-   1. in HADOOP_PREFIX: `ant compile-libhdfs -Dlibhdfs=1
-   2. in HADOOP_PREFIX: `ant package` to deploy libhdfs
-   3. in HADOOP_PREFIX: `ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1`
-
-NOTE: for amd64 architecture, libhdfs will not compile unless you edit
-the Makefile in src/c++/libhdfs/Makefile and set OS_ARCH=amd64
-(probably the same for others too). See [https://issues.apache.org/jira/browse/HADOOP-3344 HADOOP-3344]
-
-Common build problems include not finding the libjvm.so in JAVA_HOME/jre/lib/OS_ARCH/server or not finding fuse in FUSE_HOME or /usr/local.
-
-
-CONFIGURING
-
-Look at all the paths in fuse_dfs_wrapper.sh and either correct them or set them in your environment before running. (note for automount and mount as root, you probably cannot control the environment, so best to set them in the wrapper)
-
-INSTALLING
-
-1. `mkdir /export/hdfs` (or wherever you want to mount it)
-
-2. `fuse_dfs_wrapper.sh dfs://hadoop_server1.foo.com:9000 /export/hdfs -d` and from another terminal, try `ls /export/hdfs`
-
-If 2 works, try again dropping the debug mode, i.e., -d
-
-(note - common problems are that you don't have libhdfs.so or libjvm.so or libfuse.so on your LD_LIBRARY_PATH, and your CLASSPATH does not contain hadoop and other required jars.)
-
-Also note, fuse-dfs will write error/warn messages to the syslog - typically in /var/log/messages
-
-You can use fuse-dfs to mount multiple hdfs instances by just changing the server/port name and directory mount point above.
-
-DEPLOYING
-
-in a root shell do the following:
-
-1. add the following to /etc/fstab
-
-fuse_dfs#dfs://hadoop_server.foo.com:9000 /export/hdfs fuse -oallow_other,rw,-ousetrash,-oinitchecks 0 0
-
-
-2. Mount using: `mount /export/hdfs`. Expect problems with not finding fuse_dfs. You will need to probably add this to /sbin and then problems finding the above 3 libraries. Add these using ldconfig.
-
-
-Fuse DFS takes the following mount options (i.e., on the command line or the comma separated list of options in /etc/fstab:
-
--oserver=%s  (optional place to specify the server but in fstab use the format above)
--oport=%d (optional port see comment on server option)
--oentry_timeout=%d (how long directory entries are cached by fuse in seconds - see fuse docs)
--oattribute_timeout=%d (how long attributes are cached by fuse in seconds - see fuse docs)
--oprotected=%s (a colon separated list of directories that fuse-dfs should not allow to be deleted or moved - e.g., /user:/tmp)
--oprivate (not often used but means only the person who does the mount can use the filesystem - aka ! allow_others in fuse speak)
--ordbuffer=%d (in KBs how large a buffer should fuse-dfs use when doing hdfs reads)
-ro 
-rw
--ousetrash (should fuse dfs throw things in /Trash when deleting them)
--onotrash (opposite of usetrash)
--odebug (do not daemonize - aka -d in fuse speak)
--obig_writes (use fuse big_writes option so as to allow better performance of writes on kernels >= 2.6.26)
--initchecks - have fuse-dfs try to connect to hdfs to ensure all is ok upon startup. recommended to have this  on
-The defaults are:
-
-entry,attribute_timeouts = 60 seconds
-rdbuffer = 10 MB
-protected = null
-debug = 0
-notrash
-private = 0
-
-EXPORTING
-
-Add the following to /etc/exports:
-
-/export/hdfs *.foo.com(no_root_squash,rw,fsid=1,sync)
-
-NOTE - you cannot export this with a FUSE module built into the kernel
-- e.g., kernel 2.6.17. For info on this, refer to the FUSE wiki.
-
-
-RECOMMENDATIONS
-
-1. From /bin, `ln -s $HADOOP_PREFIX/contrib/fuse-dfs/fuse_dfs* .`
-
-2. Always start with debug on so you can see if you are missing a classpath or something like that.
-
-3. use -obig_writes
-
-4. use -initchecks
-
-KNOWN ISSUES 
-
-1. if you alias `ls` to `ls --color=auto` and try listing a directory with lots (over thousands) of files, expect it to be slow and at 10s of thousands, expect it to be very very slow.  This is because `--color=auto` causes ls to stat every file in the directory. Since fuse-dfs does not cache attribute entries when doing a readdir, 
-this is very slow. see [https://issues.apache.org/jira/browse/HADOOP-3797 HADOOP-3797]
-
-2. Writes are approximately 33% slower than the DFSClient. TBD how to optimize this. see: [https://issues.apache.org/jira/browse/HADOOP-3805 HADOOP-3805] - try using -obig_writes if on a >2.6.26 kernel, should perform much better since bigger writes implies less context switching.
-
-3. Reads are ~20-30% slower even with the read buffering. 
-
-4. fuse-dfs and underlying libhdfs have no support for permissions. See [https://issues.apache.org/jira/browse/HADOOP-3536 HADOOP-3536] 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build-contrib.xml b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build-contrib.xml
deleted file mode 100644
index bde5e4e..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build-contrib.xml
+++ /dev/null
@@ -1,312 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<!-- Imported by contrib/*/build.xml files to share generic targets. -->
-
-<project name="hadoopbuildcontrib" xmlns:ivy="antlib:org.apache.ivy.ant">
-
-  <property name="name" value="${ant.project.name}"/>
-  <property name="root" value="${basedir}"/>
-  <property name="hadoop.root" location="${root}/../../../"/>
-
-  <!-- Load all the default properties, and any the user wants    -->
-  <!-- to contribute (without having to type -D or edit this file -->
-  <property file="${user.home}/${name}.build.properties" />
-  <property file="${root}/build.properties" />
-  <property file="${hadoop.root}/build.properties" />
-
-  <property name="src.dir"  location="${root}/src/java"/>
-  <property name="src.test" location="${root}/src/test"/>
-  <property name="src.examples" location="${root}/src/examples"/>
-
-  <available file="${src.examples}" type="dir" property="examples.available"/>
-  <available file="${src.test}" type="dir" property="test.available"/>
-
-  <property name="conf.dir" location="${hadoop.root}/conf"/>
-  <property name="test.junit.output.format" value="plain"/>
-  <property name="test.output" value="no"/>
-  <property name="test.timeout" value="900000"/>
-  <property name="build.dir" location="${hadoop.root}/build/contrib/${name}"/>
-  <property name="build.webapps.root.dir" value="${hadoop.root}/build/web"/>
-  <property name="build.webapps" value="${build.webapps.root.dir}/webapps"/>
-  <property name="build.classes" location="${build.dir}/classes"/>
-  <!-- NB: sun.arch.data.model is not supported on all platforms -->
-  <property name="build.platform"
-            value="${os.name}-${os.arch}-${sun.arch.data.model}"/>
-  <property name="build.c++.libhdfs" value="${build.dir}/../../c++/${build.platform}/lib"/>
-  <property name="build.test" location="${build.dir}/test"/>
-  <property name="build.examples" location="${build.dir}/examples"/>
-  <property name="hadoop.log.dir" location="${build.dir}/test/logs"/>
-  <!-- all jars together -->
-  <property name="javac.deprecation" value="off"/>
-  <property name="javac.debug" value="on"/>
-  <property name="build.ivy.lib.dir" value="${hadoop.root}/build/ivy/lib"/> 
-
-  <property name="javadoc.link"
-            value="http://java.sun.com/j2se/1.4/docs/api/"/>
-
-  <property name="build.encoding" value="ISO-8859-1"/>
-
-  <fileset id="lib.jars" dir="${root}" includes="lib/*.jar"/>
-
-
-   <!-- IVY properties set here -->
-  <property name="ivy.dir" location="ivy" />
-  <property name="ivysettings.xml" location="${hadoop.root}/ivy/ivysettings.xml"/>
-  <loadproperties srcfile="${ivy.dir}/libraries.properties"/>
-  <loadproperties srcfile="ivy/libraries.properties"/>
-  <property name="ivy.jar" location="${hadoop.root}/ivy/ivy-${ivy.version}.jar"/>
-  <property name="ivy_repo_url" 
-	value="http://repo2.maven.org/maven2/org/apache/ivy/ivy/${ivy.version}/ivy-${ivy.version}.jar" />
-  <property name="build.dir" location="build" />
-  <property name="build.ivy.dir" location="${build.dir}/ivy" />
-  <property name="build.ivy.lib.dir" location="${build.ivy.dir}/lib" />
-  <property name="build.ivy.report.dir" location="${build.ivy.dir}/report" />
-  <property name="common.ivy.lib.dir" location="${build.ivy.lib.dir}/${ant.project.name}/common"/> 
-
-  <!--this is the naming policy for artifacts we want pulled down-->
-  <property name="ivy.artifact.retrieve.pattern"
-    			value="${ant.project.name}/[conf]/[artifact]-[revision](-[classifier]).[ext]"/>
-
-  <!-- the normal classpath -->
-  <path id="contrib-classpath">
-    <pathelement location="${build.classes}"/>
-    <fileset refid="lib.jars"/>
-    <pathelement location="${hadoop.root}/build/classes"/>
-    <fileset dir="${hadoop.root}/lib">
-      <include name="**/*.jar" />
-    </fileset>
-    <path refid="${ant.project.name}.common-classpath"/>
-    <pathelement path="${clover.jar}"/>
-  </path>
-
-  <!-- the unit test classpath -->
-  <path id="test.classpath">
-    <pathelement location="${build.test}"/>
-    <pathelement location="${build.webapps.root.dir}"/>
-    <pathelement location="${hadoop.root}/build/test/core/classes"/>
-    <pathelement location="${hadoop.root}/build/test/hdfs/classes"/>
-    <pathelement location="${hadoop.root}/build/test/mapred/classes"/>
-    <pathelement location="${hadoop.root}/src/contrib/test"/>
-    <pathelement location="${conf.dir}"/>
-    <pathelement location="${hadoop.root}/build"/>
-    <pathelement location="${build.examples}"/>
-    <path refid="contrib-classpath"/>
-  </path>
-
-
-  <!-- to be overridden by sub-projects -->
-  <target name="check-contrib"/>
-  <target name="init-contrib"/>
-
-  <!-- ====================================================== -->
-  <!-- Stuff needed by all targets                            -->
-  <!-- ====================================================== -->
-  <target name="init" depends="check-contrib" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <mkdir dir="${build.dir}"/>
-    <mkdir dir="${build.classes}"/>
-    <mkdir dir="${build.test}"/>
-    <mkdir dir="${build.examples}"/>
-    <mkdir dir="${hadoop.log.dir}"/>
-    <antcall target="init-contrib"/>
-  </target>
-
-
-  <!-- ====================================================== -->
-  <!-- Compile a Hadoop contrib's files                       -->
-  <!-- ====================================================== -->
-  <target name="compile" depends="init, ivy-retrieve-common" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.dir}"
-     includes="**/*.java"
-     destdir="${build.classes}"
-     debug="${javac.debug}"
-     deprecation="${javac.deprecation}">
-     <classpath refid="contrib-classpath"/>
-    </javac>
-  </target>
-
-
-  <!-- ======================================================= -->
-  <!-- Compile a Hadoop contrib's example files (if available) -->
-  <!-- ======================================================= -->
-  <target name="compile-examples" depends="compile" if="examples.available">
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.examples}"
-     includes="**/*.java"
-     destdir="${build.examples}"
-     debug="${javac.debug}">
-     <classpath refid="contrib-classpath"/>
-    </javac>
-  </target>
-
-
-  <!-- ================================================================== -->
-  <!-- Compile test code                                                  -->
-  <!-- ================================================================== -->
-  <target name="compile-test" depends="compile-examples" if="test.available">
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.test}"
-     includes="**/*.java"
-     destdir="${build.test}"
-     debug="${javac.debug}">
-    <classpath refid="test.classpath"/>
-    </javac>
-  </target>
-  
-
-  <!-- ====================================================== -->
-  <!-- Make a Hadoop contrib's jar                            -->
-  <!-- ====================================================== -->
-  <target name="jar" depends="compile" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <jar
-      jarfile="${build.dir}/hadoop-${version}-${name}.jar"
-      basedir="${build.classes}"      
-    />
-  </target>
-
-  
-  <!-- ====================================================== -->
-  <!-- Make a Hadoop contrib's examples jar                   -->
-  <!-- ====================================================== -->
-  <target name="jar-examples" depends="compile-examples"
-          if="examples.available" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <jar jarfile="${build.dir}/hadoop-${version}-${name}-examples.jar">
-      <fileset dir="${build.classes}">
-      </fileset>
-      <fileset dir="${build.examples}">
-      </fileset>
-    </jar>
-  </target>
-  
-  <!-- ====================================================== -->
-  <!-- Package a Hadoop contrib                               -->
-  <!-- ====================================================== -->
-  <target name="package" depends="jar, jar-examples" unless="skip.contrib"> 
-    <mkdir dir="${dist.dir}/contrib/${name}"/>
-    <copy todir="${dist.dir}/contrib/${name}" includeEmptyDirs="false" flatten="true">
-      <fileset dir="${build.dir}">
-        <include name="hadoop-${version}-${name}.jar" />
-      </fileset>
-    </copy>
-  </target>
-  
-  <!-- ================================================================== -->
-  <!-- Run unit tests                                                     -->
-  <!-- ================================================================== -->
-  <target name="test" depends="compile-test, compile" if="test.available">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${hadoop.log.dir}"/>
-    <mkdir dir="${hadoop.log.dir}"/>
-    <junit
-      printsummary="yes" showoutput="${test.output}" 
-      haltonfailure="no" fork="yes" maxmemory="256m"
-      errorProperty="tests.failed" failureProperty="tests.failed"
-      timeout="${test.timeout}">
-      
-      <sysproperty key="test.build.data" value="${build.test}/data"/>
-      <sysproperty key="build.test" value="${build.test}"/>
-      <sysproperty key="contrib.name" value="${name}"/>
-      
-      <!-- requires fork=yes for: 
-        relative File paths to use the specified user.dir 
-        classpath to use build/contrib/*.jar
-      -->
-      <sysproperty key="java.net.preferIPv4Stack" value="true"/>
-      <sysproperty key="user.dir" value="${build.test}/data"/>
-      
-      <sysproperty key="fs.default.name" value="${fs.default.name}"/>
-      <sysproperty key="hadoop.test.localoutputfile" value="${hadoop.test.localoutputfile}"/>
-      <sysproperty key="hadoop.log.dir" value="${hadoop.log.dir}"/> 
-      <sysproperty key="taskcontroller-path" value="${taskcontroller-path}"/>
-      <sysproperty key="taskcontroller-user" value="${taskcontroller-user}"/>
-      <classpath refid="test.classpath"/>
-      <formatter type="${test.junit.output.format}" />
-      <batchtest todir="${build.test}" unless="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/Test*.java" excludes="**/${test.exclude}.java" />
-      </batchtest>
-      <batchtest todir="${build.test}" if="testcase">
-        <fileset dir="${src.test}" includes="**/${testcase}.java"/>
-      </batchtest>
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- Clean.  Delete the build files, and their directories              -->
-  <!-- ================================================================== -->
-  <target name="clean">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${build.dir}"/>
-  </target>
-
-  <target name="ivy-probe-antlib" >
-    <condition property="ivy.found">
-      <typefound uri="antlib:org.apache.ivy.ant" name="cleancache"/>
-    </condition>
-  </target>
-
-
-  <target name="ivy-download" description="To download ivy " unless="offline">
-    <get src="${ivy_repo_url}" dest="${ivy.jar}" usetimestamp="true"/>
-  </target>
-
-  <target name="ivy-init-antlib" depends="ivy-download,ivy-probe-antlib" unless="ivy.found">
-    <typedef uri="antlib:org.apache.ivy.ant" onerror="fail"
-      loaderRef="ivyLoader">
-      <classpath>
-        <pathelement location="${ivy.jar}"/>
-      </classpath>
-    </typedef>
-    <fail >
-      <condition >
-        <not>
-          <typefound uri="antlib:org.apache.ivy.ant" name="cleancache"/>
-        </not>
-      </condition>
-      You need Apache Ivy 2.0 or later from http://ant.apache.org/
-      It could not be loaded from ${ivy_repo_url}
-    </fail>
-  </target>
-
-  <target name="ivy-init" depends="ivy-init-antlib">
-    <ivy:configure settingsid="${ant.project.name}.ivy.settings" file="${ivysettings.xml}"/>
-  </target>
-
-  <target name="ivy-resolve-common" depends="ivy-init">
-    <ivy:resolve settingsRef="${ant.project.name}.ivy.settings" conf="common" />
-  </target>
-
-  <target name="ivy-retrieve-common" depends="ivy-resolve-common"
-    description="Retrieve Ivy-managed artifacts for the compile/test configurations">
-    <ivy:retrieve settingsRef="${ant.project.name}.ivy.settings" 
-      pattern="${build.ivy.lib.dir}/${ivy.artifact.retrieve.pattern}" sync="true" />
-    <ivy:cachepath pathid="${ant.project.name}.common-classpath" conf="common" />
-  </target>
-</project>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build.xml b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build.xml
deleted file mode 100644
index ab3c92b..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/build.xml
+++ /dev/null
@@ -1,87 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<project name="fuse-dfs" default="compile" xmlns:ivy="antlib:org.apache.ivy.ant">
-
-  <import file="build-contrib.xml"/>
-
-  <target name="check-libhdfs-exists">
-    <property name="libhdfs.lib" value="${build.c++.libhdfs}/libhdfs.so"/>
-    <available file="${libhdfs.lib}" property="libhdfs-exists"/>
-    <fail message="libhdfs.so does not exist: ${libhdfs.lib}.">
-      <condition><not><isset property="libhdfs-exists"/></not></condition>
-    </fail>
-  </target>
-
-  <target name="compile">
-    <exec executable="autoreconf" dir="${basedir}" 
-          searchpath="yes" failonerror="yes">
-       <arg value="-if"/>
-    </exec>
-
-    <exec executable="${basedir}/configure" dir="${basedir}"
-          failonerror="yes">
-    </exec>
-
-    <exec executable="make" failonerror="true">
-      <env key="OS_NAME" value="${os.name}"/>
-      <env key="OS_ARCH" value="${os.arch}"/>
-      <env key="HADOOP_PREFIX" value="${hadoop.root}"/>
-      <env key="PACKAGE_VERSION" value="0.1.0"/>
-      <env key="BUILD_PLATFORM" value="${build.platform}" />
-    </exec>
-  </target>
-
-  <target name="jar" />
-  <target name="package" />
-
-  <target name="compile-test" depends="ivy-retrieve-common, check-libhdfs-exists">
-    <javac encoding="${build.encoding}"
-	   srcdir="${src.test}"
-	   includes="**/*.java"
-	   destdir="${build.test}"
-	   debug="${javac.debug}">
-      <classpath refid="test.classpath"/>
-    </javac>
-  </target>
-
-  <target name="test" depends="compile-test,check-libhdfs-exists">
-    <junit showoutput="${test.output}" fork="yes" printsummary="yes"
-           errorProperty="tests.failed" haltonfailure="no" failureProperty="tests.failed">
-      <classpath refid="test.classpath"/>
-      <sysproperty key="test.build.data" value="${build.test}/data"/>
-      <sysproperty key="build.test" value="${build.test}"/>
-      <sysproperty key="user.dir" value="${build.test}/data"/>
-      <sysproperty key="hadoop.log.dir" value="${hadoop.log.dir}"/>
-      <sysproperty key="test.src.dir" value="${test.src.dir}"/>
-      <formatter type="${test.junit.output.format}" />
-      <batchtest todir="${build.test}" unless="testcase">
-        <fileset dir="${src.test}">
-          <include name="**/Test*.java"/>
-        </fileset>
-      </batchtest>
-      <batchtest todir="${build.test}" if="testcase">
-        <fileset dir="${src.test}">
-          <include name="**/${testcase}.java"/>
-        </fileset>
-      </batchtest>
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
- </target>
-</project>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_footer.mk b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_footer.mk
deleted file mode 100644
index 80c3662..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_footer.mk
+++ /dev/null
@@ -1,18 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-thriftstyle : $(XBUILT_SOURCES)
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_header.mk b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_header.mk
deleted file mode 100644
index f67fa8b..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/global_header.mk
+++ /dev/null
@@ -1,51 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-ifneq ($$(XBUILT_SOURCES),)
-    XBUILT_SOURCES := $$(XBUILT_SOURCES) $$(XTARGET)
-else
-    XBUILT_SOURCES := $$(XTARGET)
-endif
-
-showvars:
-	@echo BUILD_SOURCES = $(BUILT_SOURCES)
-	@echo XBUILTSOURCES = $(XBUILT_SOURCES)
-	@echo DEFS = $(DEFS)
-	@echo CXXFLAGS = $(CXXFLAGS)
-	@echo AM_CXXFLAGS = $(AM_CXXFLAGS)
-	@echo CPPFLAGS = $(CPPFLAGS)
-	@echo AM_CPPFLAGS = $(AM_CPPFLAGS)
-	@echo LDFLAGS = $(LDFLAGS)
-	@echo AM_LDFLAGS = $(AM_LDFLAGS)
-	@echo LDADD = $(LDADD)
-	@echo LIBS = $(LIBS)
-	@echo EXTERNAL_LIBS = $(EXTERNAL_LIBS)
-	@echo EXTERNAL_PATH = $(EXTERNAL_PATH)
-	@echo MAKE = $(MAKE)
-	@echo MAKE_FLAGS = $(MAKE_FLAGS)
-	@echo AM_MAKEFLAGS = $(AM_MAKEFLAGS)
-	@echo top_builddir = $(top_builddir)
-	@echo top_srcdir = $(top_srcdir)
-	@echo srcdir = $(srcdir)
-	@echo PHPVAL = $(PHPVAL)
-	@echo PHPCONFIGDIR  = $(PHPCONFIGDIR)
-	@echo PHPCONFIGINCLUDEDIR = $(PHPCONFIGINCLUDEDIR)
-	@echo PHPCONFIGINCLUDES  = $(PHPCONFIGINCLUDES)
-	@echo PHPCONFIGLDFLAGS  = $(PHPCONFIGLDFLAGS)
-	@echo PHPCONFIGLIBS  = $(PHPCONFIGLIBS)
-
-clean-common:
-	rm -rf gen-*
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy.xml b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy.xml
deleted file mode 100644
index c99e16e..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy.xml
+++ /dev/null
@@ -1,71 +0,0 @@
-<?xml version="1.0" ?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-<ivy-module version="1.0" xmlns:m="http://ant.apache.org/ivy/maven">
-  <info organisation="org.apache.hadoop" module="${ant.project.name}">
-    <license name="Apache 2.0"/>
-    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
-    <description>
-        FUSE plugin for HDFS
-    </description>
-  </info>
-  <configurations defaultconfmapping="default">
-    <!--these match the Maven configurations-->
-    <conf name="default" extends="master,runtime"/>
-    <conf name="master" description="contains the artifact but no dependencies"/>
-    <conf name="runtime" description="runtime but not the artifact" />
-
-    <conf name="common" visibility="private" 
-      extends="runtime"
-      description="artifacts needed to compile/test the application"/>
-    <conf name="test" visibility="private" extends="runtime"/>
-  </configurations>
-
-  <publications>
-    <!--get the artifact from our module name-->
-    <artifact conf="master"/>
-  </publications>
-  <dependencies>
-    <dependency org="org.apache.hadoop"
-      name="hadoop-common"
-      rev="${hadoop-common.version}"
-      conf="common->default"/>
-    <dependency org="org.apache.hadoop"
-      name="hadoop-common"
-      rev="${hadoop-common.version}"
-      conf="common->default">
-      <artifact name="hadoop-common" type="tests" ext="jar" m:classifier="tests"/>
-    </dependency>
-    <dependency org="log4j"
-      name="log4j"
-      rev="${log4j.version}"
-      conf="common->master">
-      <exclude org="com.sun.jdmk"/>
-      <exclude org="com.sun.jmx"/>
-      <exclude org="javax.jms"/> 
-    </dependency>
-    
-    <dependency org="commons-logging"
-      name="commons-logging"
-      rev="${commons-logging.version}"
-      conf="common->master"/>
-    <dependency org="junit"
-      name="junit"
-      rev="${junit.version}"
-      conf="common->master"/>
-  </dependencies>
-</ivy-module>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy/libraries.properties b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy/libraries.properties
deleted file mode 100644
index a470b37..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/ivy/libraries.properties
+++ /dev/null
@@ -1,5 +0,0 @@
-#This properties file lists the versions of the various artifacts used by streaming.
-#It drives ivy and the generation of a maven POM
-
-#Please list the dependencies name with version if they are different from the ones 
-#listed in the global libraries.properties file (in alphabetical order)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/CMakeLists.txt b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/CMakeLists.txt
deleted file mode 100644
index fb3c580..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/CMakeLists.txt
+++ /dev/null
@@ -1,73 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-# Find Linux FUSE
-IF (${CMAKE_SYSTEM_NAME} MATCHES "Linux")
-    find_package(PkgConfig REQUIRED)
-    pkg_check_modules(FUSE fuse)
-    IF(FUSE_FOUND)
-        FLATTEN_LIST("${FUSE_CFLAGS}" " " FUSE_CFLAGS)
-        FLATTEN_LIST("${FUSE_LDFLAGS}" " " FUSE_LDFLAGS)
-        set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${FUSE_CFLAGS}")
-        set(CMAKE_LD_FLAGS "${CMAKE_LD_FLAGS} ${FUSE_LDFLAGS}")
-        MESSAGE(STATUS "Building Linux FUSE client.")
-        include_directories(${FUSE_INCLUDE_DIRS})
-    ELSE(FUSE_FOUND)
-        MESSAGE(STATUS "Failed to find Linux FUSE libraries or include files.  Will not build FUSE client.")
-    ENDIF(FUSE_FOUND)
-ELSE (${CMAKE_SYSTEM_NAME} MATCHES "Linux")
-    MESSAGE(STATUS "Non-Linux system detected.  Will not build FUSE client.")
-ENDIF (${CMAKE_SYSTEM_NAME} MATCHES "Linux")
-
-IF(FUSE_FOUND)
-    add_executable(fuse_dfs
-        fuse_dfs.c
-        fuse_options.c 
-        fuse_connect.c 
-        fuse_impls_access.c 
-        fuse_impls_chmod.c  
-        fuse_impls_chown.c  
-        fuse_impls_create.c  
-        fuse_impls_flush.c 
-        fuse_impls_getattr.c  
-        fuse_impls_mkdir.c  
-        fuse_impls_mknod.c  
-        fuse_impls_open.c 
-        fuse_impls_read.c 
-        fuse_impls_readdir.c 
-        fuse_impls_release.c 
-        fuse_impls_rename.c 
-        fuse_impls_rmdir.c 
-        fuse_impls_statfs.c 
-        fuse_impls_symlink.c 
-        fuse_impls_truncate.c 
-        fuse_impls_unlink.c 
-        fuse_impls_utimens.c  
-        fuse_impls_write.c
-        fuse_init.c 
-        fuse_stat_struct.c 
-        fuse_trash.c 
-        fuse_users.c 
-    )
-    target_link_libraries(fuse_dfs
-        ${FUSE_LIBRARIES}
-        ${JAVA_JVM_LIBRARY}
-        hdfs
-        m
-    )
-ENDIF(FUSE_FOUND)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.c
deleted file mode 100644
index bfb7a1e..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.c
+++ /dev/null
@@ -1,239 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hdfs.h"
-#include "fuse_dfs.h"
-#include "fuse_connect.h"
-#include "fuse_users.h" 
-
-#include <limits.h>
-#include <search.h>
-#include <stdio.h>
-#include <stdlib.h>
-
-#define HADOOP_SECURITY_AUTHENTICATION "hadoop.security.authentication"
-
-enum authConf {
-    AUTH_CONF_UNKNOWN,
-    AUTH_CONF_KERBEROS,
-    AUTH_CONF_OTHER,
-};
-
-#define MAX_ELEMENTS (16 * 1024)
-static struct hsearch_data *fsTable = NULL;
-static enum authConf hdfsAuthConf = AUTH_CONF_UNKNOWN;
-static pthread_mutex_t tableMutex = PTHREAD_MUTEX_INITIALIZER;
-
-/*
- * Allocate a hash table for fs handles. Returns 0 on success,
- * -1 on failure.
- */
-int allocFsTable(void) {
-  assert(NULL == fsTable);
-  fsTable = calloc(1, sizeof(struct hsearch_data));
-  if (0 == hcreate_r(MAX_ELEMENTS, fsTable)) {
-    ERROR("Unable to initialize connection table");
-    return -1;
-  }
-  return 0;
-}
-
-/*
- * Find a fs handle for the given key. Returns a fs handle, 
- * or NULL if there is no fs for the given key.
- */
-static hdfsFS findFs(char *key) {
-  ENTRY entry;
-  ENTRY *entryP = NULL;
-  entry.key = key;
-  if (0 == hsearch_r(entry, FIND, &entryP, fsTable)) {
-    return NULL;
-  }
-  assert(NULL != entryP->data);
-  return (hdfsFS)entryP->data;
-}
-
-/*
- * Insert the given fs handle into the table.
- * Returns 0 on success, -1 on failure.
- */
-static int insertFs(char *key, hdfsFS fs) {
-  ENTRY entry;
-  ENTRY *entryP = NULL;
-  assert(NULL != fs);
-  entry.key = strdup(key);
-  if (entry.key == NULL) {
-    return -1;
-  }
-  entry.data = (void*)fs;
-  if (0 == hsearch_r(entry, ENTER, &entryP, fsTable)) {
-    return -1;
-  }
-  return 0;
-}
-
-/** 
- * Find out what type of authentication the system administrator
- * has configured.
- *
- * @return     the type of authentication, or AUTH_CONF_UNKNOWN on error.
- */
-static enum authConf discoverAuthConf(void)
-{
-    int ret;
-    char *val = NULL;
-    enum authConf authConf;
-
-    ret = hdfsConfGet(HADOOP_SECURITY_AUTHENTICATION, &val);
-    if (ret)
-        authConf = AUTH_CONF_UNKNOWN;
-    else if (!strcmp(val, "kerberos"))
-        authConf = AUTH_CONF_KERBEROS;
-    else
-        authConf = AUTH_CONF_OTHER;
-    free(val);
-    return authConf;
-}
-
-/**
- * Find the Kerberos ticket cache path.
- *
- * This function finds the Kerberos ticket cache path from the thread ID and
- * user ID of the process making the request.
- *
- * Normally, the ticket cache path is in a well-known location in /tmp.
- * However, it's possible that the calling process could set the KRB5CCNAME
- * environment variable, indicating that its Kerberos ticket cache is at a
- * non-default location.  We try to handle this possibility by reading the
- * process' environment here.  This will be allowed if we have root
- * capabilities, or if our UID is the same as the remote process' UID.
- *
- * Note that we don't check to see if the cache file actually exists or not.
- * We're just trying to find out where it would be if it did exist. 
- *
- * @param path          (out param) the path to the ticket cache file
- * @param pathLen       length of the path buffer
- */
-static void findKerbTicketCachePath(char *path, size_t pathLen)
-{
-  struct fuse_context *ctx = fuse_get_context();
-  FILE *fp = NULL;
-  static const char * const KRB5CCNAME = "\0KRB5CCNAME=";
-  int c = '\0', pathIdx = 0, keyIdx = 0;
-  size_t KRB5CCNAME_LEN = strlen(KRB5CCNAME + 1) + 1;
-
-  // /proc/<tid>/environ contains the remote process' environment.  It is
-  // exposed to us as a series of KEY=VALUE pairs, separated by NULL bytes.
-  snprintf(path, pathLen, "/proc/%d/environ", ctx->pid);
-  fp = fopen(path, "r");
-  if (!fp)
-    goto done;
-  while (1) {
-    if (c == EOF)
-      goto done;
-    if (keyIdx == KRB5CCNAME_LEN) {
-      if (pathIdx >= pathLen - 1)
-        goto done;
-      if (c == '\0')
-        goto done;
-      path[pathIdx++] = c;
-    } else if (KRB5CCNAME[keyIdx++] != c) {
-      keyIdx = 0;
-    }
-    c = fgetc(fp);
-  }
-
-done:
-  if (fp)
-    fclose(fp);
-  if (pathIdx == 0) {
-    snprintf(path, pathLen, "/tmp/krb5cc_%d", ctx->uid);
-  } else {
-    path[pathIdx] = '\0';
-  }
-}
-
-/*
- * Connect to the NN as the current user/group.
- * Returns a fs handle on success, or NULL on failure.
- */
-hdfsFS doConnectAsUser(const char *nn_uri, int nn_port) {
-  struct hdfsBuilder *bld;
-  uid_t uid = fuse_get_context()->uid;
-  char *user = getUsername(uid);
-  char kpath[PATH_MAX];
-  int ret;
-  hdfsFS fs = NULL;
-  if (NULL == user) {
-    goto done;
-  }
-
-  ret = pthread_mutex_lock(&tableMutex);
-  assert(0 == ret);
-
-  fs = findFs(user);
-  if (NULL == fs) {
-    if (hdfsAuthConf == AUTH_CONF_UNKNOWN) {
-      hdfsAuthConf = discoverAuthConf();
-      if (hdfsAuthConf == AUTH_CONF_UNKNOWN) {
-        ERROR("Unable to determine the configured value for %s.",
-              HADOOP_SECURITY_AUTHENTICATION);
-        goto done;
-      }
-    }
-    bld = hdfsNewBuilder();
-    if (!bld) {
-      ERROR("Unable to create hdfs builder");
-      goto done;
-    }
-    hdfsBuilderSetForceNewInstance(bld);
-    hdfsBuilderSetNameNode(bld, nn_uri);
-    if (nn_port) {
-        hdfsBuilderSetNameNodePort(bld, nn_port);
-    }
-    hdfsBuilderSetUserName(bld, user);
-    if (hdfsAuthConf == AUTH_CONF_KERBEROS) {
-      findKerbTicketCachePath(kpath, sizeof(kpath));
-      hdfsBuilderSetKerbTicketCachePath(bld, kpath);
-    }
-    fs = hdfsBuilderConnect(bld);
-    if (NULL == fs) {
-      int err = errno;
-      ERROR("Unable to create fs for user %s: error code %d", user, err);
-      goto done;
-    }
-    if (-1 == insertFs(user, fs)) {
-      ERROR("Unable to cache fs for user %s", user);
-    }
-  }
-
-done:
-  ret = pthread_mutex_unlock(&tableMutex);
-  assert(0 == ret);
-  free(user);
-  return fs;
-}
-
-/*
- * We currently cache a fs handle per-user in this module rather
- * than use the FileSystem cache in the java client. Therefore
- * we do not disconnect the fs handle here.
- */
-int doDisconnect(hdfsFS fs) {
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.h
deleted file mode 100644
index 4bddeea..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_connect.h
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_CONNECT_H__
-#define __FUSE_CONNECT_H__
-
-#include "fuse_dfs.h"
-
-hdfsFS doConnectAsUser(const char *nn_uri, int nn_port);
-int doDisconnect(hdfsFS fs);
-int allocFsTable(void);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_context_handle.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_context_handle.h
deleted file mode 100644
index ae07735..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_context_handle.h
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_CONTEXT_HANDLE_H__
-#define __FUSE_CONTEXT_HANDLE_H__
-
-#include <hdfs.h>
-#include <stddef.h>
-#include <sys/types.h>
-
-//
-// Structure to store fuse_dfs specific data
-// this will be created and passed to fuse at startup
-// and fuse will pass it back to us via the context function
-// on every operation.
-//
-typedef struct dfs_context_struct {
-  int debug;
-  char *nn_uri;
-  int nn_port;
-  int read_only;
-  int usetrash;
-  int direct_io;
-  char **protectedpaths;
-  size_t rdbuffer_size;
-} dfs_context;
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.c
deleted file mode 100644
index e218c81..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.c
+++ /dev/null
@@ -1,129 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_options.h"
-#include "fuse_impls.h"
-#include "fuse_init.h"
-#include "fuse_connect.h"
-
-#include <string.h>
-#include <stdlib.h>
-
-int is_protected(const char *path) {
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-  assert(dfs != NULL);
-  assert(dfs->protectedpaths);
-
-  int i ;
-  for (i = 0; dfs->protectedpaths[i]; i++) {
-    if (strcmp(path, dfs->protectedpaths[i]) == 0) {
-      return 1;
-    }
-  }
-  return 0;
-}
-
-static struct fuse_operations dfs_oper = {
-  .getattr  = dfs_getattr,
-  .access   = dfs_access,
-  .readdir  = dfs_readdir,
-  .destroy  = dfs_destroy,
-  .init     = dfs_init,
-  .open     = dfs_open,
-  .read     = dfs_read,
-  .symlink  = dfs_symlink,
-  .statfs   = dfs_statfs,
-  .mkdir    = dfs_mkdir,
-  .rmdir    = dfs_rmdir,
-  .rename   = dfs_rename,
-  .unlink   = dfs_unlink,
-  .release  = dfs_release,
-  .create   = dfs_create,
-  .write    = dfs_write,
-  .flush    = dfs_flush,
-  .mknod    = dfs_mknod,
-  .utimens  = dfs_utimens,
-  .chmod    = dfs_chmod,
-  .chown    = dfs_chown,
-  .truncate = dfs_truncate,
-};
-
-int main(int argc, char *argv[])
-{
-  umask(0);
-
-  extern const char *program;  
-  program = argv[0];
-  struct fuse_args args = FUSE_ARGS_INIT(argc, argv);
-
-  memset(&options, 0, sizeof(struct options));
-
-  options.rdbuffer_size = 10*1024*1024; 
-  options.attribute_timeout = 60; 
-  options.entry_timeout = 60;
-
-  if (-1 == fuse_opt_parse(&args, &options, dfs_opts, dfs_options)) {
-    return -1;
-  }
-
-  if (!options.private) {
-    fuse_opt_add_arg(&args, "-oallow_other");
-  }
-
-  if (!options.no_permissions) {
-    fuse_opt_add_arg(&args, "-odefault_permissions");
-  }
-
-  {
-    char buf[1024];
-
-    snprintf(buf, sizeof buf, "-oattr_timeout=%d",options.attribute_timeout);
-    fuse_opt_add_arg(&args, buf);
-
-    snprintf(buf, sizeof buf, "-oentry_timeout=%d",options.entry_timeout);
-    fuse_opt_add_arg(&args, buf);
-  }
-
-  if (options.nn_uri == NULL) {
-    print_usage(argv[0]);
-    exit(0);
-  }
-
-  // Check connection as root
-  if (options.initchecks == 1) {
-    hdfsFS tempFS = hdfsConnectAsUser(options.nn_uri, options.nn_port, "root");
-    if (NULL == tempFS) {
-      const char *cp = getenv("CLASSPATH");
-      const char *ld = getenv("LD_LIBRARY_PATH");
-      ERROR("FATAL: misconfiguration - cannot connect to HDFS");
-      ERROR("LD_LIBRARY_PATH=%s",ld == NULL ? "NULL" : ld);
-      ERROR("CLASSPATH=%s",cp == NULL ? "NULL" : cp);
-      exit(1);
-    }
-    if (doDisconnect(tempFS)) {
-      ERROR("FATAL: unable to disconnect from test filesystem.");
-      exit(1);
-    }
-  }
-
-  int ret = fuse_main(args.argc, args.argv, &dfs_oper, NULL);
-  fuse_opt_free_args(&args);
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.h
deleted file mode 100644
index 4554dbd..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs.h
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_DFS_H__
-#define __FUSE_DFS_H__
-
-#define FUSE_USE_VERSION 26
-
-#include <stdio.h>
-#include <string.h>
-#include <errno.h>
-#include <assert.h>
-#include <strings.h>
-#include <syslog.h>
-
-#include <fuse.h>
-#include <fuse/fuse_opt.h>
-
-#include <sys/xattr.h>
-
-#include "config.h"
-
-//
-// Check if a path is in the mount option supplied protected paths.
-//
-int is_protected(const char *path);
-
-#undef INFO
-#define INFO(_fmt, ...) {                       \
-  fprintf(stdout, "INFO %s:%d " _fmt "\n",      \
-          __FILE__, __LINE__, ## __VA_ARGS__);  \
-  syslog(LOG_INFO, "INFO %s:%d " _fmt "\n",     \
-          __FILE__, __LINE__, ## __VA_ARGS__);  \
-}
-
-#undef DEBUG
-#define DEBUG(_fmt, ...) {                      \
-  fprintf(stdout, "DEBUG %s:%d " _fmt "\n",     \
-          __FILE__, __LINE__, ## __VA_ARGS__);  \
-  syslog(LOG_DEBUG, "DEBUG %s:%d " _fmt "\n",   \
-          __FILE__, __LINE__, ## __VA_ARGS__);  \
-}
-
-#undef ERROR
-#define ERROR(_fmt, ...) {                      \
-  fprintf(stderr, "ERROR %s:%d " _fmt "\n",     \
-          __FILE__, __LINE__, ## __VA_ARGS__);  \
-  syslog(LOG_ERR, "ERROR %s:%d " _fmt "\n",     \
-          __FILE__, __LINE__, ## __VA_ARGS__);  \
-}
-
-//#define DOTRACE
-#ifdef DOTRACE
-#define TRACE(x) {        \
-    DEBUG("TRACE %s", x); \
-}
-
-#define TRACE1(x,y) {             \
-    DEBUG("TRACE %s %s\n", x, y); \
-}
-#else
-#define TRACE(x) ; 
-#define TRACE1(x,y) ; 
-#endif
-
-#endif // __FUSE_DFS_H__
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs_wrapper.sh b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs_wrapper.sh
deleted file mode 100755
index 97239cc..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_dfs_wrapper.sh
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/usr/bin/env bash
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-export HADOOP_PREFIX=${HADOOP_PREFIX:-/usr/local/share/hadoop}
-
-if [ "$OS_ARCH" = "" ]; then
-export OS_ARCH=amd64
-fi
-
-if [ "$JAVA_HOME" = "" ]; then
-export  JAVA_HOME=/usr/local/java
-fi
-
-if [ "$LD_LIBRARY_PATH" = "" ]; then
-export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/lib
-fi
-
-# If dev build set paths accordingly
-if [ -d $HADOOP_PREFIX/build ]; then
-  export HADOOP_PREFIX=$HADOOP_PREFIX
-  for f in ${HADOOP_PREFIX}/build/*.jar ; do
-    export CLASSPATH=$CLASSPATH:$f
-  done
-  for f in $HADOOP_PREFIX/build/ivy/lib/hadoop-hdfs/common/*.jar ; do
-    export CLASSPATH=$CLASSPATH:$f
-  done
-  export PATH=$HADOOP_PREFIX/build/contrib/fuse-dfs:$PATH
-  export LD_LIBRARY_PATH=$HADOOP_PREFIX/build/c++/lib:$JAVA_HOME/jre/lib/$OS_ARCH/server
-fi
-
-fuse_dfs $@
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_file_handle.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_file_handle.h
deleted file mode 100644
index 70cd898..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_file_handle.h
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_FILE_HANDLE_H__
-#define __FUSE_FILE_HANDLE_H__
-
-#include <hdfs.h>
-#include <pthread.h>
-
-/**
- *
- * dfs_fh_struct is passed around for open files. Fuse provides a hook (the context) 
- * for storing file specific data.
- *
- * 2 Types of information:
- * a) a read buffer for performance reasons since fuse is typically called on 4K chunks only
- * b) the hdfs fs handle 
- *
- */
-typedef struct dfs_fh_struct {
-  hdfsFile hdfsFH;
-  char *buf;
-  tSize bufferSize;  //what is the size of the buffer we have
-  off_t buffersStartOffset; //where the buffer starts in the file
-  hdfsFS fs; // for reads/writes need to access as the real user
-  pthread_mutex_t mutex;
-} dfs_fh;
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls.h
deleted file mode 100644
index d0d93e2..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls.h
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#ifndef __FUSE_IMPLS_H__
-#define __FUSE_IMPLS_H__
-
-#include <fuse.h>
-#include <syslog.h>
-
-#include "fuse_context_handle.h"
-
-/**
- * Implementations of the various fuse hooks.
- * All of these (should be) thread safe.
- *
- */
-
-int dfs_mkdir(const char *path, mode_t mode);
-int dfs_rename(const char *from, const char *to);
-int dfs_getattr(const char *path, struct stat *st);
-int dfs_readdir(const char *path, void *buf, fuse_fill_dir_t filler,
-                off_t offset, struct fuse_file_info *fi);
-int dfs_read(const char *path, char *buf, size_t size, off_t offset,
-                    struct fuse_file_info *fi);
-int dfs_statfs(const char *path, struct statvfs *st);
-int dfs_mkdir(const char *path, mode_t mode);
-int dfs_rename(const char *from, const char *to);
-int dfs_rmdir(const char *path);
-int dfs_unlink(const char *path);
-int dfs_utimens(const char *path, const struct timespec ts[2]);
-int dfs_chmod(const char *path, mode_t mode);
-int dfs_chown(const char *path, uid_t uid, gid_t gid);
-int dfs_open(const char *path, struct fuse_file_info *fi);
-int dfs_write(const char *path, const char *buf, size_t size,
-              off_t offset, struct fuse_file_info *fi);
-int dfs_release (const char *path, struct fuse_file_info *fi);
-int dfs_mknod(const char *path, mode_t mode, dev_t rdev) ;
-int dfs_create(const char *path, mode_t mode, struct fuse_file_info *fi);
-int dfs_flush(const char *path, struct fuse_file_info *fi);
-int dfs_access(const char *path, int mask);
-int dfs_truncate(const char *path, off_t size);
-int dfs_symlink(const char *from, const char *to);
-
-#endif
-
-
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_access.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_access.c
deleted file mode 100644
index 033a1c3..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_access.c
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-
-int dfs_access(const char *path, int mask)
-{
-  TRACE1("access", path)
-  assert(path != NULL);
-  // TODO: HDFS-428
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chmod.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chmod.c
deleted file mode 100644
index 2c1e96b..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chmod.c
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_users.h"
-#include "fuse_connect.h"
-
-int dfs_chmod(const char *path, mode_t mode)
-{
-  TRACE1("chmod", path)
-  int ret = 0;
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect to HDFS");
-    ret = -EIO;
-    goto cleanup;
-  }
-
-  if (hdfsChmod(userFS, path, (short)mode)) {
-    ERROR("Could not chmod %s to %d", path, (int)mode);
-    ret = (errno > 0) ? -errno : -EIO;
-    goto cleanup;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chown.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chown.c
deleted file mode 100644
index 9c6105d..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_chown.c
+++ /dev/null
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_users.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-
-#include <stdlib.h>
-
-int dfs_chown(const char *path, uid_t uid, gid_t gid)
-{
-  TRACE1("chown", path)
-
-  int ret = 0;
-  char *user = NULL;
-  char *group = NULL;
-  hdfsFS userFS = NULL;
-
-  // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  user = getUsername(uid);
-  if (NULL == user) {
-    ERROR("Could not lookup the user id string %d",(int)uid); 
-    ret = -EIO;
-    goto cleanup;
-  }
-
-  group = getGroup(gid);
-  if (group == NULL) {
-    ERROR("Could not lookup the group id string %d",(int)gid);
-    ret = -EIO;
-    goto cleanup;
-  } 
-
-  userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect to HDFS");
-    ret = -EIO;
-    goto cleanup;
-  }
-
-  if (hdfsChown(userFS, path, user, group)) {
-    ERROR("Could not chown %s to %d:%d", path, (int)uid, gid);
-    ret = (errno > 0) ? -errno : -EIO;
-    goto cleanup;
-  }
-
-cleanup:
-  if (userFS && doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  if (user) {
-    free(user);
-  }
-  if (group) {
-    free(group);
-  }
-
-  return ret;
-
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_create.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_create.c
deleted file mode 100644
index 256e383..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_create.c
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-
-int dfs_create(const char *path, mode_t mode, struct fuse_file_info *fi)
-{
-  TRACE1("create", path)
-  fi->flags |= mode;
-  return dfs_open(path, fi);
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_flush.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_flush.c
deleted file mode 100644
index 6d4f05c..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_flush.c
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_file_handle.h"
-
-int dfs_flush(const char *path, struct fuse_file_info *fi) {
-  TRACE1("flush", path)
-
-  // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-  assert(fi);
-
-  if (NULL == (void*)fi->fh) {
-    return  0;
-  }
-
-  // note that fuse calls flush on RO files too and hdfs does not like that and will return an error
-  if (fi->flags & O_WRONLY) {
-
-    dfs_fh *fh = (dfs_fh*)fi->fh;
-    assert(fh);
-    hdfsFile file_handle = (hdfsFile)fh->hdfsFH;
-    assert(file_handle);
-
-    assert(fh->fs);
-    if (hdfsFlush(fh->fs, file_handle) != 0) {
-      ERROR("Could not flush %lx for %s\n",(long)file_handle, path);
-      return -EIO;
-    }
-  }
-
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_getattr.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_getattr.c
deleted file mode 100644
index 56f634e..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_getattr.c
+++ /dev/null
@@ -1,71 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_stat_struct.h"
-#include "fuse_connect.h"
-
-int dfs_getattr(const char *path, struct stat *st)
-{
-  TRACE1("getattr", path)
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(dfs);
-  assert(path);
-  assert(st);
-
-  hdfsFS fs = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (NULL == fs) {
-    ERROR("Could not connect to %s:%d", dfs->nn_uri, dfs->nn_port);
-    return -EIO;
-  }
-
-  int ret = 0;
-  hdfsFileInfo *info = hdfsGetPathInfo(fs,path);
-  if (NULL == info) {
-    ret = -ENOENT;
-    goto cleanup;
-  }
-  fill_stat_structure(&info[0], st);
-
-  // setup hard link info - for a file it is 1 else num entries in a dir + 2 (for . and ..)
-  if (info[0].mKind == kObjectKindDirectory) {
-    int numEntries = 0;
-    hdfsFileInfo *info = hdfsListDirectory(fs,path,&numEntries);
-
-    if (info) {
-      hdfsFreeFileInfo(info,numEntries);
-    }
-    st->st_nlink = numEntries + 2;
-  } else {
-    // not a directory
-    st->st_nlink = 1;
-  }
-
-  // free the info pointer
-  hdfsFreeFileInfo(info,1);
-
-cleanup:
-  if (doDisconnect(fs)) {
-    ERROR("Could not disconnect from filesystem");
-    ret = -EIO;
-  }
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mkdir.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mkdir.c
deleted file mode 100644
index d0624af..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mkdir.c
+++ /dev/null
@@ -1,69 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_trash.h"
-#include "fuse_connect.h"
-
-int dfs_mkdir(const char *path, mode_t mode)
-{
-  TRACE1("mkdir", path)
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  if (is_protected(path)) {
-    ERROR("HDFS trying to create directory %s", path);
-    return -EACCES;
-  }
-
-  if (dfs->read_only) {
-    ERROR("HDFS is configured read-only, cannot create directory %s", path);
-    return -EACCES;
-  }
-  
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  // In theory the create and chmod should be atomic.
-
-  int ret = 0;
-  if (hdfsCreateDirectory(userFS, path)) {
-    ERROR("HDFS could not create directory %s", path);
-    ret = (errno > 0) ? -errno : -EIO;
-    goto cleanup;
-  }
-
-  if (hdfsChmod(userFS, path, (short)mode)) {
-    ERROR("Could not chmod %s to %d", path, (int)mode);
-    ret = (errno > 0) ? -errno : -EIO;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mknod.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mknod.c
deleted file mode 100644
index c745cf1..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_mknod.c
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-
-int dfs_mknod(const char *path, mode_t mode, dev_t rdev)
-{
-  TRACE1("mknod", path);
-  DEBUG("dfs_mknod");
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_open.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_open.c
deleted file mode 100644
index 071590a..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_open.c
+++ /dev/null
@@ -1,94 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-#include "fuse_file_handle.h"
-
-int dfs_open(const char *path, struct fuse_file_info *fi)
-{
-  TRACE1("open", path)
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(path);
-  assert('/' == *path);
-  assert(dfs);
-
-  int ret = 0;
-
-  // 0x8000 is always passed in and hadoop doesn't like it, so killing it here
-  // bugbug figure out what this flag is and report problem to Hadoop JIRA
-  int flags = (fi->flags & 0x7FFF);
-
-  // retrieve dfs specific data
-  dfs_fh *fh = (dfs_fh*)calloc(1, sizeof (dfs_fh));
-  if (fh == NULL) {
-    ERROR("Malloc of new file handle failed");
-    return -EIO;
-  }
-
-  fh->fs = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (fh->fs == NULL) {
-    ERROR("Could not connect to dfs");
-    return -EIO;
-  }
-
-  if (flags & O_RDWR) {
-    hdfsFileInfo *info = hdfsGetPathInfo(fh->fs,path);
-    if (info == NULL) {
-      // File does not exist (maybe?); interpret it as a O_WRONLY
-      // If the actual error was something else, we'll get it again when
-      // we try to open the file.
-      flags ^= O_RDWR;
-      flags |= O_WRONLY;
-    } else {
-      // File exists; open this as read only.
-      flags ^= O_RDWR;
-      flags |= O_RDONLY;
-    }
-  }
-
-  if ((fh->hdfsFH = hdfsOpenFile(fh->fs, path, flags,  0, 0, 0)) == NULL) {
-    ERROR("Could not open file %s (errno=%d)", path, errno);
-    if (errno == 0 || errno == EINTERNAL) {
-      return -EIO;
-    }
-    return -errno;
-  }
-
-  pthread_mutex_init(&fh->mutex, NULL);
-
-  if (fi->flags & O_WRONLY || fi->flags & O_CREAT) {
-    fh->buf = NULL;
-  } else  {
-    assert(dfs->rdbuffer_size > 0);
-    fh->buf = (char*)malloc(dfs->rdbuffer_size * sizeof(char));
-    if (NULL == fh->buf) {
-      ERROR("Could not allocate memory for a read for file %s\n", path);
-      ret = -EIO;
-    }
-    fh->buffersStartOffset = 0;
-    fh->bufferSize = 0;
-  }
-  fi->fh = (uint64_t)fh;
-
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_read.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_read.c
deleted file mode 100644
index 5209261..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_read.c
+++ /dev/null
@@ -1,162 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_file_handle.h"
-
-static size_t min(const size_t x, const size_t y) {
-  return x < y ? x : y;
-}
-
-/**
- * dfs_read
- *
- * Reads from dfs or the open file's buffer.  Note that fuse requires that
- * either the entire read be satisfied or the EOF is hit or direct_io is enabled
- *
- */
-int dfs_read(const char *path, char *buf, size_t size, off_t offset,
-                   struct fuse_file_info *fi)
-{
-  TRACE1("read",path)
-  
-  // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(dfs);
-  assert(path);
-  assert(buf);
-  assert(offset >= 0);
-  assert(size >= 0);
-  assert(fi);
-
-  dfs_fh *fh = (dfs_fh*)fi->fh;
-
-  assert(fh != NULL);
-  assert(fh->fs != NULL);
-  assert(fh->hdfsFH != NULL);
-
-  // special case this as simplifies the rest of the logic to know the caller wanted > 0 bytes
-  if (size == 0)
-    return 0;
-
-  // If size is bigger than the read buffer, then just read right into the user supplied buffer
-  if ( size >= dfs->rdbuffer_size) {
-    int num_read;
-    size_t total_read = 0;
-    while (size - total_read > 0 && (num_read = hdfsPread(fh->fs, fh->hdfsFH, offset + total_read, buf + total_read, size - total_read)) > 0) {
-      total_read += num_read;
-    }
-    // if there was an error before satisfying the current read, this logic declares it an error
-    // and does not try to return any of the bytes read. Don't think it matters, so the code
-    // is just being conservative.
-    if (total_read < size && num_read < 0) {
-      total_read = -EIO;
-    }
-    return total_read;
-  }
-
-  //
-  // Critical section - protect from multiple reads in different threads accessing the read buffer
-  // (no returns until end)
-  //
-
-  pthread_mutex_lock(&fh->mutex);
-
-  // used only to check the postcondition of this function - namely that we satisfy
-  // the entire read or EOF is hit.
-  int isEOF = 0;
-  int ret = 0;
-
-  // check if the buffer is empty or
-  // the read starts before the buffer starts or
-  // the read ends after the buffer ends
-
-  if (fh->bufferSize == 0  || 
-      offset < fh->buffersStartOffset || 
-      offset + size > fh->buffersStartOffset + fh->bufferSize) 
-    {
-      // Read into the buffer from DFS
-      int num_read = 0;
-      size_t total_read = 0;
-
-      while (dfs->rdbuffer_size  - total_read > 0 &&
-             (num_read = hdfsPread(fh->fs, fh->hdfsFH, offset + total_read, fh->buf + total_read, dfs->rdbuffer_size - total_read)) > 0) {
-        total_read += num_read;
-      }
-
-      // if there was an error before satisfying the current read, this logic declares it an error
-      // and does not try to return any of the bytes read. Don't think it matters, so the code
-      // is just being conservative.
-      if (total_read < size && num_read < 0) {
-        // invalidate the buffer 
-        fh->bufferSize = 0; 
-        ERROR("pread failed for %s with return code %d", path, (int)num_read);
-        ret = -EIO;
-      } else {
-        // Either EOF, all read or read beyond size, but then there was an error
-        fh->bufferSize = total_read;
-        fh->buffersStartOffset = offset;
-
-        if (dfs->rdbuffer_size - total_read > 0) {
-          // assert(num_read == 0); this should be true since if num_read < 0 handled above.
-          isEOF = 1;
-        }
-      }
-    }
-
-  //
-  // NOTE on EOF, fh->bufferSize == 0 and ret = 0 ,so the logic for copying data into the caller's buffer is bypassed, and
-  //  the code returns 0 as required
-  //
-  if (ret == 0 && fh->bufferSize > 0) {
-
-    assert(offset >= fh->buffersStartOffset);
-    assert(fh->buf);
-
-    const size_t bufferReadIndex = offset - fh->buffersStartOffset;
-    assert(bufferReadIndex >= 0 && bufferReadIndex < fh->bufferSize);
-
-    const size_t amount = min(fh->buffersStartOffset + fh->bufferSize - offset, size);
-    assert(amount >= 0 && amount <= fh->bufferSize);
-
-    const char *offsetPtr = fh->buf + bufferReadIndex;
-    assert(offsetPtr >= fh->buf);
-    assert(offsetPtr + amount <= fh->buf + fh->bufferSize);
-    
-    memcpy(buf, offsetPtr, amount);
-
-    ret = amount;
-  }
-
-  //
-  // Critical section end 
-  //
-  pthread_mutex_unlock(&fh->mutex);
- 
-  // fuse requires the below and the code should guarantee this assertion
-  // 3 cases on return:
-  //   1. entire read satisfied
-  //   2. partial read and isEOF - including 0 size read
-  //   3. error 
-  assert(ret == size || isEOF || ret < 0);
-
- return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_readdir.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_readdir.c
deleted file mode 100644
index f6fe48b..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_readdir.c
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_stat_struct.h"
-#include "fuse_connect.h"
-
-int dfs_readdir(const char *path, void *buf, fuse_fill_dir_t filler,
-                       off_t offset, struct fuse_file_info *fi)
-{
-  TRACE1("readdir", path)
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(dfs);
-  assert(path);
-  assert(buf);
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  // Read dirents. Calling a variant that just returns the final path
-  // component (HDFS-975) would save us from parsing it out below.
-  int numEntries = 0;
-  hdfsFileInfo *info = hdfsListDirectory(userFS, path, &numEntries);
-
-  int ret = 0;
-  // NULL means either the directory doesn't exist or maybe IO error.
-  if (NULL == info) {
-    ret = (errno > 0) ? -errno : -ENOENT;
-    goto cleanup;
-  }
-
-  int i ;
-  for (i = 0; i < numEntries; i++) {
-    if (NULL == info[i].mName) {
-      ERROR("Path %s info[%d].mName is NULL", path, i);
-      continue;
-    }
-
-    struct stat st;
-    fill_stat_structure(&info[i], &st);
-
-    // Find the final path component
-    const char *str = strrchr(info[i].mName, '/');
-    if (NULL == str) {
-      ERROR("Invalid URI %s", info[i].mName);
-      continue;
-    }
-    str++;
-
-    // pack this entry into the fuse buffer
-    int res = 0;
-    if ((res = filler(buf,str,&st,0)) != 0) {
-      ERROR("Readdir filler failed: %d\n",res);
-    }
-  }
-
-  // insert '.' and '..'
-  const char *const dots [] = { ".",".."};
-  for (i = 0 ; i < 2 ; i++)
-    {
-      struct stat st;
-      memset(&st, 0, sizeof(struct stat));
-
-      // set to 0 to indicate not supported for directory because we cannot (efficiently) get this info for every subdirectory
-      st.st_nlink =  0;
-
-      // setup stat size and acl meta data
-      st.st_size    = 512;
-      st.st_blksize = 512;
-      st.st_blocks  =  1;
-      st.st_mode    = (S_IFDIR | 0777);
-      st.st_uid     = default_id;
-      st.st_gid     = default_id;
-      // todo fix below times
-      st.st_atime   = 0;
-      st.st_mtime   = 0;
-      st.st_ctime   = 0;
-
-      const char *const str = dots[i];
-
-      // flatten the info using fuse's function into a buffer
-      int res = 0;
-      if ((res = filler(buf,str,&st,0)) != 0) {
-	ERROR("Readdir filler failed: %d\n",res);
-      }
-    }
-  // free the info pointers
-  hdfsFreeFileInfo(info,numEntries);
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-    ERROR("Failed to disconnect %d", errno);
-  }
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_release.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_release.c
deleted file mode 100644
index e15dd57..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_release.c
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_file_handle.h"
-#include "fuse_connect.h"
-
-#include <stdlib.h>
-
-/**
- * release a fuse_file_info structure.
- *
- * When this function is invoked, there are no more references to our
- * fuse_file_info structure that exist anywhere.  So there is no need for
- * locking to protect this structure here.
- *
- * Another thread could open() the same file, and get a separate, different file
- * descriptor with a different, separate fuse_file_info structure.  In HDFS,
- * this results in one writer winning and overwriting everything the other
- * writer has done.
- */
-
-int dfs_release (const char *path, struct fuse_file_info *fi) {
-  TRACE1("release", path)
-
-  // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  int ret = 0;
-  dfs_fh *fh = (dfs_fh*)fi->fh;
-  assert(fh);
-  hdfsFile file_handle = (hdfsFile)fh->hdfsFH;
-  if (NULL != file_handle) {
-    if (hdfsCloseFile(fh->fs, file_handle) != 0) {
-      ERROR("Could not close handle %ld for %s\n",(long)file_handle, path);
-      ret = -EIO;
-    }
-  }
-  free(fh->buf);
-  if (doDisconnect(fh->fs)) {
-    ret = -EIO;
-  }
-  pthread_mutex_destroy(&fh->mutex);
-  free(fh);
-  fi->fh = 0;
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rename.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rename.c
deleted file mode 100644
index bbb0462..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rename.c
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_trash.h"
-#include "fuse_connect.h"
-
-int dfs_rename(const char *from, const char *to)
-{
-  TRACE1("rename", from) 
-
- // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(from);
-  assert(to);
-  assert(dfs);
-
-  assert('/' == *from);
-  assert('/' == *to);
-
-  if (is_protected(from) || is_protected(to)) {
-    ERROR("Could not rename %s to %s", from, to);
-    return -EACCES;
-  }
-
-  if (dfs->read_only) {
-    ERROR("HDFS configured read-only, cannot rename directory %s", from);
-    return -EACCES;
-  }
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  int ret = 0;
-  if (hdfsRename(userFS, from, to)) {
-    ERROR("Rename %s to %s failed", from, to);
-    ret = (errno > 0) ? -errno : -EIO;
-    goto cleanup;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  return ret;
-
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rmdir.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rmdir.c
deleted file mode 100644
index 259040f..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_rmdir.c
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_trash.h"
-#include "fuse_connect.h"
-
-extern const char *const TrashPrefixDir;
-
-int dfs_rmdir(const char *path)
-{
-  TRACE1("rmdir", path)
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  if (is_protected(path)) {
-    ERROR("Trying to delete protected directory %s", path);
-    return -EACCES;
-  }
-
-  if (dfs->read_only) {
-    ERROR("HDFS configured read-only, cannot delete directory %s", path);
-    return -EACCES;
-  }
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  int ret = 0;
-  int numEntries = 0;
-  hdfsFileInfo *info = hdfsListDirectory(userFS,path,&numEntries);
-
-  if (info) {
-    hdfsFreeFileInfo(info, numEntries);
-  }
-
-  if (numEntries) {
-    ret = -ENOTEMPTY;
-    goto cleanup;
-  }
-
-  if (hdfsDeleteWithTrash(userFS, path, dfs->usetrash)) {
-    ERROR("Error trying to delete directory %s", path);
-    ret = -EIO;
-    goto cleanup;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_statfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_statfs.c
deleted file mode 100644
index c7004a9..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_statfs.c
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-
-
-int dfs_statfs(const char *path, struct statvfs *st)
-{
-  TRACE1("statfs",path)
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert(st);
-  assert(dfs);
-
-  memset(st,0,sizeof(struct statvfs));
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  const tOffset cap   = hdfsGetCapacity(userFS);
-  const tOffset used  = hdfsGetUsed(userFS);
-  const tOffset bsize = hdfsGetDefaultBlockSize(userFS);
-
-  if (doDisconnect(userFS)) {
-    return -EIO;
-  }
-
-  st->f_bsize   =  bsize;
-  st->f_frsize  =  bsize;
-  st->f_blocks  =  cap/bsize;
-  st->f_bfree   =  (cap-used)/bsize;
-  st->f_bavail  =  (cap-used)/bsize;
-  st->f_files   =  1000;
-  st->f_ffree   =  500;
-  st->f_favail  =  500;
-  st->f_fsid    =  1023;
-  st->f_flag    =  ST_RDONLY | ST_NOSUID;
-  st->f_namemax =  1023;
-
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_symlink.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_symlink.c
deleted file mode 100644
index be6e7eb..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_symlink.c
+++ /dev/null
@@ -1,30 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-
-
-int dfs_symlink(const char *from, const char *to)
-{
-  TRACE1("symlink", from)
-  (void)from;
-  (void)to;
-  // bugbug we need the FileSystem to support this posix API
-  return -ENOTSUP;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_truncate.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_truncate.c
deleted file mode 100644
index d09b0c8..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_truncate.c
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-
-/**
- * For now implement truncate here and only for size == 0.
- * Weak implementation in that we just delete the file and 
- * then re-create it, but don't set the user, group, and times to the old
- * file's metadata. 
- */
-int dfs_truncate(const char *path, off_t size)
-{
-  TRACE1("truncate", path)
-
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert('/' == *path);
-  assert(dfs);
-
-  if (size != 0) {
-    return 0;
-  }
-
-  int ret = dfs_unlink(path);
-  if (ret != 0) {
-    return ret;
-  }
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    ret = -EIO;
-    goto cleanup;
-  }
-
-  int flags = O_WRONLY | O_CREAT;
-
-  hdfsFile file;
-  if ((file = (hdfsFile)hdfsOpenFile(userFS, path, flags,  0, 0, 0)) == NULL) {
-    ERROR("Could not connect open file %s", path);
-    ret = -EIO;
-    goto cleanup;
-  }
-
-  if (hdfsCloseFile(userFS, file) != 0) {
-    ERROR("Could not close file %s", path);
-    ret = -EIO;
-    goto cleanup;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_unlink.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_unlink.c
deleted file mode 100644
index a3d2034..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_unlink.c
+++ /dev/null
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-#include "fuse_trash.h"
-extern const char *const TrashPrefixDir;
-
-int dfs_unlink(const char *path)
-{
-  TRACE1("unlink", path)
-
-  int ret = 0;
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  if (is_protected(path)) {
-    ERROR("Trying to delete protected directory %s", path);
-    return -EACCES;
-  }
-
-  if (dfs->read_only) {
-    ERROR("HDFS configured read-only, cannot create directory %s", path);
-    return -EACCES;
-  }
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  if (hdfsDeleteWithTrash(userFS, path, dfs->usetrash)) {
-    ERROR("Could not delete file %s", path);
-    ret = (errno > 0) ? -errno : -EIO;
-    goto cleanup;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  return ret;
-
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_utimens.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_utimens.c
deleted file mode 100644
index f9144f8..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_utimens.c
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_connect.h"
-
-int dfs_utimens(const char *path, const struct timespec ts[2])
-{
-  TRACE1("utimens", path)
-  int ret = 0;
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-
-  time_t aTime = ts[0].tv_sec;
-  time_t mTime = ts[1].tv_sec;
-
-  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
-  if (userFS == NULL) {
-    ERROR("Could not connect");
-    return -EIO;
-  }
-
-  if (hdfsUtime(userFS, path, mTime, aTime)) {
-    hdfsFileInfo *info = hdfsGetPathInfo(userFS, path);
-    if (info == NULL) {
-      ret = (errno > 0) ? -errno : -ENOENT;
-      goto cleanup;
-    }
-    // Silently ignore utimens failure for directories, otherwise 
-    // some programs like tar will fail.
-    if (info->mKind == kObjectKindDirectory) {
-      ret = 0;
-    } else {
-      ret = (errno > 0) ? -errno : -EACCES;
-    }
-    goto cleanup;
-  }
-
-cleanup:
-  if (doDisconnect(userFS)) {
-    ret = -EIO;
-  }
-  return ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_write.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_write.c
deleted file mode 100644
index 8bb0454..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_impls_write.c
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_impls.h"
-#include "fuse_file_handle.h"
-
-int dfs_write(const char *path, const char *buf, size_t size,
-                     off_t offset, struct fuse_file_info *fi)
-{
-  TRACE1("write", path)
-
-  // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-  int ret = 0;
-
-  // check params and the context var
-  assert(path);
-  assert(dfs);
-  assert('/' == *path);
-  assert(fi);
-
-  dfs_fh *fh = (dfs_fh*)fi->fh;
-  assert(fh);
-
-  hdfsFile file_handle = (hdfsFile)fh->hdfsFH;
-  assert(file_handle);
-
-  //
-  // Critical section - make the sanity check (tell to see the writes are sequential) and the actual write 
-  // (no returns until end)
-  //
-  pthread_mutex_lock(&fh->mutex);
-
-  tSize length = 0;
-  assert(fh->fs);
-
-  tOffset cur_offset = hdfsTell(fh->fs, file_handle);
-  if (cur_offset != offset) {
-    ERROR("User trying to random access write to a file %d != %d for %s",
-	  (int)cur_offset, (int)offset, path);
-    ret =  -ENOTSUP;
-  } else {
-    length = hdfsWrite(fh->fs, file_handle, buf, size);
-    if (length <= 0) {
-      ERROR("Could not write all bytes for %s %d != %d (errno=%d)", 
-	    path, length, (int)size, errno);
-      if (errno == 0 || errno == EINTERNAL) {
-        ret = -EIO;
-      } else {
-        ret = -errno;
-      }
-    } 
-    if (length != size) {
-      ERROR("Could not write all bytes for %s %d != %d (errno=%d)", 
-	    path, length, (int)size, errno);
-    }
-  }
-
-  //
-  // Critical section end 
-  //
-
-  pthread_mutex_unlock(&fh->mutex);
-
-  return ret == 0 ? length : ret;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.c
deleted file mode 100644
index 6c1c0d0..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.c
+++ /dev/null
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_dfs.h"
-#include "fuse_init.h"
-#include "fuse_options.h"
-#include "fuse_context_handle.h"
-#include "fuse_connect.h"
-
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-
-// Hacked up function to basically do:
-//  protectedpaths = split(options.protected,':');
-
-void init_protectedpaths(dfs_context *dfs) {
-
-  char *tmp = options.protected;
-
-  // handle degenerate case up front.
-  if (tmp == NULL || 0 == *tmp) {
-    dfs->protectedpaths = (char**)malloc(sizeof(char*));
-    dfs->protectedpaths[0] = NULL;
-    return;
-  }
-  assert(tmp);
-
-  if (options.debug) {
-    print_options();
-  }
-
-  int i = 0;
-  while (tmp && (NULL != (tmp = index(tmp,':')))) {
-    tmp++; // pass the ,
-    i++;
-  }
-  i++; // for the last entry
-  i++; // for the final NULL
-  dfs->protectedpaths = (char**)malloc(sizeof(char*)*i);
-  assert(dfs->protectedpaths);
-  tmp = options.protected;
-  int j  = 0;
-  while (NULL != tmp && j < i) {
-    int length;
-    char *eos = index(tmp,':');
-    if (NULL != eos) {
-      length = eos - tmp; // length of this value
-    } else {
-      length = strlen(tmp);
-    }
-    dfs->protectedpaths[j] = (char*)malloc(sizeof(char)*length+1);
-    assert(dfs->protectedpaths[j]);
-    strncpy(dfs->protectedpaths[j], tmp, length);
-    dfs->protectedpaths[j][length] = '\0';
-    if (eos) {
-      tmp = eos + 1;
-    } else {
-      tmp = NULL;
-    }
-    j++;
-  }
-  dfs->protectedpaths[j] = NULL;
-}
-
-
-void *dfs_init(void) {
-  //
-  // Create a private struct of data we will pass to fuse here and which
-  // will then be accessible on every call.
-  //
-  dfs_context *dfs = (dfs_context*)malloc(sizeof(dfs_context));
-  if (NULL == dfs) {
-    ERROR("FATAL: could not malloc dfs_context");
-    exit(1);
-  }
-
-  // initialize the context
-  dfs->debug                 = options.debug;
-  dfs->nn_uri                = options.nn_uri;
-  dfs->nn_port               = options.nn_port;
-  dfs->read_only             = options.read_only;
-  dfs->usetrash              = options.usetrash;
-  dfs->protectedpaths        = NULL;
-  dfs->rdbuffer_size         = options.rdbuffer_size;
-  dfs->direct_io             = options.direct_io;
-
-  INFO("Mounting.  nn_uri=%s, nn_port=%d", dfs->nn_uri, dfs->nn_port);
-
-  init_protectedpaths(dfs);
-  assert(dfs->protectedpaths != NULL);
-
-  if (dfs->rdbuffer_size <= 0) {
-    DEBUG("dfs->rdbuffersize <= 0 = %ld", dfs->rdbuffer_size);
-    dfs->rdbuffer_size = 32768;
-  }
-
-  if (0 != allocFsTable()) {
-    ERROR("FATAL: could not allocate ");
-    exit(1);
-  }
-
-  return (void*)dfs;
-}
-
-
-void dfs_destroy(void *ptr)
-{
-  TRACE("destroy")
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.h
deleted file mode 100644
index 6f17af8..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_init.h
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_INIT_H__
-#define __FUSE_INIT_H__
-
-/**
- * These are responsible for initializing connections to dfs and internal
- * data structures and then freeing them.
- * i.e., what happens on mount and unmount.
- *
- */
-void *dfs_init();
-void dfs_destroy (void *ptr);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.c
deleted file mode 100644
index 3582974..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.c
+++ /dev/null
@@ -1,188 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fuse_context_handle.h"
-#include "fuse_dfs.h"
-#include "fuse_options.h"
-
-#include <getopt.h>
-#include <stdlib.h>
-
-#define OLD_HDFS_URI_LOCATION "dfs://"
-#define NEW_HDFS_URI_LOCATION "hdfs://"
-
-void print_options() {
-  printf("options:\n"
-	 "\tprotected=%s\n"
-	 "\tserver=%s\n"
-	 "\tport=%d\n"
-	 "\tdebug=%d\n"
-	 "\tread_only=%d\n"
-	 "\tusetrash=%d\n"
-	 "\tentry_timeout=%d\n"
-	 "\tattribute_timeout=%d\n"
-	 "\tprivate=%d\n"
-	 "\trdbuffer_size=%d (KBs)\n", 
-	 options.protected, options.nn_uri, options.nn_port, options.debug,
-	 options.read_only, options.usetrash, options.entry_timeout, 
-	 options.attribute_timeout, options.private, 
-	 (int)options.rdbuffer_size / 1024);
-}
-
-const char *program;
-
-/** macro to define options */
-#define DFSFS_OPT_KEY(t, p, v) { t, offsetof(struct options, p), v }
-
-void print_usage(const char *pname)
-{
-  printf("USAGE: %s [debug] [--help] [--version] "
-	 "[-oprotected=<colon_seped_list_of_paths] [rw] [-onotrash] "
-	 "[-ousetrash] [-obig_writes] [-oprivate (single user)] [ro] "
-	 "[-oserver=<hadoop_servername>] [-oport=<hadoop_port>] "
-	 "[-oentry_timeout=<secs>] [-oattribute_timeout=<secs>] "
-	 "[-odirect_io] [-onopoermissions] [-o<other fuse option>] "
-	 "<mntpoint> [fuse options]\n", pname);
-  printf("NOTE: debugging option for fuse is -debug\n");
-}
-
-
-/** keys for FUSE_OPT_ options */
-enum
-  {
-    KEY_VERSION,
-    KEY_HELP,
-    KEY_USETRASH,
-    KEY_NOTRASH,
-    KEY_RO,
-    KEY_RW,
-    KEY_PRIVATE,
-    KEY_BIGWRITES,
-    KEY_DEBUG,
-    KEY_INITCHECKS,
-    KEY_NOPERMISSIONS,
-    KEY_DIRECTIO,
-  };
-
-struct fuse_opt dfs_opts[] =
-  {
-    DFSFS_OPT_KEY("server=%s", nn_uri, 0),
-    DFSFS_OPT_KEY("entry_timeout=%d", entry_timeout, 0),
-    DFSFS_OPT_KEY("attribute_timeout=%d", attribute_timeout, 0),
-    DFSFS_OPT_KEY("protected=%s", protected, 0),
-    DFSFS_OPT_KEY("port=%d", nn_port, 0),
-    DFSFS_OPT_KEY("rdbuffer=%d", rdbuffer_size,0),
-
-    FUSE_OPT_KEY("private", KEY_PRIVATE),
-    FUSE_OPT_KEY("ro", KEY_RO),
-    FUSE_OPT_KEY("debug", KEY_DEBUG),
-    FUSE_OPT_KEY("initchecks", KEY_INITCHECKS),
-    FUSE_OPT_KEY("nopermissions", KEY_NOPERMISSIONS),
-    FUSE_OPT_KEY("big_writes", KEY_BIGWRITES),
-    FUSE_OPT_KEY("rw", KEY_RW),
-    FUSE_OPT_KEY("usetrash", KEY_USETRASH),
-    FUSE_OPT_KEY("notrash", KEY_NOTRASH),
-    FUSE_OPT_KEY("direct_io", KEY_DIRECTIO),
-    FUSE_OPT_KEY("-v",             KEY_VERSION),
-    FUSE_OPT_KEY("--version",      KEY_VERSION),
-    FUSE_OPT_KEY("-h",             KEY_HELP),
-    FUSE_OPT_KEY("--help",         KEY_HELP),
-    FUSE_OPT_END
-  };
-
-int dfs_options(void *data, const char *arg, int key,  struct fuse_args *outargs)
-{
-  (void) data;
-  int nn_uri_len;
-
-  switch (key) {
-  case FUSE_OPT_KEY_OPT:
-    INFO("Ignoring option %s", arg);
-    return 1;
-  case KEY_VERSION:
-    INFO("%s %s\n", program, _FUSE_DFS_VERSION);
-    exit(0);
-  case KEY_HELP:
-    print_usage(program);
-    exit(0);
-  case KEY_USETRASH:
-    options.usetrash = 1;
-    break;
-  case KEY_NOTRASH:
-    options.usetrash = 1;
-    break;
-  case KEY_RO:
-    options.read_only = 1;
-    break;
-  case KEY_RW:
-    options.read_only = 0;
-    break;
-  case KEY_PRIVATE:
-    options.private = 1;
-    break;
-  case KEY_DEBUG:
-    fuse_opt_add_arg(outargs, "-d");
-    options.debug = 1;
-    break;
-  case KEY_INITCHECKS:
-    options.initchecks = 1;
-    break;
-  case KEY_NOPERMISSIONS:
-    options.no_permissions = 1;
-    break;
-  case KEY_DIRECTIO:
-    options.direct_io = 1;
-    break;
-  case KEY_BIGWRITES:
-#ifdef FUSE_CAP_BIG_WRITES
-    fuse_opt_add_arg(outargs, "-obig_writes");
-#endif
-    break;
-  default: {
-    // try and see if the arg is a URI
-    if (!strstr(arg, "://")) {
-      if (strcmp(arg,"ro") == 0) {
-        options.read_only = 1;
-      } else if (strcmp(arg,"rw") == 0) {
-        options.read_only = 0;
-      } else {
-        INFO("Adding FUSE arg %s", arg);
-        fuse_opt_add_arg(outargs, arg);
-        return 0;
-      }
-    } else {
-      if (options.nn_uri) {
-        INFO("Ignoring option %s because '-server' was already "
-          "specified!", arg);
-        return 1;
-      }
-      if (strstr(arg, OLD_HDFS_URI_LOCATION) == arg) {
-        // For historical reasons, we let people refer to hdfs:// as dfs://
-        nn_uri_len = strlen(NEW_HDFS_URI_LOCATION) + 
-                strlen(arg + strlen(OLD_HDFS_URI_LOCATION)) + 1;
-        options.nn_uri = malloc(nn_uri_len);
-        snprintf(options.nn_uri, nn_uri_len, "%s%s", NEW_HDFS_URI_LOCATION, 
-              arg + strlen(OLD_HDFS_URI_LOCATION));
-      } else {
-        options.nn_uri = strdup(arg);
-      }
-    }
-  }
-  }
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.h
deleted file mode 100644
index 4bfc235..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_options.h
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_OPTIONS_H__
-#define __FUSE_OPTIONS_H__
-
-/** options for fuse_opt.h */
-struct options {
-  char* protected;
-  char* nn_uri;
-  int nn_port;
-  int debug;
-  int read_only;
-  int initchecks;
-  int no_permissions;
-  int usetrash;
-  int entry_timeout;
-  int attribute_timeout;
-  int private;
-  size_t rdbuffer_size;
-  int direct_io;
-} options;
-
-extern struct fuse_opt dfs_opts[];
-void print_options();
-void print_usage(const char *pname);
-int dfs_options(void *data, const char *arg, int key,  struct fuse_args *outargs);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.c
deleted file mode 100644
index e3a0725..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.c
+++ /dev/null
@@ -1,112 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <math.h>
-#include <pthread.h>
-#include <grp.h>
-#include <pwd.h>
-
-#include "fuse_dfs.h"
-#include "fuse_stat_struct.h"
-#include "fuse_context_handle.h"
-
-/*
- * getpwuid and getgrgid return static structs so we safeguard the contents
- * while retrieving fields using the 2 structs below.
- * NOTE: if using both, always get the passwd struct firt!
- */
-extern pthread_mutex_t passwdstruct_mutex; 
-extern pthread_mutex_t groupstruct_mutex;
-
-const int default_id = 99; // nobody  - not configurable since soon uids in dfs, yeah!
-const int blksize = 512;
-
-/**
- * Converts from a hdfs hdfsFileInfo to a POSIX stat struct
- *
- */
-int fill_stat_structure(hdfsFileInfo *info, struct stat *st) 
-{
-  assert(st);
-  assert(info);
-
-  // initialize the stat structure
-  memset(st, 0, sizeof(struct stat));
-
-  // by default: set to 0 to indicate not supported for directory because we cannot (efficiently) get this info for every subdirectory
-  st->st_nlink = (info->mKind == kObjectKindDirectory) ? 0 : 1;
-
-  uid_t owner_id = default_id;
-  if (info->mOwner != NULL) {
-    //
-    // Critical section - protect from concurrent calls in different threads since
-    // the struct below is static.
-    // (no returns until end)
-    //
-    pthread_mutex_lock(&passwdstruct_mutex);
-
-    struct passwd *passwd_info = getpwnam(info->mOwner);
-    owner_id = passwd_info == NULL ? default_id : passwd_info->pw_uid;
-
-    //
-    // End critical section 
-    // 
-    pthread_mutex_unlock(&passwdstruct_mutex);
-
-  } 
-
-  gid_t group_id = default_id;
-
-  if (info->mGroup != NULL) {
-    //
-    // Critical section - protect from concurrent calls in different threads since
-    // the struct below is static.
-    // (no returns until end)
-    //
-    pthread_mutex_lock(&groupstruct_mutex);
-
-    struct group *grp = getgrnam(info->mGroup);
-    group_id = grp == NULL ? default_id : grp->gr_gid;
-
-    //
-    // End critical section 
-    // 
-    pthread_mutex_unlock(&groupstruct_mutex);
-
-  }
-
-  short perm = (info->mKind == kObjectKindDirectory) ? (S_IFDIR | 0777) :  (S_IFREG | 0666);
-  if (info->mPermissions > 0) {
-    perm = (info->mKind == kObjectKindDirectory) ? S_IFDIR:  S_IFREG ;
-    perm |= info->mPermissions;
-  }
-
-  // set stat metadata
-  st->st_size     = (info->mKind == kObjectKindDirectory) ? 4096 : info->mSize;
-  st->st_blksize  = blksize;
-  st->st_blocks   =  ceil(st->st_size/st->st_blksize);
-  st->st_mode     = perm;
-  st->st_uid      = owner_id;
-  st->st_gid      = group_id;
-  st->st_atime    = info->mLastAccess;
-  st->st_mtime    = info->mLastMod;
-  st->st_ctime    = info->mLastMod;
-
-  return 0;
-}
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.h
deleted file mode 100644
index d42a371..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_stat_struct.h
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_STAT_STRUCT_H__
-#define __FUSE_STAT_STRUCT_H__
-
-#include <sys/types.h>
-#include <sys/stat.h>
-#include <unistd.h>
-
-#include "hdfs.h"
-
-/**
- * Converts from a hdfs hdfsFileInfo to a POSIX stat struct
- * Should be thread safe.
- */
-int fill_stat_structure(hdfsFileInfo *info, struct stat *st) ;
-
-extern const int default_id;
-extern const int blksize;
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.c
deleted file mode 100644
index 6d95b40..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.c
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#include <hdfs.h>
-#include <strings.h>
-
-#include "fuse_dfs.h"
-#include "fuse_trash.h"
-#include "fuse_context_handle.h"
-
-
-const char *const TrashPrefixDir = "/user/root/.Trash";
-const char *const TrashDir = "/user/root/.Trash/Current";
-
-#define TRASH_RENAME_TRIES  100
-
-//
-// NOTE: this function is a c implementation of org.apache.hadoop.fs.Trash.moveToTrash(Path path).
-//
-
-int move_to_trash(const char *item, hdfsFS userFS) {
-
-  // retrieve dfs specific data
-  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
-
-  // check params and the context var
-  assert(item);
-  assert(dfs);
-  assert('/' == *item);
-  assert(rindex(item,'/') >= 0);
-
-
-  char fname[4096]; // or last element of the directory path
-  char parent_dir[4096]; // the directory the fname resides in
-
-  if (strlen(item) > sizeof(fname) - strlen(TrashDir)) {
-    ERROR("Buffer too small to accomodate path of len %d", (int)strlen(item));
-    return -EIO;
-  }
-
-  // separate the file name and the parent directory of the item to be deleted
-  {
-    int length_of_parent_dir = rindex(item, '/') - item ;
-    int length_of_fname = strlen(item) - length_of_parent_dir - 1; // the '/'
-
-    // note - the below strncpys should be safe from overflow because of the check on item's string length above.
-    strncpy(parent_dir, item, length_of_parent_dir);
-    parent_dir[length_of_parent_dir ] = 0;
-    strncpy(fname, item + length_of_parent_dir + 1, strlen(item));
-    fname[length_of_fname + 1] = 0;
-  }
-
-  // create the target trash directory
-  char trash_dir[4096];
-  if (snprintf(trash_dir, sizeof(trash_dir), "%s%s", TrashDir, parent_dir) 
-      >= sizeof trash_dir) {
-    ERROR("Move to trash error target not big enough for %s", item);
-    return -EIO;
-  }
-
-  // create the target trash directory in trash (if needed)
-  if ( hdfsExists(userFS, trash_dir)) {
-    // make the directory to put it in in the Trash - NOTE
-    // hdfsCreateDirectory also creates parents, so Current will be created if it does not exist.
-    if (hdfsCreateDirectory(userFS, trash_dir)) {
-      return -EIO;
-    }
-  }
-
-  //
-  // if the target path in Trash already exists, then append with
-  // a number. Start from 1.
-  //
-  char target[4096];
-  int j ;
-  if ( snprintf(target, sizeof target,"%s/%s",trash_dir, fname) >= sizeof target) {
-    ERROR("Move to trash error target not big enough for %s", item);
-    return -EIO;
-  }
-
-  // NOTE: this loop differs from the java version by capping the #of tries
-  for (j = 1; ! hdfsExists(userFS, target) && j < TRASH_RENAME_TRIES ; j++) {
-    if (snprintf(target, sizeof target,"%s/%s.%d",trash_dir, fname, j) >= sizeof target) {
-      ERROR("Move to trash error target not big enough for %s", item);
-      return -EIO;
-    }
-  }
-  if (hdfsRename(userFS, item, target)) {
-    ERROR("Trying to rename %s to %s", item, target);
-    return -EIO;
-  }
-  return 0;
-} 
-
-
-int hdfsDeleteWithTrash(hdfsFS userFS, const char *path, int useTrash) {
-
-  // move the file to the trash if this is enabled and its not actually in the trash.
-  if (useTrash && strncmp(path, TrashPrefixDir, strlen(TrashPrefixDir)) != 0) {
-    int ret= move_to_trash(path, userFS);
-    return ret;
-  }
-
-  if (hdfsDelete(userFS, path, 1)) {
-    ERROR("Trying to delete the file %s", path);
-    return -EIO;
-  }
-
-  return 0;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.h
deleted file mode 100644
index 220ce3d..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_trash.h
+++ /dev/null
@@ -1,26 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_TRASH_H__
-#define __FUSE_TRASH_H__
-
-#include <hdfs.h>
-
-int hdfsDeleteWithTrash(hdfsFS userFS, const char *path, int useTrash);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.c b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.c
deleted file mode 100644
index d7bdfe7..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.c
+++ /dev/null
@@ -1,213 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#include <pthread.h>
-#include <grp.h>
-#include <pwd.h>
-#include <stdlib.h>
-
-#include "fuse_dfs.h"
-
-/*
- * getpwuid and getgrgid return static structs so we safeguard the contents
- * while retrieving fields using the 2 structs below.
- * NOTE: if using both, always get the passwd struct firt!
- */
-pthread_mutex_t passwdstruct_mutex = PTHREAD_MUTEX_INITIALIZER;
-pthread_mutex_t groupstruct_mutex = PTHREAD_MUTEX_INITIALIZER;
-
-/*
- * Utility for getting the user making the fuse call in char * form
- * NOTE: if non-null return, the return must be freed by the caller.
- */
-char *getUsername(uid_t uid) {
-  //
-  // Critical section - protect from concurrent calls in different threads.
-  // since the struct below is static.
-  // (no returns until end)
-  //
-
-  pthread_mutex_lock(&passwdstruct_mutex);
-
-  struct passwd *userinfo = getpwuid(uid);
-  char * ret = userinfo && userinfo->pw_name ? strdup(userinfo->pw_name) : NULL;
-
-  pthread_mutex_unlock(&passwdstruct_mutex);
-
-  //
-  // End critical section 
-  // 
-  return ret;
-}
-
-/**
- * Cleans up a char ** group pointer
- */
-
-void freeGroups(char **groups, int numgroups) {
-  if (groups == NULL) {
-    return;
-  }
-  int i ;
-  for (i = 0; i < numgroups; i++) {
-    free(groups[i]);
-  }
-  free(groups);
-}
-
-#define GROUPBUF_SIZE 5
-
-char *getGroup(gid_t gid) {
-  //
-  // Critical section - protect from concurrent calls in different threads.
-  // since the struct below is static.
-  // (no returns until end)
-  //
-
-  pthread_mutex_lock(&groupstruct_mutex);
-
-  struct group* grp = getgrgid(gid);
-  char * ret = grp && grp->gr_name ? strdup(grp->gr_name) : NULL;
-
-  //
-  // End critical section 
-  // 
-  pthread_mutex_unlock(&groupstruct_mutex);
-
-  return ret;
-}
-
-
-/**
- * Utility for getting the group from the uid
- * NOTE: if non-null return, the return must be freed by the caller.
- */
-char *getGroupUid(uid_t uid) {
-  //
-  // Critical section - protect from concurrent calls in different threads
-  // since the structs below are static.
-  // (no returns until end)
-  //
-
-  pthread_mutex_lock(&passwdstruct_mutex);
-  pthread_mutex_lock(&groupstruct_mutex);
-
-  char *ret = NULL;
-  struct passwd *userinfo = getpwuid(uid);
-  if (NULL != userinfo) {
-    struct group* grp = getgrgid( userinfo->pw_gid);
-    ret = grp && grp->gr_name ? strdup(grp->gr_name) : NULL;
-  }
-
-  //
-  // End critical section 
-  // 
-  pthread_mutex_unlock(&groupstruct_mutex);
-  pthread_mutex_unlock(&passwdstruct_mutex);
-
-  return ret;
-}
-
-
-/**
- * lookup the gid based on the uid
- */
-gid_t getGidUid(uid_t uid) {
-  //
-  // Critical section - protect from concurrent calls in different threads
-  // since the struct below is static.
-  // (no returns until end)
-  //
-
-  pthread_mutex_lock(&passwdstruct_mutex);
-
-  struct passwd *userinfo = getpwuid(uid);
-  gid_t gid = userinfo == NULL ? 0 : userinfo->pw_gid;
-
-  //
-  // End critical section 
-  // 
-  pthread_mutex_unlock(&passwdstruct_mutex);
-
-  return gid;
-}
-
-/**
- * Utility for getting the groups for the user making the fuse call in char * form
- */
-char ** getGroups(uid_t uid, int *num_groups)
-{
-  char *user = getUsername(uid);
-
-  if (user == NULL)
-    return NULL;
-
-  char **groupnames = NULL;
-
-  // see http://www.openldap.org/lists/openldap-devel/199903/msg00023.html
-
-  //#define GETGROUPS_T 1 
-#ifdef GETGROUPS_T
-  *num_groups = GROUPBUF_SIZE;
-
-  gid_t* grouplist = malloc(GROUPBUF_SIZE * sizeof(gid_t)); 
-  assert(grouplist != NULL);
-  gid_t* tmp_grouplist; 
-  int rtr;
-
-  gid_t gid = getGidUid(uid);
-
-  if ((rtr = getgrouplist(user, gid, grouplist, num_groups)) == -1) {
-    // the buffer we passed in is < *num_groups
-    if ((tmp_grouplist = realloc(grouplist, *num_groups * sizeof(gid_t))) != NULL) {
-      grouplist = tmp_grouplist;
-      getgrouplist(user, gid, grouplist, num_groups);
-    }
-  }
-
-  groupnames = (char**)malloc(sizeof(char*)* (*num_groups) + 1);
-  assert(groupnames);
-  int i;
-  for (i=0; i < *num_groups; i++)  {
-    groupnames[i] = getGroup(grouplist[i]);
-    if (groupnames[i] == NULL) {
-      ERROR("Could not lookup group %d\n", (int)grouplist[i]);
-    }
-  } 
-  free(grouplist);
-  assert(user != NULL);
-  groupnames[i] = user;
-  *num_groups = *num_groups + 1;
-#else
-
-  int i = 0;
-  assert(user != NULL);
-  groupnames[i] = user;
-  i++;
-
-  groupnames[i] = getGroupUid(uid);
-  if (groupnames[i]) {
-    i++;
-  }
-
-  *num_groups = i;
-
-#endif
-  return groupnames;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.h b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.h
deleted file mode 100644
index d63d916..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/fuse_users.h
+++ /dev/null
@@ -1,70 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef __FUSE_USERS_H__
-#define __FUSE_USERS_H__
-
-#include <grp.h>
-#include <pwd.h>
-#include <pthread.h>
-
-/**
- * Overall Note:
- * 1. all these functions should be thread safe.
- * 2. the ones that return char * or char **, generally require
- * the caller to free the return value.
- *
- */
-
-
-/**
- * Utility for getting the user making the fuse call in char * form
- * NOTE: if non-null return, the return must be freed by the caller.
- */
-char *getUsername(uid_t uid);
-
-
-/**
- * Cleans up a char ** group pointer
- */
-void freeGroups(char **groups, int numgroups);
-
-/**
- * Lookup single group. Caller responsible for free of the return value
- */
-char *getGroup(gid_t gid);
-
-/**
- * Utility for getting the group from the uid
- * NOTE: if non-null return, the return must be freed by the caller.
- */
-char *getGroupUid(uid_t uid) ;
-
-
-/**
- * lookup the gid based on the uid
- */
-
-gid_t getGidUid(uid_t uid);
-
-/**
- * Utility for getting the groups for the user making the fuse call in char * form
- */
-char ** getGroups(uid_t uid, int *num_groups);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/test/TestFuseDFS.java b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/test/TestFuseDFS.java
deleted file mode 100644
index c9827da..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/fuse-dfs/src/test/TestFuseDFS.java
+++ /dev/null
@@ -1,369 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.*;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.concurrent.atomic.*;
-
-import org.apache.log4j.Level;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.commons.logging.impl.Log4JLogger;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.fs.permission.*;
-import org.apache.hadoop.hdfs.*;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.util.StringUtils;
-
-import org.junit.Test;
-import org.junit.BeforeClass;
-import org.junit.AfterClass;
-import static org.junit.Assert.*;
-
-/**
- * Basic functional tests on a fuse-dfs mount.
- */
-public class TestFuseDFS {
-
-  private static MiniDFSCluster cluster;
-  private static FileSystem fs;
-  private static Runtime r;
-  private static String mountPoint;
-
-  private static final Log LOG = LogFactory.getLog(TestFuseDFS.class);
-  {
-    ((Log4JLogger)LOG).getLogger().setLevel(Level.ALL);
-  }
-
-  /** Dump the given intput stream to stderr */
-  private static void dumpInputStream(InputStream is) throws IOException {
-    int len;
-    do {
-      byte b[] = new byte[is.available()];
-      len = is.read(b);
-      System.out.println("Read "+len+" bytes");
-      System.out.write(b, 0, b.length);
-    } while (len > 0);
-  }
-
-  /** 
-   * Wait for the given process to return and check that it exited
-   * as required. Log if the process failed.
-   */
-  private static void checkProcessRet(Process p, boolean expectPass) 
-      throws IOException {
-    try {
-      int ret = p.waitFor();
-      if (ret != 0) {
-	dumpInputStream(p.getErrorStream());
-      }
-      if (expectPass) {
-	assertEquals(0, ret);
-      } else {
-	assertTrue(ret != 0);
-      }
-    } catch (InterruptedException ie) {
-      fail("Process interrupted: "+ie.getMessage());
-    }
-  }
-
-  /** Exec the given command and assert it executed successfully */
-  private static void execWaitRet(String cmd) throws IOException {
-    LOG.debug("EXEC "+cmd);
-    Process p = r.exec(cmd);
-    try {
-      p.waitFor();
-    } catch (InterruptedException ie) {
-      fail("Process interrupted: "+ie.getMessage());
-    }
-  }
-
-  /** Exec the given command and assert it executed successfully */
-  private static void execIgnoreRet(String cmd) throws IOException {
-    LOG.debug("EXEC "+cmd);
-    r.exec(cmd);
-  }
-
-  /** Exec the given command and assert it executed successfully */
-  private static void execAssertSucceeds(String cmd) throws IOException {
-    LOG.debug("EXEC "+cmd);
-    checkProcessRet(r.exec(cmd), true);
-  }
-
-  /** Exec the given command, assert it returned an error code */
-  private static void execAssertFails(String cmd) throws IOException {
-    LOG.debug("EXEC "+cmd);
-    checkProcessRet(r.exec(cmd), false);
-  }
-
-  /** Create and write the given file */
-  private static void createFile(File f, String s) throws IOException {
-    InputStream is = new ByteArrayInputStream(s.getBytes());
-    FileOutputStream fos = new FileOutputStream(f);
-    IOUtils.copyBytes(is, fos, s.length(), true);
-  }
-
-  /** Check that the given file exists with the given contents */
-  private static void checkFile(File f, String expectedContents) 
-      throws IOException {
-    FileInputStream fi = new FileInputStream(f);
-    int len = expectedContents.length();
-    byte[] b = new byte[len];
-    try {
-      IOUtils.readFully(fi, b, 0, len);
-    } catch (IOException ie) {
-      fail("Reading "+f.getName()+" failed with "+ie.getMessage());
-    } finally {
-      fi.close(); // NB: leaving f unclosed prevents unmount
-    }
-    String s = new String(b, 0, len);
-    assertEquals("File content differs", expectedContents, s);
-  }
-
-  /** Run a fuse-dfs process to mount the given DFS */
-  private static void establishMount(URI uri) throws IOException  {
-    Runtime r = Runtime.getRuntime();
-    String cp = System.getProperty("java.class.path");
-
-    String buildTestDir = System.getProperty("build.test");
-    String fuseCmd = buildTestDir + "/../fuse_dfs";
-    String libHdfs = buildTestDir + "/../../../c++/lib";
-
-    String arch = System.getProperty("os.arch");
-    String jvm = System.getProperty("java.home") + "/lib/" + arch + "/server";
-    String lp = System.getProperty("LD_LIBRARY_PATH")+":"+libHdfs+":"+jvm;
-    LOG.debug("LD_LIBRARY_PATH=" + lp);
-
-    String nameNode = 
-      "dfs://" + uri.getHost() + ":" + String.valueOf(uri.getPort());
-
-    // NB: We're mounting via an unprivileged user, therefore
-    // user_allow_other needs to be set in /etc/fuse.conf, which also
-    // needs to be world readable.
-    String mountCmd[] = {
-      fuseCmd, nameNode, mountPoint,
-      // "-odebug",              // Don't daemonize
-      "-obig_writes",            // Allow >4kb writes
-      "-oentry_timeout=0.1",     // Don't cache dents long
-      "-oattribute_timeout=0.1", // Don't cache attributes long
-      "-ordbuffer=32768",        // Read buffer size in kb
-      "rw"
-    };
-
-    String [] env = {
-      "CLASSPATH="+cp,
-      "LD_LIBRARY_PATH="+lp,
-      "PATH=/usr/bin:/bin"
-    };
-
-    execWaitRet("fusermount -u " + mountPoint);
-    execAssertSucceeds("rm -rf " + mountPoint);
-    execAssertSucceeds("mkdir -p " + mountPoint);
-
-    // Mount the mini cluster
-    try {
-      Process fuseProcess = r.exec(mountCmd, env);
-      assertEquals(0, fuseProcess.waitFor());
-    } catch (InterruptedException ie) {
-      fail("Failed to mount");
-    }
-  }
-
-  /** Tear down the fuse-dfs process and mount */
-  private static void teardownMount() throws IOException {
-    execWaitRet("fusermount -u " + mountPoint);
-  }
-
-  @BeforeClass
-  public static void startUp() throws IOException {
-    Configuration conf = new HdfsConfiguration();
-    r = Runtime.getRuntime();
-    mountPoint = System.getProperty("build.test") + "/mnt";
-    conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, false);
-    cluster = new MiniDFSCluster.Builder(conf).build();
-    cluster.waitClusterUp();
-    fs = cluster.getFileSystem();
-    establishMount(fs.getUri());
-  }
-
-  @AfterClass
-  public static void tearDown() throws IOException {
-    // Unmount before taking down the mini cluster
-    // so no outstanding operations hang.
-    teardownMount();
-    if (fs != null) {
-      fs.close();
-    }
-    if (cluster != null) {
-      cluster.shutdown();
-    }
-  }
-
-  /** Test basic directory creation, access, removal */
-  @Test
-  public void testBasicDir() throws IOException {
-    File d = new File(mountPoint, "dir1");
-
-    // Mkdir, access and rm via the mount
-    execAssertSucceeds("mkdir " + d.getAbsolutePath());
-    execAssertSucceeds("ls " + d.getAbsolutePath());
-    execAssertSucceeds("rmdir " + d.getAbsolutePath());
-
-    // The dir should no longer exist
-    execAssertFails("ls " + d.getAbsolutePath());
-  }
-
-  /** Test basic file creation and writing */
-  @Test
-  public void testCreate() throws IOException {
-    final String contents = "hello world";
-    File f = new File(mountPoint, "file1");
-
-    // Create and access via the mount
-    createFile(f, contents);
-
-    // XX avoids premature EOF
-    try {
-      Thread.sleep(1000);
-    } catch (InterruptedException ie) { }
-
-    checkFile(f, contents);
-
-    // Cat, stat and delete via the mount
-    execAssertSucceeds("cat " + f.getAbsolutePath());
-    execAssertSucceeds("stat " + f.getAbsolutePath());
-    execAssertSucceeds("rm " + f.getAbsolutePath());
-
-    // The file should no longer exist
-    execAssertFails("ls " + f.getAbsolutePath());
-  }
-
-  /** Test creating a file via touch */
-  @Test
-  public void testTouch() throws IOException {
-    File f = new File(mountPoint, "file1");
-    execAssertSucceeds("touch " + f.getAbsolutePath());
-    execAssertSucceeds("rm " + f.getAbsolutePath());
-  }
-
-  /** Test random access to a file */
-  @Test
-  public void testRandomAccess() throws IOException {
-    final String contents = "hello world";
-    File f = new File(mountPoint, "file1");
-
-    createFile(f, contents);
-
-    RandomAccessFile raf = new RandomAccessFile(f, "rw");
-    raf.seek(f.length());
-    try {
-      raf.write('b');
-    } catch (IOException e) {
-      // Expected: fuse-dfs not yet support append
-      assertEquals("Operation not supported", e.getMessage());
-    } finally {
-      raf.close();
-    }
-
-    raf = new RandomAccessFile(f, "rw");
-    raf.seek(0);
-    try {
-      raf.write('b');
-      fail("Over-wrote existing bytes");
-    } catch (IOException e) {
-      // Expected: can-not overwrite a file
-      assertEquals("Invalid argument", e.getMessage());
-    } finally {
-      raf.close();
-    }
-    execAssertSucceeds("rm " + f.getAbsolutePath());
-  }
-
-  /** Test copying a set of files from the mount to itself */
-  @Test
-  public void testCopyFiles() throws IOException {
-    final String contents = "hello world";
-    File d1 = new File(mountPoint, "dir1");
-    File d2 = new File(mountPoint, "dir2");
-
-    // Create and populate dir1 via the mount
-    execAssertSucceeds("mkdir " + d1.getAbsolutePath());
-    for (int i = 0; i < 5; i++) {
-      createFile(new File(d1, "file"+i), contents);
-    }
-    assertEquals(5, d1.listFiles().length);
-
-    // Copy dir from the mount to the mount
-    execAssertSucceeds("cp -r " + d1.getAbsolutePath() +
-                       " " + d2.getAbsolutePath());
-    assertEquals(5, d2.listFiles().length);
-
-    // Access all the files in the dirs and remove them
-    execAssertSucceeds("find " + d1.getAbsolutePath());
-    execAssertSucceeds("find " + d2.getAbsolutePath());
-    execAssertSucceeds("rm -r " + d1.getAbsolutePath());
-    execAssertSucceeds("rm -r " + d2.getAbsolutePath());
-  }
-
-  /** Test concurrent creation and access of the mount */
-  @Test
-  public void testMultipleThreads() throws IOException {
-    ArrayList<Thread> threads = new ArrayList<Thread>();
-    final AtomicReference<String> errorMessage = new AtomicReference<String>();
-
-    for (int i = 0; i < 10; i++) {
-      Thread t = new Thread() {
-	  public void run() {
-	    try {
-	      File d = new File(mountPoint, "dir"+getId());
-	      execWaitRet("mkdir " + d.getAbsolutePath());
-	      for (int j = 0; j < 10; j++) {
-		File f = new File(d, "file"+j);
-		final String contents = "thread "+getId()+" "+j;
-		createFile(f, contents);
-	      }
-	      for (int j = 0; j < 10; j++) {
-		File f = new File(d, "file"+j);
-		execWaitRet("cat " + f.getAbsolutePath());
-		execWaitRet("rm " + f.getAbsolutePath());
-	      }
-	      execWaitRet("rmdir " + d.getAbsolutePath());
-	    } catch (IOException ie) {
-	      errorMessage.set(
-		String.format("Exception %s", 
-			      StringUtils.stringifyException(ie)));
-	    }
-          }
-	};
-      t.start();
-      threads.add(t);
-    }
-
-    for (Thread t : threads) {
-      try {
-	t.join();
-      } catch (InterruptedException ie) {
-	fail("Thread interrupted: "+ie.getMessage());
-      }
-    }
-
-    assertNull(errorMessage.get(), errorMessage.get());
-  }
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/expect.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/expect.h
deleted file mode 100644
index 2046bd0..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/expect.h
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LIBHDFS_NATIVE_TESTS_EXPECT_H
-#define LIBHDFS_NATIVE_TESTS_EXPECT_H
-
-#include <stdio.h>
-
-#define EXPECT_ZERO(x) \
-    do { \
-        int __my_ret__ = x; \
-        if (__my_ret__) { \
-            int __my_errno__ = errno; \
-            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
-		    "code %d (errno: %d): got nonzero from %s\n", \
-		    __LINE__, __my_ret__, __my_errno__, #x); \
-            return __my_ret__; \
-        } \
-    } while (0);
-
-#define EXPECT_NULL(x) \
-    do { \
-        void* __my_ret__ = x; \
-        int __my_errno__ = errno; \
-        if (__my_ret__ != NULL) { \
-            fprintf(stderr, "TEST_ERROR: failed on line %d (errno: %d): " \
-		    "got non-NULL value %p from %s\n", \
-		    __LINE__, __my_errno__, __my_ret__, #x); \
-            return -1; \
-        } \
-    } while (0);
-
-#define EXPECT_NONNULL(x) \
-    do { \
-        void* __my_ret__ = x; \
-        int __my_errno__ = errno; \
-        if (__my_ret__ == NULL) { \
-            fprintf(stderr, "TEST_ERROR: failed on line %d (errno: %d): " \
-		    "got NULL from %s\n", __LINE__, __my_errno__, #x); \
-            return -1; \
-        } \
-    } while (0);
-
-#define EXPECT_NEGATIVE_ONE_WITH_ERRNO(x, e) \
-    do { \
-        int __my_ret__ = x; \
-        int __my_errno__ = errno; \
-        if (__my_ret__ != -1) { \
-            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
-                "code %d (errno: %d): expected -1 from %s\n", __LINE__, \
-                __my_ret__, __my_errno__, #x); \
-            return -1; \
-        } \
-        if (__my_errno__ != e) { \
-            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
-                "code %d (errno: %d): expected errno = %d from %s\n", \
-                __LINE__, __my_ret__, __my_errno__, e, #x); \
-            return -1; \
-	} \
-    } while (0);
-
-#define EXPECT_NONZERO(x) \
-    do { \
-        int __my_ret__ = x; \
-        int __my_errno__ = errno; \
-        if (__my_ret__) { \
-            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
-		    "code %d (errno: %d): got zero from %s\n", __LINE__, \
-                __my_ret__, __my_errno__, #x); \
-            return -1; \
-        } \
-    } while (0);
-
-#define EXPECT_NONNEGATIVE(x) \
-    do { \
-        int __my_ret__ = x; \
-        int __my_errno__ = errno; \
-        if (__my_ret__ < 0) { \
-            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
-                "code %d (errno: %d): got negative return from %s\n", \
-		    __LINE__, __my_ret__, __my_errno__, #x); \
-            return __my_ret__; \
-        } \
-    } while (0);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt
new file mode 100644
index 0000000..fb3c580
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt
@@ -0,0 +1,73 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Find Linux FUSE
+IF (${CMAKE_SYSTEM_NAME} MATCHES "Linux")
+    find_package(PkgConfig REQUIRED)
+    pkg_check_modules(FUSE fuse)
+    IF(FUSE_FOUND)
+        FLATTEN_LIST("${FUSE_CFLAGS}" " " FUSE_CFLAGS)
+        FLATTEN_LIST("${FUSE_LDFLAGS}" " " FUSE_LDFLAGS)
+        set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${FUSE_CFLAGS}")
+        set(CMAKE_LD_FLAGS "${CMAKE_LD_FLAGS} ${FUSE_LDFLAGS}")
+        MESSAGE(STATUS "Building Linux FUSE client.")
+        include_directories(${FUSE_INCLUDE_DIRS})
+    ELSE(FUSE_FOUND)
+        MESSAGE(STATUS "Failed to find Linux FUSE libraries or include files.  Will not build FUSE client.")
+    ENDIF(FUSE_FOUND)
+ELSE (${CMAKE_SYSTEM_NAME} MATCHES "Linux")
+    MESSAGE(STATUS "Non-Linux system detected.  Will not build FUSE client.")
+ENDIF (${CMAKE_SYSTEM_NAME} MATCHES "Linux")
+
+IF(FUSE_FOUND)
+    add_executable(fuse_dfs
+        fuse_dfs.c
+        fuse_options.c 
+        fuse_connect.c 
+        fuse_impls_access.c 
+        fuse_impls_chmod.c  
+        fuse_impls_chown.c  
+        fuse_impls_create.c  
+        fuse_impls_flush.c 
+        fuse_impls_getattr.c  
+        fuse_impls_mkdir.c  
+        fuse_impls_mknod.c  
+        fuse_impls_open.c 
+        fuse_impls_read.c 
+        fuse_impls_readdir.c 
+        fuse_impls_release.c 
+        fuse_impls_rename.c 
+        fuse_impls_rmdir.c 
+        fuse_impls_statfs.c 
+        fuse_impls_symlink.c 
+        fuse_impls_truncate.c 
+        fuse_impls_unlink.c 
+        fuse_impls_utimens.c  
+        fuse_impls_write.c
+        fuse_init.c 
+        fuse_stat_struct.c 
+        fuse_trash.c 
+        fuse_users.c 
+    )
+    target_link_libraries(fuse_dfs
+        ${FUSE_LIBRARIES}
+        ${JAVA_JVM_LIBRARY}
+        hdfs
+        m
+    )
+ENDIF(FUSE_FOUND)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/doc/README b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/doc/README
new file mode 100644
index 0000000..1744892
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/doc/README
@@ -0,0 +1,131 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+Fuse-DFS
+
+Supports reads, writes, and directory operations (e.g., cp, ls, more, cat, find, less, rm, mkdir, mv, rmdir).  Things like touch, chmod, chown, and permissions are in the works. Fuse-dfs currently shows all files as owned by nobody.
+
+Contributing
+
+It's pretty straightforward to add functionality to fuse-dfs as fuse makes things relatively simple. Some other tasks require also augmenting libhdfs to expose more hdfs functionality to C. See [http://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&mode=hide&pid=12310240&sorter/order=DESC&sorter/field=priority&resolution=-1&component=12312376  contrib/fuse-dfs JIRAs]
+
+Requirements
+
+ * Hadoop with compiled libhdfs.so
+ * Linux kernel > 2.6.9 with fuse, which is the default or Fuse 2.7.x, 2.8.x installed. See: [http://fuse.sourceforge.net/]
+ * modprobe fuse to load it
+ * fuse-dfs executable (see below)
+ * fuse_dfs_wrapper.sh installed in /bin or other appropriate location (see below)
+
+
+BUILDING
+
+   1. in HADOOP_PREFIX: `ant compile-libhdfs -Dlibhdfs=1
+   2. in HADOOP_PREFIX: `ant package` to deploy libhdfs
+   3. in HADOOP_PREFIX: `ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1`
+
+NOTE: for amd64 architecture, libhdfs will not compile unless you edit
+the Makefile in src/c++/libhdfs/Makefile and set OS_ARCH=amd64
+(probably the same for others too). See [https://issues.apache.org/jira/browse/HADOOP-3344 HADOOP-3344]
+
+Common build problems include not finding the libjvm.so in JAVA_HOME/jre/lib/OS_ARCH/server or not finding fuse in FUSE_HOME or /usr/local.
+
+
+CONFIGURING
+
+Look at all the paths in fuse_dfs_wrapper.sh and either correct them or set them in your environment before running. (note for automount and mount as root, you probably cannot control the environment, so best to set them in the wrapper)
+
+INSTALLING
+
+1. `mkdir /export/hdfs` (or wherever you want to mount it)
+
+2. `fuse_dfs_wrapper.sh dfs://hadoop_server1.foo.com:9000 /export/hdfs -d` and from another terminal, try `ls /export/hdfs`
+
+If 2 works, try again dropping the debug mode, i.e., -d
+
+(note - common problems are that you don't have libhdfs.so or libjvm.so or libfuse.so on your LD_LIBRARY_PATH, and your CLASSPATH does not contain hadoop and other required jars.)
+
+Also note, fuse-dfs will write error/warn messages to the syslog - typically in /var/log/messages
+
+You can use fuse-dfs to mount multiple hdfs instances by just changing the server/port name and directory mount point above.
+
+DEPLOYING
+
+in a root shell do the following:
+
+1. add the following to /etc/fstab
+
+fuse_dfs#dfs://hadoop_server.foo.com:9000 /export/hdfs fuse -oallow_other,rw,-ousetrash,-oinitchecks 0 0
+
+
+2. Mount using: `mount /export/hdfs`. Expect problems with not finding fuse_dfs. You will need to probably add this to /sbin and then problems finding the above 3 libraries. Add these using ldconfig.
+
+
+Fuse DFS takes the following mount options (i.e., on the command line or the comma separated list of options in /etc/fstab:
+
+-oserver=%s  (optional place to specify the server but in fstab use the format above)
+-oport=%d (optional port see comment on server option)
+-oentry_timeout=%d (how long directory entries are cached by fuse in seconds - see fuse docs)
+-oattribute_timeout=%d (how long attributes are cached by fuse in seconds - see fuse docs)
+-oprotected=%s (a colon separated list of directories that fuse-dfs should not allow to be deleted or moved - e.g., /user:/tmp)
+-oprivate (not often used but means only the person who does the mount can use the filesystem - aka ! allow_others in fuse speak)
+-ordbuffer=%d (in KBs how large a buffer should fuse-dfs use when doing hdfs reads)
+ro 
+rw
+-ousetrash (should fuse dfs throw things in /Trash when deleting them)
+-onotrash (opposite of usetrash)
+-odebug (do not daemonize - aka -d in fuse speak)
+-obig_writes (use fuse big_writes option so as to allow better performance of writes on kernels >= 2.6.26)
+-initchecks - have fuse-dfs try to connect to hdfs to ensure all is ok upon startup. recommended to have this  on
+The defaults are:
+
+entry,attribute_timeouts = 60 seconds
+rdbuffer = 10 MB
+protected = null
+debug = 0
+notrash
+private = 0
+
+EXPORTING
+
+Add the following to /etc/exports:
+
+/export/hdfs *.foo.com(no_root_squash,rw,fsid=1,sync)
+
+NOTE - you cannot export this with a FUSE module built into the kernel
+- e.g., kernel 2.6.17. For info on this, refer to the FUSE wiki.
+
+
+RECOMMENDATIONS
+
+1. From /bin, `ln -s $HADOOP_PREFIX/contrib/fuse-dfs/fuse_dfs* .`
+
+2. Always start with debug on so you can see if you are missing a classpath or something like that.
+
+3. use -obig_writes
+
+4. use -initchecks
+
+KNOWN ISSUES 
+
+1. if you alias `ls` to `ls --color=auto` and try listing a directory with lots (over thousands) of files, expect it to be slow and at 10s of thousands, expect it to be very very slow.  This is because `--color=auto` causes ls to stat every file in the directory. Since fuse-dfs does not cache attribute entries when doing a readdir, 
+this is very slow. see [https://issues.apache.org/jira/browse/HADOOP-3797 HADOOP-3797]
+
+2. Writes are approximately 33% slower than the DFSClient. TBD how to optimize this. see: [https://issues.apache.org/jira/browse/HADOOP-3805 HADOOP-3805] - try using -obig_writes if on a >2.6.26 kernel, should perform much better since bigger writes implies less context switching.
+
+3. Reads are ~20-30% slower even with the read buffering. 
+
+4. fuse-dfs and underlying libhdfs have no support for permissions. See [https://issues.apache.org/jira/browse/HADOOP-3536 HADOOP-3536] 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
new file mode 100644
index 0000000..bfb7a1e
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
@@ -0,0 +1,239 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hdfs.h"
+#include "fuse_dfs.h"
+#include "fuse_connect.h"
+#include "fuse_users.h" 
+
+#include <limits.h>
+#include <search.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+#define HADOOP_SECURITY_AUTHENTICATION "hadoop.security.authentication"
+
+enum authConf {
+    AUTH_CONF_UNKNOWN,
+    AUTH_CONF_KERBEROS,
+    AUTH_CONF_OTHER,
+};
+
+#define MAX_ELEMENTS (16 * 1024)
+static struct hsearch_data *fsTable = NULL;
+static enum authConf hdfsAuthConf = AUTH_CONF_UNKNOWN;
+static pthread_mutex_t tableMutex = PTHREAD_MUTEX_INITIALIZER;
+
+/*
+ * Allocate a hash table for fs handles. Returns 0 on success,
+ * -1 on failure.
+ */
+int allocFsTable(void) {
+  assert(NULL == fsTable);
+  fsTable = calloc(1, sizeof(struct hsearch_data));
+  if (0 == hcreate_r(MAX_ELEMENTS, fsTable)) {
+    ERROR("Unable to initialize connection table");
+    return -1;
+  }
+  return 0;
+}
+
+/*
+ * Find a fs handle for the given key. Returns a fs handle, 
+ * or NULL if there is no fs for the given key.
+ */
+static hdfsFS findFs(char *key) {
+  ENTRY entry;
+  ENTRY *entryP = NULL;
+  entry.key = key;
+  if (0 == hsearch_r(entry, FIND, &entryP, fsTable)) {
+    return NULL;
+  }
+  assert(NULL != entryP->data);
+  return (hdfsFS)entryP->data;
+}
+
+/*
+ * Insert the given fs handle into the table.
+ * Returns 0 on success, -1 on failure.
+ */
+static int insertFs(char *key, hdfsFS fs) {
+  ENTRY entry;
+  ENTRY *entryP = NULL;
+  assert(NULL != fs);
+  entry.key = strdup(key);
+  if (entry.key == NULL) {
+    return -1;
+  }
+  entry.data = (void*)fs;
+  if (0 == hsearch_r(entry, ENTER, &entryP, fsTable)) {
+    return -1;
+  }
+  return 0;
+}
+
+/** 
+ * Find out what type of authentication the system administrator
+ * has configured.
+ *
+ * @return     the type of authentication, or AUTH_CONF_UNKNOWN on error.
+ */
+static enum authConf discoverAuthConf(void)
+{
+    int ret;
+    char *val = NULL;
+    enum authConf authConf;
+
+    ret = hdfsConfGet(HADOOP_SECURITY_AUTHENTICATION, &val);
+    if (ret)
+        authConf = AUTH_CONF_UNKNOWN;
+    else if (!strcmp(val, "kerberos"))
+        authConf = AUTH_CONF_KERBEROS;
+    else
+        authConf = AUTH_CONF_OTHER;
+    free(val);
+    return authConf;
+}
+
+/**
+ * Find the Kerberos ticket cache path.
+ *
+ * This function finds the Kerberos ticket cache path from the thread ID and
+ * user ID of the process making the request.
+ *
+ * Normally, the ticket cache path is in a well-known location in /tmp.
+ * However, it's possible that the calling process could set the KRB5CCNAME
+ * environment variable, indicating that its Kerberos ticket cache is at a
+ * non-default location.  We try to handle this possibility by reading the
+ * process' environment here.  This will be allowed if we have root
+ * capabilities, or if our UID is the same as the remote process' UID.
+ *
+ * Note that we don't check to see if the cache file actually exists or not.
+ * We're just trying to find out where it would be if it did exist. 
+ *
+ * @param path          (out param) the path to the ticket cache file
+ * @param pathLen       length of the path buffer
+ */
+static void findKerbTicketCachePath(char *path, size_t pathLen)
+{
+  struct fuse_context *ctx = fuse_get_context();
+  FILE *fp = NULL;
+  static const char * const KRB5CCNAME = "\0KRB5CCNAME=";
+  int c = '\0', pathIdx = 0, keyIdx = 0;
+  size_t KRB5CCNAME_LEN = strlen(KRB5CCNAME + 1) + 1;
+
+  // /proc/<tid>/environ contains the remote process' environment.  It is
+  // exposed to us as a series of KEY=VALUE pairs, separated by NULL bytes.
+  snprintf(path, pathLen, "/proc/%d/environ", ctx->pid);
+  fp = fopen(path, "r");
+  if (!fp)
+    goto done;
+  while (1) {
+    if (c == EOF)
+      goto done;
+    if (keyIdx == KRB5CCNAME_LEN) {
+      if (pathIdx >= pathLen - 1)
+        goto done;
+      if (c == '\0')
+        goto done;
+      path[pathIdx++] = c;
+    } else if (KRB5CCNAME[keyIdx++] != c) {
+      keyIdx = 0;
+    }
+    c = fgetc(fp);
+  }
+
+done:
+  if (fp)
+    fclose(fp);
+  if (pathIdx == 0) {
+    snprintf(path, pathLen, "/tmp/krb5cc_%d", ctx->uid);
+  } else {
+    path[pathIdx] = '\0';
+  }
+}
+
+/*
+ * Connect to the NN as the current user/group.
+ * Returns a fs handle on success, or NULL on failure.
+ */
+hdfsFS doConnectAsUser(const char *nn_uri, int nn_port) {
+  struct hdfsBuilder *bld;
+  uid_t uid = fuse_get_context()->uid;
+  char *user = getUsername(uid);
+  char kpath[PATH_MAX];
+  int ret;
+  hdfsFS fs = NULL;
+  if (NULL == user) {
+    goto done;
+  }
+
+  ret = pthread_mutex_lock(&tableMutex);
+  assert(0 == ret);
+
+  fs = findFs(user);
+  if (NULL == fs) {
+    if (hdfsAuthConf == AUTH_CONF_UNKNOWN) {
+      hdfsAuthConf = discoverAuthConf();
+      if (hdfsAuthConf == AUTH_CONF_UNKNOWN) {
+        ERROR("Unable to determine the configured value for %s.",
+              HADOOP_SECURITY_AUTHENTICATION);
+        goto done;
+      }
+    }
+    bld = hdfsNewBuilder();
+    if (!bld) {
+      ERROR("Unable to create hdfs builder");
+      goto done;
+    }
+    hdfsBuilderSetForceNewInstance(bld);
+    hdfsBuilderSetNameNode(bld, nn_uri);
+    if (nn_port) {
+        hdfsBuilderSetNameNodePort(bld, nn_port);
+    }
+    hdfsBuilderSetUserName(bld, user);
+    if (hdfsAuthConf == AUTH_CONF_KERBEROS) {
+      findKerbTicketCachePath(kpath, sizeof(kpath));
+      hdfsBuilderSetKerbTicketCachePath(bld, kpath);
+    }
+    fs = hdfsBuilderConnect(bld);
+    if (NULL == fs) {
+      int err = errno;
+      ERROR("Unable to create fs for user %s: error code %d", user, err);
+      goto done;
+    }
+    if (-1 == insertFs(user, fs)) {
+      ERROR("Unable to cache fs for user %s", user);
+    }
+  }
+
+done:
+  ret = pthread_mutex_unlock(&tableMutex);
+  assert(0 == ret);
+  free(user);
+  return fs;
+}
+
+/*
+ * We currently cache a fs handle per-user in this module rather
+ * than use the FileSystem cache in the java client. Therefore
+ * we do not disconnect the fs handle here.
+ */
+int doDisconnect(hdfsFS fs) {
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.h
new file mode 100644
index 0000000..4bddeea
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.h
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_CONNECT_H__
+#define __FUSE_CONNECT_H__
+
+#include "fuse_dfs.h"
+
+hdfsFS doConnectAsUser(const char *nn_uri, int nn_port);
+int doDisconnect(hdfsFS fs);
+int allocFsTable(void);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_context_handle.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_context_handle.h
new file mode 100644
index 0000000..ae07735
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_context_handle.h
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_CONTEXT_HANDLE_H__
+#define __FUSE_CONTEXT_HANDLE_H__
+
+#include <hdfs.h>
+#include <stddef.h>
+#include <sys/types.h>
+
+//
+// Structure to store fuse_dfs specific data
+// this will be created and passed to fuse at startup
+// and fuse will pass it back to us via the context function
+// on every operation.
+//
+typedef struct dfs_context_struct {
+  int debug;
+  char *nn_uri;
+  int nn_port;
+  int read_only;
+  int usetrash;
+  int direct_io;
+  char **protectedpaths;
+  size_t rdbuffer_size;
+} dfs_context;
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.c
new file mode 100644
index 0000000..e218c81
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.c
@@ -0,0 +1,129 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_options.h"
+#include "fuse_impls.h"
+#include "fuse_init.h"
+#include "fuse_connect.h"
+
+#include <string.h>
+#include <stdlib.h>
+
+int is_protected(const char *path) {
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+  assert(dfs != NULL);
+  assert(dfs->protectedpaths);
+
+  int i ;
+  for (i = 0; dfs->protectedpaths[i]; i++) {
+    if (strcmp(path, dfs->protectedpaths[i]) == 0) {
+      return 1;
+    }
+  }
+  return 0;
+}
+
+static struct fuse_operations dfs_oper = {
+  .getattr  = dfs_getattr,
+  .access   = dfs_access,
+  .readdir  = dfs_readdir,
+  .destroy  = dfs_destroy,
+  .init     = dfs_init,
+  .open     = dfs_open,
+  .read     = dfs_read,
+  .symlink  = dfs_symlink,
+  .statfs   = dfs_statfs,
+  .mkdir    = dfs_mkdir,
+  .rmdir    = dfs_rmdir,
+  .rename   = dfs_rename,
+  .unlink   = dfs_unlink,
+  .release  = dfs_release,
+  .create   = dfs_create,
+  .write    = dfs_write,
+  .flush    = dfs_flush,
+  .mknod    = dfs_mknod,
+  .utimens  = dfs_utimens,
+  .chmod    = dfs_chmod,
+  .chown    = dfs_chown,
+  .truncate = dfs_truncate,
+};
+
+int main(int argc, char *argv[])
+{
+  umask(0);
+
+  extern const char *program;  
+  program = argv[0];
+  struct fuse_args args = FUSE_ARGS_INIT(argc, argv);
+
+  memset(&options, 0, sizeof(struct options));
+
+  options.rdbuffer_size = 10*1024*1024; 
+  options.attribute_timeout = 60; 
+  options.entry_timeout = 60;
+
+  if (-1 == fuse_opt_parse(&args, &options, dfs_opts, dfs_options)) {
+    return -1;
+  }
+
+  if (!options.private) {
+    fuse_opt_add_arg(&args, "-oallow_other");
+  }
+
+  if (!options.no_permissions) {
+    fuse_opt_add_arg(&args, "-odefault_permissions");
+  }
+
+  {
+    char buf[1024];
+
+    snprintf(buf, sizeof buf, "-oattr_timeout=%d",options.attribute_timeout);
+    fuse_opt_add_arg(&args, buf);
+
+    snprintf(buf, sizeof buf, "-oentry_timeout=%d",options.entry_timeout);
+    fuse_opt_add_arg(&args, buf);
+  }
+
+  if (options.nn_uri == NULL) {
+    print_usage(argv[0]);
+    exit(0);
+  }
+
+  // Check connection as root
+  if (options.initchecks == 1) {
+    hdfsFS tempFS = hdfsConnectAsUser(options.nn_uri, options.nn_port, "root");
+    if (NULL == tempFS) {
+      const char *cp = getenv("CLASSPATH");
+      const char *ld = getenv("LD_LIBRARY_PATH");
+      ERROR("FATAL: misconfiguration - cannot connect to HDFS");
+      ERROR("LD_LIBRARY_PATH=%s",ld == NULL ? "NULL" : ld);
+      ERROR("CLASSPATH=%s",cp == NULL ? "NULL" : cp);
+      exit(1);
+    }
+    if (doDisconnect(tempFS)) {
+      ERROR("FATAL: unable to disconnect from test filesystem.");
+      exit(1);
+    }
+  }
+
+  int ret = fuse_main(args.argc, args.argv, &dfs_oper, NULL);
+  fuse_opt_free_args(&args);
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.h
new file mode 100644
index 0000000..4554dbd
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs.h
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_DFS_H__
+#define __FUSE_DFS_H__
+
+#define FUSE_USE_VERSION 26
+
+#include <stdio.h>
+#include <string.h>
+#include <errno.h>
+#include <assert.h>
+#include <strings.h>
+#include <syslog.h>
+
+#include <fuse.h>
+#include <fuse/fuse_opt.h>
+
+#include <sys/xattr.h>
+
+#include "config.h"
+
+//
+// Check if a path is in the mount option supplied protected paths.
+//
+int is_protected(const char *path);
+
+#undef INFO
+#define INFO(_fmt, ...) {                       \
+  fprintf(stdout, "INFO %s:%d " _fmt "\n",      \
+          __FILE__, __LINE__, ## __VA_ARGS__);  \
+  syslog(LOG_INFO, "INFO %s:%d " _fmt "\n",     \
+          __FILE__, __LINE__, ## __VA_ARGS__);  \
+}
+
+#undef DEBUG
+#define DEBUG(_fmt, ...) {                      \
+  fprintf(stdout, "DEBUG %s:%d " _fmt "\n",     \
+          __FILE__, __LINE__, ## __VA_ARGS__);  \
+  syslog(LOG_DEBUG, "DEBUG %s:%d " _fmt "\n",   \
+          __FILE__, __LINE__, ## __VA_ARGS__);  \
+}
+
+#undef ERROR
+#define ERROR(_fmt, ...) {                      \
+  fprintf(stderr, "ERROR %s:%d " _fmt "\n",     \
+          __FILE__, __LINE__, ## __VA_ARGS__);  \
+  syslog(LOG_ERR, "ERROR %s:%d " _fmt "\n",     \
+          __FILE__, __LINE__, ## __VA_ARGS__);  \
+}
+
+//#define DOTRACE
+#ifdef DOTRACE
+#define TRACE(x) {        \
+    DEBUG("TRACE %s", x); \
+}
+
+#define TRACE1(x,y) {             \
+    DEBUG("TRACE %s %s\n", x, y); \
+}
+#else
+#define TRACE(x) ; 
+#define TRACE1(x,y) ; 
+#endif
+
+#endif // __FUSE_DFS_H__
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh
new file mode 100755
index 0000000..97239cc
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh
@@ -0,0 +1,46 @@
+#!/usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+export HADOOP_PREFIX=${HADOOP_PREFIX:-/usr/local/share/hadoop}
+
+if [ "$OS_ARCH" = "" ]; then
+export OS_ARCH=amd64
+fi
+
+if [ "$JAVA_HOME" = "" ]; then
+export  JAVA_HOME=/usr/local/java
+fi
+
+if [ "$LD_LIBRARY_PATH" = "" ]; then
+export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/lib
+fi
+
+# If dev build set paths accordingly
+if [ -d $HADOOP_PREFIX/build ]; then
+  export HADOOP_PREFIX=$HADOOP_PREFIX
+  for f in ${HADOOP_PREFIX}/build/*.jar ; do
+    export CLASSPATH=$CLASSPATH:$f
+  done
+  for f in $HADOOP_PREFIX/build/ivy/lib/hadoop-hdfs/common/*.jar ; do
+    export CLASSPATH=$CLASSPATH:$f
+  done
+  export PATH=$HADOOP_PREFIX/build/contrib/fuse-dfs:$PATH
+  export LD_LIBRARY_PATH=$HADOOP_PREFIX/build/c++/lib:$JAVA_HOME/jre/lib/$OS_ARCH/server
+fi
+
+fuse_dfs $@
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_file_handle.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_file_handle.h
new file mode 100644
index 0000000..70cd898
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_file_handle.h
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_FILE_HANDLE_H__
+#define __FUSE_FILE_HANDLE_H__
+
+#include <hdfs.h>
+#include <pthread.h>
+
+/**
+ *
+ * dfs_fh_struct is passed around for open files. Fuse provides a hook (the context) 
+ * for storing file specific data.
+ *
+ * 2 Types of information:
+ * a) a read buffer for performance reasons since fuse is typically called on 4K chunks only
+ * b) the hdfs fs handle 
+ *
+ */
+typedef struct dfs_fh_struct {
+  hdfsFile hdfsFH;
+  char *buf;
+  tSize bufferSize;  //what is the size of the buffer we have
+  off_t buffersStartOffset; //where the buffer starts in the file
+  hdfsFS fs; // for reads/writes need to access as the real user
+  pthread_mutex_t mutex;
+} dfs_fh;
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls.h
new file mode 100644
index 0000000..d0d93e2
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls.h
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#ifndef __FUSE_IMPLS_H__
+#define __FUSE_IMPLS_H__
+
+#include <fuse.h>
+#include <syslog.h>
+
+#include "fuse_context_handle.h"
+
+/**
+ * Implementations of the various fuse hooks.
+ * All of these (should be) thread safe.
+ *
+ */
+
+int dfs_mkdir(const char *path, mode_t mode);
+int dfs_rename(const char *from, const char *to);
+int dfs_getattr(const char *path, struct stat *st);
+int dfs_readdir(const char *path, void *buf, fuse_fill_dir_t filler,
+                off_t offset, struct fuse_file_info *fi);
+int dfs_read(const char *path, char *buf, size_t size, off_t offset,
+                    struct fuse_file_info *fi);
+int dfs_statfs(const char *path, struct statvfs *st);
+int dfs_mkdir(const char *path, mode_t mode);
+int dfs_rename(const char *from, const char *to);
+int dfs_rmdir(const char *path);
+int dfs_unlink(const char *path);
+int dfs_utimens(const char *path, const struct timespec ts[2]);
+int dfs_chmod(const char *path, mode_t mode);
+int dfs_chown(const char *path, uid_t uid, gid_t gid);
+int dfs_open(const char *path, struct fuse_file_info *fi);
+int dfs_write(const char *path, const char *buf, size_t size,
+              off_t offset, struct fuse_file_info *fi);
+int dfs_release (const char *path, struct fuse_file_info *fi);
+int dfs_mknod(const char *path, mode_t mode, dev_t rdev) ;
+int dfs_create(const char *path, mode_t mode, struct fuse_file_info *fi);
+int dfs_flush(const char *path, struct fuse_file_info *fi);
+int dfs_access(const char *path, int mask);
+int dfs_truncate(const char *path, off_t size);
+int dfs_symlink(const char *from, const char *to);
+
+#endif
+
+
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_access.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_access.c
new file mode 100644
index 0000000..033a1c3
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_access.c
@@ -0,0 +1,29 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+
+int dfs_access(const char *path, int mask)
+{
+  TRACE1("access", path)
+  assert(path != NULL);
+  // TODO: HDFS-428
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chmod.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chmod.c
new file mode 100644
index 0000000..2c1e96b
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chmod.c
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_users.h"
+#include "fuse_connect.h"
+
+int dfs_chmod(const char *path, mode_t mode)
+{
+  TRACE1("chmod", path)
+  int ret = 0;
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect to HDFS");
+    ret = -EIO;
+    goto cleanup;
+  }
+
+  if (hdfsChmod(userFS, path, (short)mode)) {
+    ERROR("Could not chmod %s to %d", path, (int)mode);
+    ret = (errno > 0) ? -errno : -EIO;
+    goto cleanup;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chown.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chown.c
new file mode 100644
index 0000000..9c6105d
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_chown.c
@@ -0,0 +1,83 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_users.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+
+#include <stdlib.h>
+
+int dfs_chown(const char *path, uid_t uid, gid_t gid)
+{
+  TRACE1("chown", path)
+
+  int ret = 0;
+  char *user = NULL;
+  char *group = NULL;
+  hdfsFS userFS = NULL;
+
+  // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  user = getUsername(uid);
+  if (NULL == user) {
+    ERROR("Could not lookup the user id string %d",(int)uid); 
+    ret = -EIO;
+    goto cleanup;
+  }
+
+  group = getGroup(gid);
+  if (group == NULL) {
+    ERROR("Could not lookup the group id string %d",(int)gid);
+    ret = -EIO;
+    goto cleanup;
+  } 
+
+  userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect to HDFS");
+    ret = -EIO;
+    goto cleanup;
+  }
+
+  if (hdfsChown(userFS, path, user, group)) {
+    ERROR("Could not chown %s to %d:%d", path, (int)uid, gid);
+    ret = (errno > 0) ? -errno : -EIO;
+    goto cleanup;
+  }
+
+cleanup:
+  if (userFS && doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  if (user) {
+    free(user);
+  }
+  if (group) {
+    free(group);
+  }
+
+  return ret;
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_create.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_create.c
new file mode 100644
index 0000000..256e383
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_create.c
@@ -0,0 +1,27 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+
+int dfs_create(const char *path, mode_t mode, struct fuse_file_info *fi)
+{
+  TRACE1("create", path)
+  fi->flags |= mode;
+  return dfs_open(path, fi);
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_flush.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_flush.c
new file mode 100644
index 0000000..6d4f05c
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_flush.c
@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_file_handle.h"
+
+int dfs_flush(const char *path, struct fuse_file_info *fi) {
+  TRACE1("flush", path)
+
+  // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+  assert(fi);
+
+  if (NULL == (void*)fi->fh) {
+    return  0;
+  }
+
+  // note that fuse calls flush on RO files too and hdfs does not like that and will return an error
+  if (fi->flags & O_WRONLY) {
+
+    dfs_fh *fh = (dfs_fh*)fi->fh;
+    assert(fh);
+    hdfsFile file_handle = (hdfsFile)fh->hdfsFH;
+    assert(file_handle);
+
+    assert(fh->fs);
+    if (hdfsFlush(fh->fs, file_handle) != 0) {
+      ERROR("Could not flush %lx for %s\n",(long)file_handle, path);
+      return -EIO;
+    }
+  }
+
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_getattr.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_getattr.c
new file mode 100644
index 0000000..56f634e
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_getattr.c
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_stat_struct.h"
+#include "fuse_connect.h"
+
+int dfs_getattr(const char *path, struct stat *st)
+{
+  TRACE1("getattr", path)
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(dfs);
+  assert(path);
+  assert(st);
+
+  hdfsFS fs = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (NULL == fs) {
+    ERROR("Could not connect to %s:%d", dfs->nn_uri, dfs->nn_port);
+    return -EIO;
+  }
+
+  int ret = 0;
+  hdfsFileInfo *info = hdfsGetPathInfo(fs,path);
+  if (NULL == info) {
+    ret = -ENOENT;
+    goto cleanup;
+  }
+  fill_stat_structure(&info[0], st);
+
+  // setup hard link info - for a file it is 1 else num entries in a dir + 2 (for . and ..)
+  if (info[0].mKind == kObjectKindDirectory) {
+    int numEntries = 0;
+    hdfsFileInfo *info = hdfsListDirectory(fs,path,&numEntries);
+
+    if (info) {
+      hdfsFreeFileInfo(info,numEntries);
+    }
+    st->st_nlink = numEntries + 2;
+  } else {
+    // not a directory
+    st->st_nlink = 1;
+  }
+
+  // free the info pointer
+  hdfsFreeFileInfo(info,1);
+
+cleanup:
+  if (doDisconnect(fs)) {
+    ERROR("Could not disconnect from filesystem");
+    ret = -EIO;
+  }
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mkdir.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mkdir.c
new file mode 100644
index 0000000..d0624af
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mkdir.c
@@ -0,0 +1,69 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_trash.h"
+#include "fuse_connect.h"
+
+int dfs_mkdir(const char *path, mode_t mode)
+{
+  TRACE1("mkdir", path)
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  if (is_protected(path)) {
+    ERROR("HDFS trying to create directory %s", path);
+    return -EACCES;
+  }
+
+  if (dfs->read_only) {
+    ERROR("HDFS is configured read-only, cannot create directory %s", path);
+    return -EACCES;
+  }
+  
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  // In theory the create and chmod should be atomic.
+
+  int ret = 0;
+  if (hdfsCreateDirectory(userFS, path)) {
+    ERROR("HDFS could not create directory %s", path);
+    ret = (errno > 0) ? -errno : -EIO;
+    goto cleanup;
+  }
+
+  if (hdfsChmod(userFS, path, (short)mode)) {
+    ERROR("Could not chmod %s to %d", path, (int)mode);
+    ret = (errno > 0) ? -errno : -EIO;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mknod.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mknod.c
new file mode 100644
index 0000000..c745cf1
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_mknod.c
@@ -0,0 +1,27 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+
+int dfs_mknod(const char *path, mode_t mode, dev_t rdev)
+{
+  TRACE1("mknod", path);
+  DEBUG("dfs_mknod");
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
new file mode 100644
index 0000000..071590a
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
@@ -0,0 +1,94 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+#include "fuse_file_handle.h"
+
+int dfs_open(const char *path, struct fuse_file_info *fi)
+{
+  TRACE1("open", path)
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(path);
+  assert('/' == *path);
+  assert(dfs);
+
+  int ret = 0;
+
+  // 0x8000 is always passed in and hadoop doesn't like it, so killing it here
+  // bugbug figure out what this flag is and report problem to Hadoop JIRA
+  int flags = (fi->flags & 0x7FFF);
+
+  // retrieve dfs specific data
+  dfs_fh *fh = (dfs_fh*)calloc(1, sizeof (dfs_fh));
+  if (fh == NULL) {
+    ERROR("Malloc of new file handle failed");
+    return -EIO;
+  }
+
+  fh->fs = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (fh->fs == NULL) {
+    ERROR("Could not connect to dfs");
+    return -EIO;
+  }
+
+  if (flags & O_RDWR) {
+    hdfsFileInfo *info = hdfsGetPathInfo(fh->fs,path);
+    if (info == NULL) {
+      // File does not exist (maybe?); interpret it as a O_WRONLY
+      // If the actual error was something else, we'll get it again when
+      // we try to open the file.
+      flags ^= O_RDWR;
+      flags |= O_WRONLY;
+    } else {
+      // File exists; open this as read only.
+      flags ^= O_RDWR;
+      flags |= O_RDONLY;
+    }
+  }
+
+  if ((fh->hdfsFH = hdfsOpenFile(fh->fs, path, flags,  0, 0, 0)) == NULL) {
+    ERROR("Could not open file %s (errno=%d)", path, errno);
+    if (errno == 0 || errno == EINTERNAL) {
+      return -EIO;
+    }
+    return -errno;
+  }
+
+  pthread_mutex_init(&fh->mutex, NULL);
+
+  if (fi->flags & O_WRONLY || fi->flags & O_CREAT) {
+    fh->buf = NULL;
+  } else  {
+    assert(dfs->rdbuffer_size > 0);
+    fh->buf = (char*)malloc(dfs->rdbuffer_size * sizeof(char));
+    if (NULL == fh->buf) {
+      ERROR("Could not allocate memory for a read for file %s\n", path);
+      ret = -EIO;
+    }
+    fh->buffersStartOffset = 0;
+    fh->bufferSize = 0;
+  }
+  fi->fh = (uint64_t)fh;
+
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_read.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_read.c
new file mode 100644
index 0000000..5209261
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_read.c
@@ -0,0 +1,162 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_file_handle.h"
+
+static size_t min(const size_t x, const size_t y) {
+  return x < y ? x : y;
+}
+
+/**
+ * dfs_read
+ *
+ * Reads from dfs or the open file's buffer.  Note that fuse requires that
+ * either the entire read be satisfied or the EOF is hit or direct_io is enabled
+ *
+ */
+int dfs_read(const char *path, char *buf, size_t size, off_t offset,
+                   struct fuse_file_info *fi)
+{
+  TRACE1("read",path)
+  
+  // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(dfs);
+  assert(path);
+  assert(buf);
+  assert(offset >= 0);
+  assert(size >= 0);
+  assert(fi);
+
+  dfs_fh *fh = (dfs_fh*)fi->fh;
+
+  assert(fh != NULL);
+  assert(fh->fs != NULL);
+  assert(fh->hdfsFH != NULL);
+
+  // special case this as simplifies the rest of the logic to know the caller wanted > 0 bytes
+  if (size == 0)
+    return 0;
+
+  // If size is bigger than the read buffer, then just read right into the user supplied buffer
+  if ( size >= dfs->rdbuffer_size) {
+    int num_read;
+    size_t total_read = 0;
+    while (size - total_read > 0 && (num_read = hdfsPread(fh->fs, fh->hdfsFH, offset + total_read, buf + total_read, size - total_read)) > 0) {
+      total_read += num_read;
+    }
+    // if there was an error before satisfying the current read, this logic declares it an error
+    // and does not try to return any of the bytes read. Don't think it matters, so the code
+    // is just being conservative.
+    if (total_read < size && num_read < 0) {
+      total_read = -EIO;
+    }
+    return total_read;
+  }
+
+  //
+  // Critical section - protect from multiple reads in different threads accessing the read buffer
+  // (no returns until end)
+  //
+
+  pthread_mutex_lock(&fh->mutex);
+
+  // used only to check the postcondition of this function - namely that we satisfy
+  // the entire read or EOF is hit.
+  int isEOF = 0;
+  int ret = 0;
+
+  // check if the buffer is empty or
+  // the read starts before the buffer starts or
+  // the read ends after the buffer ends
+
+  if (fh->bufferSize == 0  || 
+      offset < fh->buffersStartOffset || 
+      offset + size > fh->buffersStartOffset + fh->bufferSize) 
+    {
+      // Read into the buffer from DFS
+      int num_read = 0;
+      size_t total_read = 0;
+
+      while (dfs->rdbuffer_size  - total_read > 0 &&
+             (num_read = hdfsPread(fh->fs, fh->hdfsFH, offset + total_read, fh->buf + total_read, dfs->rdbuffer_size - total_read)) > 0) {
+        total_read += num_read;
+      }
+
+      // if there was an error before satisfying the current read, this logic declares it an error
+      // and does not try to return any of the bytes read. Don't think it matters, so the code
+      // is just being conservative.
+      if (total_read < size && num_read < 0) {
+        // invalidate the buffer 
+        fh->bufferSize = 0; 
+        ERROR("pread failed for %s with return code %d", path, (int)num_read);
+        ret = -EIO;
+      } else {
+        // Either EOF, all read or read beyond size, but then there was an error
+        fh->bufferSize = total_read;
+        fh->buffersStartOffset = offset;
+
+        if (dfs->rdbuffer_size - total_read > 0) {
+          // assert(num_read == 0); this should be true since if num_read < 0 handled above.
+          isEOF = 1;
+        }
+      }
+    }
+
+  //
+  // NOTE on EOF, fh->bufferSize == 0 and ret = 0 ,so the logic for copying data into the caller's buffer is bypassed, and
+  //  the code returns 0 as required
+  //
+  if (ret == 0 && fh->bufferSize > 0) {
+
+    assert(offset >= fh->buffersStartOffset);
+    assert(fh->buf);
+
+    const size_t bufferReadIndex = offset - fh->buffersStartOffset;
+    assert(bufferReadIndex >= 0 && bufferReadIndex < fh->bufferSize);
+
+    const size_t amount = min(fh->buffersStartOffset + fh->bufferSize - offset, size);
+    assert(amount >= 0 && amount <= fh->bufferSize);
+
+    const char *offsetPtr = fh->buf + bufferReadIndex;
+    assert(offsetPtr >= fh->buf);
+    assert(offsetPtr + amount <= fh->buf + fh->bufferSize);
+    
+    memcpy(buf, offsetPtr, amount);
+
+    ret = amount;
+  }
+
+  //
+  // Critical section end 
+  //
+  pthread_mutex_unlock(&fh->mutex);
+ 
+  // fuse requires the below and the code should guarantee this assertion
+  // 3 cases on return:
+  //   1. entire read satisfied
+  //   2. partial read and isEOF - including 0 size read
+  //   3. error 
+  assert(ret == size || isEOF || ret < 0);
+
+ return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_readdir.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_readdir.c
new file mode 100644
index 0000000..f6fe48b
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_readdir.c
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_stat_struct.h"
+#include "fuse_connect.h"
+
+int dfs_readdir(const char *path, void *buf, fuse_fill_dir_t filler,
+                       off_t offset, struct fuse_file_info *fi)
+{
+  TRACE1("readdir", path)
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(dfs);
+  assert(path);
+  assert(buf);
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  // Read dirents. Calling a variant that just returns the final path
+  // component (HDFS-975) would save us from parsing it out below.
+  int numEntries = 0;
+  hdfsFileInfo *info = hdfsListDirectory(userFS, path, &numEntries);
+
+  int ret = 0;
+  // NULL means either the directory doesn't exist or maybe IO error.
+  if (NULL == info) {
+    ret = (errno > 0) ? -errno : -ENOENT;
+    goto cleanup;
+  }
+
+  int i ;
+  for (i = 0; i < numEntries; i++) {
+    if (NULL == info[i].mName) {
+      ERROR("Path %s info[%d].mName is NULL", path, i);
+      continue;
+    }
+
+    struct stat st;
+    fill_stat_structure(&info[i], &st);
+
+    // Find the final path component
+    const char *str = strrchr(info[i].mName, '/');
+    if (NULL == str) {
+      ERROR("Invalid URI %s", info[i].mName);
+      continue;
+    }
+    str++;
+
+    // pack this entry into the fuse buffer
+    int res = 0;
+    if ((res = filler(buf,str,&st,0)) != 0) {
+      ERROR("Readdir filler failed: %d\n",res);
+    }
+  }
+
+  // insert '.' and '..'
+  const char *const dots [] = { ".",".."};
+  for (i = 0 ; i < 2 ; i++)
+    {
+      struct stat st;
+      memset(&st, 0, sizeof(struct stat));
+
+      // set to 0 to indicate not supported for directory because we cannot (efficiently) get this info for every subdirectory
+      st.st_nlink =  0;
+
+      // setup stat size and acl meta data
+      st.st_size    = 512;
+      st.st_blksize = 512;
+      st.st_blocks  =  1;
+      st.st_mode    = (S_IFDIR | 0777);
+      st.st_uid     = default_id;
+      st.st_gid     = default_id;
+      // todo fix below times
+      st.st_atime   = 0;
+      st.st_mtime   = 0;
+      st.st_ctime   = 0;
+
+      const char *const str = dots[i];
+
+      // flatten the info using fuse's function into a buffer
+      int res = 0;
+      if ((res = filler(buf,str,&st,0)) != 0) {
+	ERROR("Readdir filler failed: %d\n",res);
+      }
+    }
+  // free the info pointers
+  hdfsFreeFileInfo(info,numEntries);
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+    ERROR("Failed to disconnect %d", errno);
+  }
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_release.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_release.c
new file mode 100644
index 0000000..e15dd57
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_release.c
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_file_handle.h"
+#include "fuse_connect.h"
+
+#include <stdlib.h>
+
+/**
+ * release a fuse_file_info structure.
+ *
+ * When this function is invoked, there are no more references to our
+ * fuse_file_info structure that exist anywhere.  So there is no need for
+ * locking to protect this structure here.
+ *
+ * Another thread could open() the same file, and get a separate, different file
+ * descriptor with a different, separate fuse_file_info structure.  In HDFS,
+ * this results in one writer winning and overwriting everything the other
+ * writer has done.
+ */
+
+int dfs_release (const char *path, struct fuse_file_info *fi) {
+  TRACE1("release", path)
+
+  // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  int ret = 0;
+  dfs_fh *fh = (dfs_fh*)fi->fh;
+  assert(fh);
+  hdfsFile file_handle = (hdfsFile)fh->hdfsFH;
+  if (NULL != file_handle) {
+    if (hdfsCloseFile(fh->fs, file_handle) != 0) {
+      ERROR("Could not close handle %ld for %s\n",(long)file_handle, path);
+      ret = -EIO;
+    }
+  }
+  free(fh->buf);
+  if (doDisconnect(fh->fs)) {
+    ret = -EIO;
+  }
+  pthread_mutex_destroy(&fh->mutex);
+  free(fh);
+  fi->fh = 0;
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rename.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rename.c
new file mode 100644
index 0000000..bbb0462
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rename.c
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_trash.h"
+#include "fuse_connect.h"
+
+int dfs_rename(const char *from, const char *to)
+{
+  TRACE1("rename", from) 
+
+ // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(from);
+  assert(to);
+  assert(dfs);
+
+  assert('/' == *from);
+  assert('/' == *to);
+
+  if (is_protected(from) || is_protected(to)) {
+    ERROR("Could not rename %s to %s", from, to);
+    return -EACCES;
+  }
+
+  if (dfs->read_only) {
+    ERROR("HDFS configured read-only, cannot rename directory %s", from);
+    return -EACCES;
+  }
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  int ret = 0;
+  if (hdfsRename(userFS, from, to)) {
+    ERROR("Rename %s to %s failed", from, to);
+    ret = (errno > 0) ? -errno : -EIO;
+    goto cleanup;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  return ret;
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rmdir.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rmdir.c
new file mode 100644
index 0000000..259040f
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_rmdir.c
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_trash.h"
+#include "fuse_connect.h"
+
+extern const char *const TrashPrefixDir;
+
+int dfs_rmdir(const char *path)
+{
+  TRACE1("rmdir", path)
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  if (is_protected(path)) {
+    ERROR("Trying to delete protected directory %s", path);
+    return -EACCES;
+  }
+
+  if (dfs->read_only) {
+    ERROR("HDFS configured read-only, cannot delete directory %s", path);
+    return -EACCES;
+  }
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  int ret = 0;
+  int numEntries = 0;
+  hdfsFileInfo *info = hdfsListDirectory(userFS,path,&numEntries);
+
+  if (info) {
+    hdfsFreeFileInfo(info, numEntries);
+  }
+
+  if (numEntries) {
+    ret = -ENOTEMPTY;
+    goto cleanup;
+  }
+
+  if (hdfsDeleteWithTrash(userFS, path, dfs->usetrash)) {
+    ERROR("Error trying to delete directory %s", path);
+    ret = -EIO;
+    goto cleanup;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_statfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_statfs.c
new file mode 100644
index 0000000..c7004a9
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_statfs.c
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+
+
+int dfs_statfs(const char *path, struct statvfs *st)
+{
+  TRACE1("statfs",path)
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert(st);
+  assert(dfs);
+
+  memset(st,0,sizeof(struct statvfs));
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  const tOffset cap   = hdfsGetCapacity(userFS);
+  const tOffset used  = hdfsGetUsed(userFS);
+  const tOffset bsize = hdfsGetDefaultBlockSize(userFS);
+
+  if (doDisconnect(userFS)) {
+    return -EIO;
+  }
+
+  st->f_bsize   =  bsize;
+  st->f_frsize  =  bsize;
+  st->f_blocks  =  cap/bsize;
+  st->f_bfree   =  (cap-used)/bsize;
+  st->f_bavail  =  (cap-used)/bsize;
+  st->f_files   =  1000;
+  st->f_ffree   =  500;
+  st->f_favail  =  500;
+  st->f_fsid    =  1023;
+  st->f_flag    =  ST_RDONLY | ST_NOSUID;
+  st->f_namemax =  1023;
+
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_symlink.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_symlink.c
new file mode 100644
index 0000000..be6e7eb
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_symlink.c
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+
+
+int dfs_symlink(const char *from, const char *to)
+{
+  TRACE1("symlink", from)
+  (void)from;
+  (void)to;
+  // bugbug we need the FileSystem to support this posix API
+  return -ENOTSUP;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_truncate.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_truncate.c
new file mode 100644
index 0000000..d09b0c8
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_truncate.c
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+
+/**
+ * For now implement truncate here and only for size == 0.
+ * Weak implementation in that we just delete the file and 
+ * then re-create it, but don't set the user, group, and times to the old
+ * file's metadata. 
+ */
+int dfs_truncate(const char *path, off_t size)
+{
+  TRACE1("truncate", path)
+
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert('/' == *path);
+  assert(dfs);
+
+  if (size != 0) {
+    return 0;
+  }
+
+  int ret = dfs_unlink(path);
+  if (ret != 0) {
+    return ret;
+  }
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    ret = -EIO;
+    goto cleanup;
+  }
+
+  int flags = O_WRONLY | O_CREAT;
+
+  hdfsFile file;
+  if ((file = (hdfsFile)hdfsOpenFile(userFS, path, flags,  0, 0, 0)) == NULL) {
+    ERROR("Could not connect open file %s", path);
+    ret = -EIO;
+    goto cleanup;
+  }
+
+  if (hdfsCloseFile(userFS, file) != 0) {
+    ERROR("Could not close file %s", path);
+    ret = -EIO;
+    goto cleanup;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_unlink.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_unlink.c
new file mode 100644
index 0000000..a3d2034
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_unlink.c
@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+#include "fuse_trash.h"
+extern const char *const TrashPrefixDir;
+
+int dfs_unlink(const char *path)
+{
+  TRACE1("unlink", path)
+
+  int ret = 0;
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  if (is_protected(path)) {
+    ERROR("Trying to delete protected directory %s", path);
+    return -EACCES;
+  }
+
+  if (dfs->read_only) {
+    ERROR("HDFS configured read-only, cannot create directory %s", path);
+    return -EACCES;
+  }
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  if (hdfsDeleteWithTrash(userFS, path, dfs->usetrash)) {
+    ERROR("Could not delete file %s", path);
+    ret = (errno > 0) ? -errno : -EIO;
+    goto cleanup;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  return ret;
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_utimens.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_utimens.c
new file mode 100644
index 0000000..f9144f8
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_utimens.c
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_connect.h"
+
+int dfs_utimens(const char *path, const struct timespec ts[2])
+{
+  TRACE1("utimens", path)
+  int ret = 0;
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+
+  time_t aTime = ts[0].tv_sec;
+  time_t mTime = ts[1].tv_sec;
+
+  hdfsFS userFS = doConnectAsUser(dfs->nn_uri, dfs->nn_port);
+  if (userFS == NULL) {
+    ERROR("Could not connect");
+    return -EIO;
+  }
+
+  if (hdfsUtime(userFS, path, mTime, aTime)) {
+    hdfsFileInfo *info = hdfsGetPathInfo(userFS, path);
+    if (info == NULL) {
+      ret = (errno > 0) ? -errno : -ENOENT;
+      goto cleanup;
+    }
+    // Silently ignore utimens failure for directories, otherwise 
+    // some programs like tar will fail.
+    if (info->mKind == kObjectKindDirectory) {
+      ret = 0;
+    } else {
+      ret = (errno > 0) ? -errno : -EACCES;
+    }
+    goto cleanup;
+  }
+
+cleanup:
+  if (doDisconnect(userFS)) {
+    ret = -EIO;
+  }
+  return ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_write.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_write.c
new file mode 100644
index 0000000..8bb0454
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_write.c
@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_impls.h"
+#include "fuse_file_handle.h"
+
+int dfs_write(const char *path, const char *buf, size_t size,
+                     off_t offset, struct fuse_file_info *fi)
+{
+  TRACE1("write", path)
+
+  // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+  int ret = 0;
+
+  // check params and the context var
+  assert(path);
+  assert(dfs);
+  assert('/' == *path);
+  assert(fi);
+
+  dfs_fh *fh = (dfs_fh*)fi->fh;
+  assert(fh);
+
+  hdfsFile file_handle = (hdfsFile)fh->hdfsFH;
+  assert(file_handle);
+
+  //
+  // Critical section - make the sanity check (tell to see the writes are sequential) and the actual write 
+  // (no returns until end)
+  //
+  pthread_mutex_lock(&fh->mutex);
+
+  tSize length = 0;
+  assert(fh->fs);
+
+  tOffset cur_offset = hdfsTell(fh->fs, file_handle);
+  if (cur_offset != offset) {
+    ERROR("User trying to random access write to a file %d != %d for %s",
+	  (int)cur_offset, (int)offset, path);
+    ret =  -ENOTSUP;
+  } else {
+    length = hdfsWrite(fh->fs, file_handle, buf, size);
+    if (length <= 0) {
+      ERROR("Could not write all bytes for %s %d != %d (errno=%d)", 
+	    path, length, (int)size, errno);
+      if (errno == 0 || errno == EINTERNAL) {
+        ret = -EIO;
+      } else {
+        ret = -errno;
+      }
+    } 
+    if (length != size) {
+      ERROR("Could not write all bytes for %s %d != %d (errno=%d)", 
+	    path, length, (int)size, errno);
+    }
+  }
+
+  //
+  // Critical section end 
+  //
+
+  pthread_mutex_unlock(&fh->mutex);
+
+  return ret == 0 ? length : ret;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
new file mode 100644
index 0000000..6c1c0d0
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
@@ -0,0 +1,125 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_dfs.h"
+#include "fuse_init.h"
+#include "fuse_options.h"
+#include "fuse_context_handle.h"
+#include "fuse_connect.h"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+// Hacked up function to basically do:
+//  protectedpaths = split(options.protected,':');
+
+void init_protectedpaths(dfs_context *dfs) {
+
+  char *tmp = options.protected;
+
+  // handle degenerate case up front.
+  if (tmp == NULL || 0 == *tmp) {
+    dfs->protectedpaths = (char**)malloc(sizeof(char*));
+    dfs->protectedpaths[0] = NULL;
+    return;
+  }
+  assert(tmp);
+
+  if (options.debug) {
+    print_options();
+  }
+
+  int i = 0;
+  while (tmp && (NULL != (tmp = index(tmp,':')))) {
+    tmp++; // pass the ,
+    i++;
+  }
+  i++; // for the last entry
+  i++; // for the final NULL
+  dfs->protectedpaths = (char**)malloc(sizeof(char*)*i);
+  assert(dfs->protectedpaths);
+  tmp = options.protected;
+  int j  = 0;
+  while (NULL != tmp && j < i) {
+    int length;
+    char *eos = index(tmp,':');
+    if (NULL != eos) {
+      length = eos - tmp; // length of this value
+    } else {
+      length = strlen(tmp);
+    }
+    dfs->protectedpaths[j] = (char*)malloc(sizeof(char)*length+1);
+    assert(dfs->protectedpaths[j]);
+    strncpy(dfs->protectedpaths[j], tmp, length);
+    dfs->protectedpaths[j][length] = '\0';
+    if (eos) {
+      tmp = eos + 1;
+    } else {
+      tmp = NULL;
+    }
+    j++;
+  }
+  dfs->protectedpaths[j] = NULL;
+}
+
+
+void *dfs_init(void) {
+  //
+  // Create a private struct of data we will pass to fuse here and which
+  // will then be accessible on every call.
+  //
+  dfs_context *dfs = (dfs_context*)malloc(sizeof(dfs_context));
+  if (NULL == dfs) {
+    ERROR("FATAL: could not malloc dfs_context");
+    exit(1);
+  }
+
+  // initialize the context
+  dfs->debug                 = options.debug;
+  dfs->nn_uri                = options.nn_uri;
+  dfs->nn_port               = options.nn_port;
+  dfs->read_only             = options.read_only;
+  dfs->usetrash              = options.usetrash;
+  dfs->protectedpaths        = NULL;
+  dfs->rdbuffer_size         = options.rdbuffer_size;
+  dfs->direct_io             = options.direct_io;
+
+  INFO("Mounting.  nn_uri=%s, nn_port=%d", dfs->nn_uri, dfs->nn_port);
+
+  init_protectedpaths(dfs);
+  assert(dfs->protectedpaths != NULL);
+
+  if (dfs->rdbuffer_size <= 0) {
+    DEBUG("dfs->rdbuffersize <= 0 = %ld", dfs->rdbuffer_size);
+    dfs->rdbuffer_size = 32768;
+  }
+
+  if (0 != allocFsTable()) {
+    ERROR("FATAL: could not allocate ");
+    exit(1);
+  }
+
+  return (void*)dfs;
+}
+
+
+void dfs_destroy(void *ptr)
+{
+  TRACE("destroy")
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
new file mode 100644
index 0000000..6f17af8
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
@@ -0,0 +1,31 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_INIT_H__
+#define __FUSE_INIT_H__
+
+/**
+ * These are responsible for initializing connections to dfs and internal
+ * data structures and then freeing them.
+ * i.e., what happens on mount and unmount.
+ *
+ */
+void *dfs_init();
+void dfs_destroy (void *ptr);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c
new file mode 100644
index 0000000..3582974
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c
@@ -0,0 +1,188 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fuse_context_handle.h"
+#include "fuse_dfs.h"
+#include "fuse_options.h"
+
+#include <getopt.h>
+#include <stdlib.h>
+
+#define OLD_HDFS_URI_LOCATION "dfs://"
+#define NEW_HDFS_URI_LOCATION "hdfs://"
+
+void print_options() {
+  printf("options:\n"
+	 "\tprotected=%s\n"
+	 "\tserver=%s\n"
+	 "\tport=%d\n"
+	 "\tdebug=%d\n"
+	 "\tread_only=%d\n"
+	 "\tusetrash=%d\n"
+	 "\tentry_timeout=%d\n"
+	 "\tattribute_timeout=%d\n"
+	 "\tprivate=%d\n"
+	 "\trdbuffer_size=%d (KBs)\n", 
+	 options.protected, options.nn_uri, options.nn_port, options.debug,
+	 options.read_only, options.usetrash, options.entry_timeout, 
+	 options.attribute_timeout, options.private, 
+	 (int)options.rdbuffer_size / 1024);
+}
+
+const char *program;
+
+/** macro to define options */
+#define DFSFS_OPT_KEY(t, p, v) { t, offsetof(struct options, p), v }
+
+void print_usage(const char *pname)
+{
+  printf("USAGE: %s [debug] [--help] [--version] "
+	 "[-oprotected=<colon_seped_list_of_paths] [rw] [-onotrash] "
+	 "[-ousetrash] [-obig_writes] [-oprivate (single user)] [ro] "
+	 "[-oserver=<hadoop_servername>] [-oport=<hadoop_port>] "
+	 "[-oentry_timeout=<secs>] [-oattribute_timeout=<secs>] "
+	 "[-odirect_io] [-onopoermissions] [-o<other fuse option>] "
+	 "<mntpoint> [fuse options]\n", pname);
+  printf("NOTE: debugging option for fuse is -debug\n");
+}
+
+
+/** keys for FUSE_OPT_ options */
+enum
+  {
+    KEY_VERSION,
+    KEY_HELP,
+    KEY_USETRASH,
+    KEY_NOTRASH,
+    KEY_RO,
+    KEY_RW,
+    KEY_PRIVATE,
+    KEY_BIGWRITES,
+    KEY_DEBUG,
+    KEY_INITCHECKS,
+    KEY_NOPERMISSIONS,
+    KEY_DIRECTIO,
+  };
+
+struct fuse_opt dfs_opts[] =
+  {
+    DFSFS_OPT_KEY("server=%s", nn_uri, 0),
+    DFSFS_OPT_KEY("entry_timeout=%d", entry_timeout, 0),
+    DFSFS_OPT_KEY("attribute_timeout=%d", attribute_timeout, 0),
+    DFSFS_OPT_KEY("protected=%s", protected, 0),
+    DFSFS_OPT_KEY("port=%d", nn_port, 0),
+    DFSFS_OPT_KEY("rdbuffer=%d", rdbuffer_size,0),
+
+    FUSE_OPT_KEY("private", KEY_PRIVATE),
+    FUSE_OPT_KEY("ro", KEY_RO),
+    FUSE_OPT_KEY("debug", KEY_DEBUG),
+    FUSE_OPT_KEY("initchecks", KEY_INITCHECKS),
+    FUSE_OPT_KEY("nopermissions", KEY_NOPERMISSIONS),
+    FUSE_OPT_KEY("big_writes", KEY_BIGWRITES),
+    FUSE_OPT_KEY("rw", KEY_RW),
+    FUSE_OPT_KEY("usetrash", KEY_USETRASH),
+    FUSE_OPT_KEY("notrash", KEY_NOTRASH),
+    FUSE_OPT_KEY("direct_io", KEY_DIRECTIO),
+    FUSE_OPT_KEY("-v",             KEY_VERSION),
+    FUSE_OPT_KEY("--version",      KEY_VERSION),
+    FUSE_OPT_KEY("-h",             KEY_HELP),
+    FUSE_OPT_KEY("--help",         KEY_HELP),
+    FUSE_OPT_END
+  };
+
+int dfs_options(void *data, const char *arg, int key,  struct fuse_args *outargs)
+{
+  (void) data;
+  int nn_uri_len;
+
+  switch (key) {
+  case FUSE_OPT_KEY_OPT:
+    INFO("Ignoring option %s", arg);
+    return 1;
+  case KEY_VERSION:
+    INFO("%s %s\n", program, _FUSE_DFS_VERSION);
+    exit(0);
+  case KEY_HELP:
+    print_usage(program);
+    exit(0);
+  case KEY_USETRASH:
+    options.usetrash = 1;
+    break;
+  case KEY_NOTRASH:
+    options.usetrash = 1;
+    break;
+  case KEY_RO:
+    options.read_only = 1;
+    break;
+  case KEY_RW:
+    options.read_only = 0;
+    break;
+  case KEY_PRIVATE:
+    options.private = 1;
+    break;
+  case KEY_DEBUG:
+    fuse_opt_add_arg(outargs, "-d");
+    options.debug = 1;
+    break;
+  case KEY_INITCHECKS:
+    options.initchecks = 1;
+    break;
+  case KEY_NOPERMISSIONS:
+    options.no_permissions = 1;
+    break;
+  case KEY_DIRECTIO:
+    options.direct_io = 1;
+    break;
+  case KEY_BIGWRITES:
+#ifdef FUSE_CAP_BIG_WRITES
+    fuse_opt_add_arg(outargs, "-obig_writes");
+#endif
+    break;
+  default: {
+    // try and see if the arg is a URI
+    if (!strstr(arg, "://")) {
+      if (strcmp(arg,"ro") == 0) {
+        options.read_only = 1;
+      } else if (strcmp(arg,"rw") == 0) {
+        options.read_only = 0;
+      } else {
+        INFO("Adding FUSE arg %s", arg);
+        fuse_opt_add_arg(outargs, arg);
+        return 0;
+      }
+    } else {
+      if (options.nn_uri) {
+        INFO("Ignoring option %s because '-server' was already "
+          "specified!", arg);
+        return 1;
+      }
+      if (strstr(arg, OLD_HDFS_URI_LOCATION) == arg) {
+        // For historical reasons, we let people refer to hdfs:// as dfs://
+        nn_uri_len = strlen(NEW_HDFS_URI_LOCATION) + 
+                strlen(arg + strlen(OLD_HDFS_URI_LOCATION)) + 1;
+        options.nn_uri = malloc(nn_uri_len);
+        snprintf(options.nn_uri, nn_uri_len, "%s%s", NEW_HDFS_URI_LOCATION, 
+              arg + strlen(OLD_HDFS_URI_LOCATION));
+      } else {
+        options.nn_uri = strdup(arg);
+      }
+    }
+  }
+  }
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.h
new file mode 100644
index 0000000..4bfc235
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.h
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_OPTIONS_H__
+#define __FUSE_OPTIONS_H__
+
+/** options for fuse_opt.h */
+struct options {
+  char* protected;
+  char* nn_uri;
+  int nn_port;
+  int debug;
+  int read_only;
+  int initchecks;
+  int no_permissions;
+  int usetrash;
+  int entry_timeout;
+  int attribute_timeout;
+  int private;
+  size_t rdbuffer_size;
+  int direct_io;
+} options;
+
+extern struct fuse_opt dfs_opts[];
+void print_options();
+void print_usage(const char *pname);
+int dfs_options(void *data, const char *arg, int key,  struct fuse_args *outargs);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.c
new file mode 100644
index 0000000..e3a0725
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.c
@@ -0,0 +1,112 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <math.h>
+#include <pthread.h>
+#include <grp.h>
+#include <pwd.h>
+
+#include "fuse_dfs.h"
+#include "fuse_stat_struct.h"
+#include "fuse_context_handle.h"
+
+/*
+ * getpwuid and getgrgid return static structs so we safeguard the contents
+ * while retrieving fields using the 2 structs below.
+ * NOTE: if using both, always get the passwd struct firt!
+ */
+extern pthread_mutex_t passwdstruct_mutex; 
+extern pthread_mutex_t groupstruct_mutex;
+
+const int default_id = 99; // nobody  - not configurable since soon uids in dfs, yeah!
+const int blksize = 512;
+
+/**
+ * Converts from a hdfs hdfsFileInfo to a POSIX stat struct
+ *
+ */
+int fill_stat_structure(hdfsFileInfo *info, struct stat *st) 
+{
+  assert(st);
+  assert(info);
+
+  // initialize the stat structure
+  memset(st, 0, sizeof(struct stat));
+
+  // by default: set to 0 to indicate not supported for directory because we cannot (efficiently) get this info for every subdirectory
+  st->st_nlink = (info->mKind == kObjectKindDirectory) ? 0 : 1;
+
+  uid_t owner_id = default_id;
+  if (info->mOwner != NULL) {
+    //
+    // Critical section - protect from concurrent calls in different threads since
+    // the struct below is static.
+    // (no returns until end)
+    //
+    pthread_mutex_lock(&passwdstruct_mutex);
+
+    struct passwd *passwd_info = getpwnam(info->mOwner);
+    owner_id = passwd_info == NULL ? default_id : passwd_info->pw_uid;
+
+    //
+    // End critical section 
+    // 
+    pthread_mutex_unlock(&passwdstruct_mutex);
+
+  } 
+
+  gid_t group_id = default_id;
+
+  if (info->mGroup != NULL) {
+    //
+    // Critical section - protect from concurrent calls in different threads since
+    // the struct below is static.
+    // (no returns until end)
+    //
+    pthread_mutex_lock(&groupstruct_mutex);
+
+    struct group *grp = getgrnam(info->mGroup);
+    group_id = grp == NULL ? default_id : grp->gr_gid;
+
+    //
+    // End critical section 
+    // 
+    pthread_mutex_unlock(&groupstruct_mutex);
+
+  }
+
+  short perm = (info->mKind == kObjectKindDirectory) ? (S_IFDIR | 0777) :  (S_IFREG | 0666);
+  if (info->mPermissions > 0) {
+    perm = (info->mKind == kObjectKindDirectory) ? S_IFDIR:  S_IFREG ;
+    perm |= info->mPermissions;
+  }
+
+  // set stat metadata
+  st->st_size     = (info->mKind == kObjectKindDirectory) ? 4096 : info->mSize;
+  st->st_blksize  = blksize;
+  st->st_blocks   =  ceil(st->st_size/st->st_blksize);
+  st->st_mode     = perm;
+  st->st_uid      = owner_id;
+  st->st_gid      = group_id;
+  st->st_atime    = info->mLastAccess;
+  st->st_mtime    = info->mLastMod;
+  st->st_ctime    = info->mLastMod;
+
+  return 0;
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.h
new file mode 100644
index 0000000..d42a371
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_stat_struct.h
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_STAT_STRUCT_H__
+#define __FUSE_STAT_STRUCT_H__
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+
+#include "hdfs.h"
+
+/**
+ * Converts from a hdfs hdfsFileInfo to a POSIX stat struct
+ * Should be thread safe.
+ */
+int fill_stat_structure(hdfsFileInfo *info, struct stat *st) ;
+
+extern const int default_id;
+extern const int blksize;
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.c
new file mode 100644
index 0000000..6d95b40
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.c
@@ -0,0 +1,126 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include <hdfs.h>
+#include <strings.h>
+
+#include "fuse_dfs.h"
+#include "fuse_trash.h"
+#include "fuse_context_handle.h"
+
+
+const char *const TrashPrefixDir = "/user/root/.Trash";
+const char *const TrashDir = "/user/root/.Trash/Current";
+
+#define TRASH_RENAME_TRIES  100
+
+//
+// NOTE: this function is a c implementation of org.apache.hadoop.fs.Trash.moveToTrash(Path path).
+//
+
+int move_to_trash(const char *item, hdfsFS userFS) {
+
+  // retrieve dfs specific data
+  dfs_context *dfs = (dfs_context*)fuse_get_context()->private_data;
+
+  // check params and the context var
+  assert(item);
+  assert(dfs);
+  assert('/' == *item);
+  assert(rindex(item,'/') >= 0);
+
+
+  char fname[4096]; // or last element of the directory path
+  char parent_dir[4096]; // the directory the fname resides in
+
+  if (strlen(item) > sizeof(fname) - strlen(TrashDir)) {
+    ERROR("Buffer too small to accomodate path of len %d", (int)strlen(item));
+    return -EIO;
+  }
+
+  // separate the file name and the parent directory of the item to be deleted
+  {
+    int length_of_parent_dir = rindex(item, '/') - item ;
+    int length_of_fname = strlen(item) - length_of_parent_dir - 1; // the '/'
+
+    // note - the below strncpys should be safe from overflow because of the check on item's string length above.
+    strncpy(parent_dir, item, length_of_parent_dir);
+    parent_dir[length_of_parent_dir ] = 0;
+    strncpy(fname, item + length_of_parent_dir + 1, strlen(item));
+    fname[length_of_fname + 1] = 0;
+  }
+
+  // create the target trash directory
+  char trash_dir[4096];
+  if (snprintf(trash_dir, sizeof(trash_dir), "%s%s", TrashDir, parent_dir) 
+      >= sizeof trash_dir) {
+    ERROR("Move to trash error target not big enough for %s", item);
+    return -EIO;
+  }
+
+  // create the target trash directory in trash (if needed)
+  if ( hdfsExists(userFS, trash_dir)) {
+    // make the directory to put it in in the Trash - NOTE
+    // hdfsCreateDirectory also creates parents, so Current will be created if it does not exist.
+    if (hdfsCreateDirectory(userFS, trash_dir)) {
+      return -EIO;
+    }
+  }
+
+  //
+  // if the target path in Trash already exists, then append with
+  // a number. Start from 1.
+  //
+  char target[4096];
+  int j ;
+  if ( snprintf(target, sizeof target,"%s/%s",trash_dir, fname) >= sizeof target) {
+    ERROR("Move to trash error target not big enough for %s", item);
+    return -EIO;
+  }
+
+  // NOTE: this loop differs from the java version by capping the #of tries
+  for (j = 1; ! hdfsExists(userFS, target) && j < TRASH_RENAME_TRIES ; j++) {
+    if (snprintf(target, sizeof target,"%s/%s.%d",trash_dir, fname, j) >= sizeof target) {
+      ERROR("Move to trash error target not big enough for %s", item);
+      return -EIO;
+    }
+  }
+  if (hdfsRename(userFS, item, target)) {
+    ERROR("Trying to rename %s to %s", item, target);
+    return -EIO;
+  }
+  return 0;
+} 
+
+
+int hdfsDeleteWithTrash(hdfsFS userFS, const char *path, int useTrash) {
+
+  // move the file to the trash if this is enabled and its not actually in the trash.
+  if (useTrash && strncmp(path, TrashPrefixDir, strlen(TrashPrefixDir)) != 0) {
+    int ret= move_to_trash(path, userFS);
+    return ret;
+  }
+
+  if (hdfsDelete(userFS, path, 1)) {
+    ERROR("Trying to delete the file %s", path);
+    return -EIO;
+  }
+
+  return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.h
new file mode 100644
index 0000000..220ce3d
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_trash.h
@@ -0,0 +1,26 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_TRASH_H__
+#define __FUSE_TRASH_H__
+
+#include <hdfs.h>
+
+int hdfsDeleteWithTrash(hdfsFS userFS, const char *path, int useTrash);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.c
new file mode 100644
index 0000000..d7bdfe7
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.c
@@ -0,0 +1,213 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include <pthread.h>
+#include <grp.h>
+#include <pwd.h>
+#include <stdlib.h>
+
+#include "fuse_dfs.h"
+
+/*
+ * getpwuid and getgrgid return static structs so we safeguard the contents
+ * while retrieving fields using the 2 structs below.
+ * NOTE: if using both, always get the passwd struct firt!
+ */
+pthread_mutex_t passwdstruct_mutex = PTHREAD_MUTEX_INITIALIZER;
+pthread_mutex_t groupstruct_mutex = PTHREAD_MUTEX_INITIALIZER;
+
+/*
+ * Utility for getting the user making the fuse call in char * form
+ * NOTE: if non-null return, the return must be freed by the caller.
+ */
+char *getUsername(uid_t uid) {
+  //
+  // Critical section - protect from concurrent calls in different threads.
+  // since the struct below is static.
+  // (no returns until end)
+  //
+
+  pthread_mutex_lock(&passwdstruct_mutex);
+
+  struct passwd *userinfo = getpwuid(uid);
+  char * ret = userinfo && userinfo->pw_name ? strdup(userinfo->pw_name) : NULL;
+
+  pthread_mutex_unlock(&passwdstruct_mutex);
+
+  //
+  // End critical section 
+  // 
+  return ret;
+}
+
+/**
+ * Cleans up a char ** group pointer
+ */
+
+void freeGroups(char **groups, int numgroups) {
+  if (groups == NULL) {
+    return;
+  }
+  int i ;
+  for (i = 0; i < numgroups; i++) {
+    free(groups[i]);
+  }
+  free(groups);
+}
+
+#define GROUPBUF_SIZE 5
+
+char *getGroup(gid_t gid) {
+  //
+  // Critical section - protect from concurrent calls in different threads.
+  // since the struct below is static.
+  // (no returns until end)
+  //
+
+  pthread_mutex_lock(&groupstruct_mutex);
+
+  struct group* grp = getgrgid(gid);
+  char * ret = grp && grp->gr_name ? strdup(grp->gr_name) : NULL;
+
+  //
+  // End critical section 
+  // 
+  pthread_mutex_unlock(&groupstruct_mutex);
+
+  return ret;
+}
+
+
+/**
+ * Utility for getting the group from the uid
+ * NOTE: if non-null return, the return must be freed by the caller.
+ */
+char *getGroupUid(uid_t uid) {
+  //
+  // Critical section - protect from concurrent calls in different threads
+  // since the structs below are static.
+  // (no returns until end)
+  //
+
+  pthread_mutex_lock(&passwdstruct_mutex);
+  pthread_mutex_lock(&groupstruct_mutex);
+
+  char *ret = NULL;
+  struct passwd *userinfo = getpwuid(uid);
+  if (NULL != userinfo) {
+    struct group* grp = getgrgid( userinfo->pw_gid);
+    ret = grp && grp->gr_name ? strdup(grp->gr_name) : NULL;
+  }
+
+  //
+  // End critical section 
+  // 
+  pthread_mutex_unlock(&groupstruct_mutex);
+  pthread_mutex_unlock(&passwdstruct_mutex);
+
+  return ret;
+}
+
+
+/**
+ * lookup the gid based on the uid
+ */
+gid_t getGidUid(uid_t uid) {
+  //
+  // Critical section - protect from concurrent calls in different threads
+  // since the struct below is static.
+  // (no returns until end)
+  //
+
+  pthread_mutex_lock(&passwdstruct_mutex);
+
+  struct passwd *userinfo = getpwuid(uid);
+  gid_t gid = userinfo == NULL ? 0 : userinfo->pw_gid;
+
+  //
+  // End critical section 
+  // 
+  pthread_mutex_unlock(&passwdstruct_mutex);
+
+  return gid;
+}
+
+/**
+ * Utility for getting the groups for the user making the fuse call in char * form
+ */
+char ** getGroups(uid_t uid, int *num_groups)
+{
+  char *user = getUsername(uid);
+
+  if (user == NULL)
+    return NULL;
+
+  char **groupnames = NULL;
+
+  // see http://www.openldap.org/lists/openldap-devel/199903/msg00023.html
+
+  //#define GETGROUPS_T 1 
+#ifdef GETGROUPS_T
+  *num_groups = GROUPBUF_SIZE;
+
+  gid_t* grouplist = malloc(GROUPBUF_SIZE * sizeof(gid_t)); 
+  assert(grouplist != NULL);
+  gid_t* tmp_grouplist; 
+  int rtr;
+
+  gid_t gid = getGidUid(uid);
+
+  if ((rtr = getgrouplist(user, gid, grouplist, num_groups)) == -1) {
+    // the buffer we passed in is < *num_groups
+    if ((tmp_grouplist = realloc(grouplist, *num_groups * sizeof(gid_t))) != NULL) {
+      grouplist = tmp_grouplist;
+      getgrouplist(user, gid, grouplist, num_groups);
+    }
+  }
+
+  groupnames = (char**)malloc(sizeof(char*)* (*num_groups) + 1);
+  assert(groupnames);
+  int i;
+  for (i=0; i < *num_groups; i++)  {
+    groupnames[i] = getGroup(grouplist[i]);
+    if (groupnames[i] == NULL) {
+      ERROR("Could not lookup group %d\n", (int)grouplist[i]);
+    }
+  } 
+  free(grouplist);
+  assert(user != NULL);
+  groupnames[i] = user;
+  *num_groups = *num_groups + 1;
+#else
+
+  int i = 0;
+  assert(user != NULL);
+  groupnames[i] = user;
+  i++;
+
+  groupnames[i] = getGroupUid(uid);
+  if (groupnames[i]) {
+    i++;
+  }
+
+  *num_groups = i;
+
+#endif
+  return groupnames;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.h
new file mode 100644
index 0000000..d63d916
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_users.h
@@ -0,0 +1,70 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __FUSE_USERS_H__
+#define __FUSE_USERS_H__
+
+#include <grp.h>
+#include <pwd.h>
+#include <pthread.h>
+
+/**
+ * Overall Note:
+ * 1. all these functions should be thread safe.
+ * 2. the ones that return char * or char **, generally require
+ * the caller to free the return value.
+ *
+ */
+
+
+/**
+ * Utility for getting the user making the fuse call in char * form
+ * NOTE: if non-null return, the return must be freed by the caller.
+ */
+char *getUsername(uid_t uid);
+
+
+/**
+ * Cleans up a char ** group pointer
+ */
+void freeGroups(char **groups, int numgroups);
+
+/**
+ * Lookup single group. Caller responsible for free of the return value
+ */
+char *getGroup(gid_t gid);
+
+/**
+ * Utility for getting the group from the uid
+ * NOTE: if non-null return, the return must be freed by the caller.
+ */
+char *getGroupUid(uid_t uid) ;
+
+
+/**
+ * lookup the gid based on the uid
+ */
+
+gid_t getGidUid(uid_t uid);
+
+/**
+ * Utility for getting the groups for the user making the fuse call in char * form
+ */
+char ** getGroups(uid_t uid, int *num_groups);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/TestFuseDFS.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/TestFuseDFS.java
new file mode 100644
index 0000000..c9827da
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/TestFuseDFS.java
@@ -0,0 +1,369 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.*;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.concurrent.atomic.*;
+
+import org.apache.log4j.Level;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.impl.Log4JLogger;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.fs.permission.*;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.util.StringUtils;
+
+import org.junit.Test;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import static org.junit.Assert.*;
+
+/**
+ * Basic functional tests on a fuse-dfs mount.
+ */
+public class TestFuseDFS {
+
+  private static MiniDFSCluster cluster;
+  private static FileSystem fs;
+  private static Runtime r;
+  private static String mountPoint;
+
+  private static final Log LOG = LogFactory.getLog(TestFuseDFS.class);
+  {
+    ((Log4JLogger)LOG).getLogger().setLevel(Level.ALL);
+  }
+
+  /** Dump the given intput stream to stderr */
+  private static void dumpInputStream(InputStream is) throws IOException {
+    int len;
+    do {
+      byte b[] = new byte[is.available()];
+      len = is.read(b);
+      System.out.println("Read "+len+" bytes");
+      System.out.write(b, 0, b.length);
+    } while (len > 0);
+  }
+
+  /** 
+   * Wait for the given process to return and check that it exited
+   * as required. Log if the process failed.
+   */
+  private static void checkProcessRet(Process p, boolean expectPass) 
+      throws IOException {
+    try {
+      int ret = p.waitFor();
+      if (ret != 0) {
+	dumpInputStream(p.getErrorStream());
+      }
+      if (expectPass) {
+	assertEquals(0, ret);
+      } else {
+	assertTrue(ret != 0);
+      }
+    } catch (InterruptedException ie) {
+      fail("Process interrupted: "+ie.getMessage());
+    }
+  }
+
+  /** Exec the given command and assert it executed successfully */
+  private static void execWaitRet(String cmd) throws IOException {
+    LOG.debug("EXEC "+cmd);
+    Process p = r.exec(cmd);
+    try {
+      p.waitFor();
+    } catch (InterruptedException ie) {
+      fail("Process interrupted: "+ie.getMessage());
+    }
+  }
+
+  /** Exec the given command and assert it executed successfully */
+  private static void execIgnoreRet(String cmd) throws IOException {
+    LOG.debug("EXEC "+cmd);
+    r.exec(cmd);
+  }
+
+  /** Exec the given command and assert it executed successfully */
+  private static void execAssertSucceeds(String cmd) throws IOException {
+    LOG.debug("EXEC "+cmd);
+    checkProcessRet(r.exec(cmd), true);
+  }
+
+  /** Exec the given command, assert it returned an error code */
+  private static void execAssertFails(String cmd) throws IOException {
+    LOG.debug("EXEC "+cmd);
+    checkProcessRet(r.exec(cmd), false);
+  }
+
+  /** Create and write the given file */
+  private static void createFile(File f, String s) throws IOException {
+    InputStream is = new ByteArrayInputStream(s.getBytes());
+    FileOutputStream fos = new FileOutputStream(f);
+    IOUtils.copyBytes(is, fos, s.length(), true);
+  }
+
+  /** Check that the given file exists with the given contents */
+  private static void checkFile(File f, String expectedContents) 
+      throws IOException {
+    FileInputStream fi = new FileInputStream(f);
+    int len = expectedContents.length();
+    byte[] b = new byte[len];
+    try {
+      IOUtils.readFully(fi, b, 0, len);
+    } catch (IOException ie) {
+      fail("Reading "+f.getName()+" failed with "+ie.getMessage());
+    } finally {
+      fi.close(); // NB: leaving f unclosed prevents unmount
+    }
+    String s = new String(b, 0, len);
+    assertEquals("File content differs", expectedContents, s);
+  }
+
+  /** Run a fuse-dfs process to mount the given DFS */
+  private static void establishMount(URI uri) throws IOException  {
+    Runtime r = Runtime.getRuntime();
+    String cp = System.getProperty("java.class.path");
+
+    String buildTestDir = System.getProperty("build.test");
+    String fuseCmd = buildTestDir + "/../fuse_dfs";
+    String libHdfs = buildTestDir + "/../../../c++/lib";
+
+    String arch = System.getProperty("os.arch");
+    String jvm = System.getProperty("java.home") + "/lib/" + arch + "/server";
+    String lp = System.getProperty("LD_LIBRARY_PATH")+":"+libHdfs+":"+jvm;
+    LOG.debug("LD_LIBRARY_PATH=" + lp);
+
+    String nameNode = 
+      "dfs://" + uri.getHost() + ":" + String.valueOf(uri.getPort());
+
+    // NB: We're mounting via an unprivileged user, therefore
+    // user_allow_other needs to be set in /etc/fuse.conf, which also
+    // needs to be world readable.
+    String mountCmd[] = {
+      fuseCmd, nameNode, mountPoint,
+      // "-odebug",              // Don't daemonize
+      "-obig_writes",            // Allow >4kb writes
+      "-oentry_timeout=0.1",     // Don't cache dents long
+      "-oattribute_timeout=0.1", // Don't cache attributes long
+      "-ordbuffer=32768",        // Read buffer size in kb
+      "rw"
+    };
+
+    String [] env = {
+      "CLASSPATH="+cp,
+      "LD_LIBRARY_PATH="+lp,
+      "PATH=/usr/bin:/bin"
+    };
+
+    execWaitRet("fusermount -u " + mountPoint);
+    execAssertSucceeds("rm -rf " + mountPoint);
+    execAssertSucceeds("mkdir -p " + mountPoint);
+
+    // Mount the mini cluster
+    try {
+      Process fuseProcess = r.exec(mountCmd, env);
+      assertEquals(0, fuseProcess.waitFor());
+    } catch (InterruptedException ie) {
+      fail("Failed to mount");
+    }
+  }
+
+  /** Tear down the fuse-dfs process and mount */
+  private static void teardownMount() throws IOException {
+    execWaitRet("fusermount -u " + mountPoint);
+  }
+
+  @BeforeClass
+  public static void startUp() throws IOException {
+    Configuration conf = new HdfsConfiguration();
+    r = Runtime.getRuntime();
+    mountPoint = System.getProperty("build.test") + "/mnt";
+    conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, false);
+    cluster = new MiniDFSCluster.Builder(conf).build();
+    cluster.waitClusterUp();
+    fs = cluster.getFileSystem();
+    establishMount(fs.getUri());
+  }
+
+  @AfterClass
+  public static void tearDown() throws IOException {
+    // Unmount before taking down the mini cluster
+    // so no outstanding operations hang.
+    teardownMount();
+    if (fs != null) {
+      fs.close();
+    }
+    if (cluster != null) {
+      cluster.shutdown();
+    }
+  }
+
+  /** Test basic directory creation, access, removal */
+  @Test
+  public void testBasicDir() throws IOException {
+    File d = new File(mountPoint, "dir1");
+
+    // Mkdir, access and rm via the mount
+    execAssertSucceeds("mkdir " + d.getAbsolutePath());
+    execAssertSucceeds("ls " + d.getAbsolutePath());
+    execAssertSucceeds("rmdir " + d.getAbsolutePath());
+
+    // The dir should no longer exist
+    execAssertFails("ls " + d.getAbsolutePath());
+  }
+
+  /** Test basic file creation and writing */
+  @Test
+  public void testCreate() throws IOException {
+    final String contents = "hello world";
+    File f = new File(mountPoint, "file1");
+
+    // Create and access via the mount
+    createFile(f, contents);
+
+    // XX avoids premature EOF
+    try {
+      Thread.sleep(1000);
+    } catch (InterruptedException ie) { }
+
+    checkFile(f, contents);
+
+    // Cat, stat and delete via the mount
+    execAssertSucceeds("cat " + f.getAbsolutePath());
+    execAssertSucceeds("stat " + f.getAbsolutePath());
+    execAssertSucceeds("rm " + f.getAbsolutePath());
+
+    // The file should no longer exist
+    execAssertFails("ls " + f.getAbsolutePath());
+  }
+
+  /** Test creating a file via touch */
+  @Test
+  public void testTouch() throws IOException {
+    File f = new File(mountPoint, "file1");
+    execAssertSucceeds("touch " + f.getAbsolutePath());
+    execAssertSucceeds("rm " + f.getAbsolutePath());
+  }
+
+  /** Test random access to a file */
+  @Test
+  public void testRandomAccess() throws IOException {
+    final String contents = "hello world";
+    File f = new File(mountPoint, "file1");
+
+    createFile(f, contents);
+
+    RandomAccessFile raf = new RandomAccessFile(f, "rw");
+    raf.seek(f.length());
+    try {
+      raf.write('b');
+    } catch (IOException e) {
+      // Expected: fuse-dfs not yet support append
+      assertEquals("Operation not supported", e.getMessage());
+    } finally {
+      raf.close();
+    }
+
+    raf = new RandomAccessFile(f, "rw");
+    raf.seek(0);
+    try {
+      raf.write('b');
+      fail("Over-wrote existing bytes");
+    } catch (IOException e) {
+      // Expected: can-not overwrite a file
+      assertEquals("Invalid argument", e.getMessage());
+    } finally {
+      raf.close();
+    }
+    execAssertSucceeds("rm " + f.getAbsolutePath());
+  }
+
+  /** Test copying a set of files from the mount to itself */
+  @Test
+  public void testCopyFiles() throws IOException {
+    final String contents = "hello world";
+    File d1 = new File(mountPoint, "dir1");
+    File d2 = new File(mountPoint, "dir2");
+
+    // Create and populate dir1 via the mount
+    execAssertSucceeds("mkdir " + d1.getAbsolutePath());
+    for (int i = 0; i < 5; i++) {
+      createFile(new File(d1, "file"+i), contents);
+    }
+    assertEquals(5, d1.listFiles().length);
+
+    // Copy dir from the mount to the mount
+    execAssertSucceeds("cp -r " + d1.getAbsolutePath() +
+                       " " + d2.getAbsolutePath());
+    assertEquals(5, d2.listFiles().length);
+
+    // Access all the files in the dirs and remove them
+    execAssertSucceeds("find " + d1.getAbsolutePath());
+    execAssertSucceeds("find " + d2.getAbsolutePath());
+    execAssertSucceeds("rm -r " + d1.getAbsolutePath());
+    execAssertSucceeds("rm -r " + d2.getAbsolutePath());
+  }
+
+  /** Test concurrent creation and access of the mount */
+  @Test
+  public void testMultipleThreads() throws IOException {
+    ArrayList<Thread> threads = new ArrayList<Thread>();
+    final AtomicReference<String> errorMessage = new AtomicReference<String>();
+
+    for (int i = 0; i < 10; i++) {
+      Thread t = new Thread() {
+	  public void run() {
+	    try {
+	      File d = new File(mountPoint, "dir"+getId());
+	      execWaitRet("mkdir " + d.getAbsolutePath());
+	      for (int j = 0; j < 10; j++) {
+		File f = new File(d, "file"+j);
+		final String contents = "thread "+getId()+" "+j;
+		createFile(f, contents);
+	      }
+	      for (int j = 0; j < 10; j++) {
+		File f = new File(d, "file"+j);
+		execWaitRet("cat " + f.getAbsolutePath());
+		execWaitRet("rm " + f.getAbsolutePath());
+	      }
+	      execWaitRet("rmdir " + d.getAbsolutePath());
+	    } catch (IOException ie) {
+	      errorMessage.set(
+		String.format("Exception %s", 
+			      StringUtils.stringifyException(ie)));
+	    }
+          }
+	};
+      t.start();
+      threads.add(t);
+    }
+
+    for (Thread t : threads) {
+      try {
+	t.join();
+      } catch (InterruptedException ie) {
+	fail("Thread interrupted: "+ie.getMessage());
+      }
+    }
+
+    assertNull(errorMessage.get(), errorMessage.get());
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.c
deleted file mode 100644
index 8ded7bf..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.c
+++ /dev/null
@@ -1,2519 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hdfs.h"
-#include "hdfsJniHelper.h"
-
-#include <stdio.h>
-#include <string.h>
-
-/* Some frequently used Java paths */
-#define HADOOP_CONF     "org/apache/hadoop/conf/Configuration"
-#define HADOOP_PATH     "org/apache/hadoop/fs/Path"
-#define HADOOP_LOCALFS  "org/apache/hadoop/fs/LocalFileSystem"
-#define HADOOP_FS       "org/apache/hadoop/fs/FileSystem"
-#define HADOOP_FSSTATUS "org/apache/hadoop/fs/FsStatus"
-#define HADOOP_BLK_LOC  "org/apache/hadoop/fs/BlockLocation"
-#define HADOOP_DFS      "org/apache/hadoop/hdfs/DistributedFileSystem"
-#define HADOOP_ISTRM    "org/apache/hadoop/fs/FSDataInputStream"
-#define HADOOP_OSTRM    "org/apache/hadoop/fs/FSDataOutputStream"
-#define HADOOP_STAT     "org/apache/hadoop/fs/FileStatus"
-#define HADOOP_FSPERM   "org/apache/hadoop/fs/permission/FsPermission"
-#define JAVA_NET_ISA    "java/net/InetSocketAddress"
-#define JAVA_NET_URI    "java/net/URI"
-#define JAVA_STRING     "java/lang/String"
-
-#define JAVA_VOID       "V"
-
-/* Macros for constructing method signatures */
-#define JPARAM(X)           "L" X ";"
-#define JARRPARAM(X)        "[L" X ";"
-#define JMETHOD1(X, R)      "(" X ")" R
-#define JMETHOD2(X, Y, R)   "(" X Y ")" R
-#define JMETHOD3(X, Y, Z, R)   "(" X Y Z")" R
-
-#define KERBEROS_TICKET_CACHE_PATH "hadoop.security.kerberos.ticket.cache.path"
-
-// Bit fields for hdfsFile_internal flags
-#define HDFS_FILE_SUPPORTS_DIRECT_READ (1<<0)
-
-tSize readDirect(hdfsFS fs, hdfsFile f, void* buffer, tSize length);
-
-/**
- * The C equivalent of org.apache.org.hadoop.FSData(Input|Output)Stream .
- */
-enum hdfsStreamType
-{
-    UNINITIALIZED = 0,
-    INPUT = 1,
-    OUTPUT = 2,
-};
-
-/**
- * The 'file-handle' to a file in hdfs.
- */
-struct hdfsFile_internal {
-    void* file;
-    enum hdfsStreamType type;
-    int flags;
-};
-    
-int hdfsFileIsOpenForRead(hdfsFile file)
-{
-    return (file->type == INPUT);
-}
-
-int hdfsFileIsOpenForWrite(hdfsFile file)
-{
-    return (file->type == OUTPUT);
-}
-
-int hdfsFileUsesDirectRead(hdfsFile file)
-{
-    return !!(file->flags & HDFS_FILE_SUPPORTS_DIRECT_READ);
-}
-
-void hdfsFileDisableDirectRead(hdfsFile file)
-{
-    file->flags &= ~HDFS_FILE_SUPPORTS_DIRECT_READ;
-}
-
-/**
- * hdfsJniEnv: A wrapper struct to be used as 'value'
- * while saving thread -> JNIEnv* mappings
- */
-typedef struct
-{
-    JNIEnv* env;
-} hdfsJniEnv;
-
-
-
-/**
- * Helper function to destroy a local reference of java.lang.Object
- * @param env: The JNIEnv pointer. 
- * @param jFile: The local reference of java.lang.Object object
- * @return None.
- */
-static void destroyLocalReference(JNIEnv *env, jobject jObject)
-{
-  if (jObject)
-    (*env)->DeleteLocalRef(env, jObject);
-}
-
-
-/**
- * Helper function to create a org.apache.hadoop.fs.Path object.
- * @param env: The JNIEnv pointer. 
- * @param path: The file-path for which to construct org.apache.hadoop.fs.Path
- * object.
- * @return Returns a jobject on success and NULL on error.
- */
-static jobject constructNewObjectOfPath(JNIEnv *env, const char *path)
-{
-    //Construct a java.lang.String object
-    jstring jPathString = (*env)->NewStringUTF(env, path); 
-
-    //Construct the org.apache.hadoop.fs.Path object
-    jobject jPath =
-        constructNewObjectOfClass(env, NULL, "org/apache/hadoop/fs/Path",
-                                  "(Ljava/lang/String;)V", jPathString);
-    if (jPath == NULL) {
-        fprintf(stderr, "Can't construct instance of class "
-                "org.apache.hadoop.fs.Path for %s\n", path);
-        errno = EINTERNAL;
-        return NULL;
-    }
-
-    // Destroy the local reference to the java.lang.String object
-    destroyLocalReference(env, jPathString);
-
-    return jPath;
-}
-
-
-/**
- * Helper function to translate an exception into a meaningful errno value.
- * @param exc: The exception.
- * @param env: The JNIEnv Pointer.
- * @param method: The name of the method that threw the exception. This
- * may be format string to be used in conjuction with additional arguments.
- * @return Returns a meaningful errno value if possible, or EINTERNAL if not.
- */
-static int errnoFromException(jthrowable exc, JNIEnv *env,
-                              const char *method, ...)
-{
-    va_list ap;
-    int errnum = 0;
-    char *excClass = NULL;
-
-    if (exc == NULL)
-        goto default_error;
-
-    if ((excClass = classNameOfObject((jobject) exc, env)) == NULL) {
-      errnum = EINTERNAL;
-      goto done;
-    }
-
-    if (!strcmp(excClass, "java.lang.UnsupportedOperationException")) {
-      errnum = ENOTSUP;
-      goto done;
-    }
-
-    if (!strcmp(excClass, "org.apache.hadoop.security."
-                "AccessControlException")) {
-        errnum = EACCES;
-        goto done;
-    }
-
-    if (!strcmp(excClass, "org.apache.hadoop.hdfs.protocol."
-                "QuotaExceededException")) {
-        errnum = EDQUOT;
-        goto done;
-    }
-
-    if (!strcmp(excClass, "java.io.FileNotFoundException")) {
-        errnum = ENOENT;
-        goto done;
-    }
-
-    //TODO: interpret more exceptions; maybe examine exc.getMessage()
-
-default_error:
-
-    //Can't tell what went wrong, so just punt
-    (*env)->ExceptionDescribe(env);
-    fprintf(stderr, "Call to ");
-    va_start(ap, method);
-    vfprintf(stderr, method, ap);
-    va_end(ap);
-    fprintf(stderr, " failed!\n");
-    errnum = EINTERNAL;
-
-done:
-
-    (*env)->ExceptionClear(env);
-
-    if (excClass != NULL)
-        free(excClass);
-
-    return errnum;
-}
-
-/**
- * Set a configuration value.
- *
- * @param env               The JNI environment
- * @param jConfiguration    The configuration object to modify
- * @param key               The key to modify
- * @param value             The value to set the key to
- *
- * @return                  0 on success; error code otherwise
- */
-static int hadoopConfSet(JNIEnv *env, jobject jConfiguration,
-        const char *key, const char *value)
-{
-    int ret;
-    jthrowable jExc = NULL;
-    jstring jkey = NULL, jvalue = NULL;
-    char buf[1024];
-
-    jkey = (*env)->NewStringUTF(env, key);
-    if (!jkey) {
-        ret = ENOMEM;
-        goto done;
-    }
-    jvalue = (*env)->NewStringUTF(env, value);
-    if (!jvalue) {
-        ret = ENOMEM;
-        goto done;
-    }
-    ret = invokeMethod(env, NULL, &jExc, INSTANCE, jConfiguration,
-            HADOOP_CONF, "set", JMETHOD2(JPARAM(JAVA_STRING),
-                                         JPARAM(JAVA_STRING), JAVA_VOID),
-            jkey, jvalue);
-    if (ret) {
-        snprintf(buf, sizeof(buf), "hadoopConfSet(%s, %s)", key, value);
-        ret = errnoFromException(jExc, env, buf);
-        goto done;
-    }
-done:
-    if (jkey)
-        destroyLocalReference(env, jkey);
-    if (jvalue)
-        destroyLocalReference(env, jvalue);
-    if (ret)
-        errno = ret;
-    return ret;
-}
-
-/**
- * Convert a Java string into a C string.
- *
- * @param env               The JNI environment
- * @param jStr              The Java string to convert
- * @param cstr              (out param) the C string.
- *                          This will be set to a dynamically allocated
- *                          UTF-8 C string on success.
- *
- * @return                  0 on success; error code otherwise
- */
-static int jStrToCstr(JNIEnv *env, jstring jstr, char **cstr)
-{
-    char *tmp;
-
-    tmp = (*env)->GetStringUTFChars(env, jstr, NULL);
-    *cstr = strdup(tmp);
-    (*env)->ReleaseStringUTFChars(env, jstr, tmp);
-    return 0;
-}
-
-static int hadoopConfGet(JNIEnv *env, jobject jConfiguration,
-        const char *key, char **val)
-{
-    int ret;
-    jthrowable jExc = NULL;
-    jvalue jVal;
-    jstring jkey = NULL;
-    char buf[1024];
-
-    jkey = (*env)->NewStringUTF(env, key);
-    if (!jkey) {
-        ret = ENOMEM;
-        goto done;
-    }
-    ret = invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration,
-            HADOOP_CONF, "get", JMETHOD1(JPARAM(JAVA_STRING),
-                                         JPARAM(JAVA_STRING)), jkey);
-    if (ret) {
-        snprintf(buf, sizeof(buf), "hadoopConfGet(%s)", key);
-        ret = errnoFromException(jExc, env, buf);
-        goto done;
-    }
-    if (!jVal.l) {
-        *val = NULL;
-        goto done;
-    }
-
-    ret = jStrToCstr(env, jVal.l, val);
-    if (ret)
-        goto done;
-done:
-    if (jkey)
-        destroyLocalReference(env, jkey);
-    if (ret)
-        errno = ret;
-    return ret;
-}
-
-int hdfsConfGet(const char *key, char **val)
-{
-    JNIEnv *env;
-    int ret;
-    jobject jConfiguration = NULL;
-
-    env = getJNIEnv();
-    if (env == NULL) {
-      ret = EINTERNAL;
-      goto done;
-    }
-    jConfiguration = constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
-    if (jConfiguration == NULL) {
-        fprintf(stderr, "Can't construct instance of class "
-                "org.apache.hadoop.conf.Configuration\n");
-        ret = EINTERNAL;
-        goto done;
-    }
-    ret = hadoopConfGet(env, jConfiguration, key, val);
-    if (ret)
-        goto done;
-    ret = 0;
-done:
-    if (jConfiguration)
-        destroyLocalReference(env, jConfiguration);
-    if (ret)
-        errno = ret;
-    return ret;
-}
-
-void hdfsConfFree(char *val)
-{
-    free(val);
-}
-
-struct hdfsBuilder {
-    int forceNewInstance;
-    const char *nn;
-    tPort port;
-    const char *kerbTicketCachePath;
-    const char *userName;
-};
-
-struct hdfsBuilder *hdfsNewBuilder(void)
-{
-    struct hdfsBuilder *bld = calloc(1, sizeof(struct hdfsBuilder));
-    if (!bld) {
-        errno = ENOMEM;
-        return NULL;
-    }
-    return bld;
-}
-
-void hdfsFreeBuilder(struct hdfsBuilder *bld)
-{
-    free(bld);
-}
-
-void hdfsBuilderSetForceNewInstance(struct hdfsBuilder *bld)
-{
-    bld->forceNewInstance = 1;
-}
-
-void hdfsBuilderSetNameNode(struct hdfsBuilder *bld, const char *nn)
-{
-    bld->nn = nn;
-}
-
-void hdfsBuilderSetNameNodePort(struct hdfsBuilder *bld, tPort port)
-{
-    bld->port = port;
-}
-
-void hdfsBuilderSetUserName(struct hdfsBuilder *bld, const char *userName)
-{
-    bld->userName = userName;
-}
-
-void hdfsBuilderSetKerbTicketCachePath(struct hdfsBuilder *bld,
-                                       const char *kerbTicketCachePath)
-{
-    bld->kerbTicketCachePath = kerbTicketCachePath;
-}
-
-hdfsFS hdfsConnect(const char* host, tPort port)
-{
-    struct hdfsBuilder *bld = hdfsNewBuilder();
-    if (!bld)
-        return NULL;
-    hdfsBuilderSetNameNode(bld, host);
-    hdfsBuilderSetNameNodePort(bld, port);
-    return hdfsBuilderConnect(bld);
-}
-
-/** Always return a new FileSystem handle */
-hdfsFS hdfsConnectNewInstance(const char* host, tPort port)
-{
-    struct hdfsBuilder *bld = hdfsNewBuilder();
-    if (!bld)
-        return NULL;
-    hdfsBuilderSetNameNode(bld, host);
-    hdfsBuilderSetNameNodePort(bld, port);
-    hdfsBuilderSetForceNewInstance(bld);
-    return hdfsBuilderConnect(bld);
-}
-
-hdfsFS hdfsConnectAsUser(const char* host, tPort port, const char *user)
-{
-    struct hdfsBuilder *bld = hdfsNewBuilder();
-    if (!bld)
-        return NULL;
-    hdfsBuilderSetNameNode(bld, host);
-    hdfsBuilderSetNameNodePort(bld, port);
-    hdfsBuilderSetUserName(bld, user);
-    return hdfsBuilderConnect(bld);
-}
-
-/** Always return a new FileSystem handle */
-hdfsFS hdfsConnectAsUserNewInstance(const char* host, tPort port,
-        const char *user)
-{
-    struct hdfsBuilder *bld = hdfsNewBuilder();
-    if (!bld)
-        return NULL;
-    hdfsBuilderSetNameNode(bld, host);
-    hdfsBuilderSetNameNodePort(bld, port);
-    hdfsBuilderSetForceNewInstance(bld);
-    hdfsBuilderSetUserName(bld, user);
-    return hdfsBuilderConnect(bld);
-}
-
-
-/**
- * Calculate the effective URI to use, given a builder configuration.
- *
- * If there is not already a URI scheme, we prepend 'hdfs://'.
- *
- * If there is not already a port specified, and a port was given to the
- * builder, we suffix that port.  If there is a port specified but also one in
- * the URI, that is an error.
- *
- * @param bld       The hdfs builder object
- * @param uri       (out param) dynamically allocated string representing the
- *                  effective URI
- *
- * @return          0 on success; error code otherwise
- */
-static int calcEffectiveURI(struct hdfsBuilder *bld, char ** uri)
-{
-    const char *scheme;
-    char suffix[64];
-    const char *lastColon;
-    char *u;
-    size_t uriLen;
-
-    if (!bld->nn)
-        return EINVAL;
-    scheme = (strstr(bld->nn, "://")) ? "" : "hdfs://";
-    if (bld->port == 0) {
-        suffix[0] = '\0';
-    } else {
-        lastColon = rindex(bld->nn, ':');
-        if (lastColon && (strspn(lastColon + 1, "0123456789") ==
-                          strlen(lastColon + 1))) {
-            fprintf(stderr, "port %d was given, but URI '%s' already "
-                "contains a port!\n", bld->port, bld->nn);
-            return EINVAL;
-        }
-        snprintf(suffix, sizeof(suffix), ":%d", bld->port);
-    }
-
-    uriLen = strlen(scheme) + strlen(bld->nn) + strlen(suffix);
-    u = malloc((uriLen + 1) * (sizeof(char)));
-    if (!u) {
-        fprintf(stderr, "calcEffectiveURI: out of memory");
-        return ENOMEM;
-    }
-    snprintf(u, uriLen + 1, "%s%s%s", scheme, bld->nn, suffix);
-    *uri = u;
-    return 0;
-}
-
-hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
-{
-    JNIEnv *env = 0;
-    jobject gFsRef = NULL;
-    jobject jConfiguration = NULL, jFS = NULL, jURI = NULL, jCachePath = NULL;
-    jstring jURIString = NULL, jUserString = NULL;
-    jvalue  jVal;
-    jthrowable jExc = NULL;
-    char *cURI = 0;
-    int ret = 0;
-
-    //Get the JNIEnv* corresponding to current thread
-    env = getJNIEnv();
-    if (env == NULL) {
-      ret = EINTERNAL;
-      goto done;
-    }
-
-    //  jConfiguration = new Configuration();
-    jConfiguration = constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
-    if (jConfiguration == NULL) {
-        fprintf(stderr, "Can't construct instance of class "
-                "org.apache.hadoop.conf.Configuration\n");
-      goto done;
-    }
- 
-    //Check what type of FileSystem the caller wants...
-    if (bld->nn == NULL) {
-        // Get a local filesystem.
-        if (bld->forceNewInstance) {
-            // fs = FileSytem::newInstanceLocal(conf);
-            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS,
-                    "newInstanceLocal", JMETHOD1(JPARAM(HADOOP_CONF),
-                    JPARAM(HADOOP_LOCALFS)), jConfiguration)) {
-                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                           "FileSystem::newInstanceLocal");
-                goto done;
-            }
-            jFS = jVal.l;
-        } else {
-            // fs = FileSytem::getLocal(conf);
-            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS, "getLocal",
-                             JMETHOD1(JPARAM(HADOOP_CONF),
-                                      JPARAM(HADOOP_LOCALFS)),
-                             jConfiguration) != 0) {
-                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                           "FileSystem::getLocal");
-                goto done;
-            }
-            jFS = jVal.l;
-        }
-    } else {
-        if (!strcmp(bld->nn, "default")) {
-            // jURI = FileSystem.getDefaultUri(conf)
-            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS,
-                          "getDefaultUri", 
-                          "(Lorg/apache/hadoop/conf/Configuration;)Ljava/net/URI;",
-                          jConfiguration) != 0) {
-                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs.", 
-                                           "FileSystem::getDefaultUri");
-                goto done;
-            }
-            jURI = jVal.l;
-        } else {
-            // fs = FileSystem::get(URI, conf, ugi);
-            ret = calcEffectiveURI(bld, &cURI);
-            if (ret)
-                goto done;
-            jURIString = (*env)->NewStringUTF(env, cURI);
-            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, JAVA_NET_URI,
-                             "create", "(Ljava/lang/String;)Ljava/net/URI;",
-                             jURIString) != 0) {
-                ret = errnoFromException(jExc, env, "java.net.URI::create");
-                goto done;
-            }
-            jURI = jVal.l;
-        }
-
-        if (bld->kerbTicketCachePath) {
-            ret = hadoopConfSet(env, jConfiguration,
-                KERBEROS_TICKET_CACHE_PATH, bld->kerbTicketCachePath);
-            if (ret)
-                goto done;
-        }
-        if (bld->userName) {
-            jUserString = (*env)->NewStringUTF(env, bld->userName);
-        }
-        if (bld->forceNewInstance) {
-            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL,
-                    HADOOP_FS, "newInstance",
-                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
-                        JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)), jURI,
-                    jConfiguration, jUserString)) {
-                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                           "Filesystem::newInstance(URI, Configuration)");
-                goto done;
-            }
-            jFS = jVal.l;
-        } else {
-            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS, "get",
-                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
-                        JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)),
-                        jURI, jConfiguration, jUserString)) {
-                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                               "Filesystem::get(URI, Configuration, String)");
-                goto done;
-            }
-            jFS = jVal.l;
-        }
-    }
-
-done:
-    if (jFS) {
-        /* Create a global reference for this fs */
-        gFsRef = (*env)->NewGlobalRef(env, jFS);
-    }
-
-    // Release unnecessary local references
-    destroyLocalReference(env, jConfiguration);
-    destroyLocalReference(env, jFS);
-    destroyLocalReference(env, jURI);
-    destroyLocalReference(env, jCachePath);
-    destroyLocalReference(env, jURIString);
-    destroyLocalReference(env, jUserString);
-    free(cURI);
-    free(bld);
-
-    if (ret)
-        errno = ret;
-    return gFsRef;
-}
-
-int hdfsDisconnect(hdfsFS fs)
-{
-    // JAVA EQUIVALENT:
-    //  fs.close()
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -2;
-    }
-
-    //Parameters
-    jobject jFS = (jobject)fs;
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (fs == NULL) {
-        errno = EBADF;
-        return -1;
-    }
-
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "close", "()V") != 0) {
-        errno = errnoFromException(jExc, env, "Filesystem::close");
-        return -1;
-    }
-
-    //Release unnecessary references
-    (*env)->DeleteGlobalRef(env, fs);
-
-    return 0;
-}
-
-
-
-hdfsFile hdfsOpenFile(hdfsFS fs, const char* path, int flags, 
-                      int bufferSize, short replication, tSize blockSize)
-{
-    /*
-      JAVA EQUIVALENT:
-       File f = new File(path);
-       FSData{Input|Output}Stream f{is|os} = fs.create(f);
-       return f{is|os};
-    */
-    /* Get the JNIEnv* corresponding to current thread */
-    JNIEnv* env = getJNIEnv();
-
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return NULL;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    if (flags & O_RDWR) {
-      fprintf(stderr, "ERROR: cannot open an hdfs file in O_RDWR mode\n");
-      errno = ENOTSUP;
-      return NULL;
-    }
-
-    if ((flags & O_CREAT) && (flags & O_EXCL)) {
-      fprintf(stderr, "WARN: hdfs does not truly support O_CREATE && O_EXCL\n");
-    }
-
-    /* The hadoop java api/signature */
-    const char* method = ((flags & O_WRONLY) == 0) ? "open" : (flags & O_APPEND) ? "append" : "create";
-    const char* signature = ((flags & O_WRONLY) == 0) ?
-        JMETHOD2(JPARAM(HADOOP_PATH), "I", JPARAM(HADOOP_ISTRM)) :
-      (flags & O_APPEND) ?
-      JMETHOD1(JPARAM(HADOOP_PATH), JPARAM(HADOOP_OSTRM)) :
-      JMETHOD2(JPARAM(HADOOP_PATH), "ZISJ", JPARAM(HADOOP_OSTRM));
-
-    /* Return value */
-    hdfsFile file = NULL;
-
-    /* Create an object of org.apache.hadoop.fs.Path */
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return NULL; 
-    }
-
-    /* Get the Configuration object from the FileSystem object */
-    jvalue  jVal;
-    jobject jConfiguration = NULL;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "getConf", JMETHOD1("", JPARAM(HADOOP_CONF))) != 0) {
-        errno = errnoFromException(jExc, env, "get configuration object "
-                                   "from filesystem");
-        destroyLocalReference(env, jPath);
-        return NULL;
-    }
-    jConfiguration = jVal.l;
-
-    jint jBufferSize = bufferSize;
-    jshort jReplication = replication;
-    jlong jBlockSize = blockSize;
-    jstring jStrBufferSize = (*env)->NewStringUTF(env, "io.file.buffer.size"); 
-    jstring jStrReplication = (*env)->NewStringUTF(env, "dfs.replication");
-    jstring jStrBlockSize = (*env)->NewStringUTF(env, "dfs.block.size");
-
-
-    //bufferSize
-    if (!bufferSize) {
-        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration, 
-                         HADOOP_CONF, "getInt", "(Ljava/lang/String;I)I",
-                         jStrBufferSize, 4096) != 0) {
-            errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
-                                       "Configuration::getInt");
-            goto done;
-        }
-        jBufferSize = jVal.i;
-    }
-
-    if ((flags & O_WRONLY) && (flags & O_APPEND) == 0) {
-        //replication
-
-        if (!replication) {
-            if (invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration, 
-                             HADOOP_CONF, "getInt", "(Ljava/lang/String;I)I",
-                             jStrReplication, 1) != 0) {
-                errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
-                                           "Configuration::getInt");
-                goto done;
-            }
-            jReplication = jVal.i;
-        }
-        
-        //blockSize
-        if (!blockSize) {
-            if (invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration, 
-                             HADOOP_CONF, "getLong", "(Ljava/lang/String;J)J",
-                             jStrBlockSize, (jlong)67108864)) {
-                errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
-                                           "FileSystem::%s(%s)", method,
-                                           signature);
-                goto done;
-            }
-            jBlockSize = jVal.j;
-        }
-    }
- 
-    /* Create and return either the FSDataInputStream or
-       FSDataOutputStream references jobject jStream */
-
-    // READ?
-    if ((flags & O_WRONLY) == 0) {
-      if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                       method, signature, jPath, jBufferSize)) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
-                                   "FileSystem::%s(%s)", method,
-                                   signature);
-        goto done;
-      }
-    }  else if ((flags & O_WRONLY) && (flags & O_APPEND)) {
-      // WRITE/APPEND?
-       if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                       method, signature, jPath)) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
-                                   "FileSystem::%s(%s)", method,
-                                   signature);
-        goto done;
-      }
-    } else {
-        // WRITE/CREATE
-        jboolean jOverWrite = 1;
-        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                         method, signature, jPath, jOverWrite,
-                         jBufferSize, jReplication, jBlockSize)) {
-            errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
-                                       "FileSystem::%s(%s)", method,
-                                       signature);
-            goto done;
-        }
-    }
-  
-    file = malloc(sizeof(struct hdfsFile_internal));
-    if (!file) {
-        errno = ENOMEM;
-    } else {
-        file->file = (*env)->NewGlobalRef(env, jVal.l);
-        file->type = (((flags & O_WRONLY) == 0) ? INPUT : OUTPUT);
-        file->flags = 0;
-
-        destroyLocalReference(env, jVal.l);
-
-        if ((flags & O_WRONLY) == 0) {
-          // Try a test read to see if we can do direct reads
-          errno = 0;
-          char buf;
-          if (readDirect(fs, file, &buf, 0) == 0) {
-            // Success - 0-byte read should return 0
-            file->flags |= HDFS_FILE_SUPPORTS_DIRECT_READ;
-          } else {
-            if (errno != ENOTSUP) {
-              // Unexpected error. Clear it, don't set the direct flag.
-              fprintf(stderr,
-                      "WARN: Unexpected error %d when testing "
-                      "for direct read compatibility\n", errno);
-              errno = 0;
-              goto done;
-            }
-          }
-          errno = 0;
-        }
-    }
-
-    done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jStrBufferSize);
-    destroyLocalReference(env, jStrReplication);
-    destroyLocalReference(env, jStrBlockSize);
-    destroyLocalReference(env, jConfiguration); 
-    destroyLocalReference(env, jPath); 
-
-    return file;
-}
-
-
-
-int hdfsCloseFile(hdfsFS fs, hdfsFile file)
-{
-    // JAVA EQUIVALENT:
-    //  file.close 
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -2;
-    }
-
-    //Parameters
-    jobject jStream = (jobject)(file ? file->file : NULL);
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!file || file->type == UNINITIALIZED) {
-        errno = EBADF;
-        return -1;
-    }
-
-    //The interface whose 'close' method to be called
-    const char* interface = (file->type == INPUT) ? 
-        HADOOP_ISTRM : HADOOP_OSTRM;
-  
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jStream, interface,
-                     "close", "()V") != 0) {
-        errno = errnoFromException(jExc, env, "%s::close", interface);
-        return -1;
-    }
-
-    //De-allocate memory
-    free(file);
-    (*env)->DeleteGlobalRef(env, jStream);
-
-    return 0;
-}
-
-
-
-int hdfsExists(hdfsFS fs, const char *path)
-{
-    JNIEnv *env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -2;
-    }
-
-    jobject jPath = constructNewObjectOfPath(env, path);
-    jvalue  jVal;
-    jthrowable jExc = NULL;
-    jobject jFS = (jobject)fs;
-
-    if (jPath == NULL) {
-        return -1;
-    }
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "exists", JMETHOD1(JPARAM(HADOOP_PATH), "Z"),
-                     jPath) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::exists");
-        destroyLocalReference(env, jPath);
-        return -1;
-    }
-
-    destroyLocalReference(env, jPath);
-    return jVal.z ? 0 : -1;
-}
-
-// Checks input file for readiness for reading.
-static int readPrepare(JNIEnv* env, hdfsFS fs, hdfsFile f,
-                       jobject* jInputStream)
-{
-    *jInputStream = (jobject)(f ? f->file : NULL);
-
-    //Sanity check
-    if (!f || f->type == UNINITIALIZED) {
-      errno = EBADF;
-      return -1;
-    }
-
-    //Error checking... make sure that this file is 'readable'
-    if (f->type != INPUT) {
-      fprintf(stderr, "Cannot read from a non-InputStream object!\n");
-      errno = EINVAL;
-      return -1;
-    }
-
-    return 0;
-}
-
-// Common error-handling code between read paths.
-static int handleReadResult(int success, jvalue jVal, jthrowable jExc,
-                            JNIEnv* env)
-{
-  int noReadBytes;
-  if (success != 0) {
-    if ((*env)->ExceptionCheck(env)) {
-      errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                 "FSDataInputStream::read");
-    }
-    noReadBytes = -1;
-  } else {
-    noReadBytes = jVal.i;
-    if (noReadBytes == 0) {
-      // 0 from Java means try again, which is EINTR here
-      errno = EINTR;
-      noReadBytes = -1;
-    } else if (noReadBytes < 0) {
-      // -1 from Java is EOF, which is 0 here
-      errno = 0;
-      noReadBytes = 0;
-    }
-  }
-
-  return noReadBytes;
-}
-
-tSize hdfsRead(hdfsFS fs, hdfsFile f, void* buffer, tSize length)
-{
-    if (f->flags & HDFS_FILE_SUPPORTS_DIRECT_READ) {
-      return readDirect(fs, f, buffer, length);
-    }
-
-    // JAVA EQUIVALENT:
-    //  byte [] bR = new byte[length];
-    //  fis.read(bR);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jInputStream;
-    if (readPrepare(env, fs, f, &jInputStream) == -1) {
-      return -1;
-    }
-
-    jbyteArray jbRarray;
-    jint noReadBytes = 0;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-
-    //Read the requisite bytes
-    jbRarray = (*env)->NewByteArray(env, length);
-
-    int success = invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
-                               "read", "([B)I", jbRarray);
-
-    noReadBytes = handleReadResult(success, jVal, jExc, env);;
-
-    if (noReadBytes > 0) {
-      (*env)->GetByteArrayRegion(env, jbRarray, 0, noReadBytes, buffer);
-    }
-
-    destroyLocalReference(env, jbRarray);
-
-    return noReadBytes;
-}
-
-// Reads using the read(ByteBuffer) API, which does fewer copies
-tSize readDirect(hdfsFS fs, hdfsFile f, void* buffer, tSize length)
-{
-    // JAVA EQUIVALENT:
-    //  ByteBuffer bbuffer = ByteBuffer.allocateDirect(length) // wraps C buffer
-    //  fis.read(bbuffer);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jInputStream;
-    if (readPrepare(env, fs, f, &jInputStream) == -1) {
-      return -1;
-    }
-
-    jint noReadBytes = 0;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-
-    //Read the requisite bytes
-    jobject bb = (*env)->NewDirectByteBuffer(env, buffer, length);
-    if (bb == NULL) {
-      fprintf(stderr, "Could not allocate ByteBuffer");
-      if ((*env)->ExceptionCheck(env)) {
-        errno = errnoFromException(NULL, env, "JNIEnv::NewDirectByteBuffer");
-      } else {
-        errno = ENOMEM; // Best guess if there's no exception waiting
-      }
-      return -1;
-    }
-
-    int success = invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream,
-                               HADOOP_ISTRM, "read", "(Ljava/nio/ByteBuffer;)I",
-                               bb);
-
-    noReadBytes = handleReadResult(success, jVal, jExc, env);
-
-    destroyLocalReference(env, bb);
-
-    return noReadBytes;
-}
-
-
-  
-tSize hdfsPread(hdfsFS fs, hdfsFile f, tOffset position,
-                void* buffer, tSize length)
-{
-    // JAVA EQUIVALENT:
-    //  byte [] bR = new byte[length];
-    //  fis.read(pos, bR, 0, length);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jInputStream = (jobject)(f ? f->file : NULL);
-
-    jbyteArray jbRarray;
-    jint noReadBytes = 0;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!f || f->type == UNINITIALIZED) {
-        errno = EBADF;
-        return -1;
-    }
-
-    //Error checking... make sure that this file is 'readable'
-    if (f->type != INPUT) {
-        fprintf(stderr, "Cannot read from a non-InputStream object!\n");
-        errno = EINVAL;
-        return -1;
-    }
-
-    //Read the requisite bytes
-    jbRarray = (*env)->NewByteArray(env, length);
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
-                     "read", "(J[BII)I", position, jbRarray, 0, length) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FSDataInputStream::read");
-        noReadBytes = -1;
-    }
-    else {
-        noReadBytes = jVal.i;
-        if (noReadBytes > 0) {
-            (*env)->GetByteArrayRegion(env, jbRarray, 0, noReadBytes, buffer);
-        }  else {
-            //This is a valid case: there aren't any bytes left to read!
-          if (noReadBytes == 0 || noReadBytes < -1) {
-            fprintf(stderr, "WARN: FSDataInputStream.read returned invalid return code - libhdfs returning EOF, i.e., 0: %d\n", noReadBytes);
-          }
-            noReadBytes = 0;
-        }
-        errno = 0;
-    }
-    destroyLocalReference(env, jbRarray);
-
-    return noReadBytes;
-}
-
-
-
-tSize hdfsWrite(hdfsFS fs, hdfsFile f, const void* buffer, tSize length)
-{
-    // JAVA EQUIVALENT
-    // byte b[] = str.getBytes();
-    // fso.write(b);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jOutputStream = (jobject)(f ? f->file : 0);
-    jbyteArray jbWarray;
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!f || f->type == UNINITIALIZED) {
-        errno = EBADF;
-        return -1;
-    }
-    
-    if (length < 0) {
-    	errno = EINVAL;
-    	return -1;
-    }
-
-    //Error checking... make sure that this file is 'writable'
-    if (f->type != OUTPUT) {
-        fprintf(stderr, "Cannot write into a non-OutputStream object!\n");
-        errno = EINVAL;
-        return -1;
-    }
-
-    // 'length' equals 'zero' is a valid use-case according to Posix!
-    if (length != 0) {
-        //Write the requisite bytes into the file
-        jbWarray = (*env)->NewByteArray(env, length);
-        (*env)->SetByteArrayRegion(env, jbWarray, 0, length, buffer);
-        if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream,
-                         HADOOP_OSTRM, "write",
-                         "([B)V", jbWarray) != 0) {
-            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                       "FSDataOutputStream::write");
-            length = -1;
-        }
-        destroyLocalReference(env, jbWarray);
-    }
-
-    //Return no. of bytes succesfully written (libc way)
-    //i.e. 'length' itself! ;-)
-    return length;
-}
-
-
-
-int hdfsSeek(hdfsFS fs, hdfsFile f, tOffset desiredPos) 
-{
-    // JAVA EQUIVALENT
-    //  fis.seek(pos);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jInputStream = (jobject)(f ? f->file : 0);
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!f || f->type != INPUT) {
-        errno = EBADF;
-        return -1;
-    }
-
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
-                     "seek", "(J)V", desiredPos) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FSDataInputStream::seek");
-        return -1;
-    }
-
-    return 0;
-}
-
-
-
-tOffset hdfsTell(hdfsFS fs, hdfsFile f)
-{
-    // JAVA EQUIVALENT
-    //  pos = f.getPos();
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jStream = (jobject)(f ? f->file : 0);
-
-    //Sanity check
-    if (!f || f->type == UNINITIALIZED) {
-        errno = EBADF;
-        return -1;
-    }
-
-    const char* interface = (f->type == INPUT) ?
-        HADOOP_ISTRM : HADOOP_OSTRM;
-
-    jlong currentPos  = -1;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStream,
-                     interface, "getPos", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FSDataInputStream::getPos");
-        return -1;
-    }
-    currentPos = jVal.j;
-
-    return (tOffset)currentPos;
-}
-
-
-
-int hdfsFlush(hdfsFS fs, hdfsFile f) 
-{
-    // JAVA EQUIVALENT
-    //  fos.flush();
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jOutputStream = (jobject)(f ? f->file : 0);
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!f || f->type != OUTPUT) {
-        errno = EBADF;
-        return -1;
-    }
-
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream, 
-                     HADOOP_OSTRM, "flush", "()V") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FSDataInputStream::flush");
-        return -1;
-    }
-
-    return 0;
-}
-
-
-
-int hdfsHFlush(hdfsFS fs, hdfsFile f)
-{
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jOutputStream = (jobject)(f ? f->file : 0);
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!f || f->type != OUTPUT) {
-        errno = EBADF;
-        return -1;
-    }
-
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream,
-                     HADOOP_OSTRM, "hflush", "()V") != 0) {
-        errno = errnoFromException(jExc, env, HADOOP_OSTRM "::hflush");
-        return -1;
-    }
-
-    return 0;
-}
-
-
-
-int hdfsAvailable(hdfsFS fs, hdfsFile f)
-{
-    // JAVA EQUIVALENT
-    //  fis.available();
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jInputStream = (jobject)(f ? f->file : 0);
-
-    //Caught exception
-    jthrowable jExc = NULL;
-
-    //Sanity check
-    if (!f || f->type != INPUT) {
-        errno = EBADF;
-        return -1;
-    }
-
-    jint available = -1;
-    jvalue jVal;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, 
-                     HADOOP_ISTRM, "available", "()I") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FSDataInputStream::available");
-        return -1;
-    }
-    available = jVal.i;
-
-    return available;
-}
-
-
-
-int hdfsCopy(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst)
-{
-    //JAVA EQUIVALENT
-    //  FileUtil::copy(srcFS, srcPath, dstFS, dstPath,
-    //                 deleteSource = false, conf)
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    //Parameters
-    jobject jSrcFS = (jobject)srcFS;
-    jobject jDstFS = (jobject)dstFS;
-    jobject jSrcPath = NULL;
-    jobject jDstPath = NULL;
-
-    jSrcPath = constructNewObjectOfPath(env, src);
-    if (jSrcPath == NULL) {
-        return -1;
-    }
-
-    jDstPath = constructNewObjectOfPath(env, dst);
-    if (jDstPath == NULL) {
-        destroyLocalReference(env, jSrcPath);
-        return -1;
-    }
-
-    int retval = 0;
-
-    //Create the org.apache.hadoop.conf.Configuration object
-    jobject jConfiguration =
-        constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
-    if (jConfiguration == NULL) {
-        fprintf(stderr, "Can't construct instance of class "
-                "org.apache.hadoop.conf.Configuration\n");
-        errno = EINTERNAL;
-        destroyLocalReference(env, jSrcPath);
-        destroyLocalReference(env, jDstPath);
-        return -1;
-    }
-
-    //FileUtil::copy
-    jboolean deleteSource = 0; //Only copy
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, STATIC, 
-                     NULL, "org/apache/hadoop/fs/FileUtil", "copy",
-                     "(Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;ZLorg/apache/hadoop/conf/Configuration;)Z",
-                     jSrcFS, jSrcPath, jDstFS, jDstPath, deleteSource, 
-                     jConfiguration) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileUtil::copy");
-        retval = -1;
-        goto done;
-    }
-
-    done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jConfiguration);
-    destroyLocalReference(env, jSrcPath);
-    destroyLocalReference(env, jDstPath);
-  
-    return retval;
-}
-
-
-
-int hdfsMove(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst)
-{
-    //JAVA EQUIVALENT
-    //  FileUtil::copy(srcFS, srcPath, dstFS, dstPath,
-    //                 deleteSource = true, conf)
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-
-    //Parameters
-    jobject jSrcFS = (jobject)srcFS;
-    jobject jDstFS = (jobject)dstFS;
-
-    jobject jSrcPath = NULL;
-    jobject jDstPath = NULL;
-
-    jSrcPath = constructNewObjectOfPath(env, src);
-    if (jSrcPath == NULL) {
-        return -1;
-    }
-
-    jDstPath = constructNewObjectOfPath(env, dst);
-    if (jDstPath == NULL) {
-        destroyLocalReference(env, jSrcPath);
-        return -1;
-    }
-
-    int retval = 0;
-
-    //Create the org.apache.hadoop.conf.Configuration object
-    jobject jConfiguration =
-        constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
-    if (jConfiguration == NULL) {
-        fprintf(stderr, "Can't construct instance of class "
-                "org.apache.hadoop.conf.Configuration\n");
-        errno = EINTERNAL;
-        destroyLocalReference(env, jSrcPath);
-        destroyLocalReference(env, jDstPath);
-        return -1;
-    }
-
-    //FileUtil::copy
-    jboolean deleteSource = 1; //Delete src after copy
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, STATIC, NULL,
-                     "org/apache/hadoop/fs/FileUtil", "copy",
-                "(Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;ZLorg/apache/hadoop/conf/Configuration;)Z",
-                     jSrcFS, jSrcPath, jDstFS, jDstPath, deleteSource, 
-                     jConfiguration) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileUtil::copy(move)");
-        retval = -1;
-        goto done;
-    }
-
-    done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jConfiguration);
-    destroyLocalReference(env, jSrcPath);
-    destroyLocalReference(env, jDstPath);
-  
-    return retval;
-}
-
-
-
-int hdfsDelete(hdfsFS fs, const char* path, int recursive)
-{
-    // JAVA EQUIVALENT:
-    //  File f = new File(path);
-    //  bool retval = fs.delete(f);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of java.io.File
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return -1;
-    }
-
-    //Delete the file
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    jboolean jRecursive = recursive ? JNI_TRUE : JNI_FALSE;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "delete", "(Lorg/apache/hadoop/fs/Path;Z)Z",
-                     jPath, jRecursive) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::delete");
-        destroyLocalReference(env, jPath);
-        return -1;
-    }
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-
-    return (jVal.z) ? 0 : -1;
-}
-
-
-
-int hdfsRename(hdfsFS fs, const char* oldPath, const char* newPath)
-{
-    // JAVA EQUIVALENT:
-    //  Path old = new Path(oldPath);
-    //  Path new = new Path(newPath);
-    //  fs.rename(old, new);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create objects of org.apache.hadoop.fs.Path
-    jobject jOldPath = NULL;
-    jobject jNewPath = NULL;
-
-    jOldPath = constructNewObjectOfPath(env, oldPath);
-    if (jOldPath == NULL) {
-        return -1;
-    }
-
-    jNewPath = constructNewObjectOfPath(env, newPath);
-    if (jNewPath == NULL) {
-        destroyLocalReference(env, jOldPath);
-        return -1;
-    }
-
-    //Rename the file
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS, "rename",
-                     JMETHOD2(JPARAM(HADOOP_PATH), JPARAM(HADOOP_PATH), "Z"),
-                     jOldPath, jNewPath) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::rename");
-        destroyLocalReference(env, jOldPath);
-        destroyLocalReference(env, jNewPath);
-        return -1;
-    }
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jOldPath);
-    destroyLocalReference(env, jNewPath);
-
-    return (jVal.z) ? 0 : -1;
-}
-
-
-
-char* hdfsGetWorkingDirectory(hdfsFS fs, char* buffer, size_t bufferSize)
-{
-    // JAVA EQUIVALENT:
-    //  Path p = fs.getWorkingDirectory(); 
-    //  return p.toString()
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return NULL;
-    }
-
-    jobject jFS = (jobject)fs;
-    jobject jPath = NULL;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-
-    //FileSystem::getWorkingDirectory()
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS,
-                     HADOOP_FS, "getWorkingDirectory",
-                     "()Lorg/apache/hadoop/fs/Path;") != 0 ||
-        jVal.l == NULL) {
-        errno = errnoFromException(jExc, env, "FileSystem::"
-                                   "getWorkingDirectory");
-        return NULL;
-    }
-    jPath = jVal.l;
-
-    //Path::toString()
-    jstring jPathString;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jPath, 
-                     "org/apache/hadoop/fs/Path", "toString",
-                     "()Ljava/lang/String;") != 0) { 
-        errno = errnoFromException(jExc, env, "Path::toString");
-        destroyLocalReference(env, jPath);
-        return NULL;
-    }
-    jPathString = jVal.l;
-
-    const char *jPathChars = (const char*)
-        ((*env)->GetStringUTFChars(env, jPathString, NULL));
-
-    //Copy to user-provided buffer
-    strncpy(buffer, jPathChars, bufferSize);
-
-    //Delete unnecessary local references
-    (*env)->ReleaseStringUTFChars(env, jPathString, jPathChars);
-
-    destroyLocalReference(env, jPathString);
-    destroyLocalReference(env, jPath);
-
-    return buffer;
-}
-
-
-
-int hdfsSetWorkingDirectory(hdfsFS fs, const char* path)
-{
-    // JAVA EQUIVALENT:
-    //  fs.setWorkingDirectory(Path(path)); 
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-    int retval = 0;
-    jthrowable jExc = NULL;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return -1;
-    }
-
-    //FileSystem::setWorkingDirectory()
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "setWorkingDirectory", 
-                     "(Lorg/apache/hadoop/fs/Path;)V", jPath) != 0) {
-        errno = errnoFromException(jExc, env, "FileSystem::"
-                                   "setWorkingDirectory");
-        retval = -1;
-    }
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-
-    return retval;
-}
-
-
-
-int hdfsCreateDirectory(hdfsFS fs, const char* path)
-{
-    // JAVA EQUIVALENT:
-    //  fs.mkdirs(new Path(path));
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return -1;
-    }
-
-    //Create the directory
-    jvalue jVal;
-    jVal.z = 0;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "mkdirs", "(Lorg/apache/hadoop/fs/Path;)Z",
-                     jPath) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::mkdirs");
-        goto done;
-    }
-
- done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-
-    return (jVal.z) ? 0 : -1;
-}
-
-
-int hdfsSetReplication(hdfsFS fs, const char* path, int16_t replication)
-{
-    // JAVA EQUIVALENT:
-    //  fs.setReplication(new Path(path), replication);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return -1;
-    }
-
-    //Create the directory
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "setReplication", "(Lorg/apache/hadoop/fs/Path;S)Z",
-                     jPath, replication) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::setReplication");
-        goto done;
-    }
-
- done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-
-    return (jVal.z) ? 0 : -1;
-}
-
-int hdfsChown(hdfsFS fs, const char* path, const char *owner, const char *group)
-{
-    // JAVA EQUIVALENT:
-    //  fs.setOwner(path, owner, group)
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    if (owner == NULL && group == NULL) {
-      fprintf(stderr, "Both owner and group cannot be null in chown");
-      errno = EINVAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return -1;
-    }
-
-    jstring jOwnerString = (*env)->NewStringUTF(env, owner); 
-    jstring jGroupString = (*env)->NewStringUTF(env, group); 
-
-    //Create the directory
-    int ret = 0;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "setOwner", JMETHOD3(JPARAM(HADOOP_PATH), JPARAM(JAVA_STRING), JPARAM(JAVA_STRING), JAVA_VOID),
-                     jPath, jOwnerString, jGroupString) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::setOwner");
-        ret = -1;
-        goto done;
-    }
-
- done:
-    destroyLocalReference(env, jPath);
-    destroyLocalReference(env, jOwnerString);
-    destroyLocalReference(env, jGroupString);
-
-    return ret;
-}
-
-int hdfsChmod(hdfsFS fs, const char* path, short mode)
-{
-    // JAVA EQUIVALENT:
-    //  fs.setPermission(path, FsPermission)
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    // construct jPerm = FsPermission.createImmutable(short mode);
-
-    jshort jmode = mode;
-
-    jobject jPermObj =
-      constructNewObjectOfClass(env, NULL, HADOOP_FSPERM,"(S)V",jmode);
-    if (jPermObj == NULL) {
-      return -2;
-    }
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-      destroyLocalReference(env, jPermObj);
-      return -3;
-    }
-
-    //Create the directory
-    int ret = 0;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "setPermission", JMETHOD2(JPARAM(HADOOP_PATH), JPARAM(HADOOP_FSPERM), JAVA_VOID),
-                     jPath, jPermObj) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::setPermission");
-        ret = -1;
-        goto done;
-    }
-
- done:
-    destroyLocalReference(env, jPath);
-    destroyLocalReference(env, jPermObj);
-
-    return ret;
-}
-
-int hdfsUtime(hdfsFS fs, const char* path, tTime mtime, tTime atime)
-{
-    // JAVA EQUIVALENT:
-    //  fs.setTimes(src, mtime, atime)
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-      fprintf(stderr, "could not construct path object\n");
-      return -2;
-    }
-
-    const tTime NO_CHANGE = -1;
-    jlong jmtime = (mtime == NO_CHANGE) ? -1 : (mtime * (jlong)1000);
-    jlong jatime = (atime == NO_CHANGE) ? -1 : (atime * (jlong)1000);
-
-    int ret = 0;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "setTimes", JMETHOD3(JPARAM(HADOOP_PATH), "J", "J", JAVA_VOID),
-                     jPath, jmtime, jatime) != 0) {
-      fprintf(stderr, "call to setTime failed\n");
-      errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                 "FileSystem::setTimes");
-      ret = -1;
-      goto done;
-    }
-
- done:
-    destroyLocalReference(env, jPath);
-    return ret;
-}
-
-
-
-
-char***
-hdfsGetHosts(hdfsFS fs, const char* path, tOffset start, tOffset length)
-{
-    // JAVA EQUIVALENT:
-    //  fs.getFileBlockLoctions(new Path(path), start, length);
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return NULL;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return NULL;
-    }
-
-    jvalue jFSVal;
-    jthrowable jFSExc = NULL;
-    if (invokeMethod(env, &jFSVal, &jFSExc, INSTANCE, jFS,
-                     HADOOP_FS, "getFileStatus", 
-                     "(Lorg/apache/hadoop/fs/Path;)"
-                     "Lorg/apache/hadoop/fs/FileStatus;",
-                     jPath) != 0) {
-        errno = errnoFromException(jFSExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::getFileStatus");
-        destroyLocalReference(env, jPath);
-        return NULL;
-    }
-    jobject jFileStatus = jFSVal.l;
-
-    //org.apache.hadoop.fs.FileSystem::getFileBlockLocations
-    char*** blockHosts = NULL;
-    jobjectArray jBlockLocations;;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS,
-                     HADOOP_FS, "getFileBlockLocations", 
-                     "(Lorg/apache/hadoop/fs/FileStatus;JJ)"
-                     "[Lorg/apache/hadoop/fs/BlockLocation;",
-                     jFileStatus, start, length) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::getFileBlockLocations");
-        destroyLocalReference(env, jPath);
-        destroyLocalReference(env, jFileStatus);
-        return NULL;
-    }
-    jBlockLocations = jVal.l;
-
-    //Figure out no of entries in jBlockLocations
-    //Allocate memory and add NULL at the end
-    jsize jNumFileBlocks = (*env)->GetArrayLength(env, jBlockLocations);
-
-    blockHosts = malloc(sizeof(char**) * (jNumFileBlocks+1));
-    if (blockHosts == NULL) {
-        errno = ENOMEM;
-        goto done;
-    }
-    blockHosts[jNumFileBlocks] = NULL;
-    if (jNumFileBlocks == 0) {
-        errno = 0;
-        goto done;
-    }
-
-    //Now parse each block to get hostnames
-    int i = 0;
-    for (i=0; i < jNumFileBlocks; ++i) {
-        jobject jFileBlock =
-            (*env)->GetObjectArrayElement(env, jBlockLocations, i);
-        
-        jvalue jVal;
-        jobjectArray jFileBlockHosts;
-        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFileBlock, HADOOP_BLK_LOC,
-                         "getHosts", "()[Ljava/lang/String;") ||
-                jVal.l == NULL) {
-            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                       "BlockLocation::getHosts");
-            destroyLocalReference(env, jPath);
-            destroyLocalReference(env, jFileStatus);
-            destroyLocalReference(env, jBlockLocations);
-            return NULL;
-        }
-        
-        jFileBlockHosts = jVal.l;
-        //Figure out no of hosts in jFileBlockHosts
-        //Allocate memory and add NULL at the end
-        jsize jNumBlockHosts = (*env)->GetArrayLength(env, jFileBlockHosts);
-        blockHosts[i] = malloc(sizeof(char*) * (jNumBlockHosts+1));
-        if (blockHosts[i] == NULL) {
-            int x = 0;
-            for (x=0; x < i; ++x) {
-                free(blockHosts[x]);
-            }
-            free(blockHosts);
-            errno = ENOMEM;
-            goto done;
-        }
-        blockHosts[i][jNumBlockHosts] = NULL;
-
-        //Now parse each hostname
-        int j = 0;
-        const char *hostName;
-        for (j=0; j < jNumBlockHosts; ++j) {
-            jstring jHost =
-                (*env)->GetObjectArrayElement(env, jFileBlockHosts, j);
-           
-            hostName =
-                (const char*)((*env)->GetStringUTFChars(env, jHost, NULL));
-            blockHosts[i][j] = strdup(hostName);
-
-            (*env)->ReleaseStringUTFChars(env, jHost, hostName);
-            destroyLocalReference(env, jHost);
-        }
-
-        destroyLocalReference(env, jFileBlockHosts);
-    }
-  
-    done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-    destroyLocalReference(env, jFileStatus);
-    destroyLocalReference(env, jBlockLocations);
-
-    return blockHosts;
-}
-
-
-void hdfsFreeHosts(char ***blockHosts)
-{
-    int i, j;
-    for (i=0; blockHosts[i]; i++) {
-        for (j=0; blockHosts[i][j]; j++) {
-            free(blockHosts[i][j]);
-        }
-        free(blockHosts[i]);
-    }
-    free(blockHosts);
-}
-
-
-tOffset hdfsGetDefaultBlockSize(hdfsFS fs)
-{
-    // JAVA EQUIVALENT:
-    //  fs.getDefaultBlockSize();
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //FileSystem::getDefaultBlockSize()
-    tOffset blockSize = -1;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "getDefaultBlockSize", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::getDefaultBlockSize");
-        return -1;
-    }
-    blockSize = jVal.j;
-
-    return blockSize;
-}
-
-
-
-tOffset hdfsGetCapacity(hdfsFS fs)
-{
-    // JAVA EQUIVALENT:
-    //  FsStatus fss = fs.getStatus();
-    //  return Fss.getCapacity();
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //FileSystem::getStatus
-    jvalue  jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "getStatus", "()Lorg/apache/hadoop/fs/FsStatus;") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::getStatus");
-        return -1;
-    }
-    jobject fss = (jobject)jVal.l;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, fss, HADOOP_FSSTATUS,
-                     "getCapacity", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FsStatus::getCapacity");
-        destroyLocalReference(env, fss);
-        return -1;
-    }
-    destroyLocalReference(env, fss);
-    return jVal.j;
-}
-
-
-  
-tOffset hdfsGetUsed(hdfsFS fs)
-{
-    // JAVA EQUIVALENT:
-    //  FsStatus fss = fs.getStatus();
-    //  return Fss.getUsed();
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return -1;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //FileSystem::getStatus
-    jvalue  jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "getStatus", "()Lorg/apache/hadoop/fs/FsStatus;") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::getStatus");
-        return -1;
-    }
-    jobject fss = (jobject)jVal.l;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, fss, HADOOP_FSSTATUS,
-                     "getUsed", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FsStatus::getUsed");
-        destroyLocalReference(env, fss);
-        return -1;
-    }
-    destroyLocalReference(env, fss);
-    return jVal.j;
-
-}
-
-
- 
-static int
-getFileInfoFromStat(JNIEnv *env, jobject jStat, hdfsFileInfo *fileInfo)
-{
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
-                     HADOOP_STAT, "isDir", "()Z") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileStatus::isDir");
-        return -1;
-    }
-    fileInfo->mKind = jVal.z ? kObjectKindDirectory : kObjectKindFile;
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
-                     HADOOP_STAT, "getReplication", "()S") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileStatus::getReplication");
-        return -1;
-    }
-    fileInfo->mReplication = jVal.s;
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
-                     HADOOP_STAT, "getBlockSize", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileStatus::getBlockSize");
-        return -1;
-    }
-    fileInfo->mBlockSize = jVal.j;
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
-                     HADOOP_STAT, "getModificationTime", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileStatus::getModificationTime");
-        return -1;
-    }
-    fileInfo->mLastMod = jVal.j / 1000;
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
-                     HADOOP_STAT, "getAccessTime", "()J") != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileStatus::getAccessTime");
-        return -1;
-    }
-    fileInfo->mLastAccess = (tTime) (jVal.j / 1000);
-
-
-    if (fileInfo->mKind == kObjectKindFile) {
-        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
-                         HADOOP_STAT, "getLen", "()J") != 0) {
-            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                       "FileStatus::getLen");
-            return -1;
-        }
-        fileInfo->mSize = jVal.j;
-    }
-
-    jobject jPath;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
-                     "getPath", "()Lorg/apache/hadoop/fs/Path;") ||
-            jVal.l == NULL) { 
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "Path::getPath");
-        return -1;
-    }
-    jPath = jVal.l;
-
-    jstring     jPathName;
-    const char *cPathName;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jPath, HADOOP_PATH,
-                     "toString", "()Ljava/lang/String;")) { 
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "Path::toString");
-        destroyLocalReference(env, jPath);
-        return -1;
-    }
-    jPathName = jVal.l;
-    cPathName = (const char*) ((*env)->GetStringUTFChars(env, jPathName, NULL));
-    fileInfo->mName = strdup(cPathName);
-    (*env)->ReleaseStringUTFChars(env, jPathName, cPathName);
-    destroyLocalReference(env, jPath);
-    destroyLocalReference(env, jPathName);
-    jstring     jUserName;
-    const char* cUserName;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
-                    "getOwner", "()Ljava/lang/String;")) {
-        fprintf(stderr, "Call to org.apache.hadoop.fs."
-                "FileStatus::getOwner failed!\n");
-        errno = EINTERNAL;
-        return -1;
-    }
-    jUserName = jVal.l;
-    cUserName = (const char*) ((*env)->GetStringUTFChars(env, jUserName, NULL));
-    fileInfo->mOwner = strdup(cUserName);
-    (*env)->ReleaseStringUTFChars(env, jUserName, cUserName);
-    destroyLocalReference(env, jUserName);
-
-    jstring     jGroupName;
-    const char* cGroupName;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
-                    "getGroup", "()Ljava/lang/String;")) {
-        fprintf(stderr, "Call to org.apache.hadoop.fs."
-                "FileStatus::getGroup failed!\n");
-        errno = EINTERNAL;
-        return -1;
-    }
-    jGroupName = jVal.l;
-    cGroupName = (const char*) ((*env)->GetStringUTFChars(env, jGroupName, NULL));
-    fileInfo->mGroup = strdup(cGroupName);
-    (*env)->ReleaseStringUTFChars(env, jGroupName, cGroupName);
-    destroyLocalReference(env, jGroupName);
-
-    jobject jPermission;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
-                     "getPermission", "()Lorg/apache/hadoop/fs/permission/FsPermission;") ||
-            jVal.l == NULL) {
-        fprintf(stderr, "Call to org.apache.hadoop.fs."
-                "FileStatus::getPermission failed!\n");
-        errno = EINTERNAL;
-        return -1;
-    }
-    jPermission = jVal.l;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jPermission, HADOOP_FSPERM,
-                         "toShort", "()S") != 0) {
-            fprintf(stderr, "Call to org.apache.hadoop.fs.permission."
-                    "FsPermission::toShort failed!\n");
-            errno = EINTERNAL;
-            return -1;
-    }
-    fileInfo->mPermissions = jVal.s;
-    destroyLocalReference(env, jPermission);
-
-    return 0;
-}
-
-static int
-getFileInfo(JNIEnv *env, jobject jFS, jobject jPath, hdfsFileInfo *fileInfo)
-{
-    // JAVA EQUIVALENT:
-    //  fs.isDirectory(f)
-    //  fs.getModificationTime()
-    //  fs.getAccessTime()
-    //  fs.getLength(f)
-    //  f.getPath()
-    //  f.getOwner()
-    //  f.getGroup()
-    //  f.getPermission().toShort()
-
-    jobject jStat;
-    jvalue  jVal;
-    jthrowable jExc = NULL;
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "exists", JMETHOD1(JPARAM(HADOOP_PATH), "Z"),
-                     jPath) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::exists");
-        return -1;
-    }
-
-    if (jVal.z == 0) {
-      errno = ENOENT;
-      return -1;
-    }
-
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
-                     "getFileStatus", JMETHOD1(JPARAM(HADOOP_PATH), JPARAM(HADOOP_STAT)),
-                     jPath) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::getFileStatus");
-        return -1;
-    }
-    jStat = jVal.l;
-    int ret =  getFileInfoFromStat(env, jStat, fileInfo); 
-    destroyLocalReference(env, jStat);
-    return ret;
-}
-
-
-
-hdfsFileInfo* hdfsListDirectory(hdfsFS fs, const char* path, int *numEntries)
-{
-    // JAVA EQUIVALENT:
-    //  Path p(path);
-    //  Path []pathList = fs.listPaths(p)
-    //  foreach path in pathList 
-    //    getFileInfo(path)
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return NULL;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return NULL;
-    }
-
-    hdfsFileInfo *pathList = 0; 
-
-    jobjectArray jPathList = NULL;
-    jvalue jVal;
-    jthrowable jExc = NULL;
-    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_DFS, "listStatus",
-                     JMETHOD1(JPARAM(HADOOP_PATH), JARRPARAM(HADOOP_STAT)),
-                     jPath) != 0) {
-        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
-                                   "FileSystem::listStatus");
-        destroyLocalReference(env, jPath);
-        return NULL;
-    }
-    jPathList = jVal.l;
-
-    //Figure out no of entries in that directory
-    jsize jPathListSize = (*env)->GetArrayLength(env, jPathList);
-    *numEntries = jPathListSize;
-    if (jPathListSize == 0) {
-        errno = 0;
-        goto done;
-    }
-
-    //Allocate memory
-    pathList = calloc(jPathListSize, sizeof(hdfsFileInfo));
-    if (pathList == NULL) {
-        errno = ENOMEM;
-        goto done;
-    }
-
-    //Save path information in pathList
-    jsize i;
-    jobject tmpStat;
-    for (i=0; i < jPathListSize; ++i) {
-        tmpStat = (*env)->GetObjectArrayElement(env, jPathList, i);
-        if (getFileInfoFromStat(env, tmpStat, &pathList[i])) {
-            hdfsFreeFileInfo(pathList, jPathListSize);
-            destroyLocalReference(env, tmpStat);
-            pathList = NULL;
-            goto done;
-        }
-        destroyLocalReference(env, tmpStat);
-    }
-
-    done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-    destroyLocalReference(env, jPathList);
-
-    return pathList;
-}
-
-
-
-hdfsFileInfo *hdfsGetPathInfo(hdfsFS fs, const char* path)
-{
-    // JAVA EQUIVALENT:
-    //  File f(path);
-    //  fs.isDirectory(f)
-    //  fs.lastModified() ??
-    //  fs.getLength(f)
-    //  f.getPath()
-
-    //Get the JNIEnv* corresponding to current thread
-    JNIEnv* env = getJNIEnv();
-    if (env == NULL) {
-      errno = EINTERNAL;
-      return NULL;
-    }
-
-    jobject jFS = (jobject)fs;
-
-    //Create an object of org.apache.hadoop.fs.Path
-    jobject jPath = constructNewObjectOfPath(env, path);
-    if (jPath == NULL) {
-        return NULL;
-    }
-
-    hdfsFileInfo *fileInfo = calloc(1, sizeof(hdfsFileInfo));
-    if (getFileInfo(env, jFS, jPath, fileInfo)) {
-        hdfsFreeFileInfo(fileInfo, 1);
-        fileInfo = NULL;
-        goto done;
-    }
-
-    done:
-
-    //Delete unnecessary local references
-    destroyLocalReference(env, jPath);
-
-    return fileInfo;
-}
-
-
-
-void hdfsFreeFileInfo(hdfsFileInfo *hdfsFileInfo, int numEntries)
-{
-    //Free the mName, mOwner, and mGroup
-    int i;
-    for (i=0; i < numEntries; ++i) {
-        if (hdfsFileInfo[i].mName) {
-            free(hdfsFileInfo[i].mName);
-        }
-        if (hdfsFileInfo[i].mOwner) {
-            free(hdfsFileInfo[i].mOwner);
-        }
-        if (hdfsFileInfo[i].mGroup) {
-            free(hdfsFileInfo[i].mGroup);
-        }
-    }
-
-    //Free entire block
-    free(hdfsFileInfo);
-}
-
-
-
-
-/**
- * vim: ts=4: sw=4: et:
- */
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.h
deleted file mode 100644
index d5cef6e..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs.h
+++ /dev/null
@@ -1,605 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LIBHDFS_HDFS_H
-#define LIBHDFS_HDFS_H
-
-#include <errno.h> /* for EINTERNAL, etc. */
-#include <fcntl.h> /* for O_RDONLY, O_WRONLY */
-#include <stdint.h> /* for uint64_t, etc. */
-#include <time.h> /* for time_t */
-
-#ifndef O_RDONLY
-#define O_RDONLY 1
-#endif
-
-#ifndef O_WRONLY 
-#define O_WRONLY 2
-#endif
-
-#ifndef EINTERNAL
-#define EINTERNAL 255 
-#endif
-
-
-/** All APIs set errno to meaningful values */
-
-#ifdef __cplusplus
-extern  "C" {
-#endif
-    /**
-     * Some utility decls used in libhdfs.
-     */
-    struct hdfsBuilder;
-    typedef int32_t   tSize; /// size of data for read/write io ops 
-    typedef time_t    tTime; /// time type in seconds
-    typedef int64_t   tOffset;/// offset within the file
-    typedef uint16_t  tPort; /// port
-    typedef enum tObjectKind {
-        kObjectKindFile = 'F',
-        kObjectKindDirectory = 'D',
-    } tObjectKind;
-
-
-    /**
-     * The C reflection of org.apache.org.hadoop.FileSystem .
-     */
-    struct hdfs_internal;
-    typedef struct hdfs_internal* hdfsFS;
-    
-    struct hdfsFile_internal;
-    typedef struct hdfsFile_internal* hdfsFile;
-
-    /**
-     * Determine if a file is open for read.
-     *
-     * @param file     The HDFS file
-     * @return         1 if the file is open for read; 0 otherwise
-     */
-    int hdfsFileIsOpenForRead(hdfsFile file);
-
-    /**
-     * Determine if a file is open for write.
-     *
-     * @param file     The HDFS file
-     * @return         1 if the file is open for write; 0 otherwise
-     */
-    int hdfsFileIsOpenForWrite(hdfsFile file);
-
-    /**
-     * Disable the direct read optimization for a file.
-     *
-     * This is mainly provided for unit testing purposes.
-     *
-     * @param file     The HDFS file
-     */
-    void hdfsFileDisableDirectRead(hdfsFile file);
-
-    /** 
-     * hdfsConnectAsUser - Connect to a hdfs file system as a specific user
-     * Connect to the hdfs.
-     * @param nn   The NameNode.  See hdfsBuilderSetNameNode for details.
-     * @param port The port on which the server is listening.
-     * @param user the user name (this is hadoop domain user). Or NULL is equivelant to hhdfsConnect(host, port)
-     * @return Returns a handle to the filesystem or NULL on error.
-     * @deprecated Use hdfsBuilderConnect instead. 
-     */
-     hdfsFS hdfsConnectAsUser(const char* nn, tPort port, const char *user);
-
-
-    /** 
-     * hdfsConnect - Connect to a hdfs file system.
-     * Connect to the hdfs.
-     * @param nn   The NameNode.  See hdfsBuilderSetNameNode for details.
-     * @param port The port on which the server is listening.
-     * @return Returns a handle to the filesystem or NULL on error.
-     * @deprecated Use hdfsBuilderConnect instead. 
-     */
-     hdfsFS hdfsConnect(const char* nn, tPort port);
-
-    /** 
-     * hdfsConnect - Connect to an hdfs file system.
-     *
-     * Forces a new instance to be created
-     *
-     * @param nn     The NameNode.  See hdfsBuilderSetNameNode for details.
-     * @param port   The port on which the server is listening.
-     * @param user   The user name to use when connecting
-     * @return       Returns a handle to the filesystem or NULL on error.
-     * @deprecated   Use hdfsBuilderConnect instead. 
-     */
-     hdfsFS hdfsConnectAsUserNewInstance(const char* nn, tPort port, const char *user );
-
-    /** 
-     * hdfsConnect - Connect to an hdfs file system.
-     *
-     * Forces a new instance to be created
-     *
-     * @param nn     The NameNode.  See hdfsBuilderSetNameNode for details.
-     * @param port   The port on which the server is listening.
-     * @return       Returns a handle to the filesystem or NULL on error.
-     * @deprecated   Use hdfsBuilderConnect instead. 
-     */
-     hdfsFS hdfsConnectNewInstance(const char* nn, tPort port);
-
-    /** 
-     * Connect to HDFS using the parameters defined by the builder.
-     *
-     * The HDFS builder will be freed, whether or not the connection was
-     * successful.
-     *
-     * Every successful call to hdfsBuilderConnect should be matched with a call
-     * to hdfsDisconnect, when the hdfsFS is no longer needed.
-     *
-     * @param bld    The HDFS builder
-     * @return       Returns a handle to the filesystem, or NULL on error.
-     */
-     hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld);
-
-    /**
-     * Create an HDFS builder.
-     *
-     * @return The HDFS builder, or NULL on error.
-     */
-    struct hdfsBuilder *hdfsNewBuilder(void);
-
-    /**
-     * Force the builder to always create a new instance of the FileSystem,
-     * rather than possibly finding one in the cache.
-     *
-     * @param bld The HDFS builder
-     */
-    void hdfsBuilderSetForceNewInstance(struct hdfsBuilder *bld);
-
-    /**
-     * Set the HDFS NameNode to connect to.
-     *
-     * @param bld  The HDFS builder
-     * @param nn   The NameNode to use.
-     *
-     *             If the string given is 'default', the default NameNode
-     *             configuration will be used (from the XML configuration files)
-     *
-     *             If NULL is given, a LocalFileSystem will be created.
-     *
-     *             If the string starts with a protocol type such as file:// or
-     *             hdfs://, this protocol type will be used.  If not, the
-     *             hdfs:// protocol type will be used.
-     *
-     *             You may specify a NameNode port in the usual way by 
-     *             passing a string of the format hdfs://<hostname>:<port>.
-     *             Alternately, you may set the port with
-     *             hdfsBuilderSetNameNodePort.  However, you must not pass the
-     *             port in two different ways.
-     */
-    void hdfsBuilderSetNameNode(struct hdfsBuilder *bld, const char *nn);
-
-    /**
-     * Set the port of the HDFS NameNode to connect to.
-     *
-     * @param bld The HDFS builder
-     * @param port The port.
-     */
-    void hdfsBuilderSetNameNodePort(struct hdfsBuilder *bld, tPort port);
-
-    /**
-     * Set the username to use when connecting to the HDFS cluster.
-     *
-     * @param bld The HDFS builder
-     * @param userName The user name.  The string will be shallow-copied.
-     */
-    void hdfsBuilderSetUserName(struct hdfsBuilder *bld, const char *userName);
-
-    /**
-     * Set the path to the Kerberos ticket cache to use when connecting to
-     * the HDFS cluster.
-     *
-     * @param bld The HDFS builder
-     * @param kerbTicketCachePath The Kerberos ticket cache path.  The string
-     *                            will be shallow-copied.
-     */
-    void hdfsBuilderSetKerbTicketCachePath(struct hdfsBuilder *bld,
-                                   const char *kerbTicketCachePath);
-
-    /**
-     * Free an HDFS builder.
-     *
-     * It is normally not necessary to call this function since
-     * hdfsBuilderConnect frees the builder.
-     *
-     * @param bld The HDFS builder
-     */
-    void hdfsFreeBuilder(struct hdfsBuilder *bld);
-
-    /**
-     * Get a configuration string.
-     *
-     * @param key      The key to find
-     * @param val      (out param) The value.  This will be NULL if the
-     *                 key isn't found.  You must free this string with
-     *                 hdfsConfFree.
-     *
-     * @return         0 on success; nonzero error code otherwise.
-     *                 Failure to find the key is not an error.
-     */
-    int hdfsConfGet(const char *key, char **val);
-
-    /**
-     * Free a configuration string found with hdfsConfGet. 
-     *
-     * @param val      A configuration string obtained from hdfsConfGet
-     */
-    void hdfsConfFree(char *val);
-
-    /** 
-     * hdfsDisconnect - Disconnect from the hdfs file system.
-     * Disconnect from hdfs.
-     * @param fs The configured filesystem handle.
-     * @return Returns 0 on success, -1 on error.  
-     */
-    int hdfsDisconnect(hdfsFS fs);
-        
-
-    /** 
-     * hdfsOpenFile - Open a hdfs file in given mode.
-     * @param fs The configured filesystem handle.
-     * @param path The full path to the file.
-     * @param flags - an | of bits/fcntl.h file flags - supported flags are O_RDONLY, O_WRONLY (meaning create or overwrite i.e., implies O_TRUNCAT), 
-     * O_WRONLY|O_APPEND. Other flags are generally ignored other than (O_RDWR || (O_EXCL & O_CREAT)) which return NULL and set errno equal ENOTSUP.
-     * @param bufferSize Size of buffer for read/write - pass 0 if you want
-     * to use the default configured values.
-     * @param replication Block replication - pass 0 if you want to use
-     * the default configured values.
-     * @param blocksize Size of block - pass 0 if you want to use the
-     * default configured values.
-     * @return Returns the handle to the open file or NULL on error.
-     */
-    hdfsFile hdfsOpenFile(hdfsFS fs, const char* path, int flags,
-                          int bufferSize, short replication, tSize blocksize);
-
-
-    /** 
-     * hdfsCloseFile - Close an open file. 
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @return Returns 0 on success, -1 on error.  
-     */
-    int hdfsCloseFile(hdfsFS fs, hdfsFile file);
-
-
-    /** 
-     * hdfsExists - Checks if a given path exsits on the filesystem 
-     * @param fs The configured filesystem handle.
-     * @param path The path to look for
-     * @return Returns 0 on success, -1 on error.  
-     */
-    int hdfsExists(hdfsFS fs, const char *path);
-
-
-    /** 
-     * hdfsSeek - Seek to given offset in file. 
-     * This works only for files opened in read-only mode. 
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @param desiredPos Offset into the file to seek into.
-     * @return Returns 0 on success, -1 on error.  
-     */
-    int hdfsSeek(hdfsFS fs, hdfsFile file, tOffset desiredPos); 
-
-
-    /** 
-     * hdfsTell - Get the current offset in the file, in bytes.
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @return Current offset, -1 on error.
-     */
-    tOffset hdfsTell(hdfsFS fs, hdfsFile file);
-
-
-    /** 
-     * hdfsRead - Read data from an open file.
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @param buffer The buffer to copy read bytes into.
-     * @param length The length of the buffer.
-     * @return      On success, a positive number indicating how many bytes
-     *              were read.
-     *              On end-of-file, 0.
-     *              On error, -1.  Errno will be set to the error code.
-     *              Just like the POSIX read function, hdfsRead will return -1
-     *              and set errno to EINTR if data is temporarily unavailable,
-     *              but we are not yet at the end of the file.
-     */
-    tSize hdfsRead(hdfsFS fs, hdfsFile file, void* buffer, tSize length);
-
-    /** 
-     * hdfsPread - Positional read of data from an open file.
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @param position Position from which to read
-     * @param buffer The buffer to copy read bytes into.
-     * @param length The length of the buffer.
-     * @return Returns the number of bytes actually read, possibly less than
-     * than length;-1 on error.
-     */
-    tSize hdfsPread(hdfsFS fs, hdfsFile file, tOffset position,
-                    void* buffer, tSize length);
-
-
-    /** 
-     * hdfsWrite - Write data into an open file.
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @param buffer The data.
-     * @param length The no. of bytes to write. 
-     * @return Returns the number of bytes written, -1 on error.
-     */
-    tSize hdfsWrite(hdfsFS fs, hdfsFile file, const void* buffer,
-                    tSize length);
-
-
-    /** 
-     * hdfsWrite - Flush the data. 
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsFlush(hdfsFS fs, hdfsFile file);
-
-
-    /**
-     * hdfsHFlush - Flush out the data in client's user buffer. After the
-     * return of this call, new readers will see the data.
-     * @param fs configured filesystem handle
-     * @param file file handle
-     * @return 0 on success, -1 on error and sets errno
-     */
-    int hdfsHFlush(hdfsFS fs, hdfsFile file);
-
-
-    /**
-     * hdfsAvailable - Number of bytes that can be read from this
-     * input stream without blocking.
-     * @param fs The configured filesystem handle.
-     * @param file The file handle.
-     * @return Returns available bytes; -1 on error. 
-     */
-    int hdfsAvailable(hdfsFS fs, hdfsFile file);
-
-
-    /**
-     * hdfsCopy - Copy file from one filesystem to another.
-     * @param srcFS The handle to source filesystem.
-     * @param src The path of source file. 
-     * @param dstFS The handle to destination filesystem.
-     * @param dst The path of destination file. 
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsCopy(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst);
-
-
-    /**
-     * hdfsMove - Move file from one filesystem to another.
-     * @param srcFS The handle to source filesystem.
-     * @param src The path of source file. 
-     * @param dstFS The handle to destination filesystem.
-     * @param dst The path of destination file. 
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsMove(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst);
-
-
-    /**
-     * hdfsDelete - Delete file. 
-     * @param fs The configured filesystem handle.
-     * @param path The path of the file. 
-     * @param recursive if path is a directory and set to 
-     * non-zero, the directory is deleted else throws an exception. In
-     * case of a file the recursive argument is irrelevant.
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsDelete(hdfsFS fs, const char* path, int recursive);
-
-    /**
-     * hdfsRename - Rename file. 
-     * @param fs The configured filesystem handle.
-     * @param oldPath The path of the source file. 
-     * @param newPath The path of the destination file. 
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsRename(hdfsFS fs, const char* oldPath, const char* newPath);
-
-
-    /** 
-     * hdfsGetWorkingDirectory - Get the current working directory for
-     * the given filesystem.
-     * @param fs The configured filesystem handle.
-     * @param buffer The user-buffer to copy path of cwd into. 
-     * @param bufferSize The length of user-buffer.
-     * @return Returns buffer, NULL on error.
-     */
-    char* hdfsGetWorkingDirectory(hdfsFS fs, char *buffer, size_t bufferSize);
-
-
-    /** 
-     * hdfsSetWorkingDirectory - Set the working directory. All relative
-     * paths will be resolved relative to it.
-     * @param fs The configured filesystem handle.
-     * @param path The path of the new 'cwd'. 
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsSetWorkingDirectory(hdfsFS fs, const char* path);
-
-
-    /** 
-     * hdfsCreateDirectory - Make the given file and all non-existent
-     * parents into directories.
-     * @param fs The configured filesystem handle.
-     * @param path The path of the directory. 
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsCreateDirectory(hdfsFS fs, const char* path);
-
-
-    /** 
-     * hdfsSetReplication - Set the replication of the specified
-     * file to the supplied value
-     * @param fs The configured filesystem handle.
-     * @param path The path of the file. 
-     * @return Returns 0 on success, -1 on error. 
-     */
-    int hdfsSetReplication(hdfsFS fs, const char* path, int16_t replication);
-
-
-    /** 
-     * hdfsFileInfo - Information about a file/directory.
-     */
-    typedef struct  {
-        tObjectKind mKind;   /* file or directory */
-        char *mName;         /* the name of the file */
-        tTime mLastMod;      /* the last modification time for the file in seconds */
-        tOffset mSize;       /* the size of the file in bytes */
-        short mReplication;    /* the count of replicas */
-        tOffset mBlockSize;  /* the block size for the file */
-        char *mOwner;        /* the owner of the file */
-        char *mGroup;        /* the group associated with the file */
-        short mPermissions;  /* the permissions associated with the file */
-        tTime mLastAccess;    /* the last access time for the file in seconds */
-    } hdfsFileInfo;
-
-
-    /** 
-     * hdfsListDirectory - Get list of files/directories for a given
-     * directory-path. hdfsFreeFileInfo should be called to deallocate memory. 
-     * @param fs The configured filesystem handle.
-     * @param path The path of the directory. 
-     * @param numEntries Set to the number of files/directories in path.
-     * @return Returns a dynamically-allocated array of hdfsFileInfo
-     * objects; NULL on error.
-     */
-    hdfsFileInfo *hdfsListDirectory(hdfsFS fs, const char* path,
-                                    int *numEntries);
-
-
-    /** 
-     * hdfsGetPathInfo - Get information about a path as a (dynamically
-     * allocated) single hdfsFileInfo struct. hdfsFreeFileInfo should be
-     * called when the pointer is no longer needed.
-     * @param fs The configured filesystem handle.
-     * @param path The path of the file. 
-     * @return Returns a dynamically-allocated hdfsFileInfo object;
-     * NULL on error.
-     */
-    hdfsFileInfo *hdfsGetPathInfo(hdfsFS fs, const char* path);
-
-
-    /** 
-     * hdfsFreeFileInfo - Free up the hdfsFileInfo array (including fields) 
-     * @param hdfsFileInfo The array of dynamically-allocated hdfsFileInfo
-     * objects.
-     * @param numEntries The size of the array.
-     */
-    void hdfsFreeFileInfo(hdfsFileInfo *hdfsFileInfo, int numEntries);
-
-
-    /** 
-     * hdfsGetHosts - Get hostnames where a particular block (determined by
-     * pos & blocksize) of a file is stored. The last element in the array
-     * is NULL. Due to replication, a single block could be present on
-     * multiple hosts.
-     * @param fs The configured filesystem handle.
-     * @param path The path of the file. 
-     * @param start The start of the block.
-     * @param length The length of the block.
-     * @return Returns a dynamically-allocated 2-d array of blocks-hosts;
-     * NULL on error.
-     */
-    char*** hdfsGetHosts(hdfsFS fs, const char* path, 
-            tOffset start, tOffset length);
-
-
-    /** 
-     * hdfsFreeHosts - Free up the structure returned by hdfsGetHosts
-     * @param hdfsFileInfo The array of dynamically-allocated hdfsFileInfo
-     * objects.
-     * @param numEntries The size of the array.
-     */
-    void hdfsFreeHosts(char ***blockHosts);
-
-
-    /** 
-     * hdfsGetDefaultBlockSize - Get the optimum blocksize.
-     * @param fs The configured filesystem handle.
-     * @return Returns the blocksize; -1 on error. 
-     */
-    tOffset hdfsGetDefaultBlockSize(hdfsFS fs);
-
-
-    /** 
-     * hdfsGetCapacity - Return the raw capacity of the filesystem.  
-     * @param fs The configured filesystem handle.
-     * @return Returns the raw-capacity; -1 on error. 
-     */
-    tOffset hdfsGetCapacity(hdfsFS fs);
-
-
-    /** 
-     * hdfsGetUsed - Return the total raw size of all files in the filesystem.
-     * @param fs The configured filesystem handle.
-     * @return Returns the total-size; -1 on error. 
-     */
-    tOffset hdfsGetUsed(hdfsFS fs);
-
-    /** 
-     * hdfsChown 
-     * @param fs The configured filesystem handle.
-     * @param path the path to the file or directory
-     * @param owner this is a string in Hadoop land. Set to null or "" if only setting group
-     * @param group  this is a string in Hadoop land. Set to null or "" if only setting user
-     * @return 0 on success else -1
-     */
-    int hdfsChown(hdfsFS fs, const char* path, const char *owner, const char *group);
-
-    /** 
-     * hdfsChmod
-     * @param fs The configured filesystem handle.
-     * @param path the path to the file or directory
-     * @param mode the bitmask to set it to
-     * @return 0 on success else -1
-     */
-      int hdfsChmod(hdfsFS fs, const char* path, short mode);
-
-    /** 
-     * hdfsUtime
-     * @param fs The configured filesystem handle.
-     * @param path the path to the file or directory
-     * @param mtime new modification time or -1 for no change
-     * @param atime new access time or -1 for no change
-     * @return 0 on success else -1
-     */
-    int hdfsUtime(hdfsFS fs, const char* path, tTime mtime, tTime atime);
-    
-#ifdef __cplusplus
-}
-#endif
-
-#endif /*LIBHDFS_HDFS_H*/
-
-/**
- * vim: ts=4: sw=4: et
- */
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.c
deleted file mode 100644
index f6d6651..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.c
+++ /dev/null
@@ -1,589 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "config.h"
-#include "hdfsJniHelper.h"
-
-#include <stdio.h> 
-#include <string.h> 
-
-static pthread_mutex_t hdfsHashMutex = PTHREAD_MUTEX_INITIALIZER;
-static pthread_mutex_t jvmMutex = PTHREAD_MUTEX_INITIALIZER;
-static volatile int hashTableInited = 0;
-
-#define LOCK_HASH_TABLE() pthread_mutex_lock(&hdfsHashMutex)
-#define UNLOCK_HASH_TABLE() pthread_mutex_unlock(&hdfsHashMutex)
-
-
-/** The Native return types that methods could return */
-#define VOID          'V'
-#define JOBJECT       'L'
-#define JARRAYOBJECT  '['
-#define JBOOLEAN      'Z'
-#define JBYTE         'B'
-#define JCHAR         'C'
-#define JSHORT        'S'
-#define JINT          'I'
-#define JLONG         'J'
-#define JFLOAT        'F'
-#define JDOUBLE       'D'
-
-
-/**
- * MAX_HASH_TABLE_ELEM: The maximum no. of entries in the hashtable.
- * It's set to 4096 to account for (classNames + No. of threads)
- */
-#define MAX_HASH_TABLE_ELEM 4096
-
-/** Key that allows us to retrieve thread-local storage */
-static pthread_key_t gTlsKey;
-
-/** nonzero if we succeeded in initializing gTlsKey. Protected by the jvmMutex */
-static int gTlsKeyInitialized = 0;
-
-/** Pthreads thread-local storage for each library thread. */
-struct hdfsTls {
-    JNIEnv *env;
-};
-
-/**
- * The function that is called whenever a thread with libhdfs thread local data
- * is destroyed.
- *
- * @param v         The thread-local data
- */
-static void hdfsThreadDestructor(void *v)
-{
-    struct hdfsTls *tls = v;
-    JavaVM *vm;
-    JNIEnv *env = tls->env;
-    jint ret;
-
-    ret = (*env)->GetJavaVM(env, &vm);
-    if (ret) {
-        fprintf(stderr, "hdfsThreadDestructor: GetJavaVM failed with "
-                "error %d\n", ret);
-        (*env)->ExceptionDescribe(env);
-    } else {
-        (*vm)->DetachCurrentThread(vm);
-    }
-    free(tls);
-}
-
-
-static int validateMethodType(MethType methType)
-{
-    if (methType != STATIC && methType != INSTANCE) {
-        fprintf(stderr, "Unimplemented method type\n");
-        return 0;
-    }
-    return 1;
-}
-
-
-static int hashTableInit(void)
-{
-    if (!hashTableInited) {
-        LOCK_HASH_TABLE();
-        if (!hashTableInited) {
-            if (hcreate(MAX_HASH_TABLE_ELEM) == 0) {
-                fprintf(stderr, "error creating hashtable, <%d>: %s\n",
-                        errno, strerror(errno));
-                return 0;
-            } 
-            hashTableInited = 1;
-        }
-        UNLOCK_HASH_TABLE();
-    }
-    return 1;
-}
-
-
-static int insertEntryIntoTable(const char *key, void *data)
-{
-    ENTRY e, *ep;
-    if (key == NULL || data == NULL) {
-        return 0;
-    }
-    if (! hashTableInit()) {
-      return -1;
-    }
-    e.data = data;
-    e.key = (char*)key;
-    LOCK_HASH_TABLE();
-    ep = hsearch(e, ENTER);
-    UNLOCK_HASH_TABLE();
-    if (ep == NULL) {
-        fprintf(stderr, "warn adding key (%s) to hash table, <%d>: %s\n",
-                key, errno, strerror(errno));
-    }  
-    return 0;
-}
-
-
-
-static void* searchEntryFromTable(const char *key)
-{
-    ENTRY e,*ep;
-    if (key == NULL) {
-        return NULL;
-    }
-    hashTableInit();
-    e.key = (char*)key;
-    LOCK_HASH_TABLE();
-    ep = hsearch(e, FIND);
-    UNLOCK_HASH_TABLE();
-    if (ep != NULL) {
-        return ep->data;
-    }
-    return NULL;
-}
-
-
-
-int invokeMethod(JNIEnv *env, RetVal *retval, Exc *exc, MethType methType,
-                 jobject instObj, const char *className,
-                 const char *methName, const char *methSignature, ...)
-{
-    va_list args;
-    jclass cls;
-    jmethodID mid;
-    jthrowable jthr;
-    const char *str; 
-    char returnType;
-    
-    if (! validateMethodType(methType)) {
-      return -1;
-    }
-    cls = globalClassReference(className, env);
-    if (cls == NULL) {
-      return -2;
-    }
-
-    mid = methodIdFromClass(className, methName, methSignature, 
-                            methType, env);
-    if (mid == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return -3;
-    }
-   
-    str = methSignature;
-    while (*str != ')') str++;
-    str++;
-    returnType = *str;
-    va_start(args, methSignature);
-    if (returnType == JOBJECT || returnType == JARRAYOBJECT) {
-        jobject jobj = NULL;
-        if (methType == STATIC) {
-            jobj = (*env)->CallStaticObjectMethodV(env, cls, mid, args);
-        }
-        else if (methType == INSTANCE) {
-            jobj = (*env)->CallObjectMethodV(env, instObj, mid, args);
-        }
-        retval->l = jobj;
-    }
-    else if (returnType == VOID) {
-        if (methType == STATIC) {
-            (*env)->CallStaticVoidMethodV(env, cls, mid, args);
-        }
-        else if (methType == INSTANCE) {
-            (*env)->CallVoidMethodV(env, instObj, mid, args);
-        }
-    }
-    else if (returnType == JBOOLEAN) {
-        jboolean jbool = 0;
-        if (methType == STATIC) {
-            jbool = (*env)->CallStaticBooleanMethodV(env, cls, mid, args);
-        }
-        else if (methType == INSTANCE) {
-            jbool = (*env)->CallBooleanMethodV(env, instObj, mid, args);
-        }
-        retval->z = jbool;
-    }
-    else if (returnType == JSHORT) {
-        jshort js = 0;
-        if (methType == STATIC) {
-            js = (*env)->CallStaticShortMethodV(env, cls, mid, args);
-        }
-        else if (methType == INSTANCE) {
-            js = (*env)->CallShortMethodV(env, instObj, mid, args);
-        }
-        retval->s = js;
-    }
-    else if (returnType == JLONG) {
-        jlong jl = -1;
-        if (methType == STATIC) {
-            jl = (*env)->CallStaticLongMethodV(env, cls, mid, args);
-        }
-        else if (methType == INSTANCE) {
-            jl = (*env)->CallLongMethodV(env, instObj, mid, args);
-        }
-        retval->j = jl;
-    }
-    else if (returnType == JINT) {
-        jint ji = -1;
-        if (methType == STATIC) {
-            ji = (*env)->CallStaticIntMethodV(env, cls, mid, args);
-        }
-        else if (methType == INSTANCE) {
-            ji = (*env)->CallIntMethodV(env, instObj, mid, args);
-        }
-        retval->i = ji;
-    }
-    va_end(args);
-
-    jthr = (*env)->ExceptionOccurred(env);
-    if (jthr != NULL) {
-        if (exc != NULL)
-            *exc = jthr;
-        else
-            (*env)->ExceptionDescribe(env);
-        return -1;
-    }
-    return 0;
-}
-
-jarray constructNewArrayString(JNIEnv *env, Exc *exc, const char **elements, int size) {
-  const char *className = "java/lang/String";
-  jobjectArray result;
-  int i;
-  jclass arrCls = (*env)->FindClass(env, className);
-  if (arrCls == NULL) {
-    fprintf(stderr, "could not find class %s\n",className);
-    return NULL; /* exception thrown */
-  }
-  result = (*env)->NewObjectArray(env, size, arrCls,
-                                  NULL);
-  if (result == NULL) {
-    fprintf(stderr, "ERROR: could not construct new array\n");
-    return NULL; /* out of memory error thrown */
-  }
-  for (i = 0; i < size; i++) {
-    jstring jelem = (*env)->NewStringUTF(env,elements[i]);
-    if (jelem == NULL) {
-      fprintf(stderr, "ERROR: jelem == NULL\n");
-    }
-    (*env)->SetObjectArrayElement(env, result, i, jelem);
-    (*env)->DeleteLocalRef(env, jelem);
-  }
-  return result;
-}
-
-jobject constructNewObjectOfClass(JNIEnv *env, Exc *exc, const char *className, 
-                                  const char *ctorSignature, ...)
-{
-    va_list args;
-    jclass cls;
-    jmethodID mid; 
-    jobject jobj;
-    jthrowable jthr;
-
-    cls = globalClassReference(className, env);
-    if (cls == NULL) {
-        (*env)->ExceptionDescribe(env);
-      return NULL;
-    }
-
-    mid = methodIdFromClass(className, "<init>", ctorSignature, 
-                            INSTANCE, env);
-    if (mid == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    } 
-    va_start(args, ctorSignature);
-    jobj = (*env)->NewObjectV(env, cls, mid, args);
-    va_end(args);
-    jthr = (*env)->ExceptionOccurred(env);
-    if (jthr != NULL) {
-        if (exc != NULL)
-            *exc = jthr;
-        else
-            (*env)->ExceptionDescribe(env);
-    }
-    return jobj;
-}
-
-
-
-
-jmethodID methodIdFromClass(const char *className, const char *methName, 
-                            const char *methSignature, MethType methType, 
-                            JNIEnv *env)
-{
-    jclass cls = globalClassReference(className, env);
-    if (cls == NULL) {
-      fprintf(stderr, "could not find class %s\n", className);
-      return NULL;
-    }
-
-    jmethodID mid = 0;
-    if (!validateMethodType(methType)) {
-      fprintf(stderr, "invalid method type\n");
-      return NULL;
-    }
-
-    if (methType == STATIC) {
-        mid = (*env)->GetStaticMethodID(env, cls, methName, methSignature);
-    }
-    else if (methType == INSTANCE) {
-        mid = (*env)->GetMethodID(env, cls, methName, methSignature);
-    }
-    if (mid == NULL) {
-      fprintf(stderr, "could not find method %s from class %s with signature %s\n",methName, className, methSignature);
-    }
-    return mid;
-}
-
-
-jclass globalClassReference(const char *className, JNIEnv *env)
-{
-    jclass clsLocalRef;
-    jclass cls = searchEntryFromTable(className);
-    if (cls) {
-        return cls; 
-    }
-
-    clsLocalRef = (*env)->FindClass(env,className);
-    if (clsLocalRef == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    }
-    cls = (*env)->NewGlobalRef(env, clsLocalRef);
-    if (cls == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    }
-    (*env)->DeleteLocalRef(env, clsLocalRef);
-    insertEntryIntoTable(className, cls);
-    return cls;
-}
-
-
-char *classNameOfObject(jobject jobj, JNIEnv *env) {
-    jclass cls, clsClass;
-    jmethodID mid;
-    jstring str;
-    const char *cstr;
-    char *newstr;
-
-    cls = (*env)->GetObjectClass(env, jobj);
-    if (cls == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    }
-    clsClass = (*env)->FindClass(env, "java/lang/Class");
-    if (clsClass == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    }
-    mid = (*env)->GetMethodID(env, clsClass, "getName", "()Ljava/lang/String;");
-    if (mid == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    }
-    str = (*env)->CallObjectMethod(env, cls, mid);
-    if (str == NULL) {
-        (*env)->ExceptionDescribe(env);
-        return NULL;
-    }
-
-    cstr = (*env)->GetStringUTFChars(env, str, NULL);
-    newstr = strdup(cstr);
-    (*env)->ReleaseStringUTFChars(env, str, cstr);
-    if (newstr == NULL) {
-        perror("classNameOfObject: strdup");
-        return NULL;
-    }
-    return newstr;
-}
-
-/**
- * Get the global JNI environemnt.
- *
- * We only have to create the JVM once.  After that, we can use it in
- * every thread.  You must be holding the jvmMutex when you call this
- * function.
- *
- * @return          The JNIEnv on success; error code otherwise
- */
-static JNIEnv* getGlobalJNIEnv(void)
-{
-    const jsize vmBufLength = 1;
-    JavaVM* vmBuf[vmBufLength]; 
-    JNIEnv *env;
-    jint rv = 0; 
-    jint noVMs = 0;
-
-    rv = JNI_GetCreatedJavaVMs(&(vmBuf[0]), vmBufLength, &noVMs);
-    if (rv != 0) {
-        fprintf(stderr, "JNI_GetCreatedJavaVMs failed with error: %d\n", rv);
-        return NULL;
-    }
-
-    if (noVMs == 0) {
-        //Get the environment variables for initializing the JVM
-        char *hadoopClassPath = getenv("CLASSPATH");
-        if (hadoopClassPath == NULL) {
-            fprintf(stderr, "Environment variable CLASSPATH not set!\n");
-            return NULL;
-        } 
-        char *hadoopClassPathVMArg = "-Djava.class.path=";
-        size_t optHadoopClassPathLen = strlen(hadoopClassPath) + 
-          strlen(hadoopClassPathVMArg) + 1;
-        char *optHadoopClassPath = malloc(sizeof(char)*optHadoopClassPathLen);
-        snprintf(optHadoopClassPath, optHadoopClassPathLen,
-                "%s%s", hadoopClassPathVMArg, hadoopClassPath);
-
-        // Determine the # of LIBHDFS_OPTS args
-        int noArgs = 1;
-        char *hadoopJvmArgs = getenv("LIBHDFS_OPTS");
-        char jvmArgDelims[] = " ";
-        char *str, *token, *savePtr;
-        if (hadoopJvmArgs != NULL)  {
-          hadoopJvmArgs = strdup(hadoopJvmArgs);
-          for (noArgs = 1, str = hadoopJvmArgs; ; noArgs++, str = NULL) {
-            token = strtok_r(str, jvmArgDelims, &savePtr);
-            if (NULL == token) {
-              break;
-            }
-          }
-          free(hadoopJvmArgs);
-        }
-
-        // Now that we know the # args, populate the options array
-        JavaVMOption options[noArgs];
-        options[0].optionString = optHadoopClassPath;
-        hadoopJvmArgs = getenv("LIBHDFS_OPTS");
-	if (hadoopJvmArgs != NULL)  {
-          hadoopJvmArgs = strdup(hadoopJvmArgs);
-          for (noArgs = 1, str = hadoopJvmArgs; ; noArgs++, str = NULL) {
-            token = strtok_r(str, jvmArgDelims, &savePtr);
-            if (NULL == token) {
-              break;
-            }
-            options[noArgs].optionString = token;
-          }
-        }
-
-        //Create the VM
-        JavaVMInitArgs vm_args;
-        JavaVM *vm;
-        vm_args.version = JNI_VERSION_1_2;
-        vm_args.options = options;
-        vm_args.nOptions = noArgs; 
-        vm_args.ignoreUnrecognized = 1;
-
-        rv = JNI_CreateJavaVM(&vm, (void*)&env, &vm_args);
-
-        if (hadoopJvmArgs != NULL)  {
-          free(hadoopJvmArgs);
-        }
-        free(optHadoopClassPath);
-
-        if (rv != 0) {
-            fprintf(stderr, "Call to JNI_CreateJavaVM failed "
-                    "with error: %d\n", rv);
-            return NULL;
-        }
-    }
-    else {
-        //Attach this thread to the VM
-        JavaVM* vm = vmBuf[0];
-        rv = (*vm)->AttachCurrentThread(vm, (void*)&env, 0);
-        if (rv != 0) {
-            fprintf(stderr, "Call to AttachCurrentThread "
-                    "failed with error: %d\n", rv);
-            return NULL;
-        }
-    }
-
-    return env;
-}
-
-/**
- * getJNIEnv: A helper function to get the JNIEnv* for the given thread.
- * If no JVM exists, then one will be created. JVM command line arguments
- * are obtained from the LIBHDFS_OPTS environment variable.
- *
- * Implementation note: we rely on POSIX thread-local storage (tls).
- * This allows us to associate a destructor function with each thread, that
- * will detach the thread from the Java VM when the thread terminates.  If we
- * failt to do this, it will cause a memory leak.
- *
- * However, POSIX TLS is not the most efficient way to do things.  It requires a
- * key to be initialized before it can be used.  Since we don't know if this key
- * is initialized at the start of this function, we have to lock a mutex first
- * and check.  Luckily, most operating systems support the more efficient
- * __thread construct, which is initialized by the linker.
- *
- * @param: None.
- * @return The JNIEnv* corresponding to the thread.
- */
-JNIEnv* getJNIEnv(void)
-{
-    JNIEnv *env;
-    struct hdfsTls *tls;
-    int ret;
-
-#ifdef HAVE_BETTER_TLS
-    static __thread struct hdfsTls *quickTls = NULL;
-    if (quickTls)
-        return quickTls->env;
-#endif
-    pthread_mutex_lock(&jvmMutex);
-    if (!gTlsKeyInitialized) {
-        ret = pthread_key_create(&gTlsKey, hdfsThreadDestructor);
-        if (ret) {
-            pthread_mutex_unlock(&jvmMutex);
-            fprintf("pthread_key_create failed with error %d\n", ret);
-            return NULL;
-        }
-        gTlsKeyInitialized = 1;
-    }
-    tls = pthread_getspecific(gTlsKey);
-    if (tls) {
-        pthread_mutex_unlock(&jvmMutex);
-        return tls->env;
-    }
-
-    env = getGlobalJNIEnv();
-    pthread_mutex_unlock(&jvmMutex);
-    if (!env) {
-        fprintf(stderr, "getJNIEnv: getGlobalJNIEnv failed\n");
-        return NULL;
-    }
-    tls = calloc(1, sizeof(struct hdfsTls));
-    if (!tls) {
-        fprintf(stderr, "getJNIEnv: OOM allocating %d bytes\n",
-                sizeof(struct hdfsTls));
-        return NULL;
-    }
-    tls->env = env;
-    ret = pthread_setspecific(gTlsKey, tls);
-    if (ret) {
-        fprintf(stderr, "getJNIEnv: pthread_setspecific failed with "
-            "error code %d\n", ret);
-        hdfsThreadDestructor(tls);
-        return NULL;
-    }
-#ifdef HAVE_BETTER_TLS
-    quickTls = tls;
-#endif
-    return env;
-}
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.h
deleted file mode 100644
index 442eedf..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfsJniHelper.h
+++ /dev/null
@@ -1,109 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LIBHDFS_JNI_HELPER_H
-#define LIBHDFS_JNI_HELPER_H
-
-#include <jni.h>
-#include <stdio.h>
-
-#include <stdlib.h>
-#include <stdarg.h>
-#include <search.h>
-#include <pthread.h>
-#include <errno.h>
-
-#define PATH_SEPARATOR ':'
-
-
-/** Denote the method we want to invoke as STATIC or INSTANCE */
-typedef enum {
-    STATIC,
-    INSTANCE
-} MethType;
-
-
-/** Used for returning an appropriate return value after invoking
- * a method
- */
-typedef jvalue RetVal;
-
-/** Used for returning the exception after invoking a method */
-typedef jthrowable Exc;
-
-/** invokeMethod: Invoke a Static or Instance method.
- * className: Name of the class where the method can be found
- * methName: Name of the method
- * methSignature: the signature of the method "(arg-types)ret-type"
- * methType: The type of the method (STATIC or INSTANCE)
- * instObj: Required if the methType is INSTANCE. The object to invoke
-   the method on.
- * env: The JNIEnv pointer
- * retval: The pointer to a union type which will contain the result of the
-   method invocation, e.g. if the method returns an Object, retval will be
-   set to that, if the method returns boolean, retval will be set to the
-   value (JNI_TRUE or JNI_FALSE), etc.
- * exc: If the methods throws any exception, this will contain the reference
- * Arguments (the method arguments) must be passed after methSignature
- * RETURNS: -1 on error and 0 on success. If -1 is returned, exc will have 
-   a valid exception reference, and the result stored at retval is undefined.
- */
-int invokeMethod(JNIEnv *env, RetVal *retval, Exc *exc, MethType methType,
-                 jobject instObj, const char *className, const char *methName, 
-                 const char *methSignature, ...);
-
-/** constructNewObjectOfClass: Invoke a constructor.
- * className: Name of the class
- * ctorSignature: the signature of the constructor "(arg-types)V"
- * env: The JNIEnv pointer
- * exc: If the ctor throws any exception, this will contain the reference
- * Arguments to the ctor must be passed after ctorSignature 
- */
-jobject constructNewObjectOfClass(JNIEnv *env, Exc *exc, const char *className, 
-                                  const char *ctorSignature, ...);
-
-jmethodID methodIdFromClass(const char *className, const char *methName, 
-                            const char *methSignature, MethType methType, 
-                            JNIEnv *env);
-
-jclass globalClassReference(const char *className, JNIEnv *env);
-
-/** classNameOfObject: Get an object's class name.
- * @param jobj: The object.
- * @param env: The JNIEnv pointer.
- * @return Returns a pointer to a string containing the class name. This string
- * must be freed by the caller.
- */
-char *classNameOfObject(jobject jobj, JNIEnv *env);
-
-/** getJNIEnv: A helper function to get the JNIEnv* for the given thread.
- * If no JVM exists, then one will be created. JVM command line arguments
- * are obtained from the LIBHDFS_OPTS environment variable.
- * @param: None.
- * @return The JNIEnv* corresponding to the thread.
- * */
-JNIEnv* getJNIEnv(void);
-
-jarray constructNewArrayString(JNIEnv *env, Exc *exc, const char **elements, int size) ;
-
-#endif /*LIBHDFS_JNI_HELPER_H*/
-
-/**
- * vim: ts=4: sw=4: et:
- */
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_read.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_read.c
deleted file mode 100644
index 423f703..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_read.c
+++ /dev/null
@@ -1,70 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hdfs.h" 
-
-#include <stdio.h>
-#include <stdlib.h>
-
-int main(int argc, char **argv) {
-
-    if (argc != 4) {
-        fprintf(stderr, "Usage: hdfs_read <filename> <filesize> <buffersize>\n");
-        exit(-1);
-    }
-    
-    hdfsFS fs = hdfsConnect("default", 0);
-    if (!fs) {
-        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
-        exit(-1);
-    } 
- 
-    const char* rfile = argv[1];
-    tSize fileTotalSize = strtoul(argv[2], NULL, 10);
-    tSize bufferSize = strtoul(argv[3], NULL, 10);
-   
-    hdfsFile readFile = hdfsOpenFile(fs, rfile, O_RDONLY, bufferSize, 0, 0);
-    if (!readFile) {
-        fprintf(stderr, "Failed to open %s for writing!\n", rfile);
-        exit(-2);
-    }
-
-    // data to be written to the file
-    char* buffer = malloc(sizeof(char) * bufferSize);
-    if(buffer == NULL) {
-        return -2;
-    }
-    
-    // read from the file
-    tSize curSize = bufferSize;
-    for (; curSize == bufferSize;) {
-        curSize = hdfsRead(fs, readFile, (void*)buffer, curSize);
-    }
-    
-
-    free(buffer);
-    hdfsCloseFile(fs, readFile);
-    hdfsDisconnect(fs);
-
-    return 0;
-}
-
-/**
- * vim: ts=4: sw=4: et:
- */
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.c
deleted file mode 100644
index c2a0cbd..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.c
+++ /dev/null
@@ -1,525 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hdfs.h" 
-#include "hdfs_test.h" 
-
-#include <inttypes.h>
-#include <jni.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <time.h>
-#include <unistd.h>
-
-void permission_disp(short permissions, char *rtr) {
-  rtr[9] = '\0';
-  int i;
-  for(i=2;i>=0;i--)
-    {
-      short permissionsId = permissions >> (i * 3) & (short)7;
-      char* perm;
-      switch(permissionsId) {
-      case 7:
-        perm = "rwx"; break;
-      case 6:
-        perm = "rw-"; break;
-      case 5:
-        perm = "r-x"; break;
-      case 4:
-        perm = "r--"; break;
-      case 3:
-        perm = "-wx"; break;
-      case 2:
-        perm = "-w-"; break;
-      case 1:
-        perm = "--x"; break;
-      case 0:
-        perm = "---"; break;
-      default:
-        perm = "???";
-      }
-      strncpy(rtr, perm, 3);
-      rtr+=3;
-    }
-} 
-
-int main(int argc, char **argv) {
-    char buffer[32];
-    tSize num_written_bytes;
-
-    hdfsFS fs = hdfsConnectNewInstance("default", 0);
-    if(!fs) {
-        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
-        exit(-1);
-    } 
- 
-    hdfsFS lfs = hdfsConnectNewInstance(NULL, 0);
-    if(!lfs) {
-        fprintf(stderr, "Oops! Failed to connect to 'local' hdfs!\n");
-        exit(-1);
-    } 
-
-    const char* writePath = "/tmp/testfile.txt";
-    const char* fileContents = "Hello, World!";
-
-    {
-        //Write tests
-        
-        hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
-        if(!writeFile) {
-            fprintf(stderr, "Failed to open %s for writing!\n", writePath);
-            exit(-1);
-        }
-        fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
-        num_written_bytes =
-          hdfsWrite(fs, writeFile, (void*)fileContents, strlen(fileContents)+1);
-        if (num_written_bytes != strlen(fileContents) + 1) {
-          fprintf(stderr, "Failed to write correct number of bytes - expected %d, got %d\n",
-                  (int)(strlen(fileContents) + 1), (int)num_written_bytes);
-            exit(-1);
-        }
-        fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
-
-        tOffset currentPos = -1;
-        if ((currentPos = hdfsTell(fs, writeFile)) == -1) {
-            fprintf(stderr, 
-                    "Failed to get current file position correctly! Got %ld!\n",
-                    currentPos);
-            exit(-1);
-        }
-        fprintf(stderr, "Current position: %ld\n", currentPos);
-
-        if (hdfsFlush(fs, writeFile)) {
-            fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
-            exit(-1);
-        }
-        fprintf(stderr, "Flushed %s successfully!\n", writePath); 
-
-        if (hdfsHFlush(fs, writeFile)) {
-            fprintf(stderr, "Failed to 'hflush' %s\n", writePath);
-            exit(-1);
-        }
-        fprintf(stderr, "HFlushed %s successfully!\n", writePath);
-
-        hdfsCloseFile(fs, writeFile);
-    }
-
-    {
-        //Read tests
-        
-        const char* readPath = "/tmp/testfile.txt";
-        int exists = hdfsExists(fs, readPath);
-
-        if (exists) {
-          fprintf(stderr, "Failed to validate existence of %s\n", readPath);
-          exit(-1);
-        }
-
-        hdfsFile readFile = hdfsOpenFile(fs, readPath, O_RDONLY, 0, 0, 0);
-        if (!readFile) {
-            fprintf(stderr, "Failed to open %s for reading!\n", readPath);
-            exit(-1);
-        }
-
-        if (!hdfsFileIsOpenForRead(readFile)) {
-            fprintf(stderr, "hdfsFileIsOpenForRead: we just opened a file "
-                    "with O_RDONLY, and it did not show up as 'open for "
-                    "read'\n");
-            exit(-1);
-        }
-
-        fprintf(stderr, "hdfsAvailable: %d\n", hdfsAvailable(fs, readFile));
-
-        tOffset seekPos = 1;
-        if(hdfsSeek(fs, readFile, seekPos)) {
-            fprintf(stderr, "Failed to seek %s for reading!\n", readPath);
-            exit(-1);
-        }
-
-        tOffset currentPos = -1;
-        if((currentPos = hdfsTell(fs, readFile)) != seekPos) {
-            fprintf(stderr, 
-                    "Failed to get current file position correctly! Got %ld!\n", 
-                    currentPos);
-            exit(-1);
-        }
-        fprintf(stderr, "Current position: %ld\n", currentPos);
-
-        if (!hdfsFileUsesDirectRead(readFile)) {
-          fprintf(stderr, "Direct read support incorrectly not detected "
-                  "for HDFS filesystem\n");
-          exit(-1);
-        }
-
-        fprintf(stderr, "Direct read support detected for HDFS\n");
-
-        // Test the direct read path
-        if(hdfsSeek(fs, readFile, 0)) {
-            fprintf(stderr, "Failed to seek %s for reading!\n", readPath);
-            exit(-1);
-        }
-        memset(buffer, 0, sizeof(buffer));
-        tSize num_read_bytes = hdfsRead(fs, readFile, (void*)buffer,
-                sizeof(buffer));
-        if (strncmp(fileContents, buffer, strlen(fileContents)) != 0) {
-            fprintf(stderr, "Failed to read (direct). Expected %s but got %s (%d bytes)\n",
-                    fileContents, buffer, num_read_bytes);
-            exit(-1);
-        }
-        fprintf(stderr, "Read (direct) following %d bytes:\n%s\n",
-                num_read_bytes, buffer);
-        if (hdfsSeek(fs, readFile, 0L)) {
-            fprintf(stderr, "Failed to seek to file start!\n");
-            exit(-1);
-        }
-
-        // Disable the direct read path so that we really go through the slow
-        // read path
-        hdfsFileDisableDirectRead(readFile);
-
-        num_read_bytes = hdfsRead(fs, readFile, (void*)buffer, 
-                sizeof(buffer));
-        fprintf(stderr, "Read following %d bytes:\n%s\n", 
-                num_read_bytes, buffer);
-
-        memset(buffer, 0, strlen(fileContents + 1));
-
-        num_read_bytes = hdfsPread(fs, readFile, 0, (void*)buffer, 
-                sizeof(buffer));
-        fprintf(stderr, "Read following %d bytes:\n%s\n", 
-                num_read_bytes, buffer);
-
-        hdfsCloseFile(fs, readFile);
-
-        // Test correct behaviour for unsupported filesystems
-        hdfsFile localFile = hdfsOpenFile(lfs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
-        if(!localFile) {
-            fprintf(stderr, "Failed to open %s for writing!\n", writePath);
-            exit(-1);
-        }
-
-        num_written_bytes = hdfsWrite(lfs, localFile, (void*)fileContents,
-                                      strlen(fileContents) + 1);
-
-        hdfsCloseFile(lfs, localFile);
-        localFile = hdfsOpenFile(lfs, writePath, O_RDONLY, 0, 0, 0);
-
-        if (hdfsFileUsesDirectRead(localFile)) {
-          fprintf(stderr, "Direct read support incorrectly detected for local "
-                  "filesystem\n");
-          exit(-1);
-        }
-
-        hdfsCloseFile(lfs, localFile);
-    }
-
-    int totalResult = 0;
-    int result = 0;
-    {
-        //Generic file-system operations
-
-        const char* srcPath = "/tmp/testfile.txt";
-        const char* dstPath = "/tmp/testfile2.txt";
-
-        fprintf(stderr, "hdfsCopy(remote-local): %s\n", ((result = hdfsCopy(fs, srcPath, lfs, srcPath)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsCopy(remote-remote): %s\n", ((result = hdfsCopy(fs, srcPath, fs, dstPath)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsMove(local-local): %s\n", ((result = hdfsMove(lfs, srcPath, lfs, dstPath)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsMove(remote-local): %s\n", ((result = hdfsMove(fs, srcPath, lfs, srcPath)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        fprintf(stderr, "hdfsRename: %s\n", ((result = hdfsRename(fs, dstPath, srcPath)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsCopy(remote-remote): %s\n", ((result = hdfsCopy(fs, srcPath, fs, dstPath)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        const char* slashTmp = "/tmp";
-        const char* newDirectory = "/tmp/newdir";
-        fprintf(stderr, "hdfsCreateDirectory: %s\n", ((result = hdfsCreateDirectory(fs, newDirectory)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        fprintf(stderr, "hdfsSetReplication: %s\n", ((result = hdfsSetReplication(fs, srcPath, 2)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        char buffer[256];
-        const char *resp;
-        fprintf(stderr, "hdfsGetWorkingDirectory: %s\n", ((resp = hdfsGetWorkingDirectory(fs, buffer, sizeof(buffer))) ? buffer : "Failed!"));
-        totalResult += (resp ? 0 : 1);
-        fprintf(stderr, "hdfsSetWorkingDirectory: %s\n", ((result = hdfsSetWorkingDirectory(fs, slashTmp)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsGetWorkingDirectory: %s\n", ((resp = hdfsGetWorkingDirectory(fs, buffer, sizeof(buffer))) ? buffer : "Failed!"));
-        totalResult += (resp ? 0 : 1);
-
-        fprintf(stderr, "hdfsGetDefaultBlockSize: %ld\n", hdfsGetDefaultBlockSize(fs));
-        fprintf(stderr, "hdfsGetCapacity: %ld\n", hdfsGetCapacity(fs));
-        fprintf(stderr, "hdfsGetUsed: %ld\n", hdfsGetUsed(fs));
-
-        hdfsFileInfo *fileInfo = NULL;
-        if((fileInfo = hdfsGetPathInfo(fs, slashTmp)) != NULL) {
-            fprintf(stderr, "hdfsGetPathInfo - SUCCESS!\n");
-            fprintf(stderr, "Name: %s, ", fileInfo->mName);
-            fprintf(stderr, "Type: %c, ", (char)(fileInfo->mKind));
-            fprintf(stderr, "Replication: %d, ", fileInfo->mReplication);
-            fprintf(stderr, "BlockSize: %ld, ", fileInfo->mBlockSize);
-            fprintf(stderr, "Size: %ld, ", fileInfo->mSize);
-            fprintf(stderr, "LastMod: %s", ctime(&fileInfo->mLastMod)); 
-            fprintf(stderr, "Owner: %s, ", fileInfo->mOwner);
-            fprintf(stderr, "Group: %s, ", fileInfo->mGroup);
-            char permissions[10];
-            permission_disp(fileInfo->mPermissions, permissions);
-            fprintf(stderr, "Permissions: %d (%s)\n", fileInfo->mPermissions, permissions);
-            hdfsFreeFileInfo(fileInfo, 1);
-        } else {
-            totalResult++;
-            fprintf(stderr, "waah! hdfsGetPathInfo for %s - FAILED!\n", slashTmp);
-        }
-
-        hdfsFileInfo *fileList = 0;
-        int numEntries = 0;
-        if((fileList = hdfsListDirectory(fs, slashTmp, &numEntries)) != NULL) {
-            int i = 0;
-            for(i=0; i < numEntries; ++i) {
-                fprintf(stderr, "Name: %s, ", fileList[i].mName);
-                fprintf(stderr, "Type: %c, ", (char)fileList[i].mKind);
-                fprintf(stderr, "Replication: %d, ", fileList[i].mReplication);
-                fprintf(stderr, "BlockSize: %ld, ", fileList[i].mBlockSize);
-                fprintf(stderr, "Size: %ld, ", fileList[i].mSize);
-                fprintf(stderr, "LastMod: %s", ctime(&fileList[i].mLastMod));
-                fprintf(stderr, "Owner: %s, ", fileList[i].mOwner);
-                fprintf(stderr, "Group: %s, ", fileList[i].mGroup);
-                char permissions[10];
-                permission_disp(fileList[i].mPermissions, permissions);
-                fprintf(stderr, "Permissions: %d (%s)\n", fileList[i].mPermissions, permissions);
-            }
-            hdfsFreeFileInfo(fileList, numEntries);
-        } else {
-            if (errno) {
-                totalResult++;
-                fprintf(stderr, "waah! hdfsListDirectory - FAILED!\n");
-            } else {
-                fprintf(stderr, "Empty directory!\n");
-            }
-        }
-
-        char*** hosts = hdfsGetHosts(fs, srcPath, 0, 1);
-        if(hosts) {
-            fprintf(stderr, "hdfsGetHosts - SUCCESS! ... \n");
-            int i=0; 
-            while(hosts[i]) {
-                int j = 0;
-                while(hosts[i][j]) {
-                    fprintf(stderr, 
-                            "\thosts[%d][%d] - %s\n", i, j, hosts[i][j]);
-                    ++j;
-                }
-                ++i;
-            }
-        } else {
-            totalResult++;
-            fprintf(stderr, "waah! hdfsGetHosts - FAILED!\n");
-        }
-       
-        char *newOwner = "root";
-        // setting tmp dir to 777 so later when connectAsUser nobody, we can write to it
-        short newPerm = 0666;
-
-        // chown write
-        fprintf(stderr, "hdfsChown: %s\n", ((result = hdfsChown(fs, writePath, NULL, "users")) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsChown: %s\n", ((result = hdfsChown(fs, writePath, newOwner, NULL)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        // chmod write
-        fprintf(stderr, "hdfsChmod: %s\n", ((result = hdfsChmod(fs, writePath, newPerm)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-
-
-        sleep(2);
-        tTime newMtime = time(NULL);
-        tTime newAtime = time(NULL);
-
-        // utime write
-        fprintf(stderr, "hdfsUtime: %s\n", ((result = hdfsUtime(fs, writePath, newMtime, newAtime)) ? "Failed!" : "Success!"));
-
-        totalResult += result;
-
-        // chown/chmod/utime read
-        hdfsFileInfo *finfo = hdfsGetPathInfo(fs, writePath);
-
-        fprintf(stderr, "hdfsChown read: %s\n", ((result = (strcmp(finfo->mOwner, newOwner) != 0)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        fprintf(stderr, "hdfsChmod read: %s\n", ((result = (finfo->mPermissions != newPerm)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        // will later use /tmp/ as a different user so enable it
-        fprintf(stderr, "hdfsChmod: %s\n", ((result = hdfsChmod(fs, "/tmp/", 0777)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        fprintf(stderr,"newMTime=%ld\n",newMtime);
-        fprintf(stderr,"curMTime=%ld\n",finfo->mLastMod);
-
-
-        fprintf(stderr, "hdfsUtime read (mtime): %s\n", ((result = (finfo->mLastMod != newMtime)) ? "Failed!" : "Success!"));
-        totalResult += result;
-
-        // No easy way to turn on access times from hdfs_test right now
-        //        fprintf(stderr, "hdfsUtime read (atime): %s\n", ((result = (finfo->mLastAccess != newAtime)) ? "Failed!" : "Success!"));
-        //        totalResult += result;
-
-        hdfsFreeFileInfo(finfo, 1);
-
-        // Clean up
-        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(fs, newDirectory, 1)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(fs, srcPath, 1)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(lfs, srcPath, 1)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(lfs, dstPath, 1)) ? "Failed!" : "Success!"));
-        totalResult += result;
-        fprintf(stderr, "hdfsExists: %s\n", ((result = hdfsExists(fs, newDirectory)) ? "Success!" : "Failed!"));
-        totalResult += (result ? 0 : 1);
-    }
-
-    {
-      // TEST APPENDS
-      const char *writePath = "/tmp/appends";
-
-      // CREATE
-      hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY, 0, 0, 0);
-      if(!writeFile) {
-        fprintf(stderr, "Failed to open %s for writing!\n", writePath);
-        exit(-1);
-      }
-      fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
-
-      char* buffer = "Hello,";
-      tSize num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer));
-      fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
-
-      if (hdfsFlush(fs, writeFile)) {
-        fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
-        exit(-1);
-        }
-      fprintf(stderr, "Flushed %s successfully!\n", writePath); 
-
-      hdfsCloseFile(fs, writeFile);
-
-      // RE-OPEN
-      writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_APPEND, 0, 0, 0);
-      if(!writeFile) {
-        fprintf(stderr, "Failed to open %s for writing!\n", writePath);
-        exit(-1);
-      }
-      fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
-
-      buffer = " World";
-      num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer) + 1);
-      fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
-
-      if (hdfsFlush(fs, writeFile)) {
-        fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
-        exit(-1);
-      }
-      fprintf(stderr, "Flushed %s successfully!\n", writePath); 
-
-      hdfsCloseFile(fs, writeFile);
-
-      // CHECK size
-      hdfsFileInfo *finfo = hdfsGetPathInfo(fs, writePath);
-      fprintf(stderr, "fileinfo->mSize: == total %s\n", ((result = (finfo->mSize == strlen("Hello, World") + 1)) ? "Success!" : "Failed!"));
-      totalResult += (result ? 0 : 1);
-
-      // READ and check data
-      hdfsFile readFile = hdfsOpenFile(fs, writePath, O_RDONLY, 0, 0, 0);
-      if (!readFile) {
-        fprintf(stderr, "Failed to open %s for reading!\n", writePath);
-        exit(-1);
-      }
-
-      char rdbuffer[32];
-      tSize num_read_bytes = hdfsRead(fs, readFile, (void*)rdbuffer, sizeof(rdbuffer));
-      fprintf(stderr, "Read following %d bytes:\n%s\n", 
-              num_read_bytes, rdbuffer);
-
-      fprintf(stderr, "read == Hello, World %s\n", (result = (strcmp(rdbuffer, "Hello, World") == 0)) ? "Success!" : "Failed!");
-
-      hdfsCloseFile(fs, readFile);
-
-      // DONE test appends
-    }
-      
-      
-    totalResult += (hdfsDisconnect(fs) != 0);
-
-    {
-      //
-      // Now test as connecting as a specific user
-      // This is only meant to test that we connected as that user, not to test
-      // the actual fs user capabilities. Thus just create a file and read
-      // the owner is correct.
-
-      const char *tuser = "nobody";
-      const char* writePath = "/tmp/usertestfile.txt";
-
-      fs = hdfsConnectAsUserNewInstance("default", 0, tuser);
-      if(!fs) {
-        fprintf(stderr, "Oops! Failed to connect to hdfs as user %s!\n",tuser);
-        exit(-1);
-      } 
-
-        hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
-        if(!writeFile) {
-            fprintf(stderr, "Failed to open %s for writing!\n", writePath);
-            exit(-1);
-        }
-        fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
-
-        char* buffer = "Hello, World!";
-        tSize num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer)+1);
-        fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
-
-        if (hdfsFlush(fs, writeFile)) {
-            fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
-            exit(-1);
-        }
-        fprintf(stderr, "Flushed %s successfully!\n", writePath); 
-
-        hdfsCloseFile(fs, writeFile);
-
-        hdfsFileInfo *finfo = hdfsGetPathInfo(fs, writePath);
-        fprintf(stderr, "hdfs new file user is correct: %s\n", ((result = (strcmp(finfo->mOwner, tuser) != 0)) ? "Failed!" : "Success!"));
-        totalResult += result;
-    }
-    
-    totalResult += (hdfsDisconnect(fs) != 0);
-
-    if (totalResult != 0) {
-        return -1;
-    } else {
-        return 0;
-    }
-}
-
-/**
- * vim: ts=4: sw=4: et:
- */
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.h
deleted file mode 100644
index ec59ba3..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_test.h
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LIBHDFS_HDFS_TEST_H
-#define LIBHDFS_HDFS_TEST_H
-
-struct hdfs_internal;
-
-/**
- * Some functions that are visible only for testing.
- *
- * This header is not meant to be exported or used outside of the libhdfs unit
- * tests.
- */
-
-#ifdef __cplusplus
-extern  "C" {
-#endif
-    /**
-     * Determine if a file is using the "direct read" optimization.
-     *
-     * @param file     The HDFS file
-     * @return         1 if the file is using the direct read optimization,
-     *                 0 otherwise.
-     */
-    int hdfsFileUsesDirectRead(struct hdfs_internal *file);
-#ifdef __cplusplus
-}
-#endif
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_write.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_write.c
deleted file mode 100644
index b0f320c..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/hdfs_write.c
+++ /dev/null
@@ -1,94 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hdfs.h" 
-
-#include <limits.h>
-#include <stdio.h>
-#include <stdlib.h>
-
-int main(int argc, char **argv) {
-
-    if (argc != 4) {
-        fprintf(stderr, "Usage: hdfs_write <filename> <filesize> <buffersize>\n");
-        exit(-1);
-    }
-    
-    hdfsFS fs = hdfsConnect("default", 0);
-    if (!fs) {
-        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
-        exit(-1);
-    } 
- 
-    const char* writeFileName = argv[1];
-    off_t fileTotalSize = strtoul(argv[2], NULL, 10);
-    long long tmpBufferSize = strtoul(argv[3], NULL, 10);
-
-    // sanity check
-    if(fileTotalSize == ULONG_MAX && errno == ERANGE) {
-      fprintf(stderr, "invalid file size %s - must be <= %lu\n", argv[2], ULONG_MAX);
-      exit(-3);
-    }
-
-    // currently libhdfs writes are of tSize which is int32
-    if(tmpBufferSize > INT_MAX) {
-      fprintf(stderr, "invalid buffer size libhdfs API write chunks must be <= %d\n",INT_MAX);
-      exit(-3);
-    }
-
-    tSize bufferSize = tmpBufferSize;
-
-    hdfsFile writeFile = hdfsOpenFile(fs, writeFileName, O_WRONLY, bufferSize, 0, 0);
-    if (!writeFile) {
-        fprintf(stderr, "Failed to open %s for writing!\n", writeFileName);
-        exit(-2);
-    }
-
-    // data to be written to the file
-    char* buffer = malloc(sizeof(char) * bufferSize);
-    if(buffer == NULL) {
-        fprintf(stderr, "Could not allocate buffer of size %d\n", bufferSize);
-        return -2;
-    }
-    int i = 0;
-    for (i=0; i < bufferSize; ++i) {
-        buffer[i] = 'a' + (i%26);
-    }
-
-    // write to the file
-    off_t nrRemaining;
-    for (nrRemaining = fileTotalSize; nrRemaining > 0; nrRemaining -= bufferSize ) {
-      tSize curSize = ( bufferSize < nrRemaining ) ? bufferSize : (tSize)nrRemaining; 
-      tSize written;
-      if ((written = hdfsWrite(fs, writeFile, (void*)buffer, curSize)) != curSize) {
-        fprintf(stderr, "ERROR: hdfsWrite returned an error on write: %d\n", written);
-        exit(-3);
-      }
-    }
-
-    free(buffer);
-    hdfsCloseFile(fs, writeFile);
-    hdfsDisconnect(fs);
-
-    return 0;
-}
-
-/**
- * vim: ts=4: sw=4: et:
- */
-
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/expect.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/expect.h
new file mode 100644
index 0000000..2046bd0
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/expect.h
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef LIBHDFS_NATIVE_TESTS_EXPECT_H
+#define LIBHDFS_NATIVE_TESTS_EXPECT_H
+
+#include <stdio.h>
+
+#define EXPECT_ZERO(x) \
+    do { \
+        int __my_ret__ = x; \
+        if (__my_ret__) { \
+            int __my_errno__ = errno; \
+            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
+		    "code %d (errno: %d): got nonzero from %s\n", \
+		    __LINE__, __my_ret__, __my_errno__, #x); \
+            return __my_ret__; \
+        } \
+    } while (0);
+
+#define EXPECT_NULL(x) \
+    do { \
+        void* __my_ret__ = x; \
+        int __my_errno__ = errno; \
+        if (__my_ret__ != NULL) { \
+            fprintf(stderr, "TEST_ERROR: failed on line %d (errno: %d): " \
+		    "got non-NULL value %p from %s\n", \
+		    __LINE__, __my_errno__, __my_ret__, #x); \
+            return -1; \
+        } \
+    } while (0);
+
+#define EXPECT_NONNULL(x) \
+    do { \
+        void* __my_ret__ = x; \
+        int __my_errno__ = errno; \
+        if (__my_ret__ == NULL) { \
+            fprintf(stderr, "TEST_ERROR: failed on line %d (errno: %d): " \
+		    "got NULL from %s\n", __LINE__, __my_errno__, #x); \
+            return -1; \
+        } \
+    } while (0);
+
+#define EXPECT_NEGATIVE_ONE_WITH_ERRNO(x, e) \
+    do { \
+        int __my_ret__ = x; \
+        int __my_errno__ = errno; \
+        if (__my_ret__ != -1) { \
+            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
+                "code %d (errno: %d): expected -1 from %s\n", __LINE__, \
+                __my_ret__, __my_errno__, #x); \
+            return -1; \
+        } \
+        if (__my_errno__ != e) { \
+            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
+                "code %d (errno: %d): expected errno = %d from %s\n", \
+                __LINE__, __my_ret__, __my_errno__, e, #x); \
+            return -1; \
+	} \
+    } while (0);
+
+#define EXPECT_NONZERO(x) \
+    do { \
+        int __my_ret__ = x; \
+        int __my_errno__ = errno; \
+        if (__my_ret__) { \
+            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
+		    "code %d (errno: %d): got zero from %s\n", __LINE__, \
+                __my_ret__, __my_errno__, #x); \
+            return -1; \
+        } \
+    } while (0);
+
+#define EXPECT_NONNEGATIVE(x) \
+    do { \
+        int __my_ret__ = x; \
+        int __my_errno__ = errno; \
+        if (__my_ret__ < 0) { \
+            fprintf(stderr, "TEST_ERROR: failed on line %d with return " \
+                "code %d (errno: %d): got negative return from %s\n", \
+		    __LINE__, __my_ret__, __my_errno__, #x); \
+            return __my_ret__; \
+        } \
+    } while (0);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c
new file mode 100644
index 0000000..acbda54
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c
@@ -0,0 +1,2519 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hdfs.h"
+#include "jni_helper.h"
+
+#include <stdio.h>
+#include <string.h>
+
+/* Some frequently used Java paths */
+#define HADOOP_CONF     "org/apache/hadoop/conf/Configuration"
+#define HADOOP_PATH     "org/apache/hadoop/fs/Path"
+#define HADOOP_LOCALFS  "org/apache/hadoop/fs/LocalFileSystem"
+#define HADOOP_FS       "org/apache/hadoop/fs/FileSystem"
+#define HADOOP_FSSTATUS "org/apache/hadoop/fs/FsStatus"
+#define HADOOP_BLK_LOC  "org/apache/hadoop/fs/BlockLocation"
+#define HADOOP_DFS      "org/apache/hadoop/hdfs/DistributedFileSystem"
+#define HADOOP_ISTRM    "org/apache/hadoop/fs/FSDataInputStream"
+#define HADOOP_OSTRM    "org/apache/hadoop/fs/FSDataOutputStream"
+#define HADOOP_STAT     "org/apache/hadoop/fs/FileStatus"
+#define HADOOP_FSPERM   "org/apache/hadoop/fs/permission/FsPermission"
+#define JAVA_NET_ISA    "java/net/InetSocketAddress"
+#define JAVA_NET_URI    "java/net/URI"
+#define JAVA_STRING     "java/lang/String"
+
+#define JAVA_VOID       "V"
+
+/* Macros for constructing method signatures */
+#define JPARAM(X)           "L" X ";"
+#define JARRPARAM(X)        "[L" X ";"
+#define JMETHOD1(X, R)      "(" X ")" R
+#define JMETHOD2(X, Y, R)   "(" X Y ")" R
+#define JMETHOD3(X, Y, Z, R)   "(" X Y Z")" R
+
+#define KERBEROS_TICKET_CACHE_PATH "hadoop.security.kerberos.ticket.cache.path"
+
+// Bit fields for hdfsFile_internal flags
+#define HDFS_FILE_SUPPORTS_DIRECT_READ (1<<0)
+
+tSize readDirect(hdfsFS fs, hdfsFile f, void* buffer, tSize length);
+
+/**
+ * The C equivalent of org.apache.org.hadoop.FSData(Input|Output)Stream .
+ */
+enum hdfsStreamType
+{
+    UNINITIALIZED = 0,
+    INPUT = 1,
+    OUTPUT = 2,
+};
+
+/**
+ * The 'file-handle' to a file in hdfs.
+ */
+struct hdfsFile_internal {
+    void* file;
+    enum hdfsStreamType type;
+    int flags;
+};
+    
+int hdfsFileIsOpenForRead(hdfsFile file)
+{
+    return (file->type == INPUT);
+}
+
+int hdfsFileIsOpenForWrite(hdfsFile file)
+{
+    return (file->type == OUTPUT);
+}
+
+int hdfsFileUsesDirectRead(hdfsFile file)
+{
+    return !!(file->flags & HDFS_FILE_SUPPORTS_DIRECT_READ);
+}
+
+void hdfsFileDisableDirectRead(hdfsFile file)
+{
+    file->flags &= ~HDFS_FILE_SUPPORTS_DIRECT_READ;
+}
+
+/**
+ * hdfsJniEnv: A wrapper struct to be used as 'value'
+ * while saving thread -> JNIEnv* mappings
+ */
+typedef struct
+{
+    JNIEnv* env;
+} hdfsJniEnv;
+
+
+
+/**
+ * Helper function to destroy a local reference of java.lang.Object
+ * @param env: The JNIEnv pointer. 
+ * @param jFile: The local reference of java.lang.Object object
+ * @return None.
+ */
+static void destroyLocalReference(JNIEnv *env, jobject jObject)
+{
+  if (jObject)
+    (*env)->DeleteLocalRef(env, jObject);
+}
+
+
+/**
+ * Helper function to create a org.apache.hadoop.fs.Path object.
+ * @param env: The JNIEnv pointer. 
+ * @param path: The file-path for which to construct org.apache.hadoop.fs.Path
+ * object.
+ * @return Returns a jobject on success and NULL on error.
+ */
+static jobject constructNewObjectOfPath(JNIEnv *env, const char *path)
+{
+    //Construct a java.lang.String object
+    jstring jPathString = (*env)->NewStringUTF(env, path); 
+
+    //Construct the org.apache.hadoop.fs.Path object
+    jobject jPath =
+        constructNewObjectOfClass(env, NULL, "org/apache/hadoop/fs/Path",
+                                  "(Ljava/lang/String;)V", jPathString);
+    if (jPath == NULL) {
+        fprintf(stderr, "Can't construct instance of class "
+                "org.apache.hadoop.fs.Path for %s\n", path);
+        errno = EINTERNAL;
+        return NULL;
+    }
+
+    // Destroy the local reference to the java.lang.String object
+    destroyLocalReference(env, jPathString);
+
+    return jPath;
+}
+
+
+/**
+ * Helper function to translate an exception into a meaningful errno value.
+ * @param exc: The exception.
+ * @param env: The JNIEnv Pointer.
+ * @param method: The name of the method that threw the exception. This
+ * may be format string to be used in conjuction with additional arguments.
+ * @return Returns a meaningful errno value if possible, or EINTERNAL if not.
+ */
+static int errnoFromException(jthrowable exc, JNIEnv *env,
+                              const char *method, ...)
+{
+    va_list ap;
+    int errnum = 0;
+    char *excClass = NULL;
+
+    if (exc == NULL)
+        goto default_error;
+
+    if ((excClass = classNameOfObject((jobject) exc, env)) == NULL) {
+      errnum = EINTERNAL;
+      goto done;
+    }
+
+    if (!strcmp(excClass, "java.lang.UnsupportedOperationException")) {
+      errnum = ENOTSUP;
+      goto done;
+    }
+
+    if (!strcmp(excClass, "org.apache.hadoop.security."
+                "AccessControlException")) {
+        errnum = EACCES;
+        goto done;
+    }
+
+    if (!strcmp(excClass, "org.apache.hadoop.hdfs.protocol."
+                "QuotaExceededException")) {
+        errnum = EDQUOT;
+        goto done;
+    }
+
+    if (!strcmp(excClass, "java.io.FileNotFoundException")) {
+        errnum = ENOENT;
+        goto done;
+    }
+
+    //TODO: interpret more exceptions; maybe examine exc.getMessage()
+
+default_error:
+
+    //Can't tell what went wrong, so just punt
+    (*env)->ExceptionDescribe(env);
+    fprintf(stderr, "Call to ");
+    va_start(ap, method);
+    vfprintf(stderr, method, ap);
+    va_end(ap);
+    fprintf(stderr, " failed!\n");
+    errnum = EINTERNAL;
+
+done:
+
+    (*env)->ExceptionClear(env);
+
+    if (excClass != NULL)
+        free(excClass);
+
+    return errnum;
+}
+
+/**
+ * Set a configuration value.
+ *
+ * @param env               The JNI environment
+ * @param jConfiguration    The configuration object to modify
+ * @param key               The key to modify
+ * @param value             The value to set the key to
+ *
+ * @return                  0 on success; error code otherwise
+ */
+static int hadoopConfSet(JNIEnv *env, jobject jConfiguration,
+        const char *key, const char *value)
+{
+    int ret;
+    jthrowable jExc = NULL;
+    jstring jkey = NULL, jvalue = NULL;
+    char buf[1024];
+
+    jkey = (*env)->NewStringUTF(env, key);
+    if (!jkey) {
+        ret = ENOMEM;
+        goto done;
+    }
+    jvalue = (*env)->NewStringUTF(env, value);
+    if (!jvalue) {
+        ret = ENOMEM;
+        goto done;
+    }
+    ret = invokeMethod(env, NULL, &jExc, INSTANCE, jConfiguration,
+            HADOOP_CONF, "set", JMETHOD2(JPARAM(JAVA_STRING),
+                                         JPARAM(JAVA_STRING), JAVA_VOID),
+            jkey, jvalue);
+    if (ret) {
+        snprintf(buf, sizeof(buf), "hadoopConfSet(%s, %s)", key, value);
+        ret = errnoFromException(jExc, env, buf);
+        goto done;
+    }
+done:
+    if (jkey)
+        destroyLocalReference(env, jkey);
+    if (jvalue)
+        destroyLocalReference(env, jvalue);
+    if (ret)
+        errno = ret;
+    return ret;
+}
+
+/**
+ * Convert a Java string into a C string.
+ *
+ * @param env               The JNI environment
+ * @param jStr              The Java string to convert
+ * @param cstr              (out param) the C string.
+ *                          This will be set to a dynamically allocated
+ *                          UTF-8 C string on success.
+ *
+ * @return                  0 on success; error code otherwise
+ */
+static int jStrToCstr(JNIEnv *env, jstring jstr, char **cstr)
+{
+    char *tmp;
+
+    tmp = (*env)->GetStringUTFChars(env, jstr, NULL);
+    *cstr = strdup(tmp);
+    (*env)->ReleaseStringUTFChars(env, jstr, tmp);
+    return 0;
+}
+
+static int hadoopConfGet(JNIEnv *env, jobject jConfiguration,
+        const char *key, char **val)
+{
+    int ret;
+    jthrowable jExc = NULL;
+    jvalue jVal;
+    jstring jkey = NULL;
+    char buf[1024];
+
+    jkey = (*env)->NewStringUTF(env, key);
+    if (!jkey) {
+        ret = ENOMEM;
+        goto done;
+    }
+    ret = invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration,
+            HADOOP_CONF, "get", JMETHOD1(JPARAM(JAVA_STRING),
+                                         JPARAM(JAVA_STRING)), jkey);
+    if (ret) {
+        snprintf(buf, sizeof(buf), "hadoopConfGet(%s)", key);
+        ret = errnoFromException(jExc, env, buf);
+        goto done;
+    }
+    if (!jVal.l) {
+        *val = NULL;
+        goto done;
+    }
+
+    ret = jStrToCstr(env, jVal.l, val);
+    if (ret)
+        goto done;
+done:
+    if (jkey)
+        destroyLocalReference(env, jkey);
+    if (ret)
+        errno = ret;
+    return ret;
+}
+
+int hdfsConfGet(const char *key, char **val)
+{
+    JNIEnv *env;
+    int ret;
+    jobject jConfiguration = NULL;
+
+    env = getJNIEnv();
+    if (env == NULL) {
+      ret = EINTERNAL;
+      goto done;
+    }
+    jConfiguration = constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
+    if (jConfiguration == NULL) {
+        fprintf(stderr, "Can't construct instance of class "
+                "org.apache.hadoop.conf.Configuration\n");
+        ret = EINTERNAL;
+        goto done;
+    }
+    ret = hadoopConfGet(env, jConfiguration, key, val);
+    if (ret)
+        goto done;
+    ret = 0;
+done:
+    if (jConfiguration)
+        destroyLocalReference(env, jConfiguration);
+    if (ret)
+        errno = ret;
+    return ret;
+}
+
+void hdfsConfFree(char *val)
+{
+    free(val);
+}
+
+struct hdfsBuilder {
+    int forceNewInstance;
+    const char *nn;
+    tPort port;
+    const char *kerbTicketCachePath;
+    const char *userName;
+};
+
+struct hdfsBuilder *hdfsNewBuilder(void)
+{
+    struct hdfsBuilder *bld = calloc(1, sizeof(struct hdfsBuilder));
+    if (!bld) {
+        errno = ENOMEM;
+        return NULL;
+    }
+    return bld;
+}
+
+void hdfsFreeBuilder(struct hdfsBuilder *bld)
+{
+    free(bld);
+}
+
+void hdfsBuilderSetForceNewInstance(struct hdfsBuilder *bld)
+{
+    bld->forceNewInstance = 1;
+}
+
+void hdfsBuilderSetNameNode(struct hdfsBuilder *bld, const char *nn)
+{
+    bld->nn = nn;
+}
+
+void hdfsBuilderSetNameNodePort(struct hdfsBuilder *bld, tPort port)
+{
+    bld->port = port;
+}
+
+void hdfsBuilderSetUserName(struct hdfsBuilder *bld, const char *userName)
+{
+    bld->userName = userName;
+}
+
+void hdfsBuilderSetKerbTicketCachePath(struct hdfsBuilder *bld,
+                                       const char *kerbTicketCachePath)
+{
+    bld->kerbTicketCachePath = kerbTicketCachePath;
+}
+
+hdfsFS hdfsConnect(const char* host, tPort port)
+{
+    struct hdfsBuilder *bld = hdfsNewBuilder();
+    if (!bld)
+        return NULL;
+    hdfsBuilderSetNameNode(bld, host);
+    hdfsBuilderSetNameNodePort(bld, port);
+    return hdfsBuilderConnect(bld);
+}
+
+/** Always return a new FileSystem handle */
+hdfsFS hdfsConnectNewInstance(const char* host, tPort port)
+{
+    struct hdfsBuilder *bld = hdfsNewBuilder();
+    if (!bld)
+        return NULL;
+    hdfsBuilderSetNameNode(bld, host);
+    hdfsBuilderSetNameNodePort(bld, port);
+    hdfsBuilderSetForceNewInstance(bld);
+    return hdfsBuilderConnect(bld);
+}
+
+hdfsFS hdfsConnectAsUser(const char* host, tPort port, const char *user)
+{
+    struct hdfsBuilder *bld = hdfsNewBuilder();
+    if (!bld)
+        return NULL;
+    hdfsBuilderSetNameNode(bld, host);
+    hdfsBuilderSetNameNodePort(bld, port);
+    hdfsBuilderSetUserName(bld, user);
+    return hdfsBuilderConnect(bld);
+}
+
+/** Always return a new FileSystem handle */
+hdfsFS hdfsConnectAsUserNewInstance(const char* host, tPort port,
+        const char *user)
+{
+    struct hdfsBuilder *bld = hdfsNewBuilder();
+    if (!bld)
+        return NULL;
+    hdfsBuilderSetNameNode(bld, host);
+    hdfsBuilderSetNameNodePort(bld, port);
+    hdfsBuilderSetForceNewInstance(bld);
+    hdfsBuilderSetUserName(bld, user);
+    return hdfsBuilderConnect(bld);
+}
+
+
+/**
+ * Calculate the effective URI to use, given a builder configuration.
+ *
+ * If there is not already a URI scheme, we prepend 'hdfs://'.
+ *
+ * If there is not already a port specified, and a port was given to the
+ * builder, we suffix that port.  If there is a port specified but also one in
+ * the URI, that is an error.
+ *
+ * @param bld       The hdfs builder object
+ * @param uri       (out param) dynamically allocated string representing the
+ *                  effective URI
+ *
+ * @return          0 on success; error code otherwise
+ */
+static int calcEffectiveURI(struct hdfsBuilder *bld, char ** uri)
+{
+    const char *scheme;
+    char suffix[64];
+    const char *lastColon;
+    char *u;
+    size_t uriLen;
+
+    if (!bld->nn)
+        return EINVAL;
+    scheme = (strstr(bld->nn, "://")) ? "" : "hdfs://";
+    if (bld->port == 0) {
+        suffix[0] = '\0';
+    } else {
+        lastColon = rindex(bld->nn, ':');
+        if (lastColon && (strspn(lastColon + 1, "0123456789") ==
+                          strlen(lastColon + 1))) {
+            fprintf(stderr, "port %d was given, but URI '%s' already "
+                "contains a port!\n", bld->port, bld->nn);
+            return EINVAL;
+        }
+        snprintf(suffix, sizeof(suffix), ":%d", bld->port);
+    }
+
+    uriLen = strlen(scheme) + strlen(bld->nn) + strlen(suffix);
+    u = malloc((uriLen + 1) * (sizeof(char)));
+    if (!u) {
+        fprintf(stderr, "calcEffectiveURI: out of memory");
+        return ENOMEM;
+    }
+    snprintf(u, uriLen + 1, "%s%s%s", scheme, bld->nn, suffix);
+    *uri = u;
+    return 0;
+}
+
+hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
+{
+    JNIEnv *env = 0;
+    jobject gFsRef = NULL;
+    jobject jConfiguration = NULL, jFS = NULL, jURI = NULL, jCachePath = NULL;
+    jstring jURIString = NULL, jUserString = NULL;
+    jvalue  jVal;
+    jthrowable jExc = NULL;
+    char *cURI = 0;
+    int ret = 0;
+
+    //Get the JNIEnv* corresponding to current thread
+    env = getJNIEnv();
+    if (env == NULL) {
+      ret = EINTERNAL;
+      goto done;
+    }
+
+    //  jConfiguration = new Configuration();
+    jConfiguration = constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
+    if (jConfiguration == NULL) {
+        fprintf(stderr, "Can't construct instance of class "
+                "org.apache.hadoop.conf.Configuration\n");
+      goto done;
+    }
+ 
+    //Check what type of FileSystem the caller wants...
+    if (bld->nn == NULL) {
+        // Get a local filesystem.
+        if (bld->forceNewInstance) {
+            // fs = FileSytem::newInstanceLocal(conf);
+            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS,
+                    "newInstanceLocal", JMETHOD1(JPARAM(HADOOP_CONF),
+                    JPARAM(HADOOP_LOCALFS)), jConfiguration)) {
+                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                           "FileSystem::newInstanceLocal");
+                goto done;
+            }
+            jFS = jVal.l;
+        } else {
+            // fs = FileSytem::getLocal(conf);
+            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS, "getLocal",
+                             JMETHOD1(JPARAM(HADOOP_CONF),
+                                      JPARAM(HADOOP_LOCALFS)),
+                             jConfiguration) != 0) {
+                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                           "FileSystem::getLocal");
+                goto done;
+            }
+            jFS = jVal.l;
+        }
+    } else {
+        if (!strcmp(bld->nn, "default")) {
+            // jURI = FileSystem.getDefaultUri(conf)
+            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS,
+                          "getDefaultUri", 
+                          "(Lorg/apache/hadoop/conf/Configuration;)Ljava/net/URI;",
+                          jConfiguration) != 0) {
+                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs.", 
+                                           "FileSystem::getDefaultUri");
+                goto done;
+            }
+            jURI = jVal.l;
+        } else {
+            // fs = FileSystem::get(URI, conf, ugi);
+            ret = calcEffectiveURI(bld, &cURI);
+            if (ret)
+                goto done;
+            jURIString = (*env)->NewStringUTF(env, cURI);
+            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, JAVA_NET_URI,
+                             "create", "(Ljava/lang/String;)Ljava/net/URI;",
+                             jURIString) != 0) {
+                ret = errnoFromException(jExc, env, "java.net.URI::create");
+                goto done;
+            }
+            jURI = jVal.l;
+        }
+
+        if (bld->kerbTicketCachePath) {
+            ret = hadoopConfSet(env, jConfiguration,
+                KERBEROS_TICKET_CACHE_PATH, bld->kerbTicketCachePath);
+            if (ret)
+                goto done;
+        }
+        if (bld->userName) {
+            jUserString = (*env)->NewStringUTF(env, bld->userName);
+        }
+        if (bld->forceNewInstance) {
+            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL,
+                    HADOOP_FS, "newInstance",
+                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
+                        JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)), jURI,
+                    jConfiguration, jUserString)) {
+                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                           "Filesystem::newInstance(URI, Configuration)");
+                goto done;
+            }
+            jFS = jVal.l;
+        } else {
+            if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS, "get",
+                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
+                        JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)),
+                        jURI, jConfiguration, jUserString)) {
+                ret = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                               "Filesystem::get(URI, Configuration, String)");
+                goto done;
+            }
+            jFS = jVal.l;
+        }
+    }
+
+done:
+    if (jFS) {
+        /* Create a global reference for this fs */
+        gFsRef = (*env)->NewGlobalRef(env, jFS);
+    }
+
+    // Release unnecessary local references
+    destroyLocalReference(env, jConfiguration);
+    destroyLocalReference(env, jFS);
+    destroyLocalReference(env, jURI);
+    destroyLocalReference(env, jCachePath);
+    destroyLocalReference(env, jURIString);
+    destroyLocalReference(env, jUserString);
+    free(cURI);
+    free(bld);
+
+    if (ret)
+        errno = ret;
+    return gFsRef;
+}
+
+int hdfsDisconnect(hdfsFS fs)
+{
+    // JAVA EQUIVALENT:
+    //  fs.close()
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -2;
+    }
+
+    //Parameters
+    jobject jFS = (jobject)fs;
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (fs == NULL) {
+        errno = EBADF;
+        return -1;
+    }
+
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "close", "()V") != 0) {
+        errno = errnoFromException(jExc, env, "Filesystem::close");
+        return -1;
+    }
+
+    //Release unnecessary references
+    (*env)->DeleteGlobalRef(env, fs);
+
+    return 0;
+}
+
+
+
+hdfsFile hdfsOpenFile(hdfsFS fs, const char* path, int flags, 
+                      int bufferSize, short replication, tSize blockSize)
+{
+    /*
+      JAVA EQUIVALENT:
+       File f = new File(path);
+       FSData{Input|Output}Stream f{is|os} = fs.create(f);
+       return f{is|os};
+    */
+    /* Get the JNIEnv* corresponding to current thread */
+    JNIEnv* env = getJNIEnv();
+
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return NULL;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    if (flags & O_RDWR) {
+      fprintf(stderr, "ERROR: cannot open an hdfs file in O_RDWR mode\n");
+      errno = ENOTSUP;
+      return NULL;
+    }
+
+    if ((flags & O_CREAT) && (flags & O_EXCL)) {
+      fprintf(stderr, "WARN: hdfs does not truly support O_CREATE && O_EXCL\n");
+    }
+
+    /* The hadoop java api/signature */
+    const char* method = ((flags & O_WRONLY) == 0) ? "open" : (flags & O_APPEND) ? "append" : "create";
+    const char* signature = ((flags & O_WRONLY) == 0) ?
+        JMETHOD2(JPARAM(HADOOP_PATH), "I", JPARAM(HADOOP_ISTRM)) :
+      (flags & O_APPEND) ?
+      JMETHOD1(JPARAM(HADOOP_PATH), JPARAM(HADOOP_OSTRM)) :
+      JMETHOD2(JPARAM(HADOOP_PATH), "ZISJ", JPARAM(HADOOP_OSTRM));
+
+    /* Return value */
+    hdfsFile file = NULL;
+
+    /* Create an object of org.apache.hadoop.fs.Path */
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return NULL; 
+    }
+
+    /* Get the Configuration object from the FileSystem object */
+    jvalue  jVal;
+    jobject jConfiguration = NULL;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "getConf", JMETHOD1("", JPARAM(HADOOP_CONF))) != 0) {
+        errno = errnoFromException(jExc, env, "get configuration object "
+                                   "from filesystem");
+        destroyLocalReference(env, jPath);
+        return NULL;
+    }
+    jConfiguration = jVal.l;
+
+    jint jBufferSize = bufferSize;
+    jshort jReplication = replication;
+    jlong jBlockSize = blockSize;
+    jstring jStrBufferSize = (*env)->NewStringUTF(env, "io.file.buffer.size"); 
+    jstring jStrReplication = (*env)->NewStringUTF(env, "dfs.replication");
+    jstring jStrBlockSize = (*env)->NewStringUTF(env, "dfs.block.size");
+
+
+    //bufferSize
+    if (!bufferSize) {
+        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration, 
+                         HADOOP_CONF, "getInt", "(Ljava/lang/String;I)I",
+                         jStrBufferSize, 4096) != 0) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
+                                       "Configuration::getInt");
+            goto done;
+        }
+        jBufferSize = jVal.i;
+    }
+
+    if ((flags & O_WRONLY) && (flags & O_APPEND) == 0) {
+        //replication
+
+        if (!replication) {
+            if (invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration, 
+                             HADOOP_CONF, "getInt", "(Ljava/lang/String;I)I",
+                             jStrReplication, 1) != 0) {
+                errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
+                                           "Configuration::getInt");
+                goto done;
+            }
+            jReplication = jVal.i;
+        }
+        
+        //blockSize
+        if (!blockSize) {
+            if (invokeMethod(env, &jVal, &jExc, INSTANCE, jConfiguration, 
+                             HADOOP_CONF, "getLong", "(Ljava/lang/String;J)J",
+                             jStrBlockSize, (jlong)67108864)) {
+                errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
+                                           "FileSystem::%s(%s)", method,
+                                           signature);
+                goto done;
+            }
+            jBlockSize = jVal.j;
+        }
+    }
+ 
+    /* Create and return either the FSDataInputStream or
+       FSDataOutputStream references jobject jStream */
+
+    // READ?
+    if ((flags & O_WRONLY) == 0) {
+      if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                       method, signature, jPath, jBufferSize)) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
+                                   "FileSystem::%s(%s)", method,
+                                   signature);
+        goto done;
+      }
+    }  else if ((flags & O_WRONLY) && (flags & O_APPEND)) {
+      // WRITE/APPEND?
+       if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                       method, signature, jPath)) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
+                                   "FileSystem::%s(%s)", method,
+                                   signature);
+        goto done;
+      }
+    } else {
+        // WRITE/CREATE
+        jboolean jOverWrite = 1;
+        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                         method, signature, jPath, jOverWrite,
+                         jBufferSize, jReplication, jBlockSize)) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.conf."
+                                       "FileSystem::%s(%s)", method,
+                                       signature);
+            goto done;
+        }
+    }
+  
+    file = malloc(sizeof(struct hdfsFile_internal));
+    if (!file) {
+        errno = ENOMEM;
+    } else {
+        file->file = (*env)->NewGlobalRef(env, jVal.l);
+        file->type = (((flags & O_WRONLY) == 0) ? INPUT : OUTPUT);
+        file->flags = 0;
+
+        destroyLocalReference(env, jVal.l);
+
+        if ((flags & O_WRONLY) == 0) {
+          // Try a test read to see if we can do direct reads
+          errno = 0;
+          char buf;
+          if (readDirect(fs, file, &buf, 0) == 0) {
+            // Success - 0-byte read should return 0
+            file->flags |= HDFS_FILE_SUPPORTS_DIRECT_READ;
+          } else {
+            if (errno != ENOTSUP) {
+              // Unexpected error. Clear it, don't set the direct flag.
+              fprintf(stderr,
+                      "WARN: Unexpected error %d when testing "
+                      "for direct read compatibility\n", errno);
+              errno = 0;
+              goto done;
+            }
+          }
+          errno = 0;
+        }
+    }
+
+    done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jStrBufferSize);
+    destroyLocalReference(env, jStrReplication);
+    destroyLocalReference(env, jStrBlockSize);
+    destroyLocalReference(env, jConfiguration); 
+    destroyLocalReference(env, jPath); 
+
+    return file;
+}
+
+
+
+int hdfsCloseFile(hdfsFS fs, hdfsFile file)
+{
+    // JAVA EQUIVALENT:
+    //  file.close 
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -2;
+    }
+
+    //Parameters
+    jobject jStream = (jobject)(file ? file->file : NULL);
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!file || file->type == UNINITIALIZED) {
+        errno = EBADF;
+        return -1;
+    }
+
+    //The interface whose 'close' method to be called
+    const char* interface = (file->type == INPUT) ? 
+        HADOOP_ISTRM : HADOOP_OSTRM;
+  
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jStream, interface,
+                     "close", "()V") != 0) {
+        errno = errnoFromException(jExc, env, "%s::close", interface);
+        return -1;
+    }
+
+    //De-allocate memory
+    free(file);
+    (*env)->DeleteGlobalRef(env, jStream);
+
+    return 0;
+}
+
+
+
+int hdfsExists(hdfsFS fs, const char *path)
+{
+    JNIEnv *env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -2;
+    }
+
+    jobject jPath = constructNewObjectOfPath(env, path);
+    jvalue  jVal;
+    jthrowable jExc = NULL;
+    jobject jFS = (jobject)fs;
+
+    if (jPath == NULL) {
+        return -1;
+    }
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "exists", JMETHOD1(JPARAM(HADOOP_PATH), "Z"),
+                     jPath) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::exists");
+        destroyLocalReference(env, jPath);
+        return -1;
+    }
+
+    destroyLocalReference(env, jPath);
+    return jVal.z ? 0 : -1;
+}
+
+// Checks input file for readiness for reading.
+static int readPrepare(JNIEnv* env, hdfsFS fs, hdfsFile f,
+                       jobject* jInputStream)
+{
+    *jInputStream = (jobject)(f ? f->file : NULL);
+
+    //Sanity check
+    if (!f || f->type == UNINITIALIZED) {
+      errno = EBADF;
+      return -1;
+    }
+
+    //Error checking... make sure that this file is 'readable'
+    if (f->type != INPUT) {
+      fprintf(stderr, "Cannot read from a non-InputStream object!\n");
+      errno = EINVAL;
+      return -1;
+    }
+
+    return 0;
+}
+
+// Common error-handling code between read paths.
+static int handleReadResult(int success, jvalue jVal, jthrowable jExc,
+                            JNIEnv* env)
+{
+  int noReadBytes;
+  if (success != 0) {
+    if ((*env)->ExceptionCheck(env)) {
+      errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                 "FSDataInputStream::read");
+    }
+    noReadBytes = -1;
+  } else {
+    noReadBytes = jVal.i;
+    if (noReadBytes == 0) {
+      // 0 from Java means try again, which is EINTR here
+      errno = EINTR;
+      noReadBytes = -1;
+    } else if (noReadBytes < 0) {
+      // -1 from Java is EOF, which is 0 here
+      errno = 0;
+      noReadBytes = 0;
+    }
+  }
+
+  return noReadBytes;
+}
+
+tSize hdfsRead(hdfsFS fs, hdfsFile f, void* buffer, tSize length)
+{
+    if (f->flags & HDFS_FILE_SUPPORTS_DIRECT_READ) {
+      return readDirect(fs, f, buffer, length);
+    }
+
+    // JAVA EQUIVALENT:
+    //  byte [] bR = new byte[length];
+    //  fis.read(bR);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jInputStream;
+    if (readPrepare(env, fs, f, &jInputStream) == -1) {
+      return -1;
+    }
+
+    jbyteArray jbRarray;
+    jint noReadBytes = 0;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+
+    //Read the requisite bytes
+    jbRarray = (*env)->NewByteArray(env, length);
+
+    int success = invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
+                               "read", "([B)I", jbRarray);
+
+    noReadBytes = handleReadResult(success, jVal, jExc, env);;
+
+    if (noReadBytes > 0) {
+      (*env)->GetByteArrayRegion(env, jbRarray, 0, noReadBytes, buffer);
+    }
+
+    destroyLocalReference(env, jbRarray);
+
+    return noReadBytes;
+}
+
+// Reads using the read(ByteBuffer) API, which does fewer copies
+tSize readDirect(hdfsFS fs, hdfsFile f, void* buffer, tSize length)
+{
+    // JAVA EQUIVALENT:
+    //  ByteBuffer bbuffer = ByteBuffer.allocateDirect(length) // wraps C buffer
+    //  fis.read(bbuffer);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jInputStream;
+    if (readPrepare(env, fs, f, &jInputStream) == -1) {
+      return -1;
+    }
+
+    jint noReadBytes = 0;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+
+    //Read the requisite bytes
+    jobject bb = (*env)->NewDirectByteBuffer(env, buffer, length);
+    if (bb == NULL) {
+      fprintf(stderr, "Could not allocate ByteBuffer");
+      if ((*env)->ExceptionCheck(env)) {
+        errno = errnoFromException(NULL, env, "JNIEnv::NewDirectByteBuffer");
+      } else {
+        errno = ENOMEM; // Best guess if there's no exception waiting
+      }
+      return -1;
+    }
+
+    int success = invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream,
+                               HADOOP_ISTRM, "read", "(Ljava/nio/ByteBuffer;)I",
+                               bb);
+
+    noReadBytes = handleReadResult(success, jVal, jExc, env);
+
+    destroyLocalReference(env, bb);
+
+    return noReadBytes;
+}
+
+
+  
+tSize hdfsPread(hdfsFS fs, hdfsFile f, tOffset position,
+                void* buffer, tSize length)
+{
+    // JAVA EQUIVALENT:
+    //  byte [] bR = new byte[length];
+    //  fis.read(pos, bR, 0, length);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jInputStream = (jobject)(f ? f->file : NULL);
+
+    jbyteArray jbRarray;
+    jint noReadBytes = 0;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!f || f->type == UNINITIALIZED) {
+        errno = EBADF;
+        return -1;
+    }
+
+    //Error checking... make sure that this file is 'readable'
+    if (f->type != INPUT) {
+        fprintf(stderr, "Cannot read from a non-InputStream object!\n");
+        errno = EINVAL;
+        return -1;
+    }
+
+    //Read the requisite bytes
+    jbRarray = (*env)->NewByteArray(env, length);
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
+                     "read", "(J[BII)I", position, jbRarray, 0, length) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FSDataInputStream::read");
+        noReadBytes = -1;
+    }
+    else {
+        noReadBytes = jVal.i;
+        if (noReadBytes > 0) {
+            (*env)->GetByteArrayRegion(env, jbRarray, 0, noReadBytes, buffer);
+        }  else {
+            //This is a valid case: there aren't any bytes left to read!
+          if (noReadBytes == 0 || noReadBytes < -1) {
+            fprintf(stderr, "WARN: FSDataInputStream.read returned invalid return code - libhdfs returning EOF, i.e., 0: %d\n", noReadBytes);
+          }
+            noReadBytes = 0;
+        }
+        errno = 0;
+    }
+    destroyLocalReference(env, jbRarray);
+
+    return noReadBytes;
+}
+
+
+
+tSize hdfsWrite(hdfsFS fs, hdfsFile f, const void* buffer, tSize length)
+{
+    // JAVA EQUIVALENT
+    // byte b[] = str.getBytes();
+    // fso.write(b);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jOutputStream = (jobject)(f ? f->file : 0);
+    jbyteArray jbWarray;
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!f || f->type == UNINITIALIZED) {
+        errno = EBADF;
+        return -1;
+    }
+    
+    if (length < 0) {
+    	errno = EINVAL;
+    	return -1;
+    }
+
+    //Error checking... make sure that this file is 'writable'
+    if (f->type != OUTPUT) {
+        fprintf(stderr, "Cannot write into a non-OutputStream object!\n");
+        errno = EINVAL;
+        return -1;
+    }
+
+    // 'length' equals 'zero' is a valid use-case according to Posix!
+    if (length != 0) {
+        //Write the requisite bytes into the file
+        jbWarray = (*env)->NewByteArray(env, length);
+        (*env)->SetByteArrayRegion(env, jbWarray, 0, length, buffer);
+        if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream,
+                         HADOOP_OSTRM, "write",
+                         "([B)V", jbWarray) != 0) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                       "FSDataOutputStream::write");
+            length = -1;
+        }
+        destroyLocalReference(env, jbWarray);
+    }
+
+    //Return no. of bytes succesfully written (libc way)
+    //i.e. 'length' itself! ;-)
+    return length;
+}
+
+
+
+int hdfsSeek(hdfsFS fs, hdfsFile f, tOffset desiredPos) 
+{
+    // JAVA EQUIVALENT
+    //  fis.seek(pos);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jInputStream = (jobject)(f ? f->file : 0);
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!f || f->type != INPUT) {
+        errno = EBADF;
+        return -1;
+    }
+
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
+                     "seek", "(J)V", desiredPos) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FSDataInputStream::seek");
+        return -1;
+    }
+
+    return 0;
+}
+
+
+
+tOffset hdfsTell(hdfsFS fs, hdfsFile f)
+{
+    // JAVA EQUIVALENT
+    //  pos = f.getPos();
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jStream = (jobject)(f ? f->file : 0);
+
+    //Sanity check
+    if (!f || f->type == UNINITIALIZED) {
+        errno = EBADF;
+        return -1;
+    }
+
+    const char* interface = (f->type == INPUT) ?
+        HADOOP_ISTRM : HADOOP_OSTRM;
+
+    jlong currentPos  = -1;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStream,
+                     interface, "getPos", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FSDataInputStream::getPos");
+        return -1;
+    }
+    currentPos = jVal.j;
+
+    return (tOffset)currentPos;
+}
+
+
+
+int hdfsFlush(hdfsFS fs, hdfsFile f) 
+{
+    // JAVA EQUIVALENT
+    //  fos.flush();
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jOutputStream = (jobject)(f ? f->file : 0);
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!f || f->type != OUTPUT) {
+        errno = EBADF;
+        return -1;
+    }
+
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream, 
+                     HADOOP_OSTRM, "flush", "()V") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FSDataInputStream::flush");
+        return -1;
+    }
+
+    return 0;
+}
+
+
+
+int hdfsHFlush(hdfsFS fs, hdfsFile f)
+{
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jOutputStream = (jobject)(f ? f->file : 0);
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!f || f->type != OUTPUT) {
+        errno = EBADF;
+        return -1;
+    }
+
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream,
+                     HADOOP_OSTRM, "hflush", "()V") != 0) {
+        errno = errnoFromException(jExc, env, HADOOP_OSTRM "::hflush");
+        return -1;
+    }
+
+    return 0;
+}
+
+
+
+int hdfsAvailable(hdfsFS fs, hdfsFile f)
+{
+    // JAVA EQUIVALENT
+    //  fis.available();
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jInputStream = (jobject)(f ? f->file : 0);
+
+    //Caught exception
+    jthrowable jExc = NULL;
+
+    //Sanity check
+    if (!f || f->type != INPUT) {
+        errno = EBADF;
+        return -1;
+    }
+
+    jint available = -1;
+    jvalue jVal;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, 
+                     HADOOP_ISTRM, "available", "()I") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FSDataInputStream::available");
+        return -1;
+    }
+    available = jVal.i;
+
+    return available;
+}
+
+
+
+int hdfsCopy(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst)
+{
+    //JAVA EQUIVALENT
+    //  FileUtil::copy(srcFS, srcPath, dstFS, dstPath,
+    //                 deleteSource = false, conf)
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jSrcFS = (jobject)srcFS;
+    jobject jDstFS = (jobject)dstFS;
+    jobject jSrcPath = NULL;
+    jobject jDstPath = NULL;
+
+    jSrcPath = constructNewObjectOfPath(env, src);
+    if (jSrcPath == NULL) {
+        return -1;
+    }
+
+    jDstPath = constructNewObjectOfPath(env, dst);
+    if (jDstPath == NULL) {
+        destroyLocalReference(env, jSrcPath);
+        return -1;
+    }
+
+    int retval = 0;
+
+    //Create the org.apache.hadoop.conf.Configuration object
+    jobject jConfiguration =
+        constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
+    if (jConfiguration == NULL) {
+        fprintf(stderr, "Can't construct instance of class "
+                "org.apache.hadoop.conf.Configuration\n");
+        errno = EINTERNAL;
+        destroyLocalReference(env, jSrcPath);
+        destroyLocalReference(env, jDstPath);
+        return -1;
+    }
+
+    //FileUtil::copy
+    jboolean deleteSource = 0; //Only copy
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, STATIC, 
+                     NULL, "org/apache/hadoop/fs/FileUtil", "copy",
+                     "(Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;ZLorg/apache/hadoop/conf/Configuration;)Z",
+                     jSrcFS, jSrcPath, jDstFS, jDstPath, deleteSource, 
+                     jConfiguration) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileUtil::copy");
+        retval = -1;
+        goto done;
+    }
+
+    done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jConfiguration);
+    destroyLocalReference(env, jSrcPath);
+    destroyLocalReference(env, jDstPath);
+  
+    return retval;
+}
+
+
+
+int hdfsMove(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst)
+{
+    //JAVA EQUIVALENT
+    //  FileUtil::copy(srcFS, srcPath, dstFS, dstPath,
+    //                 deleteSource = true, conf)
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+
+    //Parameters
+    jobject jSrcFS = (jobject)srcFS;
+    jobject jDstFS = (jobject)dstFS;
+
+    jobject jSrcPath = NULL;
+    jobject jDstPath = NULL;
+
+    jSrcPath = constructNewObjectOfPath(env, src);
+    if (jSrcPath == NULL) {
+        return -1;
+    }
+
+    jDstPath = constructNewObjectOfPath(env, dst);
+    if (jDstPath == NULL) {
+        destroyLocalReference(env, jSrcPath);
+        return -1;
+    }
+
+    int retval = 0;
+
+    //Create the org.apache.hadoop.conf.Configuration object
+    jobject jConfiguration =
+        constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
+    if (jConfiguration == NULL) {
+        fprintf(stderr, "Can't construct instance of class "
+                "org.apache.hadoop.conf.Configuration\n");
+        errno = EINTERNAL;
+        destroyLocalReference(env, jSrcPath);
+        destroyLocalReference(env, jDstPath);
+        return -1;
+    }
+
+    //FileUtil::copy
+    jboolean deleteSource = 1; //Delete src after copy
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, STATIC, NULL,
+                     "org/apache/hadoop/fs/FileUtil", "copy",
+                "(Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;Lorg/apache/hadoop/fs/FileSystem;Lorg/apache/hadoop/fs/Path;ZLorg/apache/hadoop/conf/Configuration;)Z",
+                     jSrcFS, jSrcPath, jDstFS, jDstPath, deleteSource, 
+                     jConfiguration) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileUtil::copy(move)");
+        retval = -1;
+        goto done;
+    }
+
+    done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jConfiguration);
+    destroyLocalReference(env, jSrcPath);
+    destroyLocalReference(env, jDstPath);
+  
+    return retval;
+}
+
+
+
+int hdfsDelete(hdfsFS fs, const char* path, int recursive)
+{
+    // JAVA EQUIVALENT:
+    //  File f = new File(path);
+    //  bool retval = fs.delete(f);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of java.io.File
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return -1;
+    }
+
+    //Delete the file
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    jboolean jRecursive = recursive ? JNI_TRUE : JNI_FALSE;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "delete", "(Lorg/apache/hadoop/fs/Path;Z)Z",
+                     jPath, jRecursive) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::delete");
+        destroyLocalReference(env, jPath);
+        return -1;
+    }
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+
+    return (jVal.z) ? 0 : -1;
+}
+
+
+
+int hdfsRename(hdfsFS fs, const char* oldPath, const char* newPath)
+{
+    // JAVA EQUIVALENT:
+    //  Path old = new Path(oldPath);
+    //  Path new = new Path(newPath);
+    //  fs.rename(old, new);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create objects of org.apache.hadoop.fs.Path
+    jobject jOldPath = NULL;
+    jobject jNewPath = NULL;
+
+    jOldPath = constructNewObjectOfPath(env, oldPath);
+    if (jOldPath == NULL) {
+        return -1;
+    }
+
+    jNewPath = constructNewObjectOfPath(env, newPath);
+    if (jNewPath == NULL) {
+        destroyLocalReference(env, jOldPath);
+        return -1;
+    }
+
+    //Rename the file
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS, "rename",
+                     JMETHOD2(JPARAM(HADOOP_PATH), JPARAM(HADOOP_PATH), "Z"),
+                     jOldPath, jNewPath) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::rename");
+        destroyLocalReference(env, jOldPath);
+        destroyLocalReference(env, jNewPath);
+        return -1;
+    }
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jOldPath);
+    destroyLocalReference(env, jNewPath);
+
+    return (jVal.z) ? 0 : -1;
+}
+
+
+
+char* hdfsGetWorkingDirectory(hdfsFS fs, char* buffer, size_t bufferSize)
+{
+    // JAVA EQUIVALENT:
+    //  Path p = fs.getWorkingDirectory(); 
+    //  return p.toString()
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return NULL;
+    }
+
+    jobject jFS = (jobject)fs;
+    jobject jPath = NULL;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+
+    //FileSystem::getWorkingDirectory()
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS,
+                     HADOOP_FS, "getWorkingDirectory",
+                     "()Lorg/apache/hadoop/fs/Path;") != 0 ||
+        jVal.l == NULL) {
+        errno = errnoFromException(jExc, env, "FileSystem::"
+                                   "getWorkingDirectory");
+        return NULL;
+    }
+    jPath = jVal.l;
+
+    //Path::toString()
+    jstring jPathString;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jPath, 
+                     "org/apache/hadoop/fs/Path", "toString",
+                     "()Ljava/lang/String;") != 0) { 
+        errno = errnoFromException(jExc, env, "Path::toString");
+        destroyLocalReference(env, jPath);
+        return NULL;
+    }
+    jPathString = jVal.l;
+
+    const char *jPathChars = (const char*)
+        ((*env)->GetStringUTFChars(env, jPathString, NULL));
+
+    //Copy to user-provided buffer
+    strncpy(buffer, jPathChars, bufferSize);
+
+    //Delete unnecessary local references
+    (*env)->ReleaseStringUTFChars(env, jPathString, jPathChars);
+
+    destroyLocalReference(env, jPathString);
+    destroyLocalReference(env, jPath);
+
+    return buffer;
+}
+
+
+
+int hdfsSetWorkingDirectory(hdfsFS fs, const char* path)
+{
+    // JAVA EQUIVALENT:
+    //  fs.setWorkingDirectory(Path(path)); 
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+    int retval = 0;
+    jthrowable jExc = NULL;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return -1;
+    }
+
+    //FileSystem::setWorkingDirectory()
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "setWorkingDirectory", 
+                     "(Lorg/apache/hadoop/fs/Path;)V", jPath) != 0) {
+        errno = errnoFromException(jExc, env, "FileSystem::"
+                                   "setWorkingDirectory");
+        retval = -1;
+    }
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+
+    return retval;
+}
+
+
+
+int hdfsCreateDirectory(hdfsFS fs, const char* path)
+{
+    // JAVA EQUIVALENT:
+    //  fs.mkdirs(new Path(path));
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return -1;
+    }
+
+    //Create the directory
+    jvalue jVal;
+    jVal.z = 0;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "mkdirs", "(Lorg/apache/hadoop/fs/Path;)Z",
+                     jPath) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::mkdirs");
+        goto done;
+    }
+
+ done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+
+    return (jVal.z) ? 0 : -1;
+}
+
+
+int hdfsSetReplication(hdfsFS fs, const char* path, int16_t replication)
+{
+    // JAVA EQUIVALENT:
+    //  fs.setReplication(new Path(path), replication);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return -1;
+    }
+
+    //Create the directory
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "setReplication", "(Lorg/apache/hadoop/fs/Path;S)Z",
+                     jPath, replication) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::setReplication");
+        goto done;
+    }
+
+ done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+
+    return (jVal.z) ? 0 : -1;
+}
+
+int hdfsChown(hdfsFS fs, const char* path, const char *owner, const char *group)
+{
+    // JAVA EQUIVALENT:
+    //  fs.setOwner(path, owner, group)
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    if (owner == NULL && group == NULL) {
+      fprintf(stderr, "Both owner and group cannot be null in chown");
+      errno = EINVAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return -1;
+    }
+
+    jstring jOwnerString = (*env)->NewStringUTF(env, owner); 
+    jstring jGroupString = (*env)->NewStringUTF(env, group); 
+
+    //Create the directory
+    int ret = 0;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "setOwner", JMETHOD3(JPARAM(HADOOP_PATH), JPARAM(JAVA_STRING), JPARAM(JAVA_STRING), JAVA_VOID),
+                     jPath, jOwnerString, jGroupString) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::setOwner");
+        ret = -1;
+        goto done;
+    }
+
+ done:
+    destroyLocalReference(env, jPath);
+    destroyLocalReference(env, jOwnerString);
+    destroyLocalReference(env, jGroupString);
+
+    return ret;
+}
+
+int hdfsChmod(hdfsFS fs, const char* path, short mode)
+{
+    // JAVA EQUIVALENT:
+    //  fs.setPermission(path, FsPermission)
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    // construct jPerm = FsPermission.createImmutable(short mode);
+
+    jshort jmode = mode;
+
+    jobject jPermObj =
+      constructNewObjectOfClass(env, NULL, HADOOP_FSPERM,"(S)V",jmode);
+    if (jPermObj == NULL) {
+      return -2;
+    }
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+      destroyLocalReference(env, jPermObj);
+      return -3;
+    }
+
+    //Create the directory
+    int ret = 0;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "setPermission", JMETHOD2(JPARAM(HADOOP_PATH), JPARAM(HADOOP_FSPERM), JAVA_VOID),
+                     jPath, jPermObj) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::setPermission");
+        ret = -1;
+        goto done;
+    }
+
+ done:
+    destroyLocalReference(env, jPath);
+    destroyLocalReference(env, jPermObj);
+
+    return ret;
+}
+
+int hdfsUtime(hdfsFS fs, const char* path, tTime mtime, tTime atime)
+{
+    // JAVA EQUIVALENT:
+    //  fs.setTimes(src, mtime, atime)
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+      fprintf(stderr, "could not construct path object\n");
+      return -2;
+    }
+
+    const tTime NO_CHANGE = -1;
+    jlong jmtime = (mtime == NO_CHANGE) ? -1 : (mtime * (jlong)1000);
+    jlong jatime = (atime == NO_CHANGE) ? -1 : (atime * (jlong)1000);
+
+    int ret = 0;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "setTimes", JMETHOD3(JPARAM(HADOOP_PATH), "J", "J", JAVA_VOID),
+                     jPath, jmtime, jatime) != 0) {
+      fprintf(stderr, "call to setTime failed\n");
+      errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                 "FileSystem::setTimes");
+      ret = -1;
+      goto done;
+    }
+
+ done:
+    destroyLocalReference(env, jPath);
+    return ret;
+}
+
+
+
+
+char***
+hdfsGetHosts(hdfsFS fs, const char* path, tOffset start, tOffset length)
+{
+    // JAVA EQUIVALENT:
+    //  fs.getFileBlockLoctions(new Path(path), start, length);
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return NULL;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return NULL;
+    }
+
+    jvalue jFSVal;
+    jthrowable jFSExc = NULL;
+    if (invokeMethod(env, &jFSVal, &jFSExc, INSTANCE, jFS,
+                     HADOOP_FS, "getFileStatus", 
+                     "(Lorg/apache/hadoop/fs/Path;)"
+                     "Lorg/apache/hadoop/fs/FileStatus;",
+                     jPath) != 0) {
+        errno = errnoFromException(jFSExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::getFileStatus");
+        destroyLocalReference(env, jPath);
+        return NULL;
+    }
+    jobject jFileStatus = jFSVal.l;
+
+    //org.apache.hadoop.fs.FileSystem::getFileBlockLocations
+    char*** blockHosts = NULL;
+    jobjectArray jBlockLocations;;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS,
+                     HADOOP_FS, "getFileBlockLocations", 
+                     "(Lorg/apache/hadoop/fs/FileStatus;JJ)"
+                     "[Lorg/apache/hadoop/fs/BlockLocation;",
+                     jFileStatus, start, length) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::getFileBlockLocations");
+        destroyLocalReference(env, jPath);
+        destroyLocalReference(env, jFileStatus);
+        return NULL;
+    }
+    jBlockLocations = jVal.l;
+
+    //Figure out no of entries in jBlockLocations
+    //Allocate memory and add NULL at the end
+    jsize jNumFileBlocks = (*env)->GetArrayLength(env, jBlockLocations);
+
+    blockHosts = malloc(sizeof(char**) * (jNumFileBlocks+1));
+    if (blockHosts == NULL) {
+        errno = ENOMEM;
+        goto done;
+    }
+    blockHosts[jNumFileBlocks] = NULL;
+    if (jNumFileBlocks == 0) {
+        errno = 0;
+        goto done;
+    }
+
+    //Now parse each block to get hostnames
+    int i = 0;
+    for (i=0; i < jNumFileBlocks; ++i) {
+        jobject jFileBlock =
+            (*env)->GetObjectArrayElement(env, jBlockLocations, i);
+        
+        jvalue jVal;
+        jobjectArray jFileBlockHosts;
+        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFileBlock, HADOOP_BLK_LOC,
+                         "getHosts", "()[Ljava/lang/String;") ||
+                jVal.l == NULL) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                       "BlockLocation::getHosts");
+            destroyLocalReference(env, jPath);
+            destroyLocalReference(env, jFileStatus);
+            destroyLocalReference(env, jBlockLocations);
+            return NULL;
+        }
+        
+        jFileBlockHosts = jVal.l;
+        //Figure out no of hosts in jFileBlockHosts
+        //Allocate memory and add NULL at the end
+        jsize jNumBlockHosts = (*env)->GetArrayLength(env, jFileBlockHosts);
+        blockHosts[i] = malloc(sizeof(char*) * (jNumBlockHosts+1));
+        if (blockHosts[i] == NULL) {
+            int x = 0;
+            for (x=0; x < i; ++x) {
+                free(blockHosts[x]);
+            }
+            free(blockHosts);
+            errno = ENOMEM;
+            goto done;
+        }
+        blockHosts[i][jNumBlockHosts] = NULL;
+
+        //Now parse each hostname
+        int j = 0;
+        const char *hostName;
+        for (j=0; j < jNumBlockHosts; ++j) {
+            jstring jHost =
+                (*env)->GetObjectArrayElement(env, jFileBlockHosts, j);
+           
+            hostName =
+                (const char*)((*env)->GetStringUTFChars(env, jHost, NULL));
+            blockHosts[i][j] = strdup(hostName);
+
+            (*env)->ReleaseStringUTFChars(env, jHost, hostName);
+            destroyLocalReference(env, jHost);
+        }
+
+        destroyLocalReference(env, jFileBlockHosts);
+    }
+  
+    done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+    destroyLocalReference(env, jFileStatus);
+    destroyLocalReference(env, jBlockLocations);
+
+    return blockHosts;
+}
+
+
+void hdfsFreeHosts(char ***blockHosts)
+{
+    int i, j;
+    for (i=0; blockHosts[i]; i++) {
+        for (j=0; blockHosts[i][j]; j++) {
+            free(blockHosts[i][j]);
+        }
+        free(blockHosts[i]);
+    }
+    free(blockHosts);
+}
+
+
+tOffset hdfsGetDefaultBlockSize(hdfsFS fs)
+{
+    // JAVA EQUIVALENT:
+    //  fs.getDefaultBlockSize();
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //FileSystem::getDefaultBlockSize()
+    tOffset blockSize = -1;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "getDefaultBlockSize", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::getDefaultBlockSize");
+        return -1;
+    }
+    blockSize = jVal.j;
+
+    return blockSize;
+}
+
+
+
+tOffset hdfsGetCapacity(hdfsFS fs)
+{
+    // JAVA EQUIVALENT:
+    //  FsStatus fss = fs.getStatus();
+    //  return Fss.getCapacity();
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //FileSystem::getStatus
+    jvalue  jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "getStatus", "()Lorg/apache/hadoop/fs/FsStatus;") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::getStatus");
+        return -1;
+    }
+    jobject fss = (jobject)jVal.l;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, fss, HADOOP_FSSTATUS,
+                     "getCapacity", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FsStatus::getCapacity");
+        destroyLocalReference(env, fss);
+        return -1;
+    }
+    destroyLocalReference(env, fss);
+    return jVal.j;
+}
+
+
+  
+tOffset hdfsGetUsed(hdfsFS fs)
+{
+    // JAVA EQUIVALENT:
+    //  FsStatus fss = fs.getStatus();
+    //  return Fss.getUsed();
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //FileSystem::getStatus
+    jvalue  jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "getStatus", "()Lorg/apache/hadoop/fs/FsStatus;") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::getStatus");
+        return -1;
+    }
+    jobject fss = (jobject)jVal.l;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, fss, HADOOP_FSSTATUS,
+                     "getUsed", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FsStatus::getUsed");
+        destroyLocalReference(env, fss);
+        return -1;
+    }
+    destroyLocalReference(env, fss);
+    return jVal.j;
+
+}
+
+
+ 
+static int
+getFileInfoFromStat(JNIEnv *env, jobject jStat, hdfsFileInfo *fileInfo)
+{
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
+                     HADOOP_STAT, "isDir", "()Z") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileStatus::isDir");
+        return -1;
+    }
+    fileInfo->mKind = jVal.z ? kObjectKindDirectory : kObjectKindFile;
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
+                     HADOOP_STAT, "getReplication", "()S") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileStatus::getReplication");
+        return -1;
+    }
+    fileInfo->mReplication = jVal.s;
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
+                     HADOOP_STAT, "getBlockSize", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileStatus::getBlockSize");
+        return -1;
+    }
+    fileInfo->mBlockSize = jVal.j;
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
+                     HADOOP_STAT, "getModificationTime", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileStatus::getModificationTime");
+        return -1;
+    }
+    fileInfo->mLastMod = jVal.j / 1000;
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
+                     HADOOP_STAT, "getAccessTime", "()J") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileStatus::getAccessTime");
+        return -1;
+    }
+    fileInfo->mLastAccess = (tTime) (jVal.j / 1000);
+
+
+    if (fileInfo->mKind == kObjectKindFile) {
+        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat,
+                         HADOOP_STAT, "getLen", "()J") != 0) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                       "FileStatus::getLen");
+            return -1;
+        }
+        fileInfo->mSize = jVal.j;
+    }
+
+    jobject jPath;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
+                     "getPath", "()Lorg/apache/hadoop/fs/Path;") ||
+            jVal.l == NULL) { 
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "Path::getPath");
+        return -1;
+    }
+    jPath = jVal.l;
+
+    jstring     jPathName;
+    const char *cPathName;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jPath, HADOOP_PATH,
+                     "toString", "()Ljava/lang/String;")) { 
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "Path::toString");
+        destroyLocalReference(env, jPath);
+        return -1;
+    }
+    jPathName = jVal.l;
+    cPathName = (const char*) ((*env)->GetStringUTFChars(env, jPathName, NULL));
+    fileInfo->mName = strdup(cPathName);
+    (*env)->ReleaseStringUTFChars(env, jPathName, cPathName);
+    destroyLocalReference(env, jPath);
+    destroyLocalReference(env, jPathName);
+    jstring     jUserName;
+    const char* cUserName;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
+                    "getOwner", "()Ljava/lang/String;")) {
+        fprintf(stderr, "Call to org.apache.hadoop.fs."
+                "FileStatus::getOwner failed!\n");
+        errno = EINTERNAL;
+        return -1;
+    }
+    jUserName = jVal.l;
+    cUserName = (const char*) ((*env)->GetStringUTFChars(env, jUserName, NULL));
+    fileInfo->mOwner = strdup(cUserName);
+    (*env)->ReleaseStringUTFChars(env, jUserName, cUserName);
+    destroyLocalReference(env, jUserName);
+
+    jstring     jGroupName;
+    const char* cGroupName;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
+                    "getGroup", "()Ljava/lang/String;")) {
+        fprintf(stderr, "Call to org.apache.hadoop.fs."
+                "FileStatus::getGroup failed!\n");
+        errno = EINTERNAL;
+        return -1;
+    }
+    jGroupName = jVal.l;
+    cGroupName = (const char*) ((*env)->GetStringUTFChars(env, jGroupName, NULL));
+    fileInfo->mGroup = strdup(cGroupName);
+    (*env)->ReleaseStringUTFChars(env, jGroupName, cGroupName);
+    destroyLocalReference(env, jGroupName);
+
+    jobject jPermission;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jStat, HADOOP_STAT,
+                     "getPermission", "()Lorg/apache/hadoop/fs/permission/FsPermission;") ||
+            jVal.l == NULL) {
+        fprintf(stderr, "Call to org.apache.hadoop.fs."
+                "FileStatus::getPermission failed!\n");
+        errno = EINTERNAL;
+        return -1;
+    }
+    jPermission = jVal.l;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jPermission, HADOOP_FSPERM,
+                         "toShort", "()S") != 0) {
+            fprintf(stderr, "Call to org.apache.hadoop.fs.permission."
+                    "FsPermission::toShort failed!\n");
+            errno = EINTERNAL;
+            return -1;
+    }
+    fileInfo->mPermissions = jVal.s;
+    destroyLocalReference(env, jPermission);
+
+    return 0;
+}
+
+static int
+getFileInfo(JNIEnv *env, jobject jFS, jobject jPath, hdfsFileInfo *fileInfo)
+{
+    // JAVA EQUIVALENT:
+    //  fs.isDirectory(f)
+    //  fs.getModificationTime()
+    //  fs.getAccessTime()
+    //  fs.getLength(f)
+    //  f.getPath()
+    //  f.getOwner()
+    //  f.getGroup()
+    //  f.getPermission().toShort()
+
+    jobject jStat;
+    jvalue  jVal;
+    jthrowable jExc = NULL;
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "exists", JMETHOD1(JPARAM(HADOOP_PATH), "Z"),
+                     jPath) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::exists");
+        return -1;
+    }
+
+    if (jVal.z == 0) {
+      errno = ENOENT;
+      return -1;
+    }
+
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_FS,
+                     "getFileStatus", JMETHOD1(JPARAM(HADOOP_PATH), JPARAM(HADOOP_STAT)),
+                     jPath) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::getFileStatus");
+        return -1;
+    }
+    jStat = jVal.l;
+    int ret =  getFileInfoFromStat(env, jStat, fileInfo); 
+    destroyLocalReference(env, jStat);
+    return ret;
+}
+
+
+
+hdfsFileInfo* hdfsListDirectory(hdfsFS fs, const char* path, int *numEntries)
+{
+    // JAVA EQUIVALENT:
+    //  Path p(path);
+    //  Path []pathList = fs.listPaths(p)
+    //  foreach path in pathList 
+    //    getFileInfo(path)
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return NULL;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return NULL;
+    }
+
+    hdfsFileInfo *pathList = 0; 
+
+    jobjectArray jPathList = NULL;
+    jvalue jVal;
+    jthrowable jExc = NULL;
+    if (invokeMethod(env, &jVal, &jExc, INSTANCE, jFS, HADOOP_DFS, "listStatus",
+                     JMETHOD1(JPARAM(HADOOP_PATH), JARRPARAM(HADOOP_STAT)),
+                     jPath) != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FileSystem::listStatus");
+        destroyLocalReference(env, jPath);
+        return NULL;
+    }
+    jPathList = jVal.l;
+
+    //Figure out no of entries in that directory
+    jsize jPathListSize = (*env)->GetArrayLength(env, jPathList);
+    *numEntries = jPathListSize;
+    if (jPathListSize == 0) {
+        errno = 0;
+        goto done;
+    }
+
+    //Allocate memory
+    pathList = calloc(jPathListSize, sizeof(hdfsFileInfo));
+    if (pathList == NULL) {
+        errno = ENOMEM;
+        goto done;
+    }
+
+    //Save path information in pathList
+    jsize i;
+    jobject tmpStat;
+    for (i=0; i < jPathListSize; ++i) {
+        tmpStat = (*env)->GetObjectArrayElement(env, jPathList, i);
+        if (getFileInfoFromStat(env, tmpStat, &pathList[i])) {
+            hdfsFreeFileInfo(pathList, jPathListSize);
+            destroyLocalReference(env, tmpStat);
+            pathList = NULL;
+            goto done;
+        }
+        destroyLocalReference(env, tmpStat);
+    }
+
+    done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+    destroyLocalReference(env, jPathList);
+
+    return pathList;
+}
+
+
+
+hdfsFileInfo *hdfsGetPathInfo(hdfsFS fs, const char* path)
+{
+    // JAVA EQUIVALENT:
+    //  File f(path);
+    //  fs.isDirectory(f)
+    //  fs.lastModified() ??
+    //  fs.getLength(f)
+    //  f.getPath()
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return NULL;
+    }
+
+    jobject jFS = (jobject)fs;
+
+    //Create an object of org.apache.hadoop.fs.Path
+    jobject jPath = constructNewObjectOfPath(env, path);
+    if (jPath == NULL) {
+        return NULL;
+    }
+
+    hdfsFileInfo *fileInfo = calloc(1, sizeof(hdfsFileInfo));
+    if (getFileInfo(env, jFS, jPath, fileInfo)) {
+        hdfsFreeFileInfo(fileInfo, 1);
+        fileInfo = NULL;
+        goto done;
+    }
+
+    done:
+
+    //Delete unnecessary local references
+    destroyLocalReference(env, jPath);
+
+    return fileInfo;
+}
+
+
+
+void hdfsFreeFileInfo(hdfsFileInfo *hdfsFileInfo, int numEntries)
+{
+    //Free the mName, mOwner, and mGroup
+    int i;
+    for (i=0; i < numEntries; ++i) {
+        if (hdfsFileInfo[i].mName) {
+            free(hdfsFileInfo[i].mName);
+        }
+        if (hdfsFileInfo[i].mOwner) {
+            free(hdfsFileInfo[i].mOwner);
+        }
+        if (hdfsFileInfo[i].mGroup) {
+            free(hdfsFileInfo[i].mGroup);
+        }
+    }
+
+    //Free entire block
+    free(hdfsFileInfo);
+}
+
+
+
+
+/**
+ * vim: ts=4: sw=4: et:
+ */
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.h
new file mode 100644
index 0000000..d5cef6e
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.h
@@ -0,0 +1,605 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef LIBHDFS_HDFS_H
+#define LIBHDFS_HDFS_H
+
+#include <errno.h> /* for EINTERNAL, etc. */
+#include <fcntl.h> /* for O_RDONLY, O_WRONLY */
+#include <stdint.h> /* for uint64_t, etc. */
+#include <time.h> /* for time_t */
+
+#ifndef O_RDONLY
+#define O_RDONLY 1
+#endif
+
+#ifndef O_WRONLY 
+#define O_WRONLY 2
+#endif
+
+#ifndef EINTERNAL
+#define EINTERNAL 255 
+#endif
+
+
+/** All APIs set errno to meaningful values */
+
+#ifdef __cplusplus
+extern  "C" {
+#endif
+    /**
+     * Some utility decls used in libhdfs.
+     */
+    struct hdfsBuilder;
+    typedef int32_t   tSize; /// size of data for read/write io ops 
+    typedef time_t    tTime; /// time type in seconds
+    typedef int64_t   tOffset;/// offset within the file
+    typedef uint16_t  tPort; /// port
+    typedef enum tObjectKind {
+        kObjectKindFile = 'F',
+        kObjectKindDirectory = 'D',
+    } tObjectKind;
+
+
+    /**
+     * The C reflection of org.apache.org.hadoop.FileSystem .
+     */
+    struct hdfs_internal;
+    typedef struct hdfs_internal* hdfsFS;
+    
+    struct hdfsFile_internal;
+    typedef struct hdfsFile_internal* hdfsFile;
+
+    /**
+     * Determine if a file is open for read.
+     *
+     * @param file     The HDFS file
+     * @return         1 if the file is open for read; 0 otherwise
+     */
+    int hdfsFileIsOpenForRead(hdfsFile file);
+
+    /**
+     * Determine if a file is open for write.
+     *
+     * @param file     The HDFS file
+     * @return         1 if the file is open for write; 0 otherwise
+     */
+    int hdfsFileIsOpenForWrite(hdfsFile file);
+
+    /**
+     * Disable the direct read optimization for a file.
+     *
+     * This is mainly provided for unit testing purposes.
+     *
+     * @param file     The HDFS file
+     */
+    void hdfsFileDisableDirectRead(hdfsFile file);
+
+    /** 
+     * hdfsConnectAsUser - Connect to a hdfs file system as a specific user
+     * Connect to the hdfs.
+     * @param nn   The NameNode.  See hdfsBuilderSetNameNode for details.
+     * @param port The port on which the server is listening.
+     * @param user the user name (this is hadoop domain user). Or NULL is equivelant to hhdfsConnect(host, port)
+     * @return Returns a handle to the filesystem or NULL on error.
+     * @deprecated Use hdfsBuilderConnect instead. 
+     */
+     hdfsFS hdfsConnectAsUser(const char* nn, tPort port, const char *user);
+
+
+    /** 
+     * hdfsConnect - Connect to a hdfs file system.
+     * Connect to the hdfs.
+     * @param nn   The NameNode.  See hdfsBuilderSetNameNode for details.
+     * @param port The port on which the server is listening.
+     * @return Returns a handle to the filesystem or NULL on error.
+     * @deprecated Use hdfsBuilderConnect instead. 
+     */
+     hdfsFS hdfsConnect(const char* nn, tPort port);
+
+    /** 
+     * hdfsConnect - Connect to an hdfs file system.
+     *
+     * Forces a new instance to be created
+     *
+     * @param nn     The NameNode.  See hdfsBuilderSetNameNode for details.
+     * @param port   The port on which the server is listening.
+     * @param user   The user name to use when connecting
+     * @return       Returns a handle to the filesystem or NULL on error.
+     * @deprecated   Use hdfsBuilderConnect instead. 
+     */
+     hdfsFS hdfsConnectAsUserNewInstance(const char* nn, tPort port, const char *user );
+
+    /** 
+     * hdfsConnect - Connect to an hdfs file system.
+     *
+     * Forces a new instance to be created
+     *
+     * @param nn     The NameNode.  See hdfsBuilderSetNameNode for details.
+     * @param port   The port on which the server is listening.
+     * @return       Returns a handle to the filesystem or NULL on error.
+     * @deprecated   Use hdfsBuilderConnect instead. 
+     */
+     hdfsFS hdfsConnectNewInstance(const char* nn, tPort port);
+
+    /** 
+     * Connect to HDFS using the parameters defined by the builder.
+     *
+     * The HDFS builder will be freed, whether or not the connection was
+     * successful.
+     *
+     * Every successful call to hdfsBuilderConnect should be matched with a call
+     * to hdfsDisconnect, when the hdfsFS is no longer needed.
+     *
+     * @param bld    The HDFS builder
+     * @return       Returns a handle to the filesystem, or NULL on error.
+     */
+     hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld);
+
+    /**
+     * Create an HDFS builder.
+     *
+     * @return The HDFS builder, or NULL on error.
+     */
+    struct hdfsBuilder *hdfsNewBuilder(void);
+
+    /**
+     * Force the builder to always create a new instance of the FileSystem,
+     * rather than possibly finding one in the cache.
+     *
+     * @param bld The HDFS builder
+     */
+    void hdfsBuilderSetForceNewInstance(struct hdfsBuilder *bld);
+
+    /**
+     * Set the HDFS NameNode to connect to.
+     *
+     * @param bld  The HDFS builder
+     * @param nn   The NameNode to use.
+     *
+     *             If the string given is 'default', the default NameNode
+     *             configuration will be used (from the XML configuration files)
+     *
+     *             If NULL is given, a LocalFileSystem will be created.
+     *
+     *             If the string starts with a protocol type such as file:// or
+     *             hdfs://, this protocol type will be used.  If not, the
+     *             hdfs:// protocol type will be used.
+     *
+     *             You may specify a NameNode port in the usual way by 
+     *             passing a string of the format hdfs://<hostname>:<port>.
+     *             Alternately, you may set the port with
+     *             hdfsBuilderSetNameNodePort.  However, you must not pass the
+     *             port in two different ways.
+     */
+    void hdfsBuilderSetNameNode(struct hdfsBuilder *bld, const char *nn);
+
+    /**
+     * Set the port of the HDFS NameNode to connect to.
+     *
+     * @param bld The HDFS builder
+     * @param port The port.
+     */
+    void hdfsBuilderSetNameNodePort(struct hdfsBuilder *bld, tPort port);
+
+    /**
+     * Set the username to use when connecting to the HDFS cluster.
+     *
+     * @param bld The HDFS builder
+     * @param userName The user name.  The string will be shallow-copied.
+     */
+    void hdfsBuilderSetUserName(struct hdfsBuilder *bld, const char *userName);
+
+    /**
+     * Set the path to the Kerberos ticket cache to use when connecting to
+     * the HDFS cluster.
+     *
+     * @param bld The HDFS builder
+     * @param kerbTicketCachePath The Kerberos ticket cache path.  The string
+     *                            will be shallow-copied.
+     */
+    void hdfsBuilderSetKerbTicketCachePath(struct hdfsBuilder *bld,
+                                   const char *kerbTicketCachePath);
+
+    /**
+     * Free an HDFS builder.
+     *
+     * It is normally not necessary to call this function since
+     * hdfsBuilderConnect frees the builder.
+     *
+     * @param bld The HDFS builder
+     */
+    void hdfsFreeBuilder(struct hdfsBuilder *bld);
+
+    /**
+     * Get a configuration string.
+     *
+     * @param key      The key to find
+     * @param val      (out param) The value.  This will be NULL if the
+     *                 key isn't found.  You must free this string with
+     *                 hdfsConfFree.
+     *
+     * @return         0 on success; nonzero error code otherwise.
+     *                 Failure to find the key is not an error.
+     */
+    int hdfsConfGet(const char *key, char **val);
+
+    /**
+     * Free a configuration string found with hdfsConfGet. 
+     *
+     * @param val      A configuration string obtained from hdfsConfGet
+     */
+    void hdfsConfFree(char *val);
+
+    /** 
+     * hdfsDisconnect - Disconnect from the hdfs file system.
+     * Disconnect from hdfs.
+     * @param fs The configured filesystem handle.
+     * @return Returns 0 on success, -1 on error.  
+     */
+    int hdfsDisconnect(hdfsFS fs);
+        
+
+    /** 
+     * hdfsOpenFile - Open a hdfs file in given mode.
+     * @param fs The configured filesystem handle.
+     * @param path The full path to the file.
+     * @param flags - an | of bits/fcntl.h file flags - supported flags are O_RDONLY, O_WRONLY (meaning create or overwrite i.e., implies O_TRUNCAT), 
+     * O_WRONLY|O_APPEND. Other flags are generally ignored other than (O_RDWR || (O_EXCL & O_CREAT)) which return NULL and set errno equal ENOTSUP.
+     * @param bufferSize Size of buffer for read/write - pass 0 if you want
+     * to use the default configured values.
+     * @param replication Block replication - pass 0 if you want to use
+     * the default configured values.
+     * @param blocksize Size of block - pass 0 if you want to use the
+     * default configured values.
+     * @return Returns the handle to the open file or NULL on error.
+     */
+    hdfsFile hdfsOpenFile(hdfsFS fs, const char* path, int flags,
+                          int bufferSize, short replication, tSize blocksize);
+
+
+    /** 
+     * hdfsCloseFile - Close an open file. 
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @return Returns 0 on success, -1 on error.  
+     */
+    int hdfsCloseFile(hdfsFS fs, hdfsFile file);
+
+
+    /** 
+     * hdfsExists - Checks if a given path exsits on the filesystem 
+     * @param fs The configured filesystem handle.
+     * @param path The path to look for
+     * @return Returns 0 on success, -1 on error.  
+     */
+    int hdfsExists(hdfsFS fs, const char *path);
+
+
+    /** 
+     * hdfsSeek - Seek to given offset in file. 
+     * This works only for files opened in read-only mode. 
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @param desiredPos Offset into the file to seek into.
+     * @return Returns 0 on success, -1 on error.  
+     */
+    int hdfsSeek(hdfsFS fs, hdfsFile file, tOffset desiredPos); 
+
+
+    /** 
+     * hdfsTell - Get the current offset in the file, in bytes.
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @return Current offset, -1 on error.
+     */
+    tOffset hdfsTell(hdfsFS fs, hdfsFile file);
+
+
+    /** 
+     * hdfsRead - Read data from an open file.
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @param buffer The buffer to copy read bytes into.
+     * @param length The length of the buffer.
+     * @return      On success, a positive number indicating how many bytes
+     *              were read.
+     *              On end-of-file, 0.
+     *              On error, -1.  Errno will be set to the error code.
+     *              Just like the POSIX read function, hdfsRead will return -1
+     *              and set errno to EINTR if data is temporarily unavailable,
+     *              but we are not yet at the end of the file.
+     */
+    tSize hdfsRead(hdfsFS fs, hdfsFile file, void* buffer, tSize length);
+
+    /** 
+     * hdfsPread - Positional read of data from an open file.
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @param position Position from which to read
+     * @param buffer The buffer to copy read bytes into.
+     * @param length The length of the buffer.
+     * @return Returns the number of bytes actually read, possibly less than
+     * than length;-1 on error.
+     */
+    tSize hdfsPread(hdfsFS fs, hdfsFile file, tOffset position,
+                    void* buffer, tSize length);
+
+
+    /** 
+     * hdfsWrite - Write data into an open file.
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @param buffer The data.
+     * @param length The no. of bytes to write. 
+     * @return Returns the number of bytes written, -1 on error.
+     */
+    tSize hdfsWrite(hdfsFS fs, hdfsFile file, const void* buffer,
+                    tSize length);
+
+
+    /** 
+     * hdfsWrite - Flush the data. 
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsFlush(hdfsFS fs, hdfsFile file);
+
+
+    /**
+     * hdfsHFlush - Flush out the data in client's user buffer. After the
+     * return of this call, new readers will see the data.
+     * @param fs configured filesystem handle
+     * @param file file handle
+     * @return 0 on success, -1 on error and sets errno
+     */
+    int hdfsHFlush(hdfsFS fs, hdfsFile file);
+
+
+    /**
+     * hdfsAvailable - Number of bytes that can be read from this
+     * input stream without blocking.
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @return Returns available bytes; -1 on error. 
+     */
+    int hdfsAvailable(hdfsFS fs, hdfsFile file);
+
+
+    /**
+     * hdfsCopy - Copy file from one filesystem to another.
+     * @param srcFS The handle to source filesystem.
+     * @param src The path of source file. 
+     * @param dstFS The handle to destination filesystem.
+     * @param dst The path of destination file. 
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsCopy(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst);
+
+
+    /**
+     * hdfsMove - Move file from one filesystem to another.
+     * @param srcFS The handle to source filesystem.
+     * @param src The path of source file. 
+     * @param dstFS The handle to destination filesystem.
+     * @param dst The path of destination file. 
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsMove(hdfsFS srcFS, const char* src, hdfsFS dstFS, const char* dst);
+
+
+    /**
+     * hdfsDelete - Delete file. 
+     * @param fs The configured filesystem handle.
+     * @param path The path of the file. 
+     * @param recursive if path is a directory and set to 
+     * non-zero, the directory is deleted else throws an exception. In
+     * case of a file the recursive argument is irrelevant.
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsDelete(hdfsFS fs, const char* path, int recursive);
+
+    /**
+     * hdfsRename - Rename file. 
+     * @param fs The configured filesystem handle.
+     * @param oldPath The path of the source file. 
+     * @param newPath The path of the destination file. 
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsRename(hdfsFS fs, const char* oldPath, const char* newPath);
+
+
+    /** 
+     * hdfsGetWorkingDirectory - Get the current working directory for
+     * the given filesystem.
+     * @param fs The configured filesystem handle.
+     * @param buffer The user-buffer to copy path of cwd into. 
+     * @param bufferSize The length of user-buffer.
+     * @return Returns buffer, NULL on error.
+     */
+    char* hdfsGetWorkingDirectory(hdfsFS fs, char *buffer, size_t bufferSize);
+
+
+    /** 
+     * hdfsSetWorkingDirectory - Set the working directory. All relative
+     * paths will be resolved relative to it.
+     * @param fs The configured filesystem handle.
+     * @param path The path of the new 'cwd'. 
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsSetWorkingDirectory(hdfsFS fs, const char* path);
+
+
+    /** 
+     * hdfsCreateDirectory - Make the given file and all non-existent
+     * parents into directories.
+     * @param fs The configured filesystem handle.
+     * @param path The path of the directory. 
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsCreateDirectory(hdfsFS fs, const char* path);
+
+
+    /** 
+     * hdfsSetReplication - Set the replication of the specified
+     * file to the supplied value
+     * @param fs The configured filesystem handle.
+     * @param path The path of the file. 
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsSetReplication(hdfsFS fs, const char* path, int16_t replication);
+
+
+    /** 
+     * hdfsFileInfo - Information about a file/directory.
+     */
+    typedef struct  {
+        tObjectKind mKind;   /* file or directory */
+        char *mName;         /* the name of the file */
+        tTime mLastMod;      /* the last modification time for the file in seconds */
+        tOffset mSize;       /* the size of the file in bytes */
+        short mReplication;    /* the count of replicas */
+        tOffset mBlockSize;  /* the block size for the file */
+        char *mOwner;        /* the owner of the file */
+        char *mGroup;        /* the group associated with the file */
+        short mPermissions;  /* the permissions associated with the file */
+        tTime mLastAccess;    /* the last access time for the file in seconds */
+    } hdfsFileInfo;
+
+
+    /** 
+     * hdfsListDirectory - Get list of files/directories for a given
+     * directory-path. hdfsFreeFileInfo should be called to deallocate memory. 
+     * @param fs The configured filesystem handle.
+     * @param path The path of the directory. 
+     * @param numEntries Set to the number of files/directories in path.
+     * @return Returns a dynamically-allocated array of hdfsFileInfo
+     * objects; NULL on error.
+     */
+    hdfsFileInfo *hdfsListDirectory(hdfsFS fs, const char* path,
+                                    int *numEntries);
+
+
+    /** 
+     * hdfsGetPathInfo - Get information about a path as a (dynamically
+     * allocated) single hdfsFileInfo struct. hdfsFreeFileInfo should be
+     * called when the pointer is no longer needed.
+     * @param fs The configured filesystem handle.
+     * @param path The path of the file. 
+     * @return Returns a dynamically-allocated hdfsFileInfo object;
+     * NULL on error.
+     */
+    hdfsFileInfo *hdfsGetPathInfo(hdfsFS fs, const char* path);
+
+
+    /** 
+     * hdfsFreeFileInfo - Free up the hdfsFileInfo array (including fields) 
+     * @param hdfsFileInfo The array of dynamically-allocated hdfsFileInfo
+     * objects.
+     * @param numEntries The size of the array.
+     */
+    void hdfsFreeFileInfo(hdfsFileInfo *hdfsFileInfo, int numEntries);
+
+
+    /** 
+     * hdfsGetHosts - Get hostnames where a particular block (determined by
+     * pos & blocksize) of a file is stored. The last element in the array
+     * is NULL. Due to replication, a single block could be present on
+     * multiple hosts.
+     * @param fs The configured filesystem handle.
+     * @param path The path of the file. 
+     * @param start The start of the block.
+     * @param length The length of the block.
+     * @return Returns a dynamically-allocated 2-d array of blocks-hosts;
+     * NULL on error.
+     */
+    char*** hdfsGetHosts(hdfsFS fs, const char* path, 
+            tOffset start, tOffset length);
+
+
+    /** 
+     * hdfsFreeHosts - Free up the structure returned by hdfsGetHosts
+     * @param hdfsFileInfo The array of dynamically-allocated hdfsFileInfo
+     * objects.
+     * @param numEntries The size of the array.
+     */
+    void hdfsFreeHosts(char ***blockHosts);
+
+
+    /** 
+     * hdfsGetDefaultBlockSize - Get the optimum blocksize.
+     * @param fs The configured filesystem handle.
+     * @return Returns the blocksize; -1 on error. 
+     */
+    tOffset hdfsGetDefaultBlockSize(hdfsFS fs);
+
+
+    /** 
+     * hdfsGetCapacity - Return the raw capacity of the filesystem.  
+     * @param fs The configured filesystem handle.
+     * @return Returns the raw-capacity; -1 on error. 
+     */
+    tOffset hdfsGetCapacity(hdfsFS fs);
+
+
+    /** 
+     * hdfsGetUsed - Return the total raw size of all files in the filesystem.
+     * @param fs The configured filesystem handle.
+     * @return Returns the total-size; -1 on error. 
+     */
+    tOffset hdfsGetUsed(hdfsFS fs);
+
+    /** 
+     * hdfsChown 
+     * @param fs The configured filesystem handle.
+     * @param path the path to the file or directory
+     * @param owner this is a string in Hadoop land. Set to null or "" if only setting group
+     * @param group  this is a string in Hadoop land. Set to null or "" if only setting user
+     * @return 0 on success else -1
+     */
+    int hdfsChown(hdfsFS fs, const char* path, const char *owner, const char *group);
+
+    /** 
+     * hdfsChmod
+     * @param fs The configured filesystem handle.
+     * @param path the path to the file or directory
+     * @param mode the bitmask to set it to
+     * @return 0 on success else -1
+     */
+      int hdfsChmod(hdfsFS fs, const char* path, short mode);
+
+    /** 
+     * hdfsUtime
+     * @param fs The configured filesystem handle.
+     * @param path the path to the file or directory
+     * @param mtime new modification time or -1 for no change
+     * @param atime new access time or -1 for no change
+     * @return 0 on success else -1
+     */
+    int hdfsUtime(hdfsFS fs, const char* path, tTime mtime, tTime atime);
+    
+#ifdef __cplusplus
+}
+#endif
+
+#endif /*LIBHDFS_HDFS_H*/
+
+/**
+ * vim: ts=4: sw=4: et
+ */
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs_test.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs_test.h
new file mode 100644
index 0000000..ec59ba3
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs_test.h
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef LIBHDFS_HDFS_TEST_H
+#define LIBHDFS_HDFS_TEST_H
+
+struct hdfs_internal;
+
+/**
+ * Some functions that are visible only for testing.
+ *
+ * This header is not meant to be exported or used outside of the libhdfs unit
+ * tests.
+ */
+
+#ifdef __cplusplus
+extern  "C" {
+#endif
+    /**
+     * Determine if a file is using the "direct read" optimization.
+     *
+     * @param file     The HDFS file
+     * @return         1 if the file is using the direct read optimization,
+     *                 0 otherwise.
+     */
+    int hdfsFileUsesDirectRead(struct hdfs_internal *file);
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c
new file mode 100644
index 0000000..6978655
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c
@@ -0,0 +1,589 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "config.h"
+#include "jni_helper.h"
+
+#include <stdio.h> 
+#include <string.h> 
+
+static pthread_mutex_t hdfsHashMutex = PTHREAD_MUTEX_INITIALIZER;
+static pthread_mutex_t jvmMutex = PTHREAD_MUTEX_INITIALIZER;
+static volatile int hashTableInited = 0;
+
+#define LOCK_HASH_TABLE() pthread_mutex_lock(&hdfsHashMutex)
+#define UNLOCK_HASH_TABLE() pthread_mutex_unlock(&hdfsHashMutex)
+
+
+/** The Native return types that methods could return */
+#define VOID          'V'
+#define JOBJECT       'L'
+#define JARRAYOBJECT  '['
+#define JBOOLEAN      'Z'
+#define JBYTE         'B'
+#define JCHAR         'C'
+#define JSHORT        'S'
+#define JINT          'I'
+#define JLONG         'J'
+#define JFLOAT        'F'
+#define JDOUBLE       'D'
+
+
+/**
+ * MAX_HASH_TABLE_ELEM: The maximum no. of entries in the hashtable.
+ * It's set to 4096 to account for (classNames + No. of threads)
+ */
+#define MAX_HASH_TABLE_ELEM 4096
+
+/** Key that allows us to retrieve thread-local storage */
+static pthread_key_t gTlsKey;
+
+/** nonzero if we succeeded in initializing gTlsKey. Protected by the jvmMutex */
+static int gTlsKeyInitialized = 0;
+
+/** Pthreads thread-local storage for each library thread. */
+struct hdfsTls {
+    JNIEnv *env;
+};
+
+/**
+ * The function that is called whenever a thread with libhdfs thread local data
+ * is destroyed.
+ *
+ * @param v         The thread-local data
+ */
+static void hdfsThreadDestructor(void *v)
+{
+    struct hdfsTls *tls = v;
+    JavaVM *vm;
+    JNIEnv *env = tls->env;
+    jint ret;
+
+    ret = (*env)->GetJavaVM(env, &vm);
+    if (ret) {
+        fprintf(stderr, "hdfsThreadDestructor: GetJavaVM failed with "
+                "error %d\n", ret);
+        (*env)->ExceptionDescribe(env);
+    } else {
+        (*vm)->DetachCurrentThread(vm);
+    }
+    free(tls);
+}
+
+
+static int validateMethodType(MethType methType)
+{
+    if (methType != STATIC && methType != INSTANCE) {
+        fprintf(stderr, "Unimplemented method type\n");
+        return 0;
+    }
+    return 1;
+}
+
+
+static int hashTableInit(void)
+{
+    if (!hashTableInited) {
+        LOCK_HASH_TABLE();
+        if (!hashTableInited) {
+            if (hcreate(MAX_HASH_TABLE_ELEM) == 0) {
+                fprintf(stderr, "error creating hashtable, <%d>: %s\n",
+                        errno, strerror(errno));
+                return 0;
+            } 
+            hashTableInited = 1;
+        }
+        UNLOCK_HASH_TABLE();
+    }
+    return 1;
+}
+
+
+static int insertEntryIntoTable(const char *key, void *data)
+{
+    ENTRY e, *ep;
+    if (key == NULL || data == NULL) {
+        return 0;
+    }
+    if (! hashTableInit()) {
+      return -1;
+    }
+    e.data = data;
+    e.key = (char*)key;
+    LOCK_HASH_TABLE();
+    ep = hsearch(e, ENTER);
+    UNLOCK_HASH_TABLE();
+    if (ep == NULL) {
+        fprintf(stderr, "warn adding key (%s) to hash table, <%d>: %s\n",
+                key, errno, strerror(errno));
+    }  
+    return 0;
+}
+
+
+
+static void* searchEntryFromTable(const char *key)
+{
+    ENTRY e,*ep;
+    if (key == NULL) {
+        return NULL;
+    }
+    hashTableInit();
+    e.key = (char*)key;
+    LOCK_HASH_TABLE();
+    ep = hsearch(e, FIND);
+    UNLOCK_HASH_TABLE();
+    if (ep != NULL) {
+        return ep->data;
+    }
+    return NULL;
+}
+
+
+
+int invokeMethod(JNIEnv *env, RetVal *retval, Exc *exc, MethType methType,
+                 jobject instObj, const char *className,
+                 const char *methName, const char *methSignature, ...)
+{
+    va_list args;
+    jclass cls;
+    jmethodID mid;
+    jthrowable jthr;
+    const char *str; 
+    char returnType;
+    
+    if (! validateMethodType(methType)) {
+      return -1;
+    }
+    cls = globalClassReference(className, env);
+    if (cls == NULL) {
+      return -2;
+    }
+
+    mid = methodIdFromClass(className, methName, methSignature, 
+                            methType, env);
+    if (mid == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return -3;
+    }
+   
+    str = methSignature;
+    while (*str != ')') str++;
+    str++;
+    returnType = *str;
+    va_start(args, methSignature);
+    if (returnType == JOBJECT || returnType == JARRAYOBJECT) {
+        jobject jobj = NULL;
+        if (methType == STATIC) {
+            jobj = (*env)->CallStaticObjectMethodV(env, cls, mid, args);
+        }
+        else if (methType == INSTANCE) {
+            jobj = (*env)->CallObjectMethodV(env, instObj, mid, args);
+        }
+        retval->l = jobj;
+    }
+    else if (returnType == VOID) {
+        if (methType == STATIC) {
+            (*env)->CallStaticVoidMethodV(env, cls, mid, args);
+        }
+        else if (methType == INSTANCE) {
+            (*env)->CallVoidMethodV(env, instObj, mid, args);
+        }
+    }
+    else if (returnType == JBOOLEAN) {
+        jboolean jbool = 0;
+        if (methType == STATIC) {
+            jbool = (*env)->CallStaticBooleanMethodV(env, cls, mid, args);
+        }
+        else if (methType == INSTANCE) {
+            jbool = (*env)->CallBooleanMethodV(env, instObj, mid, args);
+        }
+        retval->z = jbool;
+    }
+    else if (returnType == JSHORT) {
+        jshort js = 0;
+        if (methType == STATIC) {
+            js = (*env)->CallStaticShortMethodV(env, cls, mid, args);
+        }
+        else if (methType == INSTANCE) {
+            js = (*env)->CallShortMethodV(env, instObj, mid, args);
+        }
+        retval->s = js;
+    }
+    else if (returnType == JLONG) {
+        jlong jl = -1;
+        if (methType == STATIC) {
+            jl = (*env)->CallStaticLongMethodV(env, cls, mid, args);
+        }
+        else if (methType == INSTANCE) {
+            jl = (*env)->CallLongMethodV(env, instObj, mid, args);
+        }
+        retval->j = jl;
+    }
+    else if (returnType == JINT) {
+        jint ji = -1;
+        if (methType == STATIC) {
+            ji = (*env)->CallStaticIntMethodV(env, cls, mid, args);
+        }
+        else if (methType == INSTANCE) {
+            ji = (*env)->CallIntMethodV(env, instObj, mid, args);
+        }
+        retval->i = ji;
+    }
+    va_end(args);
+
+    jthr = (*env)->ExceptionOccurred(env);
+    if (jthr != NULL) {
+        if (exc != NULL)
+            *exc = jthr;
+        else
+            (*env)->ExceptionDescribe(env);
+        return -1;
+    }
+    return 0;
+}
+
+jarray constructNewArrayString(JNIEnv *env, Exc *exc, const char **elements, int size) {
+  const char *className = "java/lang/String";
+  jobjectArray result;
+  int i;
+  jclass arrCls = (*env)->FindClass(env, className);
+  if (arrCls == NULL) {
+    fprintf(stderr, "could not find class %s\n",className);
+    return NULL; /* exception thrown */
+  }
+  result = (*env)->NewObjectArray(env, size, arrCls,
+                                  NULL);
+  if (result == NULL) {
+    fprintf(stderr, "ERROR: could not construct new array\n");
+    return NULL; /* out of memory error thrown */
+  }
+  for (i = 0; i < size; i++) {
+    jstring jelem = (*env)->NewStringUTF(env,elements[i]);
+    if (jelem == NULL) {
+      fprintf(stderr, "ERROR: jelem == NULL\n");
+    }
+    (*env)->SetObjectArrayElement(env, result, i, jelem);
+    (*env)->DeleteLocalRef(env, jelem);
+  }
+  return result;
+}
+
+jobject constructNewObjectOfClass(JNIEnv *env, Exc *exc, const char *className, 
+                                  const char *ctorSignature, ...)
+{
+    va_list args;
+    jclass cls;
+    jmethodID mid; 
+    jobject jobj;
+    jthrowable jthr;
+
+    cls = globalClassReference(className, env);
+    if (cls == NULL) {
+        (*env)->ExceptionDescribe(env);
+      return NULL;
+    }
+
+    mid = methodIdFromClass(className, "<init>", ctorSignature, 
+                            INSTANCE, env);
+    if (mid == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    } 
+    va_start(args, ctorSignature);
+    jobj = (*env)->NewObjectV(env, cls, mid, args);
+    va_end(args);
+    jthr = (*env)->ExceptionOccurred(env);
+    if (jthr != NULL) {
+        if (exc != NULL)
+            *exc = jthr;
+        else
+            (*env)->ExceptionDescribe(env);
+    }
+    return jobj;
+}
+
+
+
+
+jmethodID methodIdFromClass(const char *className, const char *methName, 
+                            const char *methSignature, MethType methType, 
+                            JNIEnv *env)
+{
+    jclass cls = globalClassReference(className, env);
+    if (cls == NULL) {
+      fprintf(stderr, "could not find class %s\n", className);
+      return NULL;
+    }
+
+    jmethodID mid = 0;
+    if (!validateMethodType(methType)) {
+      fprintf(stderr, "invalid method type\n");
+      return NULL;
+    }
+
+    if (methType == STATIC) {
+        mid = (*env)->GetStaticMethodID(env, cls, methName, methSignature);
+    }
+    else if (methType == INSTANCE) {
+        mid = (*env)->GetMethodID(env, cls, methName, methSignature);
+    }
+    if (mid == NULL) {
+      fprintf(stderr, "could not find method %s from class %s with signature %s\n",methName, className, methSignature);
+    }
+    return mid;
+}
+
+
+jclass globalClassReference(const char *className, JNIEnv *env)
+{
+    jclass clsLocalRef;
+    jclass cls = searchEntryFromTable(className);
+    if (cls) {
+        return cls; 
+    }
+
+    clsLocalRef = (*env)->FindClass(env,className);
+    if (clsLocalRef == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    }
+    cls = (*env)->NewGlobalRef(env, clsLocalRef);
+    if (cls == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    }
+    (*env)->DeleteLocalRef(env, clsLocalRef);
+    insertEntryIntoTable(className, cls);
+    return cls;
+}
+
+
+char *classNameOfObject(jobject jobj, JNIEnv *env) {
+    jclass cls, clsClass;
+    jmethodID mid;
+    jstring str;
+    const char *cstr;
+    char *newstr;
+
+    cls = (*env)->GetObjectClass(env, jobj);
+    if (cls == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    }
+    clsClass = (*env)->FindClass(env, "java/lang/Class");
+    if (clsClass == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    }
+    mid = (*env)->GetMethodID(env, clsClass, "getName", "()Ljava/lang/String;");
+    if (mid == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    }
+    str = (*env)->CallObjectMethod(env, cls, mid);
+    if (str == NULL) {
+        (*env)->ExceptionDescribe(env);
+        return NULL;
+    }
+
+    cstr = (*env)->GetStringUTFChars(env, str, NULL);
+    newstr = strdup(cstr);
+    (*env)->ReleaseStringUTFChars(env, str, cstr);
+    if (newstr == NULL) {
+        perror("classNameOfObject: strdup");
+        return NULL;
+    }
+    return newstr;
+}
+
+/**
+ * Get the global JNI environemnt.
+ *
+ * We only have to create the JVM once.  After that, we can use it in
+ * every thread.  You must be holding the jvmMutex when you call this
+ * function.
+ *
+ * @return          The JNIEnv on success; error code otherwise
+ */
+static JNIEnv* getGlobalJNIEnv(void)
+{
+    const jsize vmBufLength = 1;
+    JavaVM* vmBuf[vmBufLength]; 
+    JNIEnv *env;
+    jint rv = 0; 
+    jint noVMs = 0;
+
+    rv = JNI_GetCreatedJavaVMs(&(vmBuf[0]), vmBufLength, &noVMs);
+    if (rv != 0) {
+        fprintf(stderr, "JNI_GetCreatedJavaVMs failed with error: %d\n", rv);
+        return NULL;
+    }
+
+    if (noVMs == 0) {
+        //Get the environment variables for initializing the JVM
+        char *hadoopClassPath = getenv("CLASSPATH");
+        if (hadoopClassPath == NULL) {
+            fprintf(stderr, "Environment variable CLASSPATH not set!\n");
+            return NULL;
+        } 
+        char *hadoopClassPathVMArg = "-Djava.class.path=";
+        size_t optHadoopClassPathLen = strlen(hadoopClassPath) + 
+          strlen(hadoopClassPathVMArg) + 1;
+        char *optHadoopClassPath = malloc(sizeof(char)*optHadoopClassPathLen);
+        snprintf(optHadoopClassPath, optHadoopClassPathLen,
+                "%s%s", hadoopClassPathVMArg, hadoopClassPath);
+
+        // Determine the # of LIBHDFS_OPTS args
+        int noArgs = 1;
+        char *hadoopJvmArgs = getenv("LIBHDFS_OPTS");
+        char jvmArgDelims[] = " ";
+        char *str, *token, *savePtr;
+        if (hadoopJvmArgs != NULL)  {
+          hadoopJvmArgs = strdup(hadoopJvmArgs);
+          for (noArgs = 1, str = hadoopJvmArgs; ; noArgs++, str = NULL) {
+            token = strtok_r(str, jvmArgDelims, &savePtr);
+            if (NULL == token) {
+              break;
+            }
+          }
+          free(hadoopJvmArgs);
+        }
+
+        // Now that we know the # args, populate the options array
+        JavaVMOption options[noArgs];
+        options[0].optionString = optHadoopClassPath;
+        hadoopJvmArgs = getenv("LIBHDFS_OPTS");
+	if (hadoopJvmArgs != NULL)  {
+          hadoopJvmArgs = strdup(hadoopJvmArgs);
+          for (noArgs = 1, str = hadoopJvmArgs; ; noArgs++, str = NULL) {
+            token = strtok_r(str, jvmArgDelims, &savePtr);
+            if (NULL == token) {
+              break;
+            }
+            options[noArgs].optionString = token;
+          }
+        }
+
+        //Create the VM
+        JavaVMInitArgs vm_args;
+        JavaVM *vm;
+        vm_args.version = JNI_VERSION_1_2;
+        vm_args.options = options;
+        vm_args.nOptions = noArgs; 
+        vm_args.ignoreUnrecognized = 1;
+
+        rv = JNI_CreateJavaVM(&vm, (void*)&env, &vm_args);
+
+        if (hadoopJvmArgs != NULL)  {
+          free(hadoopJvmArgs);
+        }
+        free(optHadoopClassPath);
+
+        if (rv != 0) {
+            fprintf(stderr, "Call to JNI_CreateJavaVM failed "
+                    "with error: %d\n", rv);
+            return NULL;
+        }
+    }
+    else {
+        //Attach this thread to the VM
+        JavaVM* vm = vmBuf[0];
+        rv = (*vm)->AttachCurrentThread(vm, (void*)&env, 0);
+        if (rv != 0) {
+            fprintf(stderr, "Call to AttachCurrentThread "
+                    "failed with error: %d\n", rv);
+            return NULL;
+        }
+    }
+
+    return env;
+}
+
+/**
+ * getJNIEnv: A helper function to get the JNIEnv* for the given thread.
+ * If no JVM exists, then one will be created. JVM command line arguments
+ * are obtained from the LIBHDFS_OPTS environment variable.
+ *
+ * Implementation note: we rely on POSIX thread-local storage (tls).
+ * This allows us to associate a destructor function with each thread, that
+ * will detach the thread from the Java VM when the thread terminates.  If we
+ * failt to do this, it will cause a memory leak.
+ *
+ * However, POSIX TLS is not the most efficient way to do things.  It requires a
+ * key to be initialized before it can be used.  Since we don't know if this key
+ * is initialized at the start of this function, we have to lock a mutex first
+ * and check.  Luckily, most operating systems support the more efficient
+ * __thread construct, which is initialized by the linker.
+ *
+ * @param: None.
+ * @return The JNIEnv* corresponding to the thread.
+ */
+JNIEnv* getJNIEnv(void)
+{
+    JNIEnv *env;
+    struct hdfsTls *tls;
+    int ret;
+
+#ifdef HAVE_BETTER_TLS
+    static __thread struct hdfsTls *quickTls = NULL;
+    if (quickTls)
+        return quickTls->env;
+#endif
+    pthread_mutex_lock(&jvmMutex);
+    if (!gTlsKeyInitialized) {
+        ret = pthread_key_create(&gTlsKey, hdfsThreadDestructor);
+        if (ret) {
+            pthread_mutex_unlock(&jvmMutex);
+            fprintf("pthread_key_create failed with error %d\n", ret);
+            return NULL;
+        }
+        gTlsKeyInitialized = 1;
+    }
+    tls = pthread_getspecific(gTlsKey);
+    if (tls) {
+        pthread_mutex_unlock(&jvmMutex);
+        return tls->env;
+    }
+
+    env = getGlobalJNIEnv();
+    pthread_mutex_unlock(&jvmMutex);
+    if (!env) {
+        fprintf(stderr, "getJNIEnv: getGlobalJNIEnv failed\n");
+        return NULL;
+    }
+    tls = calloc(1, sizeof(struct hdfsTls));
+    if (!tls) {
+        fprintf(stderr, "getJNIEnv: OOM allocating %d bytes\n",
+                sizeof(struct hdfsTls));
+        return NULL;
+    }
+    tls->env = env;
+    ret = pthread_setspecific(gTlsKey, tls);
+    if (ret) {
+        fprintf(stderr, "getJNIEnv: pthread_setspecific failed with "
+            "error code %d\n", ret);
+        hdfsThreadDestructor(tls);
+        return NULL;
+    }
+#ifdef HAVE_BETTER_TLS
+    quickTls = tls;
+#endif
+    return env;
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.h
new file mode 100644
index 0000000..442eedf
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.h
@@ -0,0 +1,109 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef LIBHDFS_JNI_HELPER_H
+#define LIBHDFS_JNI_HELPER_H
+
+#include <jni.h>
+#include <stdio.h>
+
+#include <stdlib.h>
+#include <stdarg.h>
+#include <search.h>
+#include <pthread.h>
+#include <errno.h>
+
+#define PATH_SEPARATOR ':'
+
+
+/** Denote the method we want to invoke as STATIC or INSTANCE */
+typedef enum {
+    STATIC,
+    INSTANCE
+} MethType;
+
+
+/** Used for returning an appropriate return value after invoking
+ * a method
+ */
+typedef jvalue RetVal;
+
+/** Used for returning the exception after invoking a method */
+typedef jthrowable Exc;
+
+/** invokeMethod: Invoke a Static or Instance method.
+ * className: Name of the class where the method can be found
+ * methName: Name of the method
+ * methSignature: the signature of the method "(arg-types)ret-type"
+ * methType: The type of the method (STATIC or INSTANCE)
+ * instObj: Required if the methType is INSTANCE. The object to invoke
+   the method on.
+ * env: The JNIEnv pointer
+ * retval: The pointer to a union type which will contain the result of the
+   method invocation, e.g. if the method returns an Object, retval will be
+   set to that, if the method returns boolean, retval will be set to the
+   value (JNI_TRUE or JNI_FALSE), etc.
+ * exc: If the methods throws any exception, this will contain the reference
+ * Arguments (the method arguments) must be passed after methSignature
+ * RETURNS: -1 on error and 0 on success. If -1 is returned, exc will have 
+   a valid exception reference, and the result stored at retval is undefined.
+ */
+int invokeMethod(JNIEnv *env, RetVal *retval, Exc *exc, MethType methType,
+                 jobject instObj, const char *className, const char *methName, 
+                 const char *methSignature, ...);
+
+/** constructNewObjectOfClass: Invoke a constructor.
+ * className: Name of the class
+ * ctorSignature: the signature of the constructor "(arg-types)V"
+ * env: The JNIEnv pointer
+ * exc: If the ctor throws any exception, this will contain the reference
+ * Arguments to the ctor must be passed after ctorSignature 
+ */
+jobject constructNewObjectOfClass(JNIEnv *env, Exc *exc, const char *className, 
+                                  const char *ctorSignature, ...);
+
+jmethodID methodIdFromClass(const char *className, const char *methName, 
+                            const char *methSignature, MethType methType, 
+                            JNIEnv *env);
+
+jclass globalClassReference(const char *className, JNIEnv *env);
+
+/** classNameOfObject: Get an object's class name.
+ * @param jobj: The object.
+ * @param env: The JNIEnv pointer.
+ * @return Returns a pointer to a string containing the class name. This string
+ * must be freed by the caller.
+ */
+char *classNameOfObject(jobject jobj, JNIEnv *env);
+
+/** getJNIEnv: A helper function to get the JNIEnv* for the given thread.
+ * If no JVM exists, then one will be created. JVM command line arguments
+ * are obtained from the LIBHDFS_OPTS environment variable.
+ * @param: None.
+ * @return The JNIEnv* corresponding to the thread.
+ * */
+JNIEnv* getJNIEnv(void);
+
+jarray constructNewArrayString(JNIEnv *env, Exc *exc, const char **elements, int size) ;
+
+#endif /*LIBHDFS_JNI_HELPER_H*/
+
+/**
+ * vim: ts=4: sw=4: et:
+ */
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.c
new file mode 100644
index 0000000..7ffa148
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.c
@@ -0,0 +1,165 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "jni_helper.h"
+#include "native_mini_dfs.h"
+
+#include <errno.h>
+#include <jni.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+#define MINIDFS_CLUSTER_BUILDER "org/apache/hadoop/hdfs/MiniDFSCluster$Builder"
+#define MINIDFS_CLUSTER "org/apache/hadoop/hdfs/MiniDFSCluster"
+#define HADOOP_CONF     "org/apache/hadoop/conf/Configuration"
+
+struct NativeMiniDfsCluster {
+    /**
+     * The NativeMiniDfsCluster object
+     */
+    jobject obj;
+};
+
+struct NativeMiniDfsCluster* nmdCreate(struct NativeMiniDfsConf *conf)
+{
+    struct NativeMiniDfsCluster* cl = NULL;
+    jobject bld = NULL, bld2 = NULL, cobj = NULL;
+    jvalue  val;
+    JNIEnv *env = getJNIEnv();
+    if (!env) {
+        fprintf(stderr, "nmdCreate: unable to construct JNIEnv.\n");
+        goto error;
+    }
+    cl = calloc(1, sizeof(struct NativeMiniDfsCluster));
+    if (!cl) {
+        fprintf(stderr, "nmdCreate: OOM");
+        goto error;
+    }
+    cobj = constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
+    if (!cobj) {
+        fprintf(stderr, "nmdCreate: unable to construct Configuration\n");
+        goto error_free_cl;
+    }
+    bld = constructNewObjectOfClass(env, NULL, MINIDFS_CLUSTER_BUILDER,
+                    "(L"HADOOP_CONF";)V", cobj);
+    if (!bld) {
+        fprintf(stderr, "nmdCreate: unable to construct "
+                "NativeMiniDfsCluster#Builder\n");
+        goto error_dlr_cobj;
+    }
+    if (invokeMethod(env, &val, NULL, INSTANCE, bld,
+            MINIDFS_CLUSTER_BUILDER, "format",
+            "(Z)L" MINIDFS_CLUSTER_BUILDER ";", conf->doFormat)) {
+        fprintf(stderr, "nmdCreate: failed to call Builder#doFormat\n");
+        goto error_dlr_bld;
+    }
+    bld2 = val.l;
+    if (invokeMethod(env, &val, NULL, INSTANCE, bld,
+            MINIDFS_CLUSTER_BUILDER, "build",
+            "()L" MINIDFS_CLUSTER ";")) {
+        fprintf(stderr, "nmdCreate: failed to call Builder#build\n");
+        goto error_dlr_bld2;
+    }
+	cl->obj = (*env)->NewGlobalRef(env, val.l);
+    if (!cl->obj) {
+        fprintf(stderr, "nmdCreate: failed to create global reference to "
+            "MiniDFSCluster\n");
+        goto error_dlr_val;
+    }
+    (*env)->DeleteLocalRef(env, val.l);
+    (*env)->DeleteLocalRef(env, bld2);
+    (*env)->DeleteLocalRef(env, bld);
+    (*env)->DeleteLocalRef(env, cobj);
+    return cl;
+
+error_dlr_val:
+    (*env)->DeleteLocalRef(env, val.l);
+error_dlr_bld2:
+    (*env)->DeleteLocalRef(env, bld2);
+error_dlr_bld:
+    (*env)->DeleteLocalRef(env, bld);
+error_dlr_cobj:
+    (*env)->DeleteLocalRef(env, cobj);
+error_free_cl:
+    free(cl);
+error:
+    return NULL;
+}
+
+void nmdFree(struct NativeMiniDfsCluster* cl)
+{
+    JNIEnv *env = getJNIEnv();
+    if (!env) {
+        fprintf(stderr, "nmdFree: getJNIEnv failed\n");
+        free(cl);
+        return;
+    }
+    (*env)->DeleteGlobalRef(env, cl->obj);
+    free(cl);
+}
+
+int nmdShutdown(struct NativeMiniDfsCluster* cl)
+{
+    JNIEnv *env = getJNIEnv();
+    if (!env) {
+        fprintf(stderr, "nmdShutdown: getJNIEnv failed\n");
+        return -EIO;
+    }
+    if (invokeMethod(env, NULL, NULL, INSTANCE, cl->obj,
+            MINIDFS_CLUSTER, "shutdown", "()V")) {
+        fprintf(stderr, "nmdShutdown: MiniDFSCluster#shutdown failure\n");
+        return -EIO;
+    }
+    return 0;
+}
+
+int nmdWaitClusterUp(struct NativeMiniDfsCluster *cl)
+{
+    JNIEnv *env = getJNIEnv();
+    if (!env) {
+        fprintf(stderr, "nmdWaitClusterUp: getJNIEnv failed\n");
+        return -EIO;
+    }
+    if (invokeMethod(env, NULL, NULL, INSTANCE, cl->obj,
+            MINIDFS_CLUSTER, "waitClusterUp", "()V")) {
+        fprintf(stderr, "nmdWaitClusterUp: MiniDFSCluster#waitClusterUp "
+                "failure\n");
+        return -EIO;
+    }
+    return 0;
+}
+
+int nmdGetNameNodePort(struct NativeMiniDfsCluster *cl)
+{
+    JNIEnv *env = getJNIEnv();
+    jvalue jVal;
+
+    if (!env) {
+        fprintf(stderr, "nmdHdfsConnect: getJNIEnv failed\n");
+        return -EIO;
+    }
+    // Note: this will have to be updated when HA nativeMiniDfs clusters are
+    // supported
+    if (invokeMethod(env, &jVal, NULL, INSTANCE, cl->obj,
+            MINIDFS_CLUSTER, "getNameNodePort", "()I")) {
+        fprintf(stderr, "nmdHdfsConnect: MiniDFSCluster#getNameNodePort "
+                "failure\n");
+        return -EIO;
+    }
+    return jVal.i;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.h
new file mode 100644
index 0000000..88a4b47
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.h
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef LIBHDFS_NATIVE_MINI_DFS_H
+#define LIBHDFS_NATIVE_MINI_DFS_H
+
+#include <jni.h> /* for jboolean */
+
+struct NativeMiniDfsCluster; 
+
+/**
+ * Represents a configuration to use for creating a Native MiniDFSCluster
+ */
+struct NativeMiniDfsConf {
+    /**
+     * Nonzero if the cluster should be formatted prior to startup
+     */
+    jboolean doFormat;
+};
+
+/**
+ * Create a NativeMiniDfsBuilder
+ *
+ * @param conf      (inout) The cluster configuration
+ *
+ * @return      a NativeMiniDfsBuilder, or a NULL pointer on error.
+ */
+struct NativeMiniDfsCluster* nmdCreate(struct NativeMiniDfsConf *conf);
+
+/**
+ * Wait until a MiniDFSCluster comes out of safe mode.
+ *
+ * @param cl        The cluster
+ *
+ * @return          0 on success; a non-zero error code if the cluster fails to
+ *                  come out of safe mode.
+ */
+int nmdWaitClusterUp(struct NativeMiniDfsCluster *cl);
+
+/**
+ * Shut down a NativeMiniDFS cluster
+ *
+ * @param cl        The cluster
+ *
+ * @return          0 on success; a non-zero error code if an exception is
+ *                  thrown.
+ */
+int nmdShutdown(struct NativeMiniDfsCluster *cl);
+
+/**
+ * Destroy a Native MiniDFSCluster
+ *
+ * @param cl        The cluster to destroy
+ */
+void nmdFree(struct NativeMiniDfsCluster* cl);
+
+/**
+ * Get the port that's in use by the given (non-HA) nativeMiniDfs
+ *
+ * @param cl        The initialized NativeMiniDfsCluster
+ *
+ * @return          the port, or a negative error code
+ */
+int nmdGetNameNodePort(struct NativeMiniDfsCluster *cl);
+
+#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_ops.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_ops.c
new file mode 100644
index 0000000..c2a0cbd
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_ops.c
@@ -0,0 +1,525 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hdfs.h" 
+#include "hdfs_test.h" 
+
+#include <inttypes.h>
+#include <jni.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <time.h>
+#include <unistd.h>
+
+void permission_disp(short permissions, char *rtr) {
+  rtr[9] = '\0';
+  int i;
+  for(i=2;i>=0;i--)
+    {
+      short permissionsId = permissions >> (i * 3) & (short)7;
+      char* perm;
+      switch(permissionsId) {
+      case 7:
+        perm = "rwx"; break;
+      case 6:
+        perm = "rw-"; break;
+      case 5:
+        perm = "r-x"; break;
+      case 4:
+        perm = "r--"; break;
+      case 3:
+        perm = "-wx"; break;
+      case 2:
+        perm = "-w-"; break;
+      case 1:
+        perm = "--x"; break;
+      case 0:
+        perm = "---"; break;
+      default:
+        perm = "???";
+      }
+      strncpy(rtr, perm, 3);
+      rtr+=3;
+    }
+} 
+
+int main(int argc, char **argv) {
+    char buffer[32];
+    tSize num_written_bytes;
+
+    hdfsFS fs = hdfsConnectNewInstance("default", 0);
+    if(!fs) {
+        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
+        exit(-1);
+    } 
+ 
+    hdfsFS lfs = hdfsConnectNewInstance(NULL, 0);
+    if(!lfs) {
+        fprintf(stderr, "Oops! Failed to connect to 'local' hdfs!\n");
+        exit(-1);
+    } 
+
+    const char* writePath = "/tmp/testfile.txt";
+    const char* fileContents = "Hello, World!";
+
+    {
+        //Write tests
+        
+        hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
+        if(!writeFile) {
+            fprintf(stderr, "Failed to open %s for writing!\n", writePath);
+            exit(-1);
+        }
+        fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
+        num_written_bytes =
+          hdfsWrite(fs, writeFile, (void*)fileContents, strlen(fileContents)+1);
+        if (num_written_bytes != strlen(fileContents) + 1) {
+          fprintf(stderr, "Failed to write correct number of bytes - expected %d, got %d\n",
+                  (int)(strlen(fileContents) + 1), (int)num_written_bytes);
+            exit(-1);
+        }
+        fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
+
+        tOffset currentPos = -1;
+        if ((currentPos = hdfsTell(fs, writeFile)) == -1) {
+            fprintf(stderr, 
+                    "Failed to get current file position correctly! Got %ld!\n",
+                    currentPos);
+            exit(-1);
+        }
+        fprintf(stderr, "Current position: %ld\n", currentPos);
+
+        if (hdfsFlush(fs, writeFile)) {
+            fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
+            exit(-1);
+        }
+        fprintf(stderr, "Flushed %s successfully!\n", writePath); 
+
+        if (hdfsHFlush(fs, writeFile)) {
+            fprintf(stderr, "Failed to 'hflush' %s\n", writePath);
+            exit(-1);
+        }
+        fprintf(stderr, "HFlushed %s successfully!\n", writePath);
+
+        hdfsCloseFile(fs, writeFile);
+    }
+
+    {
+        //Read tests
+        
+        const char* readPath = "/tmp/testfile.txt";
+        int exists = hdfsExists(fs, readPath);
+
+        if (exists) {
+          fprintf(stderr, "Failed to validate existence of %s\n", readPath);
+          exit(-1);
+        }
+
+        hdfsFile readFile = hdfsOpenFile(fs, readPath, O_RDONLY, 0, 0, 0);
+        if (!readFile) {
+            fprintf(stderr, "Failed to open %s for reading!\n", readPath);
+            exit(-1);
+        }
+
+        if (!hdfsFileIsOpenForRead(readFile)) {
+            fprintf(stderr, "hdfsFileIsOpenForRead: we just opened a file "
+                    "with O_RDONLY, and it did not show up as 'open for "
+                    "read'\n");
+            exit(-1);
+        }
+
+        fprintf(stderr, "hdfsAvailable: %d\n", hdfsAvailable(fs, readFile));
+
+        tOffset seekPos = 1;
+        if(hdfsSeek(fs, readFile, seekPos)) {
+            fprintf(stderr, "Failed to seek %s for reading!\n", readPath);
+            exit(-1);
+        }
+
+        tOffset currentPos = -1;
+        if((currentPos = hdfsTell(fs, readFile)) != seekPos) {
+            fprintf(stderr, 
+                    "Failed to get current file position correctly! Got %ld!\n", 
+                    currentPos);
+            exit(-1);
+        }
+        fprintf(stderr, "Current position: %ld\n", currentPos);
+
+        if (!hdfsFileUsesDirectRead(readFile)) {
+          fprintf(stderr, "Direct read support incorrectly not detected "
+                  "for HDFS filesystem\n");
+          exit(-1);
+        }
+
+        fprintf(stderr, "Direct read support detected for HDFS\n");
+
+        // Test the direct read path
+        if(hdfsSeek(fs, readFile, 0)) {
+            fprintf(stderr, "Failed to seek %s for reading!\n", readPath);
+            exit(-1);
+        }
+        memset(buffer, 0, sizeof(buffer));
+        tSize num_read_bytes = hdfsRead(fs, readFile, (void*)buffer,
+                sizeof(buffer));
+        if (strncmp(fileContents, buffer, strlen(fileContents)) != 0) {
+            fprintf(stderr, "Failed to read (direct). Expected %s but got %s (%d bytes)\n",
+                    fileContents, buffer, num_read_bytes);
+            exit(-1);
+        }
+        fprintf(stderr, "Read (direct) following %d bytes:\n%s\n",
+                num_read_bytes, buffer);
+        if (hdfsSeek(fs, readFile, 0L)) {
+            fprintf(stderr, "Failed to seek to file start!\n");
+            exit(-1);
+        }
+
+        // Disable the direct read path so that we really go through the slow
+        // read path
+        hdfsFileDisableDirectRead(readFile);
+
+        num_read_bytes = hdfsRead(fs, readFile, (void*)buffer, 
+                sizeof(buffer));
+        fprintf(stderr, "Read following %d bytes:\n%s\n", 
+                num_read_bytes, buffer);
+
+        memset(buffer, 0, strlen(fileContents + 1));
+
+        num_read_bytes = hdfsPread(fs, readFile, 0, (void*)buffer, 
+                sizeof(buffer));
+        fprintf(stderr, "Read following %d bytes:\n%s\n", 
+                num_read_bytes, buffer);
+
+        hdfsCloseFile(fs, readFile);
+
+        // Test correct behaviour for unsupported filesystems
+        hdfsFile localFile = hdfsOpenFile(lfs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
+        if(!localFile) {
+            fprintf(stderr, "Failed to open %s for writing!\n", writePath);
+            exit(-1);
+        }
+
+        num_written_bytes = hdfsWrite(lfs, localFile, (void*)fileContents,
+                                      strlen(fileContents) + 1);
+
+        hdfsCloseFile(lfs, localFile);
+        localFile = hdfsOpenFile(lfs, writePath, O_RDONLY, 0, 0, 0);
+
+        if (hdfsFileUsesDirectRead(localFile)) {
+          fprintf(stderr, "Direct read support incorrectly detected for local "
+                  "filesystem\n");
+          exit(-1);
+        }
+
+        hdfsCloseFile(lfs, localFile);
+    }
+
+    int totalResult = 0;
+    int result = 0;
+    {
+        //Generic file-system operations
+
+        const char* srcPath = "/tmp/testfile.txt";
+        const char* dstPath = "/tmp/testfile2.txt";
+
+        fprintf(stderr, "hdfsCopy(remote-local): %s\n", ((result = hdfsCopy(fs, srcPath, lfs, srcPath)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsCopy(remote-remote): %s\n", ((result = hdfsCopy(fs, srcPath, fs, dstPath)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsMove(local-local): %s\n", ((result = hdfsMove(lfs, srcPath, lfs, dstPath)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsMove(remote-local): %s\n", ((result = hdfsMove(fs, srcPath, lfs, srcPath)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        fprintf(stderr, "hdfsRename: %s\n", ((result = hdfsRename(fs, dstPath, srcPath)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsCopy(remote-remote): %s\n", ((result = hdfsCopy(fs, srcPath, fs, dstPath)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        const char* slashTmp = "/tmp";
+        const char* newDirectory = "/tmp/newdir";
+        fprintf(stderr, "hdfsCreateDirectory: %s\n", ((result = hdfsCreateDirectory(fs, newDirectory)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        fprintf(stderr, "hdfsSetReplication: %s\n", ((result = hdfsSetReplication(fs, srcPath, 2)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        char buffer[256];
+        const char *resp;
+        fprintf(stderr, "hdfsGetWorkingDirectory: %s\n", ((resp = hdfsGetWorkingDirectory(fs, buffer, sizeof(buffer))) ? buffer : "Failed!"));
+        totalResult += (resp ? 0 : 1);
+        fprintf(stderr, "hdfsSetWorkingDirectory: %s\n", ((result = hdfsSetWorkingDirectory(fs, slashTmp)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsGetWorkingDirectory: %s\n", ((resp = hdfsGetWorkingDirectory(fs, buffer, sizeof(buffer))) ? buffer : "Failed!"));
+        totalResult += (resp ? 0 : 1);
+
+        fprintf(stderr, "hdfsGetDefaultBlockSize: %ld\n", hdfsGetDefaultBlockSize(fs));
+        fprintf(stderr, "hdfsGetCapacity: %ld\n", hdfsGetCapacity(fs));
+        fprintf(stderr, "hdfsGetUsed: %ld\n", hdfsGetUsed(fs));
+
+        hdfsFileInfo *fileInfo = NULL;
+        if((fileInfo = hdfsGetPathInfo(fs, slashTmp)) != NULL) {
+            fprintf(stderr, "hdfsGetPathInfo - SUCCESS!\n");
+            fprintf(stderr, "Name: %s, ", fileInfo->mName);
+            fprintf(stderr, "Type: %c, ", (char)(fileInfo->mKind));
+            fprintf(stderr, "Replication: %d, ", fileInfo->mReplication);
+            fprintf(stderr, "BlockSize: %ld, ", fileInfo->mBlockSize);
+            fprintf(stderr, "Size: %ld, ", fileInfo->mSize);
+            fprintf(stderr, "LastMod: %s", ctime(&fileInfo->mLastMod)); 
+            fprintf(stderr, "Owner: %s, ", fileInfo->mOwner);
+            fprintf(stderr, "Group: %s, ", fileInfo->mGroup);
+            char permissions[10];
+            permission_disp(fileInfo->mPermissions, permissions);
+            fprintf(stderr, "Permissions: %d (%s)\n", fileInfo->mPermissions, permissions);
+            hdfsFreeFileInfo(fileInfo, 1);
+        } else {
+            totalResult++;
+            fprintf(stderr, "waah! hdfsGetPathInfo for %s - FAILED!\n", slashTmp);
+        }
+
+        hdfsFileInfo *fileList = 0;
+        int numEntries = 0;
+        if((fileList = hdfsListDirectory(fs, slashTmp, &numEntries)) != NULL) {
+            int i = 0;
+            for(i=0; i < numEntries; ++i) {
+                fprintf(stderr, "Name: %s, ", fileList[i].mName);
+                fprintf(stderr, "Type: %c, ", (char)fileList[i].mKind);
+                fprintf(stderr, "Replication: %d, ", fileList[i].mReplication);
+                fprintf(stderr, "BlockSize: %ld, ", fileList[i].mBlockSize);
+                fprintf(stderr, "Size: %ld, ", fileList[i].mSize);
+                fprintf(stderr, "LastMod: %s", ctime(&fileList[i].mLastMod));
+                fprintf(stderr, "Owner: %s, ", fileList[i].mOwner);
+                fprintf(stderr, "Group: %s, ", fileList[i].mGroup);
+                char permissions[10];
+                permission_disp(fileList[i].mPermissions, permissions);
+                fprintf(stderr, "Permissions: %d (%s)\n", fileList[i].mPermissions, permissions);
+            }
+            hdfsFreeFileInfo(fileList, numEntries);
+        } else {
+            if (errno) {
+                totalResult++;
+                fprintf(stderr, "waah! hdfsListDirectory - FAILED!\n");
+            } else {
+                fprintf(stderr, "Empty directory!\n");
+            }
+        }
+
+        char*** hosts = hdfsGetHosts(fs, srcPath, 0, 1);
+        if(hosts) {
+            fprintf(stderr, "hdfsGetHosts - SUCCESS! ... \n");
+            int i=0; 
+            while(hosts[i]) {
+                int j = 0;
+                while(hosts[i][j]) {
+                    fprintf(stderr, 
+                            "\thosts[%d][%d] - %s\n", i, j, hosts[i][j]);
+                    ++j;
+                }
+                ++i;
+            }
+        } else {
+            totalResult++;
+            fprintf(stderr, "waah! hdfsGetHosts - FAILED!\n");
+        }
+       
+        char *newOwner = "root";
+        // setting tmp dir to 777 so later when connectAsUser nobody, we can write to it
+        short newPerm = 0666;
+
+        // chown write
+        fprintf(stderr, "hdfsChown: %s\n", ((result = hdfsChown(fs, writePath, NULL, "users")) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsChown: %s\n", ((result = hdfsChown(fs, writePath, newOwner, NULL)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        // chmod write
+        fprintf(stderr, "hdfsChmod: %s\n", ((result = hdfsChmod(fs, writePath, newPerm)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+
+
+        sleep(2);
+        tTime newMtime = time(NULL);
+        tTime newAtime = time(NULL);
+
+        // utime write
+        fprintf(stderr, "hdfsUtime: %s\n", ((result = hdfsUtime(fs, writePath, newMtime, newAtime)) ? "Failed!" : "Success!"));
+
+        totalResult += result;
+
+        // chown/chmod/utime read
+        hdfsFileInfo *finfo = hdfsGetPathInfo(fs, writePath);
+
+        fprintf(stderr, "hdfsChown read: %s\n", ((result = (strcmp(finfo->mOwner, newOwner) != 0)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        fprintf(stderr, "hdfsChmod read: %s\n", ((result = (finfo->mPermissions != newPerm)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        // will later use /tmp/ as a different user so enable it
+        fprintf(stderr, "hdfsChmod: %s\n", ((result = hdfsChmod(fs, "/tmp/", 0777)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        fprintf(stderr,"newMTime=%ld\n",newMtime);
+        fprintf(stderr,"curMTime=%ld\n",finfo->mLastMod);
+
+
+        fprintf(stderr, "hdfsUtime read (mtime): %s\n", ((result = (finfo->mLastMod != newMtime)) ? "Failed!" : "Success!"));
+        totalResult += result;
+
+        // No easy way to turn on access times from hdfs_test right now
+        //        fprintf(stderr, "hdfsUtime read (atime): %s\n", ((result = (finfo->mLastAccess != newAtime)) ? "Failed!" : "Success!"));
+        //        totalResult += result;
+
+        hdfsFreeFileInfo(finfo, 1);
+
+        // Clean up
+        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(fs, newDirectory, 1)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(fs, srcPath, 1)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(lfs, srcPath, 1)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsDelete: %s\n", ((result = hdfsDelete(lfs, dstPath, 1)) ? "Failed!" : "Success!"));
+        totalResult += result;
+        fprintf(stderr, "hdfsExists: %s\n", ((result = hdfsExists(fs, newDirectory)) ? "Success!" : "Failed!"));
+        totalResult += (result ? 0 : 1);
+    }
+
+    {
+      // TEST APPENDS
+      const char *writePath = "/tmp/appends";
+
+      // CREATE
+      hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY, 0, 0, 0);
+      if(!writeFile) {
+        fprintf(stderr, "Failed to open %s for writing!\n", writePath);
+        exit(-1);
+      }
+      fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
+
+      char* buffer = "Hello,";
+      tSize num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer));
+      fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
+
+      if (hdfsFlush(fs, writeFile)) {
+        fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
+        exit(-1);
+        }
+      fprintf(stderr, "Flushed %s successfully!\n", writePath); 
+
+      hdfsCloseFile(fs, writeFile);
+
+      // RE-OPEN
+      writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_APPEND, 0, 0, 0);
+      if(!writeFile) {
+        fprintf(stderr, "Failed to open %s for writing!\n", writePath);
+        exit(-1);
+      }
+      fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
+
+      buffer = " World";
+      num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer) + 1);
+      fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
+
+      if (hdfsFlush(fs, writeFile)) {
+        fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
+        exit(-1);
+      }
+      fprintf(stderr, "Flushed %s successfully!\n", writePath); 
+
+      hdfsCloseFile(fs, writeFile);
+
+      // CHECK size
+      hdfsFileInfo *finfo = hdfsGetPathInfo(fs, writePath);
+      fprintf(stderr, "fileinfo->mSize: == total %s\n", ((result = (finfo->mSize == strlen("Hello, World") + 1)) ? "Success!" : "Failed!"));
+      totalResult += (result ? 0 : 1);
+
+      // READ and check data
+      hdfsFile readFile = hdfsOpenFile(fs, writePath, O_RDONLY, 0, 0, 0);
+      if (!readFile) {
+        fprintf(stderr, "Failed to open %s for reading!\n", writePath);
+        exit(-1);
+      }
+
+      char rdbuffer[32];
+      tSize num_read_bytes = hdfsRead(fs, readFile, (void*)rdbuffer, sizeof(rdbuffer));
+      fprintf(stderr, "Read following %d bytes:\n%s\n", 
+              num_read_bytes, rdbuffer);
+
+      fprintf(stderr, "read == Hello, World %s\n", (result = (strcmp(rdbuffer, "Hello, World") == 0)) ? "Success!" : "Failed!");
+
+      hdfsCloseFile(fs, readFile);
+
+      // DONE test appends
+    }
+      
+      
+    totalResult += (hdfsDisconnect(fs) != 0);
+
+    {
+      //
+      // Now test as connecting as a specific user
+      // This is only meant to test that we connected as that user, not to test
+      // the actual fs user capabilities. Thus just create a file and read
+      // the owner is correct.
+
+      const char *tuser = "nobody";
+      const char* writePath = "/tmp/usertestfile.txt";
+
+      fs = hdfsConnectAsUserNewInstance("default", 0, tuser);
+      if(!fs) {
+        fprintf(stderr, "Oops! Failed to connect to hdfs as user %s!\n",tuser);
+        exit(-1);
+      } 
+
+        hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
+        if(!writeFile) {
+            fprintf(stderr, "Failed to open %s for writing!\n", writePath);
+            exit(-1);
+        }
+        fprintf(stderr, "Opened %s for writing successfully...\n", writePath);
+
+        char* buffer = "Hello, World!";
+        tSize num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer)+1);
+        fprintf(stderr, "Wrote %d bytes\n", num_written_bytes);
+
+        if (hdfsFlush(fs, writeFile)) {
+            fprintf(stderr, "Failed to 'flush' %s\n", writePath); 
+            exit(-1);
+        }
+        fprintf(stderr, "Flushed %s successfully!\n", writePath); 
+
+        hdfsCloseFile(fs, writeFile);
+
+        hdfsFileInfo *finfo = hdfsGetPathInfo(fs, writePath);
+        fprintf(stderr, "hdfs new file user is correct: %s\n", ((result = (strcmp(finfo->mOwner, tuser) != 0)) ? "Failed!" : "Success!"));
+        totalResult += result;
+    }
+    
+    totalResult += (hdfsDisconnect(fs) != 0);
+
+    if (totalResult != 0) {
+        return -1;
+    } else {
+        return 0;
+    }
+}
+
+/**
+ * vim: ts=4: sw=4: et:
+ */
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_read.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_read.c
new file mode 100644
index 0000000..423f703
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_read.c
@@ -0,0 +1,70 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hdfs.h" 
+
+#include <stdio.h>
+#include <stdlib.h>
+
+int main(int argc, char **argv) {
+
+    if (argc != 4) {
+        fprintf(stderr, "Usage: hdfs_read <filename> <filesize> <buffersize>\n");
+        exit(-1);
+    }
+    
+    hdfsFS fs = hdfsConnect("default", 0);
+    if (!fs) {
+        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
+        exit(-1);
+    } 
+ 
+    const char* rfile = argv[1];
+    tSize fileTotalSize = strtoul(argv[2], NULL, 10);
+    tSize bufferSize = strtoul(argv[3], NULL, 10);
+   
+    hdfsFile readFile = hdfsOpenFile(fs, rfile, O_RDONLY, bufferSize, 0, 0);
+    if (!readFile) {
+        fprintf(stderr, "Failed to open %s for writing!\n", rfile);
+        exit(-2);
+    }
+
+    // data to be written to the file
+    char* buffer = malloc(sizeof(char) * bufferSize);
+    if(buffer == NULL) {
+        return -2;
+    }
+    
+    // read from the file
+    tSize curSize = bufferSize;
+    for (; curSize == bufferSize;) {
+        curSize = hdfsRead(fs, readFile, (void*)buffer, curSize);
+    }
+    
+
+    free(buffer);
+    hdfsCloseFile(fs, readFile);
+    hdfsDisconnect(fs);
+
+    return 0;
+}
+
+/**
+ * vim: ts=4: sw=4: et:
+ */
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_write.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_write.c
new file mode 100644
index 0000000..b0f320c
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/test_libhdfs_write.c
@@ -0,0 +1,94 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hdfs.h" 
+
+#include <limits.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+int main(int argc, char **argv) {
+
+    if (argc != 4) {
+        fprintf(stderr, "Usage: hdfs_write <filename> <filesize> <buffersize>\n");
+        exit(-1);
+    }
+    
+    hdfsFS fs = hdfsConnect("default", 0);
+    if (!fs) {
+        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
+        exit(-1);
+    } 
+ 
+    const char* writeFileName = argv[1];
+    off_t fileTotalSize = strtoul(argv[2], NULL, 10);
+    long long tmpBufferSize = strtoul(argv[3], NULL, 10);
+
+    // sanity check
+    if(fileTotalSize == ULONG_MAX && errno == ERANGE) {
+      fprintf(stderr, "invalid file size %s - must be <= %lu\n", argv[2], ULONG_MAX);
+      exit(-3);
+    }
+
+    // currently libhdfs writes are of tSize which is int32
+    if(tmpBufferSize > INT_MAX) {
+      fprintf(stderr, "invalid buffer size libhdfs API write chunks must be <= %d\n",INT_MAX);
+      exit(-3);
+    }
+
+    tSize bufferSize = tmpBufferSize;
+
+    hdfsFile writeFile = hdfsOpenFile(fs, writeFileName, O_WRONLY, bufferSize, 0, 0);
+    if (!writeFile) {
+        fprintf(stderr, "Failed to open %s for writing!\n", writeFileName);
+        exit(-2);
+    }
+
+    // data to be written to the file
+    char* buffer = malloc(sizeof(char) * bufferSize);
+    if(buffer == NULL) {
+        fprintf(stderr, "Could not allocate buffer of size %d\n", bufferSize);
+        return -2;
+    }
+    int i = 0;
+    for (i=0; i < bufferSize; ++i) {
+        buffer[i] = 'a' + (i%26);
+    }
+
+    // write to the file
+    off_t nrRemaining;
+    for (nrRemaining = fileTotalSize; nrRemaining > 0; nrRemaining -= bufferSize ) {
+      tSize curSize = ( bufferSize < nrRemaining ) ? bufferSize : (tSize)nrRemaining; 
+      tSize written;
+      if ((written = hdfsWrite(fs, writeFile, (void*)buffer, curSize)) != curSize) {
+        fprintf(stderr, "ERROR: hdfsWrite returned an error on write: %d\n", written);
+        exit(-3);
+      }
+    }
+
+    free(buffer);
+    hdfsCloseFile(fs, writeFile);
+    hdfsDisconnect(fs);
+
+    return 0;
+}
+
+/**
+ * vim: ts=4: sw=4: et:
+ */
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c
new file mode 100644
index 0000000..5c40426
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c
@@ -0,0 +1,221 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "expect.h"
+#include "hdfs.h"
+#include "native_mini_dfs.h"
+
+#include <errno.h>
+#include <semaphore.h>
+#include <pthread.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+#define TLH_MAX_THREADS 100
+
+static sem_t tlhSem;
+
+static struct NativeMiniDfsCluster* tlhCluster;
+
+struct tlhThreadInfo {
+    /** Thread index */
+    int threadIdx;
+    /** 0 = thread was successful; error code otherwise */
+    int success;
+    /** pthread identifier */
+    pthread_t thread;
+};
+
+static int hdfsSingleNameNodeConnect(struct NativeMiniDfsCluster *cl, hdfsFS *fs)
+{
+    int ret, port;
+    hdfsFS hdfs;
+    
+    port = nmdGetNameNodePort(cl);
+    if (port < 0) {
+        fprintf(stderr, "hdfsSingleNameNodeConnect: nmdGetNameNodePort "
+                "returned error %d\n", port);
+        return port;
+    }
+    hdfs = hdfsConnectNewInstance("localhost", port);
+    if (!hdfs) {
+        ret = -errno;
+        return ret;
+    }
+    *fs = hdfs;
+    return 0;
+}
+
+static int doTestHdfsOperations(struct tlhThreadInfo *ti, hdfsFS fs)
+{
+    char prefix[256], tmp[256];
+    hdfsFile file;
+    int ret, expected;
+
+    snprintf(prefix, sizeof(prefix), "/tlhData%04d", ti->threadIdx);
+
+    if (hdfsExists(fs, prefix) == 0) {
+        EXPECT_ZERO(hdfsDelete(fs, prefix, 1));
+    }
+    EXPECT_ZERO(hdfsCreateDirectory(fs, prefix));
+    snprintf(tmp, sizeof(tmp), "%s/file", prefix);
+
+    /* There should not be any file to open for reading. */
+    EXPECT_NULL(hdfsOpenFile(fs, tmp, O_RDONLY, 0, 0, 0));
+
+    file = hdfsOpenFile(fs, tmp, O_WRONLY, 0, 0, 0);
+    EXPECT_NONNULL(file);
+
+    /* TODO: implement writeFully and use it here */
+    expected = strlen(prefix);
+    ret = hdfsWrite(fs, file, prefix, expected);
+    if (ret < 0) {
+        ret = errno;
+        fprintf(stderr, "hdfsWrite failed and set errno %d\n", ret);
+        return ret;
+    }
+    if (ret != expected) {
+        fprintf(stderr, "hdfsWrite was supposed to write %d bytes, but "
+                "it wrote %d\n", ret, expected);
+        return EIO;
+    }
+    EXPECT_ZERO(hdfsFlush(fs, file));
+    EXPECT_ZERO(hdfsCloseFile(fs, file));
+
+    /* Let's re-open the file for reading */
+    file = hdfsOpenFile(fs, tmp, O_RDONLY, 0, 0, 0);
+    EXPECT_NONNULL(file);
+
+    /* TODO: implement readFully and use it here */
+    ret = hdfsRead(fs, file, tmp, sizeof(tmp));
+    if (ret < 0) {
+        ret = errno;
+        fprintf(stderr, "hdfsRead failed and set errno %d\n", ret);
+        return ret;
+    }
+    if (ret != expected) {
+        fprintf(stderr, "hdfsRead was supposed to read %d bytes, but "
+                "it read %d\n", ret, expected);
+        return EIO;
+    }
+    EXPECT_ZERO(memcmp(prefix, tmp, expected));
+    EXPECT_ZERO(hdfsCloseFile(fs, file));
+
+    // TODO: Non-recursive delete should fail?
+    //EXPECT_NONZERO(hdfsDelete(fs, prefix, 0));
+
+    EXPECT_ZERO(hdfsDelete(fs, prefix, 1));
+    return 0;
+}
+
+static void *testHdfsOperations(void *v)
+{
+    struct tlhThreadInfo *ti = (struct tlhThreadInfo*)v;
+    hdfsFS fs = NULL;
+    int ret;
+
+    fprintf(stderr, "testHdfsOperations(threadIdx=%d): starting\n",
+        ti->threadIdx);
+    ret = hdfsSingleNameNodeConnect(tlhCluster, &fs);
+    if (ret) {
+        fprintf(stderr, "testHdfsOperations(threadIdx=%d): "
+            "hdfsSingleNameNodeConnect failed with error %d.\n",
+            ti->threadIdx, ret);
+        ti->success = EIO;
+        return NULL;
+    }
+    ti->success = doTestHdfsOperations(ti, fs);
+    if (hdfsDisconnect(fs)) {
+        ret = errno;
+        fprintf(stderr, "hdfsDisconnect error %d\n", ret);
+        ti->success = ret;
+    }
+    return NULL;
+}
+
+static int checkFailures(struct tlhThreadInfo *ti, int tlhNumThreads)
+{
+    int i, threadsFailed = 0;
+    const char *sep = "";
+
+    for (i = 0; i < tlhNumThreads; i++) {
+        if (ti[i].success != 0) {
+            threadsFailed = 1;
+        }
+    }
+    if (!threadsFailed) {
+        fprintf(stderr, "testLibHdfs: all threads succeeded.  SUCCESS.\n");
+        return EXIT_SUCCESS;
+    }
+    fprintf(stderr, "testLibHdfs: some threads failed: [");
+    for (i = 0; i < tlhNumThreads; i++) {
+        if (ti[i].success != 0) {
+            fprintf(stderr, "%s%d", sep, i);
+            sep = ", "; 
+        }
+    }
+    fprintf(stderr, "].  FAILURE.\n");
+    return EXIT_FAILURE;
+}
+
+/**
+ * Test that we can write a file with libhdfs and then read it back
+ */
+int main(void)
+{
+    int i, tlhNumThreads;
+    const char *tlhNumThreadsStr;
+    struct tlhThreadInfo ti[TLH_MAX_THREADS];
+    struct NativeMiniDfsConf conf = {
+        .doFormat = 1,
+    };
+
+    tlhNumThreadsStr = getenv("TLH_NUM_THREADS");
+    if (!tlhNumThreadsStr) {
+        tlhNumThreadsStr = "3";
+    }
+    tlhNumThreads = atoi(tlhNumThreadsStr);
+    if ((tlhNumThreads <= 0) || (tlhNumThreads > TLH_MAX_THREADS)) {
+        fprintf(stderr, "testLibHdfs: must have a number of threads "
+                "between 1 and %d inclusive, not %d\n",
+                TLH_MAX_THREADS, tlhNumThreads);
+        return EXIT_FAILURE;
+    }
+    memset(&ti[0], 0, sizeof(ti));
+    for (i = 0; i < tlhNumThreads; i++) {
+        ti[i].threadIdx = i;
+    }
+
+    EXPECT_ZERO(sem_init(&tlhSem, 0, tlhNumThreads));
+    tlhCluster = nmdCreate(&conf);
+    EXPECT_NONNULL(tlhCluster);
+    EXPECT_ZERO(nmdWaitClusterUp(tlhCluster));
+
+    for (i = 0; i < tlhNumThreads; i++) {
+        EXPECT_ZERO(pthread_create(&ti[i].thread, NULL,
+            testHdfsOperations, &ti[i]));
+    }
+    for (i = 0; i < tlhNumThreads; i++) {
+        EXPECT_ZERO(pthread_join(ti[i].thread, NULL));
+    }
+
+    EXPECT_ZERO(nmdShutdown(tlhCluster));
+    nmdFree(tlhCluster);
+    EXPECT_ZERO(sem_destroy(&tlhSem));
+    return checkFailures(ti, tlhNumThreads);
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_native_mini_dfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_native_mini_dfs.c
new file mode 100644
index 0000000..b97ef95
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_native_mini_dfs.c
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "expect.h"
+#include "native_mini_dfs.h"
+
+#include <errno.h>
+
+static struct NativeMiniDfsConf conf = {
+    .doFormat = 1,
+};
+
+/**
+ * Test that we can create a MiniDFSCluster and shut it down.
+ */
+int main(void) {
+    struct NativeMiniDfsCluster* cl;
+    
+    cl = nmdCreate(&conf);
+    EXPECT_NONNULL(cl);
+    EXPECT_ZERO(nmdWaitClusterUp(cl));
+    EXPECT_ZERO(nmdShutdown(cl));
+    nmdFree(cl);
+
+    return 0;
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.c
deleted file mode 100644
index 63139e6..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.c
+++ /dev/null
@@ -1,165 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hdfsJniHelper.h"
-#include "native_mini_dfs.h"
-
-#include <errno.h>
-#include <jni.h>
-#include <stdio.h>
-#include <stdlib.h>
-
-#define MINIDFS_CLUSTER_BUILDER "org/apache/hadoop/hdfs/MiniDFSCluster$Builder"
-#define MINIDFS_CLUSTER "org/apache/hadoop/hdfs/MiniDFSCluster"
-#define HADOOP_CONF     "org/apache/hadoop/conf/Configuration"
-
-struct NativeMiniDfsCluster {
-    /**
-     * The NativeMiniDfsCluster object
-     */
-    jobject obj;
-};
-
-struct NativeMiniDfsCluster* nmdCreate(struct NativeMiniDfsConf *conf)
-{
-    struct NativeMiniDfsCluster* cl = NULL;
-    jobject bld = NULL, bld2 = NULL, cobj = NULL;
-    jvalue  val;
-    JNIEnv *env = getJNIEnv();
-    if (!env) {
-        fprintf(stderr, "nmdCreate: unable to construct JNIEnv.\n");
-        goto error;
-    }
-    cl = calloc(1, sizeof(struct NativeMiniDfsCluster));
-    if (!cl) {
-        fprintf(stderr, "nmdCreate: OOM");
-        goto error;
-    }
-    cobj = constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
-    if (!cobj) {
-        fprintf(stderr, "nmdCreate: unable to construct Configuration\n");
-        goto error_free_cl;
-    }
-    bld = constructNewObjectOfClass(env, NULL, MINIDFS_CLUSTER_BUILDER,
-                    "(L"HADOOP_CONF";)V", cobj);
-    if (!bld) {
-        fprintf(stderr, "nmdCreate: unable to construct "
-                "NativeMiniDfsCluster#Builder\n");
-        goto error_dlr_cobj;
-    }
-    if (invokeMethod(env, &val, NULL, INSTANCE, bld,
-            MINIDFS_CLUSTER_BUILDER, "format",
-            "(Z)L" MINIDFS_CLUSTER_BUILDER ";", conf->doFormat)) {
-        fprintf(stderr, "nmdCreate: failed to call Builder#doFormat\n");
-        goto error_dlr_bld;
-    }
-    bld2 = val.l;
-    if (invokeMethod(env, &val, NULL, INSTANCE, bld,
-            MINIDFS_CLUSTER_BUILDER, "build",
-            "()L" MINIDFS_CLUSTER ";")) {
-        fprintf(stderr, "nmdCreate: failed to call Builder#build\n");
-        goto error_dlr_bld2;
-    }
-	cl->obj = (*env)->NewGlobalRef(env, val.l);
-    if (!cl->obj) {
-        fprintf(stderr, "nmdCreate: failed to create global reference to "
-            "MiniDFSCluster\n");
-        goto error_dlr_val;
-    }
-    (*env)->DeleteLocalRef(env, val.l);
-    (*env)->DeleteLocalRef(env, bld2);
-    (*env)->DeleteLocalRef(env, bld);
-    (*env)->DeleteLocalRef(env, cobj);
-    return cl;
-
-error_dlr_val:
-    (*env)->DeleteLocalRef(env, val.l);
-error_dlr_bld2:
-    (*env)->DeleteLocalRef(env, bld2);
-error_dlr_bld:
-    (*env)->DeleteLocalRef(env, bld);
-error_dlr_cobj:
-    (*env)->DeleteLocalRef(env, cobj);
-error_free_cl:
-    free(cl);
-error:
-    return NULL;
-}
-
-void nmdFree(struct NativeMiniDfsCluster* cl)
-{
-    JNIEnv *env = getJNIEnv();
-    if (!env) {
-        fprintf(stderr, "nmdFree: getJNIEnv failed\n");
-        free(cl);
-        return;
-    }
-    (*env)->DeleteGlobalRef(env, cl->obj);
-    free(cl);
-}
-
-int nmdShutdown(struct NativeMiniDfsCluster* cl)
-{
-    JNIEnv *env = getJNIEnv();
-    if (!env) {
-        fprintf(stderr, "nmdShutdown: getJNIEnv failed\n");
-        return -EIO;
-    }
-    if (invokeMethod(env, NULL, NULL, INSTANCE, cl->obj,
-            MINIDFS_CLUSTER, "shutdown", "()V")) {
-        fprintf(stderr, "nmdShutdown: MiniDFSCluster#shutdown failure\n");
-        return -EIO;
-    }
-    return 0;
-}
-
-int nmdWaitClusterUp(struct NativeMiniDfsCluster *cl)
-{
-    JNIEnv *env = getJNIEnv();
-    if (!env) {
-        fprintf(stderr, "nmdWaitClusterUp: getJNIEnv failed\n");
-        return -EIO;
-    }
-    if (invokeMethod(env, NULL, NULL, INSTANCE, cl->obj,
-            MINIDFS_CLUSTER, "waitClusterUp", "()V")) {
-        fprintf(stderr, "nmdWaitClusterUp: MiniDFSCluster#waitClusterUp "
-                "failure\n");
-        return -EIO;
-    }
-    return 0;
-}
-
-int nmdGetNameNodePort(struct NativeMiniDfsCluster *cl)
-{
-    JNIEnv *env = getJNIEnv();
-    jvalue jVal;
-
-    if (!env) {
-        fprintf(stderr, "nmdHdfsConnect: getJNIEnv failed\n");
-        return -EIO;
-    }
-    // Note: this will have to be updated when HA nativeMiniDfs clusters are
-    // supported
-    if (invokeMethod(env, &jVal, NULL, INSTANCE, cl->obj,
-            MINIDFS_CLUSTER, "getNameNodePort", "()I")) {
-        fprintf(stderr, "nmdHdfsConnect: MiniDFSCluster#getNameNodePort "
-                "failure\n");
-        return -EIO;
-    }
-    return jVal.i;
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.h b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.h
deleted file mode 100644
index 88a4b47..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/native_mini_dfs.h
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LIBHDFS_NATIVE_MINI_DFS_H
-#define LIBHDFS_NATIVE_MINI_DFS_H
-
-#include <jni.h> /* for jboolean */
-
-struct NativeMiniDfsCluster; 
-
-/**
- * Represents a configuration to use for creating a Native MiniDFSCluster
- */
-struct NativeMiniDfsConf {
-    /**
-     * Nonzero if the cluster should be formatted prior to startup
-     */
-    jboolean doFormat;
-};
-
-/**
- * Create a NativeMiniDfsBuilder
- *
- * @param conf      (inout) The cluster configuration
- *
- * @return      a NativeMiniDfsBuilder, or a NULL pointer on error.
- */
-struct NativeMiniDfsCluster* nmdCreate(struct NativeMiniDfsConf *conf);
-
-/**
- * Wait until a MiniDFSCluster comes out of safe mode.
- *
- * @param cl        The cluster
- *
- * @return          0 on success; a non-zero error code if the cluster fails to
- *                  come out of safe mode.
- */
-int nmdWaitClusterUp(struct NativeMiniDfsCluster *cl);
-
-/**
- * Shut down a NativeMiniDFS cluster
- *
- * @param cl        The cluster
- *
- * @return          0 on success; a non-zero error code if an exception is
- *                  thrown.
- */
-int nmdShutdown(struct NativeMiniDfsCluster *cl);
-
-/**
- * Destroy a Native MiniDFSCluster
- *
- * @param cl        The cluster to destroy
- */
-void nmdFree(struct NativeMiniDfsCluster* cl);
-
-/**
- * Get the port that's in use by the given (non-HA) nativeMiniDfs
- *
- * @param cl        The initialized NativeMiniDfsCluster
- *
- * @return          the port, or a negative error code
- */
-int nmdGetNameNodePort(struct NativeMiniDfsCluster *cl);
-
-#endif
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_libhdfs_threaded.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_libhdfs_threaded.c
deleted file mode 100644
index 5c40426..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_libhdfs_threaded.c
+++ /dev/null
@@ -1,221 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "expect.h"
-#include "hdfs.h"
-#include "native_mini_dfs.h"
-
-#include <errno.h>
-#include <semaphore.h>
-#include <pthread.h>
-#include <stdio.h>
-#include <stdlib.h>
-
-#define TLH_MAX_THREADS 100
-
-static sem_t tlhSem;
-
-static struct NativeMiniDfsCluster* tlhCluster;
-
-struct tlhThreadInfo {
-    /** Thread index */
-    int threadIdx;
-    /** 0 = thread was successful; error code otherwise */
-    int success;
-    /** pthread identifier */
-    pthread_t thread;
-};
-
-static int hdfsSingleNameNodeConnect(struct NativeMiniDfsCluster *cl, hdfsFS *fs)
-{
-    int ret, port;
-    hdfsFS hdfs;
-    
-    port = nmdGetNameNodePort(cl);
-    if (port < 0) {
-        fprintf(stderr, "hdfsSingleNameNodeConnect: nmdGetNameNodePort "
-                "returned error %d\n", port);
-        return port;
-    }
-    hdfs = hdfsConnectNewInstance("localhost", port);
-    if (!hdfs) {
-        ret = -errno;
-        return ret;
-    }
-    *fs = hdfs;
-    return 0;
-}
-
-static int doTestHdfsOperations(struct tlhThreadInfo *ti, hdfsFS fs)
-{
-    char prefix[256], tmp[256];
-    hdfsFile file;
-    int ret, expected;
-
-    snprintf(prefix, sizeof(prefix), "/tlhData%04d", ti->threadIdx);
-
-    if (hdfsExists(fs, prefix) == 0) {
-        EXPECT_ZERO(hdfsDelete(fs, prefix, 1));
-    }
-    EXPECT_ZERO(hdfsCreateDirectory(fs, prefix));
-    snprintf(tmp, sizeof(tmp), "%s/file", prefix);
-
-    /* There should not be any file to open for reading. */
-    EXPECT_NULL(hdfsOpenFile(fs, tmp, O_RDONLY, 0, 0, 0));
-
-    file = hdfsOpenFile(fs, tmp, O_WRONLY, 0, 0, 0);
-    EXPECT_NONNULL(file);
-
-    /* TODO: implement writeFully and use it here */
-    expected = strlen(prefix);
-    ret = hdfsWrite(fs, file, prefix, expected);
-    if (ret < 0) {
-        ret = errno;
-        fprintf(stderr, "hdfsWrite failed and set errno %d\n", ret);
-        return ret;
-    }
-    if (ret != expected) {
-        fprintf(stderr, "hdfsWrite was supposed to write %d bytes, but "
-                "it wrote %d\n", ret, expected);
-        return EIO;
-    }
-    EXPECT_ZERO(hdfsFlush(fs, file));
-    EXPECT_ZERO(hdfsCloseFile(fs, file));
-
-    /* Let's re-open the file for reading */
-    file = hdfsOpenFile(fs, tmp, O_RDONLY, 0, 0, 0);
-    EXPECT_NONNULL(file);
-
-    /* TODO: implement readFully and use it here */
-    ret = hdfsRead(fs, file, tmp, sizeof(tmp));
-    if (ret < 0) {
-        ret = errno;
-        fprintf(stderr, "hdfsRead failed and set errno %d\n", ret);
-        return ret;
-    }
-    if (ret != expected) {
-        fprintf(stderr, "hdfsRead was supposed to read %d bytes, but "
-                "it read %d\n", ret, expected);
-        return EIO;
-    }
-    EXPECT_ZERO(memcmp(prefix, tmp, expected));
-    EXPECT_ZERO(hdfsCloseFile(fs, file));
-
-    // TODO: Non-recursive delete should fail?
-    //EXPECT_NONZERO(hdfsDelete(fs, prefix, 0));
-
-    EXPECT_ZERO(hdfsDelete(fs, prefix, 1));
-    return 0;
-}
-
-static void *testHdfsOperations(void *v)
-{
-    struct tlhThreadInfo *ti = (struct tlhThreadInfo*)v;
-    hdfsFS fs = NULL;
-    int ret;
-
-    fprintf(stderr, "testHdfsOperations(threadIdx=%d): starting\n",
-        ti->threadIdx);
-    ret = hdfsSingleNameNodeConnect(tlhCluster, &fs);
-    if (ret) {
-        fprintf(stderr, "testHdfsOperations(threadIdx=%d): "
-            "hdfsSingleNameNodeConnect failed with error %d.\n",
-            ti->threadIdx, ret);
-        ti->success = EIO;
-        return NULL;
-    }
-    ti->success = doTestHdfsOperations(ti, fs);
-    if (hdfsDisconnect(fs)) {
-        ret = errno;
-        fprintf(stderr, "hdfsDisconnect error %d\n", ret);
-        ti->success = ret;
-    }
-    return NULL;
-}
-
-static int checkFailures(struct tlhThreadInfo *ti, int tlhNumThreads)
-{
-    int i, threadsFailed = 0;
-    const char *sep = "";
-
-    for (i = 0; i < tlhNumThreads; i++) {
-        if (ti[i].success != 0) {
-            threadsFailed = 1;
-        }
-    }
-    if (!threadsFailed) {
-        fprintf(stderr, "testLibHdfs: all threads succeeded.  SUCCESS.\n");
-        return EXIT_SUCCESS;
-    }
-    fprintf(stderr, "testLibHdfs: some threads failed: [");
-    for (i = 0; i < tlhNumThreads; i++) {
-        if (ti[i].success != 0) {
-            fprintf(stderr, "%s%d", sep, i);
-            sep = ", "; 
-        }
-    }
-    fprintf(stderr, "].  FAILURE.\n");
-    return EXIT_FAILURE;
-}
-
-/**
- * Test that we can write a file with libhdfs and then read it back
- */
-int main(void)
-{
-    int i, tlhNumThreads;
-    const char *tlhNumThreadsStr;
-    struct tlhThreadInfo ti[TLH_MAX_THREADS];
-    struct NativeMiniDfsConf conf = {
-        .doFormat = 1,
-    };
-
-    tlhNumThreadsStr = getenv("TLH_NUM_THREADS");
-    if (!tlhNumThreadsStr) {
-        tlhNumThreadsStr = "3";
-    }
-    tlhNumThreads = atoi(tlhNumThreadsStr);
-    if ((tlhNumThreads <= 0) || (tlhNumThreads > TLH_MAX_THREADS)) {
-        fprintf(stderr, "testLibHdfs: must have a number of threads "
-                "between 1 and %d inclusive, not %d\n",
-                TLH_MAX_THREADS, tlhNumThreads);
-        return EXIT_FAILURE;
-    }
-    memset(&ti[0], 0, sizeof(ti));
-    for (i = 0; i < tlhNumThreads; i++) {
-        ti[i].threadIdx = i;
-    }
-
-    EXPECT_ZERO(sem_init(&tlhSem, 0, tlhNumThreads));
-    tlhCluster = nmdCreate(&conf);
-    EXPECT_NONNULL(tlhCluster);
-    EXPECT_ZERO(nmdWaitClusterUp(tlhCluster));
-
-    for (i = 0; i < tlhNumThreads; i++) {
-        EXPECT_ZERO(pthread_create(&ti[i].thread, NULL,
-            testHdfsOperations, &ti[i]));
-    }
-    for (i = 0; i < tlhNumThreads; i++) {
-        EXPECT_ZERO(pthread_join(ti[i].thread, NULL));
-    }
-
-    EXPECT_ZERO(nmdShutdown(tlhCluster));
-    nmdFree(tlhCluster);
-    EXPECT_ZERO(sem_destroy(&tlhSem));
-    return checkFailures(ti, tlhNumThreads);
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_native_mini_dfs.c b/hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_native_mini_dfs.c
deleted file mode 100644
index b97ef95..0000000
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/native/test_native_mini_dfs.c
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "expect.h"
-#include "native_mini_dfs.h"
-
-#include <errno.h>
-
-static struct NativeMiniDfsConf conf = {
-    .doFormat = 1,
-};
-
-/**
- * Test that we can create a MiniDFSCluster and shut it down.
- */
-int main(void) {
-    struct NativeMiniDfsCluster* cl;
-    
-    cl = nmdCreate(&conf);
-    EXPECT_NONNULL(cl);
-    EXPECT_ZERO(nmdWaitClusterUp(cl));
-    EXPECT_ZERO(nmdShutdown(cl));
-    nmdFree(cl);
-
-    return 0;
-}
-- 
1.7.0.4

