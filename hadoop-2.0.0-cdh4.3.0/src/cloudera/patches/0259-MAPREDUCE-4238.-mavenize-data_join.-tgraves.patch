From 829f93a8ff782d8d43ba7c4589d49d9307f0dd58 Mon Sep 17 00:00:00 2001
From: Thomas Graves <tgraves@apache.org>
Date: Tue, 15 May 2012 19:09:59 +0000
Subject: [PATCH 0259/1357] MAPREDUCE-4238. mavenize data_join. (tgraves)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338835 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 4900a83054b309d979a884ece699f68de730edc8)
---
 .../apache/hadoop/contrib/utils/join/README.txt    |   50 ----
 .../contrib/utils/join/SampleDataJoinMapper.java   |   54 -----
 .../contrib/utils/join/SampleDataJoinReducer.java  |   58 -----
 .../contrib/utils/join/SampleTaggedMapOutput.java  |   60 -----
 .../utils/join/ArrayListBackedIterator.java        |   70 ------
 .../hadoop/contrib/utils/join/DataJoinJob.java     |  174 --------------
 .../contrib/utils/join/DataJoinMapperBase.java     |  122 ----------
 .../contrib/utils/join/DataJoinReducerBase.java    |  237 --------------------
 .../apache/hadoop/contrib/utils/join/JobBase.java  |  173 --------------
 .../contrib/utils/join/ResetableIterator.java      |   35 ---
 .../hadoop/contrib/utils/join/TaggedMapOutput.java |   56 -----
 .../hadoop/contrib/utils/join/TestDataJoin.java    |  154 -------------
 hadoop-project/pom.xml                             |    5 +
 hadoop-tools/hadoop-datajoin/pom.xml               |  120 ++++++++++
 .../utils/join/ArrayListBackedIterator.java        |   70 ++++++
 .../hadoop/contrib/utils/join/DataJoinJob.java     |  174 ++++++++++++++
 .../contrib/utils/join/DataJoinMapperBase.java     |  122 ++++++++++
 .../contrib/utils/join/DataJoinReducerBase.java    |  237 ++++++++++++++++++++
 .../apache/hadoop/contrib/utils/join/JobBase.java  |  173 ++++++++++++++
 .../contrib/utils/join/ResetableIterator.java      |   35 +++
 .../hadoop/contrib/utils/join/TaggedMapOutput.java |   56 +++++
 .../hadoop-datajoin/src/test/java/README.txt       |   50 ++++
 .../src/test/java/SampleDataJoinMapper.java        |   54 +++++
 .../src/test/java/SampleDataJoinReducer.java       |   58 +++++
 .../src/test/java/SampleTaggedMapOutput.java       |   60 +++++
 .../hadoop/contrib/utils/join/TestDataJoin.java    |  155 +++++++++++++
 hadoop-tools/hadoop-tools-dist/pom.xml             |    5 +
 hadoop-tools/pom.xml                               |    1 +
 28 files changed, 1375 insertions(+), 1243 deletions(-)
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/README.txt
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinMapper.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinReducer.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleTaggedMapOutput.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/JobBase.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java
 delete mode 100644 hadoop-mapreduce-project/src/contrib/data_join/src/test/org/apache/hadoop/contrib/utils/join/TestDataJoin.java
 create mode 100644 hadoop-tools/hadoop-datajoin/pom.xml
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/JobBase.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/test/java/README.txt
 create mode 100644 hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinMapper.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinReducer.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/test/java/SampleTaggedMapOutput.java
 create mode 100644 hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/TestDataJoin.java

diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/README.txt b/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/README.txt
deleted file mode 100644
index 73fd6ef..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/README.txt
+++ /dev/null
@@ -1,50 +0,0 @@
-*************************************************
-*** Input Files (Note: tab-separated columns) ***
-*************************************************
-[:~]$ cat datajoin/input/A
-A.a11   A.a12
-A.a21   A.a22
-B.a21   A.a32
-A.a31   A.a32
-B.a31   A.a32
-
-[:~]$ cat datajoin/input/B
-A.a11   B.a12
-A.a11   B.a13
-B.a11   B.a12
-B.a21   B.a22
-A.a31   B.a32
-B.a31   B.a32
-
-
-*****************************
-*** Invoke SampleDataJoin ***
-*****************************
-[:~]$ $HADOOP_PREFIX/bin/hadoop jar hadoop-datajoin-examples.jar org.apache.hadoop.contrib.utils.join.DataJoinJob datajoin/input datajoin/output Text 1 org.apache.hadoop.contrib.utils.join.SampleDataJoinMapper org.apache.hadoop.contrib.utils.join.SampleDataJoinReducer org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput Text
-Using TextInputFormat: Text
-Using TextOutputFormat: Text
-07/06/01 19:58:23 INFO mapred.FileInputFormat: Total input paths to process : 2
-Job job_kkzk08 is submitted
-Job job_kkzk08 is still running.
-07/06/01 19:58:24 INFO mapred.LocalJobRunner: collectedCount    5
-totalCount      5
-
-07/06/01 19:58:24 INFO mapred.LocalJobRunner: collectedCount    6
-totalCount      6
-
-07/06/01 19:58:24 INFO datajoin.job: key: A.a11 this.largestNumOfValues: 3
-07/06/01 19:58:24 INFO mapred.LocalJobRunner: actuallyCollectedCount    5
-collectedCount  7
-groupCount      6
- > reduce
-
-
-*******************
-*** Output File ***
-*******************
-[:~]$ cat datajoin/output/part-00000  
-A.a11   A.a12   B.a12
-A.a11   A.a12   B.a13
-A.a31   A.a32   B.a32
-B.a21   A.a32   B.a22
-B.a31   A.a32   B.a32
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinMapper.java b/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinMapper.java
deleted file mode 100644
index 3f1d4f0..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinMapper.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import org.apache.hadoop.io.Text;
-
-import org.apache.hadoop.contrib.utils.join.DataJoinMapperBase;
-import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
-import org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput;
-
-/**
- * This is a subclass of DataJoinMapperBase that is used to
- * demonstrate the functionality of INNER JOIN between 2 data
- * sources (TAB separated text files) based on the first column.
- */
-public class SampleDataJoinMapper extends DataJoinMapperBase {
-
-
-  protected Text generateInputTag(String inputFile) {
-    // tag the row with input file name (data source)
-    return new Text(inputFile);
-  }
-
-  protected Text generateGroupKey(TaggedMapOutput aRecord) {
-    // first column in the input tab separated files becomes the key (to perform the JOIN)
-    String line = ((Text) aRecord.getData()).toString();
-    String groupKey = "";
-    String[] tokens = line.split("\\t", 2);
-    groupKey = tokens[0];
-    return new Text(groupKey);
-  }
-
-  protected TaggedMapOutput generateTaggedMapOutput(Object value) {
-    TaggedMapOutput retv = new SampleTaggedMapOutput((Text) value);
-    retv.setTag(new Text(this.inputTag));
-    return retv;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinReducer.java b/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinReducer.java
deleted file mode 100644
index f8eb00d..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleDataJoinReducer.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import org.apache.hadoop.io.Text;
-
-import org.apache.hadoop.contrib.utils.join.DataJoinReducerBase;
-import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
-
-/**
- * This is a subclass of DataJoinReducerBase that is used to
- * demonstrate the functionality of INNER JOIN between 2 data
- * sources (TAB separated text files) based on the first column.
- */
-public class SampleDataJoinReducer extends DataJoinReducerBase {
-
-  /**
-   * 
-   * @param tags
-   *          a list of source tags
-   * @param values
-   *          a value per source
-   * @return combined value derived from values of the sources
-   */
-  protected TaggedMapOutput combine(Object[] tags, Object[] values) {
-    // eliminate rows which didnot match in one of the two tables (for INNER JOIN)
-    if (tags.length < 2)
-       return null;  
-    String joinedStr = ""; 
-    for (int i=0; i<tags.length; i++) {
-      if (i > 0)
-         joinedStr += "\t";
-      // strip first column as it is the key on which we joined
-      String line = ((Text) (((TaggedMapOutput) values[i]).getData())).toString();
-      String[] tokens = line.split("\\t", 2);
-      joinedStr += tokens[1];
-    }
-    TaggedMapOutput retv = new SampleTaggedMapOutput(new Text(joinedStr));
-    retv.setTag((Text) tags[0]); 
-    return retv;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleTaggedMapOutput.java b/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleTaggedMapOutput.java
deleted file mode 100644
index 59f1bd1..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/examples/org/apache/hadoop/contrib/utils/join/SampleTaggedMapOutput.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.Text;
-
-import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
-
-/**
- * This is a subclass of TaggedMapOutput that is used to
- * demonstrate the functionality of INNER JOIN between 2 data
- * sources (TAB separated text files) based on the first column.
- */
-public class SampleTaggedMapOutput extends TaggedMapOutput {
-
-  private Text data;
-
-  public SampleTaggedMapOutput() {
-    this.data = new Text("");
-  }
-
-  public SampleTaggedMapOutput(Text data) {
-    this.data = data;
-  }
-
-  public Writable getData() {
-    return data;
-  }
-
-  public void write(DataOutput out) throws IOException {
-    this.tag.write(out);
-    this.data.write(out);
-  }
-
-  public void readFields(DataInput in) throws IOException {
-    this.tag.readFields(in);
-    this.data.readFields(in);
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java
deleted file mode 100644
index 1ffd4b5..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-
-/**
- * This class provides an implementation of ResetableIterator. The
- * implementation will be based on ArrayList.
- * 
- * 
- */
-public class ArrayListBackedIterator implements ResetableIterator {
-
-  private Iterator iter;
-
-  private ArrayList<Object> data;
-
-  public ArrayListBackedIterator() {
-    this(new ArrayList<Object>());
-  }
-
-  public ArrayListBackedIterator(ArrayList<Object> data) {
-    this.data = data;
-    this.iter = this.data.iterator();
-  }
-
-  public void add(Object item) {
-    this.data.add(item);
-  }
-
-  public boolean hasNext() {
-    return this.iter.hasNext();
-  }
-
-  public Object next() {
-    return this.iter.next();
-  }
-
-  public void remove() {
-
-  }
-
-  public void reset() {
-    this.iter = this.data.iterator();
-  }
-
-  public void close() throws IOException {
-    this.iter = null;
-    this.data = null;
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java
deleted file mode 100644
index 9a1b8f1..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java
+++ /dev/null
@@ -1,174 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.mapred.SequenceFileOutputFormat;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.mapred.JobID;
-
-/**
- * This class implements the main function for creating a map/reduce
- * job to join data of different sources. To create sucn a job, the 
- * user must implement a mapper class that extends DataJoinMapperBase class,
- * and a reducer class that extends DataJoinReducerBase. 
- * 
- */
-public class DataJoinJob {
-
-  public static Class getClassByName(String className) {
-    Class retv = null;
-    try {
-      ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
-      retv = Class.forName(className, true, classLoader);
-    } catch (Exception e) {
-      throw new RuntimeException(e);
-    }
-    return retv;
-  }
-
-  public static JobConf createDataJoinJob(String args[]) throws IOException {
-
-    String inputDir = args[0];
-    String outputDir = args[1];
-    Class inputFormat = SequenceFileInputFormat.class;
-    if (args[2].compareToIgnoreCase("text") != 0) {
-      System.out.println("Using SequenceFileInputFormat: " + args[2]);
-    } else {
-      System.out.println("Using TextInputFormat: " + args[2]);
-      inputFormat = TextInputFormat.class;
-    }
-    int numOfReducers = Integer.parseInt(args[3]);
-    Class mapper = getClassByName(args[4]);
-    Class reducer = getClassByName(args[5]);
-    Class mapoutputValueClass = getClassByName(args[6]);
-    Class outputFormat = TextOutputFormat.class;
-    Class outputValueClass = Text.class;
-    if (args[7].compareToIgnoreCase("text") != 0) {
-      System.out.println("Using SequenceFileOutputFormat: " + args[7]);
-      outputFormat = SequenceFileOutputFormat.class;
-      outputValueClass = getClassByName(args[7]);
-    } else {
-      System.out.println("Using TextOutputFormat: " + args[7]);
-    }
-    long maxNumOfValuesPerGroup = 100;
-    String jobName = "";
-    if (args.length > 8) {
-      maxNumOfValuesPerGroup = Long.parseLong(args[8]);
-    }
-    if (args.length > 9) {
-      jobName = args[9];
-    }
-    Configuration defaults = new Configuration();
-    JobConf job = new JobConf(defaults, DataJoinJob.class);
-    job.setJobName("DataJoinJob: " + jobName);
-
-    FileSystem fs = FileSystem.get(defaults);
-    fs.delete(new Path(outputDir), true);
-    FileInputFormat.setInputPaths(job, inputDir);
-
-    job.setInputFormat(inputFormat);
-
-    job.setMapperClass(mapper);
-    FileOutputFormat.setOutputPath(job, new Path(outputDir));
-    job.setOutputFormat(outputFormat);
-    SequenceFileOutputFormat.setOutputCompressionType(job,
-            SequenceFile.CompressionType.BLOCK);
-    job.setMapOutputKeyClass(Text.class);
-    job.setMapOutputValueClass(mapoutputValueClass);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(outputValueClass);
-    job.setReducerClass(reducer);
-
-    job.setNumMapTasks(1);
-    job.setNumReduceTasks(numOfReducers);
-    job.setLong("datajoin.maxNumOfValuesPerGroup", maxNumOfValuesPerGroup);
-    return job;
-  }
-
-  /**
-   * Submit/run a map/reduce job.
-   * 
-   * @param job
-   * @return true for success
-   * @throws IOException
-   */
-  public static boolean runJob(JobConf job) throws IOException {
-    JobClient jc = new JobClient(job);
-    boolean sucess = true;
-    RunningJob running = null;
-    try {
-      running = jc.submitJob(job);
-      JobID jobId = running.getID();
-      System.out.println("Job " + jobId + " is submitted");
-      while (!running.isComplete()) {
-        System.out.println("Job " + jobId + " is still running.");
-        try {
-          Thread.sleep(60000);
-        } catch (InterruptedException e) {
-        }
-        running = jc.getJob(jobId);
-      }
-      sucess = running.isSuccessful();
-    } finally {
-      if (!sucess && (running != null)) {
-        running.killJob();
-      }
-      jc.close();
-    }
-    return sucess;
-  }
-
-  /**
-   * @param args
-   */
-  public static void main(String[] args) {
-    boolean success;
-    if (args.length < 8 || args.length > 10) {
-      System.out.println("usage: DataJoinJob " + "inputdirs outputdir map_input_file_format "
-                         + "numofParts " + "mapper_class " + "reducer_class "
-                         + "map_output_value_class "
-                         + "output_value_class [maxNumOfValuesPerGroup [descriptionOfJob]]]");
-      System.exit(-1);
-    }
-
-    try {
-      JobConf job = DataJoinJob.createDataJoinJob(args);
-      success = DataJoinJob.runJob(job);
-      if (!success) {
-        System.out.println("Job failed");
-      }
-    } catch (IOException ioe) {
-      ioe.printStackTrace();
-    }
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java
deleted file mode 100644
index 1e1296a..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-
-/**
- * This abstract class serves as the base class for the mapper class of a data
- * join job. This class expects its subclasses to implement methods for the
- * following functionalities:
- * 
- * 1. Compute the source tag of input values 2. Compute the map output value
- * object 3. Compute the map output key object
- * 
- * The source tag will be used by the reducer to determine from which source
- * (which table in SQL terminology) a value comes. Computing the map output
- * value object amounts to performing projecting/filtering work in a SQL
- * statement (through the select/where clauses). Computing the map output key
- * amounts to choosing the join key. This class provides the appropriate plugin
- * points for the user defined subclasses to implement the appropriate logic.
- * 
- */
-public abstract class DataJoinMapperBase extends JobBase {
-
-  protected String inputFile = null;
-
-  protected JobConf job = null;
-
-  protected Text inputTag = null;
-
-  protected Reporter reporter = null;
-
-  public void configure(JobConf job) {
-    super.configure(job);
-    this.job = job;
-    this.inputFile = job.get(MRJobConfig.MAP_INPUT_FILE);
-    this.inputTag = generateInputTag(this.inputFile);
-  }
-
-  /**
-   * Determine the source tag based on the input file name.
-   * 
-   * @param inputFile
-   * @return the source tag computed from the given file name.
-   */
-  protected abstract Text generateInputTag(String inputFile);
-
-  /**
-   * Generate a tagged map output value. The user code can also perform
-   * projection/filtering. If it decides to discard the input record when
-   * certain conditions are met,it can simply return a null.
-   * 
-   * @param value
-   * @return an object of TaggedMapOutput computed from the given value.
-   */
-  protected abstract TaggedMapOutput generateTaggedMapOutput(Object value);
-
-  /**
-   * Generate a map output key. The user code can compute the key
-   * programmatically, not just selecting the values of some fields. In this
-   * sense, it is more general than the joining capabilities of SQL.
-   * 
-   * @param aRecord
-   * @return the group key for the given record
-   */
-  protected abstract Text generateGroupKey(TaggedMapOutput aRecord);
-
-  public void map(Object key, Object value,
-                  OutputCollector output, Reporter reporter) throws IOException {
-    if (this.reporter == null) {
-      this.reporter = reporter;
-    }
-    addLongValue("totalCount", 1);
-    TaggedMapOutput aRecord = generateTaggedMapOutput(value);
-    if (aRecord == null) {
-      addLongValue("discardedCount", 1);
-      return;
-    }
-    Text groupKey = generateGroupKey(aRecord);
-    if (groupKey == null) {
-      addLongValue("nullGroupKeyCount", 1);
-      return;
-    }
-    output.collect(groupKey, aRecord);
-    addLongValue("collectedCount", 1);
-  }
-
-  public void close() throws IOException {
-    if (this.reporter != null) {
-      this.reporter.setStatus(super.getReport());
-    }
-  }
-
-  public void reduce(Object arg0, Iterator arg1,
-                     OutputCollector arg2, Reporter arg3) throws IOException {
-    // TODO Auto-generated method stub
-
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java
deleted file mode 100644
index e340d79..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java
+++ /dev/null
@@ -1,237 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-
-/**
- * This abstract class serves as the base class for the reducer class of a data
- * join job. The reduce function will first group the values according to their
- * input tags, and then compute the cross product of over the groups. For each
- * tuple in the cross product, it calls the following method, which is expected
- * to be implemented in a subclass.
- * 
- * protected abstract TaggedMapOutput combine(Object[] tags, Object[] values);
- * 
- * The above method is expected to produce one output value from an array of
- * records of different sources. The user code can also perform filtering here.
- * It can return null if it decides to the records do not meet certain
- * conditions.
- * 
- */
-public abstract class DataJoinReducerBase extends JobBase {
-
-  protected Reporter reporter = null;
-
-  private long maxNumOfValuesPerGroup = 100;
-
-  protected long largestNumOfValues = 0;
-
-  protected long numOfValues = 0;
-
-  protected long collected = 0;
-
-  protected JobConf job;
-
-  public void close() throws IOException {
-    if (this.reporter != null) {
-      this.reporter.setStatus(super.getReport());
-    }
-  }
-
-  public void configure(JobConf job) {
-    super.configure(job);
-    this.job = job;
-    this.maxNumOfValuesPerGroup = job.getLong("datajoin.maxNumOfValuesPerGroup", 100);
-  }
-
-  /**
-   * The subclass can provide a different implementation on ResetableIterator.
-   * This is necessary if the number of values in a reduce call is very high.
-   * 
-   * The default provided here uses ArrayListBackedIterator
-   * 
-   * @return an Object of ResetableIterator.
-   */
-  protected ResetableIterator createResetableIterator() {
-    return new ArrayListBackedIterator();
-  }
-
-  /**
-   * This is the function that re-groups values for a key into sub-groups based
-   * on a secondary key (input tag).
-   * 
-   * @param arg1
-   * @return
-   */
-  private SortedMap<Object, ResetableIterator> regroup(Object key,
-                                                       Iterator arg1, Reporter reporter) throws IOException {
-    this.numOfValues = 0;
-    SortedMap<Object, ResetableIterator> retv = new TreeMap<Object, ResetableIterator>();
-    TaggedMapOutput aRecord = null;
-    while (arg1.hasNext()) {
-      this.numOfValues += 1;
-      if (this.numOfValues % 100 == 0) {
-        reporter.setStatus("key: " + key.toString() + " numOfValues: "
-                           + this.numOfValues);
-      }
-      if (this.numOfValues > this.maxNumOfValuesPerGroup) {
-        continue;
-      }
-      aRecord = ((TaggedMapOutput) arg1.next()).clone(job);
-      Text tag = aRecord.getTag();
-      ResetableIterator data = retv.get(tag);
-      if (data == null) {
-        data = createResetableIterator();
-        retv.put(tag, data);
-      }
-      data.add(aRecord);
-    }
-    if (this.numOfValues > this.largestNumOfValues) {
-      this.largestNumOfValues = numOfValues;
-      LOG.info("key: " + key.toString() + " this.largestNumOfValues: "
-               + this.largestNumOfValues);
-    }
-    return retv;
-  }
-
-  public void reduce(Object key, Iterator values,
-                     OutputCollector output, Reporter reporter) throws IOException {
-    if (this.reporter == null) {
-      this.reporter = reporter;
-    }
-
-    SortedMap<Object, ResetableIterator> groups = regroup(key, values, reporter);
-    Object[] tags = groups.keySet().toArray();
-    ResetableIterator[] groupValues = new ResetableIterator[tags.length];
-    for (int i = 0; i < tags.length; i++) {
-      groupValues[i] = groups.get(tags[i]);
-    }
-    joinAndCollect(tags, groupValues, key, output, reporter);
-    addLongValue("groupCount", 1);
-    for (int i = 0; i < tags.length; i++) {
-      groupValues[i].close();
-    }
-  }
-
-  /**
-   * The subclass can overwrite this method to perform additional filtering
-   * and/or other processing logic before a value is collected.
-   * 
-   * @param key
-   * @param aRecord
-   * @param output
-   * @param reporter
-   * @throws IOException
-   */
-  protected void collect(Object key, TaggedMapOutput aRecord,
-                         OutputCollector output, Reporter reporter) throws IOException {
-    this.collected += 1;
-    addLongValue("collectedCount", 1);
-    if (aRecord != null) {
-      output.collect(key, aRecord.getData());
-      reporter.setStatus("key: " + key.toString() + " collected: " + collected);
-      addLongValue("actuallyCollectedCount", 1);
-    }
-  }
-
-  /**
-   * join the list of the value lists, and collect the results.
-   * 
-   * @param tags
-   *          a list of input tags
-   * @param values
-   *          a list of value lists, each corresponding to one input source
-   * @param key
-   * @param output
-   * @throws IOException
-   */
-  private void joinAndCollect(Object[] tags, ResetableIterator[] values,
-                              Object key, OutputCollector output, Reporter reporter)
-    throws IOException {
-    if (values.length < 1) {
-      return;
-    }
-    Object[] partialList = new Object[values.length];
-    joinAndCollect(tags, values, 0, partialList, key, output, reporter);
-  }
-
-  /**
-   * Perform the actual join recursively.
-   * 
-   * @param tags
-   *          a list of input tags
-   * @param values
-   *          a list of value lists, each corresponding to one input source
-   * @param pos
-   *          indicating the next value list to be joined
-   * @param partialList
-   *          a list of values, each from one value list considered so far.
-   * @param key
-   * @param output
-   * @throws IOException
-   */
-  private void joinAndCollect(Object[] tags, ResetableIterator[] values,
-                              int pos, Object[] partialList, Object key,
-                              OutputCollector output, Reporter reporter) throws IOException {
-
-    if (values.length == pos) {
-      // get a value from each source. Combine them
-      TaggedMapOutput combined = combine(tags, partialList);
-      collect(key, combined, output, reporter);
-      return;
-    }
-    ResetableIterator nextValues = values[pos];
-    nextValues.reset();
-    while (nextValues.hasNext()) {
-      Object v = nextValues.next();
-      partialList[pos] = v;
-      joinAndCollect(tags, values, pos + 1, partialList, key, output, reporter);
-    }
-  }
-
-  public static Text SOURCE_TAGS_FIELD = new Text("SOURCE_TAGS");
-
-  public static Text NUM_OF_VALUES_FIELD = new Text("NUM_OF_VALUES");
-
-  /**
-   * 
-   * @param tags
-   *          a list of source tags
-   * @param values
-   *          a value per source
-   * @return combined value derived from values of the sources
-   */
-  protected abstract TaggedMapOutput combine(Object[] tags, Object[] values);
-
-  public void map(Object arg0, Object arg1, OutputCollector arg2,
-                  Reporter arg3) throws IOException {
-    // TODO Auto-generated method stub
-
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/JobBase.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/JobBase.java
deleted file mode 100644
index dd34a4b..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/JobBase.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.Map.Entry;
-import java.util.Iterator;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.Reducer;
-
-/**
- * A common base implementing some statics collecting mechanisms that are
- * commonly used in a typical map/reduce job.
- * 
- */
-public abstract class JobBase implements Mapper, Reducer {
-
-  public static final Log LOG = LogFactory.getLog("datajoin.job");
-
-  private SortedMap<Object, Long> longCounters = null;
-
-  private SortedMap<Object, Double> doubleCounters = null;
-
-  /**
-   * Set the given counter to the given value
-   * 
-   * @param name
-   *          the counter name
-   * @param value
-   *          the value for the counter
-   */
-  protected void setLongValue(Object name, long value) {
-    this.longCounters.put(name, new Long(value));
-  }
-
-  /**
-   * Set the given counter to the given value
-   * 
-   * @param name
-   *          the counter name
-   * @param value
-   *          the value for the counter
-   */
-  protected void setDoubleValue(Object name, double value) {
-    this.doubleCounters.put(name, new Double(value));
-  }
-
-  /**
-   * 
-   * @param name
-   *          the counter name
-   * @return return the value of the given counter.
-   */
-  protected Long getLongValue(Object name) {
-    return this.longCounters.get(name);
-  }
-
-  /**
-   * 
-   * @param name
-   *          the counter name
-   * @return return the value of the given counter.
-   */
-  protected Double getDoubleValue(Object name) {
-    return this.doubleCounters.get(name);
-  }
-
-  /**
-   * Increment the given counter by the given incremental value If the counter
-   * does not exist, one is created with value 0.
-   * 
-   * @param name
-   *          the counter name
-   * @param inc
-   *          the incremental value
-   * @return the updated value.
-   */
-  protected Long addLongValue(Object name, long inc) {
-    Long val = this.longCounters.get(name);
-    Long retv = null;
-    if (val == null) {
-      retv = new Long(inc);
-    } else {
-      retv = new Long(val.longValue() + inc);
-    }
-    this.longCounters.put(name, retv);
-    return retv;
-  }
-
-  /**
-   * Increment the given counter by the given incremental value If the counter
-   * does not exist, one is created with value 0.
-   * 
-   * @param name
-   *          the counter name
-   * @param inc
-   *          the incremental value
-   * @return the updated value.
-   */
-  protected Double addDoubleValue(Object name, double inc) {
-    Double val = this.doubleCounters.get(name);
-    Double retv = null;
-    if (val == null) {
-      retv = new Double(inc);
-    } else {
-      retv = new Double(val.doubleValue() + inc);
-    }
-    this.doubleCounters.put(name, retv);
-    return retv;
-  }
-
-  /**
-   * log the counters
-   * 
-   */
-  protected void report() {
-    LOG.info(getReport());
-  }
-
-  /**
-   * log the counters
-   * 
-   */
-  protected String getReport() {
-    StringBuffer sb = new StringBuffer();
-
-    Iterator iter = this.longCounters.entrySet().iterator();
-    while (iter.hasNext()) {
-      Entry e = (Entry) iter.next();
-      sb.append(e.getKey().toString()).append("\t").append(e.getValue())
-        .append("\n");
-    }
-    iter = this.doubleCounters.entrySet().iterator();
-    while (iter.hasNext()) {
-      Entry e = (Entry) iter.next();
-      sb.append(e.getKey().toString()).append("\t").append(e.getValue())
-        .append("\n");
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Initializes a new instance from a {@link JobConf}.
-   * 
-   * @param job
-   *          the configuration
-   */
-  public void configure(JobConf job) {
-    this.longCounters = new TreeMap<Object, Long>();
-    this.doubleCounters = new TreeMap<Object, Double>();
-  }
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java
deleted file mode 100644
index f20f2e4..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-/**
- * This defines an iterator interface that will help the reducer class
- * re-group its input by source tags. Once the values are re-grouped,
- * the reducer will receive the cross product of values from different groups.
- */
-public interface ResetableIterator extends Iterator {
-  public void reset();
-
-  public void add(Object item);
-
-  public void close() throws IOException;
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java b/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java
deleted file mode 100644
index 71a5d85..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.JobConf;
-
-/**
- * This abstract class serves as the base class for the values that 
- * flow from the mappers to the reducers in a data join job. 
- * Typically, in such a job, the mappers will compute the source
- * tag of an input record based on its attributes or based on the 
- * file name of the input file. This tag will be used by the reducers
- * to re-group the values of a given key according to their source tags.
- * 
- */
-public abstract class TaggedMapOutput implements Writable {
-  protected Text tag;
-
-  public TaggedMapOutput() {
-    this.tag = new Text("");
-  }
-
-  public Text getTag() {
-    return tag;
-  }
-
-  public void setTag(Text tag) {
-    this.tag = tag;
-  }
-
-  public abstract Writable getData();
-  
-  public TaggedMapOutput clone(JobConf job) {
-    return (TaggedMapOutput) WritableUtils.clone(this, job);
-  }
-
-}
diff --git a/hadoop-mapreduce-project/src/contrib/data_join/src/test/org/apache/hadoop/contrib/utils/join/TestDataJoin.java b/hadoop-mapreduce-project/src/contrib/data_join/src/test/org/apache/hadoop/contrib/utils/join/TestDataJoin.java
deleted file mode 100644
index 1eab959..0000000
--- a/hadoop-mapreduce-project/src/contrib/data_join/src/test/org/apache/hadoop/contrib/utils/join/TestDataJoin.java
+++ /dev/null
@@ -1,154 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.contrib.utils.join;
-
-import java.io.IOException;
-
-import junit.framework.Test;
-import junit.framework.TestCase;
-import junit.framework.TestSuite;
-import junit.extensions.TestSetup;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.*;
-
-public class TestDataJoin extends TestCase {
-
-  private static MiniDFSCluster cluster = null;
-  public static Test suite() {
-    TestSetup setup = new TestSetup(new TestSuite(TestDataJoin.class)) {
-      protected void setUp() throws Exception {
-        Configuration conf = new Configuration();
-        cluster = new MiniDFSCluster(conf, 2, true, null);
-      }
-      protected void tearDown() throws Exception {
-        if (cluster != null) {
-          cluster.shutdown();
-        }
-      }
-    };
-    return setup;
-  }
-
-  public void testDataJoin() throws Exception {
-    final int srcs = 4;
-    JobConf job = new JobConf();
-    Path base = cluster.getFileSystem().makeQualified(new Path("/inner"));
-    Path[] src = writeSimpleSrc(base, job, srcs);
-    job.setInputFormat(SequenceFileInputFormat.class);
-    Path outdir = new Path(base, "out");
-    FileOutputFormat.setOutputPath(job, outdir);
-
-    job.setMapperClass(SampleDataJoinMapper.class);
-    job.setReducerClass(SampleDataJoinReducer.class);
-    job.setMapOutputKeyClass(Text.class);
-    job.setMapOutputValueClass(SampleTaggedMapOutput.class);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(Text.class);
-    job.setOutputFormat(TextOutputFormat.class);
-    job.setNumMapTasks(1);
-    job.setNumReduceTasks(1);
-    FileInputFormat.setInputPaths(job, src);
-    try {
-      JobClient.runJob(job);
-      confirmOutput(outdir, job, srcs);
-    } finally {
-      base.getFileSystem(job).delete(base, true);
-    }
-  }
-
-  private static void confirmOutput(Path out, JobConf job, int srcs)
-      throws IOException {
-    FileSystem fs = out.getFileSystem(job);
-    FileStatus[] outlist = fs.listStatus(out);
-    assertEquals(1, outlist.length);
-    assertTrue(0 < outlist[0].getLen());
-    FSDataInputStream in = fs.open(outlist[0].getPath());
-    LineRecordReader rr = new LineRecordReader(in, 0, Integer.MAX_VALUE, job);
-    LongWritable k = new LongWritable();
-    Text v = new Text();
-    int count = 0;
-    while (rr.next(k, v)) {
-      String[] vals = v.toString().split("\t");
-      assertEquals(srcs + 1, vals.length);
-      int[] ivals = new int[vals.length];
-      for (int i = 0; i < vals.length; ++i)
-        ivals[i] = Integer.parseInt(vals[i]);
-      assertEquals(0, ivals[0] % (srcs * srcs));
-      for (int i = 1; i < vals.length; ++i) {
-        assertEquals((ivals[i] - (i - 1)) * srcs, 10 * ivals[0]);
-      }
-      ++count;
-    }
-    assertEquals(4, count);
-  }
-
-  private static SequenceFile.Writer[] createWriters(Path testdir,
-      JobConf conf, int srcs, Path[] src) throws IOException {
-    for (int i = 0; i < srcs; ++i) {
-      src[i] = new Path(testdir, Integer.toString(i + 10, 36));
-    }
-    SequenceFile.Writer out[] = new SequenceFile.Writer[srcs];
-    for (int i = 0; i < srcs; ++i) {
-      out[i] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf,
-          src[i], Text.class, Text.class);
-    }
-    return out;
-  }
-
-  private static Path[] writeSimpleSrc(Path testdir, JobConf conf,
-      int srcs) throws IOException {
-    SequenceFile.Writer out[] = null;
-    Path[] src = new Path[srcs];
-    try {
-      out = createWriters(testdir, conf, srcs, src);
-      final int capacity = srcs * 2 + 1;
-      Text key = new Text();
-      key.set("ignored");
-      Text val = new Text();
-      for (int k = 0; k < capacity; ++k) {
-        for (int i = 0; i < srcs; ++i) {
-          val.set(Integer.toString(k % srcs == 0 ? k * srcs : k * srcs + i) +
-              "\t" + Integer.toString(10 * k + i));
-          out[i].append(key, val);
-          if (i == k) {
-            // add duplicate key
-            out[i].append(key, val);
-          }
-        }
-      }
-    } finally {
-      if (out != null) {
-        for (int i = 0; i < srcs; ++i) {
-          if (out[i] != null)
-            out[i].close();
-        }
-      }
-    }
-    return src;
-  }
-}
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index 6fe4c79..05e4d16 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -238,6 +238,11 @@
       </dependency>
       <dependency>
         <groupId>org.apache.hadoop</groupId>
+        <artifactId>hadoop-datajoin</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>org.apache.hadoop</groupId>
         <artifactId>hadoop-rumen</artifactId>
         <version>${project.version}</version>
       </dependency>
diff --git a/hadoop-tools/hadoop-datajoin/pom.xml b/hadoop-tools/hadoop-datajoin/pom.xml
new file mode 100644
index 0000000..984682f
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/pom.xml
@@ -0,0 +1,120 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+<project>
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.hadoop</groupId>
+    <artifactId>hadoop-project</artifactId>
+    <version>3.0.0-SNAPSHOT</version>
+    <relativePath>../../hadoop-project</relativePath>
+  </parent>
+  <groupId>org.apache.hadoop</groupId>
+  <artifactId>hadoop-datajoin</artifactId>
+  <version>3.0.0-SNAPSHOT</version>
+  <description>Apache Hadoop Data Join</description>
+  <name>Apache Hadoop Data Join</name>
+  <packaging>jar</packaging>
+
+  <properties>
+    <hadoop.log.dir>${project.build.directory}/log</hadoop.log.dir>
+  </properties>
+
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-annotations</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-hs</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-core</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-common</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-common</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <scope>test</scope>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-server-tests</artifactId>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+  </dependencies>
+
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-antrun-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>create-log-dir</id>
+            <phase>process-test-resources</phase>
+            <goals>
+              <goal>run</goal>
+            </goals>
+            <configuration>
+              <target>
+                <delete dir="${test.build.data}"/>
+                <mkdir dir="${test.build.data}"/>
+                <mkdir dir="${hadoop.log.dir}"/>
+              </target>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+       </plugin>
+    </plugins>
+  </build>
+</project>
+
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java
new file mode 100644
index 0000000..1ffd4b5
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java
@@ -0,0 +1,70 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+
+/**
+ * This class provides an implementation of ResetableIterator. The
+ * implementation will be based on ArrayList.
+ * 
+ * 
+ */
+public class ArrayListBackedIterator implements ResetableIterator {
+
+  private Iterator iter;
+
+  private ArrayList<Object> data;
+
+  public ArrayListBackedIterator() {
+    this(new ArrayList<Object>());
+  }
+
+  public ArrayListBackedIterator(ArrayList<Object> data) {
+    this.data = data;
+    this.iter = this.data.iterator();
+  }
+
+  public void add(Object item) {
+    this.data.add(item);
+  }
+
+  public boolean hasNext() {
+    return this.iter.hasNext();
+  }
+
+  public Object next() {
+    return this.iter.next();
+  }
+
+  public void remove() {
+
+  }
+
+  public void reset() {
+    this.iter = this.data.iterator();
+  }
+
+  public void close() throws IOException {
+    this.iter = null;
+    this.data = null;
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java
new file mode 100644
index 0000000..9a1b8f1
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java
@@ -0,0 +1,174 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.SequenceFileInputFormat;
+import org.apache.hadoop.mapred.SequenceFileOutputFormat;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.TextOutputFormat;
+import org.apache.hadoop.mapred.JobID;
+
+/**
+ * This class implements the main function for creating a map/reduce
+ * job to join data of different sources. To create sucn a job, the 
+ * user must implement a mapper class that extends DataJoinMapperBase class,
+ * and a reducer class that extends DataJoinReducerBase. 
+ * 
+ */
+public class DataJoinJob {
+
+  public static Class getClassByName(String className) {
+    Class retv = null;
+    try {
+      ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
+      retv = Class.forName(className, true, classLoader);
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+    return retv;
+  }
+
+  public static JobConf createDataJoinJob(String args[]) throws IOException {
+
+    String inputDir = args[0];
+    String outputDir = args[1];
+    Class inputFormat = SequenceFileInputFormat.class;
+    if (args[2].compareToIgnoreCase("text") != 0) {
+      System.out.println("Using SequenceFileInputFormat: " + args[2]);
+    } else {
+      System.out.println("Using TextInputFormat: " + args[2]);
+      inputFormat = TextInputFormat.class;
+    }
+    int numOfReducers = Integer.parseInt(args[3]);
+    Class mapper = getClassByName(args[4]);
+    Class reducer = getClassByName(args[5]);
+    Class mapoutputValueClass = getClassByName(args[6]);
+    Class outputFormat = TextOutputFormat.class;
+    Class outputValueClass = Text.class;
+    if (args[7].compareToIgnoreCase("text") != 0) {
+      System.out.println("Using SequenceFileOutputFormat: " + args[7]);
+      outputFormat = SequenceFileOutputFormat.class;
+      outputValueClass = getClassByName(args[7]);
+    } else {
+      System.out.println("Using TextOutputFormat: " + args[7]);
+    }
+    long maxNumOfValuesPerGroup = 100;
+    String jobName = "";
+    if (args.length > 8) {
+      maxNumOfValuesPerGroup = Long.parseLong(args[8]);
+    }
+    if (args.length > 9) {
+      jobName = args[9];
+    }
+    Configuration defaults = new Configuration();
+    JobConf job = new JobConf(defaults, DataJoinJob.class);
+    job.setJobName("DataJoinJob: " + jobName);
+
+    FileSystem fs = FileSystem.get(defaults);
+    fs.delete(new Path(outputDir), true);
+    FileInputFormat.setInputPaths(job, inputDir);
+
+    job.setInputFormat(inputFormat);
+
+    job.setMapperClass(mapper);
+    FileOutputFormat.setOutputPath(job, new Path(outputDir));
+    job.setOutputFormat(outputFormat);
+    SequenceFileOutputFormat.setOutputCompressionType(job,
+            SequenceFile.CompressionType.BLOCK);
+    job.setMapOutputKeyClass(Text.class);
+    job.setMapOutputValueClass(mapoutputValueClass);
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(outputValueClass);
+    job.setReducerClass(reducer);
+
+    job.setNumMapTasks(1);
+    job.setNumReduceTasks(numOfReducers);
+    job.setLong("datajoin.maxNumOfValuesPerGroup", maxNumOfValuesPerGroup);
+    return job;
+  }
+
+  /**
+   * Submit/run a map/reduce job.
+   * 
+   * @param job
+   * @return true for success
+   * @throws IOException
+   */
+  public static boolean runJob(JobConf job) throws IOException {
+    JobClient jc = new JobClient(job);
+    boolean sucess = true;
+    RunningJob running = null;
+    try {
+      running = jc.submitJob(job);
+      JobID jobId = running.getID();
+      System.out.println("Job " + jobId + " is submitted");
+      while (!running.isComplete()) {
+        System.out.println("Job " + jobId + " is still running.");
+        try {
+          Thread.sleep(60000);
+        } catch (InterruptedException e) {
+        }
+        running = jc.getJob(jobId);
+      }
+      sucess = running.isSuccessful();
+    } finally {
+      if (!sucess && (running != null)) {
+        running.killJob();
+      }
+      jc.close();
+    }
+    return sucess;
+  }
+
+  /**
+   * @param args
+   */
+  public static void main(String[] args) {
+    boolean success;
+    if (args.length < 8 || args.length > 10) {
+      System.out.println("usage: DataJoinJob " + "inputdirs outputdir map_input_file_format "
+                         + "numofParts " + "mapper_class " + "reducer_class "
+                         + "map_output_value_class "
+                         + "output_value_class [maxNumOfValuesPerGroup [descriptionOfJob]]]");
+      System.exit(-1);
+    }
+
+    try {
+      JobConf job = DataJoinJob.createDataJoinJob(args);
+      success = DataJoinJob.runJob(job);
+      if (!success) {
+        System.out.println("Job failed");
+      }
+    } catch (IOException ioe) {
+      ioe.printStackTrace();
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java
new file mode 100644
index 0000000..1e1296a
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+
+/**
+ * This abstract class serves as the base class for the mapper class of a data
+ * join job. This class expects its subclasses to implement methods for the
+ * following functionalities:
+ * 
+ * 1. Compute the source tag of input values 2. Compute the map output value
+ * object 3. Compute the map output key object
+ * 
+ * The source tag will be used by the reducer to determine from which source
+ * (which table in SQL terminology) a value comes. Computing the map output
+ * value object amounts to performing projecting/filtering work in a SQL
+ * statement (through the select/where clauses). Computing the map output key
+ * amounts to choosing the join key. This class provides the appropriate plugin
+ * points for the user defined subclasses to implement the appropriate logic.
+ * 
+ */
+public abstract class DataJoinMapperBase extends JobBase {
+
+  protected String inputFile = null;
+
+  protected JobConf job = null;
+
+  protected Text inputTag = null;
+
+  protected Reporter reporter = null;
+
+  public void configure(JobConf job) {
+    super.configure(job);
+    this.job = job;
+    this.inputFile = job.get(MRJobConfig.MAP_INPUT_FILE);
+    this.inputTag = generateInputTag(this.inputFile);
+  }
+
+  /**
+   * Determine the source tag based on the input file name.
+   * 
+   * @param inputFile
+   * @return the source tag computed from the given file name.
+   */
+  protected abstract Text generateInputTag(String inputFile);
+
+  /**
+   * Generate a tagged map output value. The user code can also perform
+   * projection/filtering. If it decides to discard the input record when
+   * certain conditions are met,it can simply return a null.
+   * 
+   * @param value
+   * @return an object of TaggedMapOutput computed from the given value.
+   */
+  protected abstract TaggedMapOutput generateTaggedMapOutput(Object value);
+
+  /**
+   * Generate a map output key. The user code can compute the key
+   * programmatically, not just selecting the values of some fields. In this
+   * sense, it is more general than the joining capabilities of SQL.
+   * 
+   * @param aRecord
+   * @return the group key for the given record
+   */
+  protected abstract Text generateGroupKey(TaggedMapOutput aRecord);
+
+  public void map(Object key, Object value,
+                  OutputCollector output, Reporter reporter) throws IOException {
+    if (this.reporter == null) {
+      this.reporter = reporter;
+    }
+    addLongValue("totalCount", 1);
+    TaggedMapOutput aRecord = generateTaggedMapOutput(value);
+    if (aRecord == null) {
+      addLongValue("discardedCount", 1);
+      return;
+    }
+    Text groupKey = generateGroupKey(aRecord);
+    if (groupKey == null) {
+      addLongValue("nullGroupKeyCount", 1);
+      return;
+    }
+    output.collect(groupKey, aRecord);
+    addLongValue("collectedCount", 1);
+  }
+
+  public void close() throws IOException {
+    if (this.reporter != null) {
+      this.reporter.setStatus(super.getReport());
+    }
+  }
+
+  public void reduce(Object arg0, Iterator arg1,
+                     OutputCollector arg2, Reporter arg3) throws IOException {
+    // TODO Auto-generated method stub
+
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java
new file mode 100644
index 0000000..e340d79
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java
@@ -0,0 +1,237 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reporter;
+
+/**
+ * This abstract class serves as the base class for the reducer class of a data
+ * join job. The reduce function will first group the values according to their
+ * input tags, and then compute the cross product of over the groups. For each
+ * tuple in the cross product, it calls the following method, which is expected
+ * to be implemented in a subclass.
+ * 
+ * protected abstract TaggedMapOutput combine(Object[] tags, Object[] values);
+ * 
+ * The above method is expected to produce one output value from an array of
+ * records of different sources. The user code can also perform filtering here.
+ * It can return null if it decides to the records do not meet certain
+ * conditions.
+ * 
+ */
+public abstract class DataJoinReducerBase extends JobBase {
+
+  protected Reporter reporter = null;
+
+  private long maxNumOfValuesPerGroup = 100;
+
+  protected long largestNumOfValues = 0;
+
+  protected long numOfValues = 0;
+
+  protected long collected = 0;
+
+  protected JobConf job;
+
+  public void close() throws IOException {
+    if (this.reporter != null) {
+      this.reporter.setStatus(super.getReport());
+    }
+  }
+
+  public void configure(JobConf job) {
+    super.configure(job);
+    this.job = job;
+    this.maxNumOfValuesPerGroup = job.getLong("datajoin.maxNumOfValuesPerGroup", 100);
+  }
+
+  /**
+   * The subclass can provide a different implementation on ResetableIterator.
+   * This is necessary if the number of values in a reduce call is very high.
+   * 
+   * The default provided here uses ArrayListBackedIterator
+   * 
+   * @return an Object of ResetableIterator.
+   */
+  protected ResetableIterator createResetableIterator() {
+    return new ArrayListBackedIterator();
+  }
+
+  /**
+   * This is the function that re-groups values for a key into sub-groups based
+   * on a secondary key (input tag).
+   * 
+   * @param arg1
+   * @return
+   */
+  private SortedMap<Object, ResetableIterator> regroup(Object key,
+                                                       Iterator arg1, Reporter reporter) throws IOException {
+    this.numOfValues = 0;
+    SortedMap<Object, ResetableIterator> retv = new TreeMap<Object, ResetableIterator>();
+    TaggedMapOutput aRecord = null;
+    while (arg1.hasNext()) {
+      this.numOfValues += 1;
+      if (this.numOfValues % 100 == 0) {
+        reporter.setStatus("key: " + key.toString() + " numOfValues: "
+                           + this.numOfValues);
+      }
+      if (this.numOfValues > this.maxNumOfValuesPerGroup) {
+        continue;
+      }
+      aRecord = ((TaggedMapOutput) arg1.next()).clone(job);
+      Text tag = aRecord.getTag();
+      ResetableIterator data = retv.get(tag);
+      if (data == null) {
+        data = createResetableIterator();
+        retv.put(tag, data);
+      }
+      data.add(aRecord);
+    }
+    if (this.numOfValues > this.largestNumOfValues) {
+      this.largestNumOfValues = numOfValues;
+      LOG.info("key: " + key.toString() + " this.largestNumOfValues: "
+               + this.largestNumOfValues);
+    }
+    return retv;
+  }
+
+  public void reduce(Object key, Iterator values,
+                     OutputCollector output, Reporter reporter) throws IOException {
+    if (this.reporter == null) {
+      this.reporter = reporter;
+    }
+
+    SortedMap<Object, ResetableIterator> groups = regroup(key, values, reporter);
+    Object[] tags = groups.keySet().toArray();
+    ResetableIterator[] groupValues = new ResetableIterator[tags.length];
+    for (int i = 0; i < tags.length; i++) {
+      groupValues[i] = groups.get(tags[i]);
+    }
+    joinAndCollect(tags, groupValues, key, output, reporter);
+    addLongValue("groupCount", 1);
+    for (int i = 0; i < tags.length; i++) {
+      groupValues[i].close();
+    }
+  }
+
+  /**
+   * The subclass can overwrite this method to perform additional filtering
+   * and/or other processing logic before a value is collected.
+   * 
+   * @param key
+   * @param aRecord
+   * @param output
+   * @param reporter
+   * @throws IOException
+   */
+  protected void collect(Object key, TaggedMapOutput aRecord,
+                         OutputCollector output, Reporter reporter) throws IOException {
+    this.collected += 1;
+    addLongValue("collectedCount", 1);
+    if (aRecord != null) {
+      output.collect(key, aRecord.getData());
+      reporter.setStatus("key: " + key.toString() + " collected: " + collected);
+      addLongValue("actuallyCollectedCount", 1);
+    }
+  }
+
+  /**
+   * join the list of the value lists, and collect the results.
+   * 
+   * @param tags
+   *          a list of input tags
+   * @param values
+   *          a list of value lists, each corresponding to one input source
+   * @param key
+   * @param output
+   * @throws IOException
+   */
+  private void joinAndCollect(Object[] tags, ResetableIterator[] values,
+                              Object key, OutputCollector output, Reporter reporter)
+    throws IOException {
+    if (values.length < 1) {
+      return;
+    }
+    Object[] partialList = new Object[values.length];
+    joinAndCollect(tags, values, 0, partialList, key, output, reporter);
+  }
+
+  /**
+   * Perform the actual join recursively.
+   * 
+   * @param tags
+   *          a list of input tags
+   * @param values
+   *          a list of value lists, each corresponding to one input source
+   * @param pos
+   *          indicating the next value list to be joined
+   * @param partialList
+   *          a list of values, each from one value list considered so far.
+   * @param key
+   * @param output
+   * @throws IOException
+   */
+  private void joinAndCollect(Object[] tags, ResetableIterator[] values,
+                              int pos, Object[] partialList, Object key,
+                              OutputCollector output, Reporter reporter) throws IOException {
+
+    if (values.length == pos) {
+      // get a value from each source. Combine them
+      TaggedMapOutput combined = combine(tags, partialList);
+      collect(key, combined, output, reporter);
+      return;
+    }
+    ResetableIterator nextValues = values[pos];
+    nextValues.reset();
+    while (nextValues.hasNext()) {
+      Object v = nextValues.next();
+      partialList[pos] = v;
+      joinAndCollect(tags, values, pos + 1, partialList, key, output, reporter);
+    }
+  }
+
+  public static Text SOURCE_TAGS_FIELD = new Text("SOURCE_TAGS");
+
+  public static Text NUM_OF_VALUES_FIELD = new Text("NUM_OF_VALUES");
+
+  /**
+   * 
+   * @param tags
+   *          a list of source tags
+   * @param values
+   *          a value per source
+   * @return combined value derived from values of the sources
+   */
+  protected abstract TaggedMapOutput combine(Object[] tags, Object[] values);
+
+  public void map(Object arg0, Object arg1, OutputCollector arg2,
+                  Reporter arg3) throws IOException {
+    // TODO Auto-generated method stub
+
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/JobBase.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/JobBase.java
new file mode 100644
index 0000000..9ef21b3
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/JobBase.java
@@ -0,0 +1,173 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.Map.Entry;
+import java.util.Iterator;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.Reducer;
+
+/**
+ * A common base implementing some statics collecting mechanisms that are
+ * commonly used in a typical map/reduce job.
+ * 
+ */
+public abstract class JobBase implements Mapper, Reducer {
+
+  public static final Log LOG = LogFactory.getLog("datajoin.job");
+
+  private SortedMap<Object, Long> longCounters = null;
+
+  private SortedMap<Object, Double> doubleCounters = null;
+
+  /**
+   * Set the given counter to the given value
+   * 
+   * @param name
+   *          the counter name
+   * @param value
+   *          the value for the counter
+   */
+  protected void setLongValue(Object name, long value) {
+    this.longCounters.put(name, Long.valueOf(value));
+  }
+
+  /**
+   * Set the given counter to the given value
+   * 
+   * @param name
+   *          the counter name
+   * @param value
+   *          the value for the counter
+   */
+  protected void setDoubleValue(Object name, double value) {
+    this.doubleCounters.put(name, new Double(value));
+  }
+
+  /**
+   * 
+   * @param name
+   *          the counter name
+   * @return return the value of the given counter.
+   */
+  protected Long getLongValue(Object name) {
+    return this.longCounters.get(name);
+  }
+
+  /**
+   * 
+   * @param name
+   *          the counter name
+   * @return return the value of the given counter.
+   */
+  protected Double getDoubleValue(Object name) {
+    return this.doubleCounters.get(name);
+  }
+
+  /**
+   * Increment the given counter by the given incremental value If the counter
+   * does not exist, one is created with value 0.
+   * 
+   * @param name
+   *          the counter name
+   * @param inc
+   *          the incremental value
+   * @return the updated value.
+   */
+  protected Long addLongValue(Object name, long inc) {
+    Long val = this.longCounters.get(name);
+    Long retv = null;
+    if (val == null) {
+      retv = Long.valueOf(inc);
+    } else {
+      retv = Long.valueOf(val.longValue() + inc);
+    }
+    this.longCounters.put(name, retv);
+    return retv;
+  }
+
+  /**
+   * Increment the given counter by the given incremental value If the counter
+   * does not exist, one is created with value 0.
+   * 
+   * @param name
+   *          the counter name
+   * @param inc
+   *          the incremental value
+   * @return the updated value.
+   */
+  protected Double addDoubleValue(Object name, double inc) {
+    Double val = this.doubleCounters.get(name);
+    Double retv = null;
+    if (val == null) {
+      retv = new Double(inc);
+    } else {
+      retv = new Double(val.doubleValue() + inc);
+    }
+    this.doubleCounters.put(name, retv);
+    return retv;
+  }
+
+  /**
+   * log the counters
+   * 
+   */
+  protected void report() {
+    LOG.info(getReport());
+  }
+
+  /**
+   * log the counters
+   * 
+   */
+  protected String getReport() {
+    StringBuffer sb = new StringBuffer();
+
+    Iterator iter = this.longCounters.entrySet().iterator();
+    while (iter.hasNext()) {
+      Entry e = (Entry) iter.next();
+      sb.append(e.getKey().toString()).append("\t").append(e.getValue())
+        .append("\n");
+    }
+    iter = this.doubleCounters.entrySet().iterator();
+    while (iter.hasNext()) {
+      Entry e = (Entry) iter.next();
+      sb.append(e.getKey().toString()).append("\t").append(e.getValue())
+        .append("\n");
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Initializes a new instance from a {@link JobConf}.
+   * 
+   * @param job
+   *          the configuration
+   */
+  public void configure(JobConf job) {
+    this.longCounters = new TreeMap<Object, Long>();
+    this.doubleCounters = new TreeMap<Object, Double>();
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java
new file mode 100644
index 0000000..f20f2e4
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.IOException;
+import java.util.Iterator;
+
+/**
+ * This defines an iterator interface that will help the reducer class
+ * re-group its input by source tags. Once the values are re-grouped,
+ * the reducer will receive the cross product of values from different groups.
+ */
+public interface ResetableIterator extends Iterator {
+  public void reset();
+
+  public void add(Object item);
+
+  public void close() throws IOException;
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java
new file mode 100644
index 0000000..71a5d85
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapred.JobConf;
+
+/**
+ * This abstract class serves as the base class for the values that 
+ * flow from the mappers to the reducers in a data join job. 
+ * Typically, in such a job, the mappers will compute the source
+ * tag of an input record based on its attributes or based on the 
+ * file name of the input file. This tag will be used by the reducers
+ * to re-group the values of a given key according to their source tags.
+ * 
+ */
+public abstract class TaggedMapOutput implements Writable {
+  protected Text tag;
+
+  public TaggedMapOutput() {
+    this.tag = new Text("");
+  }
+
+  public Text getTag() {
+    return tag;
+  }
+
+  public void setTag(Text tag) {
+    this.tag = tag;
+  }
+
+  public abstract Writable getData();
+  
+  public TaggedMapOutput clone(JobConf job) {
+    return (TaggedMapOutput) WritableUtils.clone(this, job);
+  }
+
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/test/java/README.txt b/hadoop-tools/hadoop-datajoin/src/test/java/README.txt
new file mode 100644
index 0000000..73fd6ef
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/test/java/README.txt
@@ -0,0 +1,50 @@
+*************************************************
+*** Input Files (Note: tab-separated columns) ***
+*************************************************
+[:~]$ cat datajoin/input/A
+A.a11   A.a12
+A.a21   A.a22
+B.a21   A.a32
+A.a31   A.a32
+B.a31   A.a32
+
+[:~]$ cat datajoin/input/B
+A.a11   B.a12
+A.a11   B.a13
+B.a11   B.a12
+B.a21   B.a22
+A.a31   B.a32
+B.a31   B.a32
+
+
+*****************************
+*** Invoke SampleDataJoin ***
+*****************************
+[:~]$ $HADOOP_PREFIX/bin/hadoop jar hadoop-datajoin-examples.jar org.apache.hadoop.contrib.utils.join.DataJoinJob datajoin/input datajoin/output Text 1 org.apache.hadoop.contrib.utils.join.SampleDataJoinMapper org.apache.hadoop.contrib.utils.join.SampleDataJoinReducer org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput Text
+Using TextInputFormat: Text
+Using TextOutputFormat: Text
+07/06/01 19:58:23 INFO mapred.FileInputFormat: Total input paths to process : 2
+Job job_kkzk08 is submitted
+Job job_kkzk08 is still running.
+07/06/01 19:58:24 INFO mapred.LocalJobRunner: collectedCount    5
+totalCount      5
+
+07/06/01 19:58:24 INFO mapred.LocalJobRunner: collectedCount    6
+totalCount      6
+
+07/06/01 19:58:24 INFO datajoin.job: key: A.a11 this.largestNumOfValues: 3
+07/06/01 19:58:24 INFO mapred.LocalJobRunner: actuallyCollectedCount    5
+collectedCount  7
+groupCount      6
+ > reduce
+
+
+*******************
+*** Output File ***
+*******************
+[:~]$ cat datajoin/output/part-00000  
+A.a11   A.a12   B.a12
+A.a11   A.a12   B.a13
+A.a31   A.a32   B.a32
+B.a21   A.a32   B.a22
+B.a31   A.a32   B.a32
diff --git a/hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinMapper.java b/hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinMapper.java
new file mode 100644
index 0000000..3f1d4f0
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinMapper.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.contrib.utils.join.DataJoinMapperBase;
+import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
+import org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput;
+
+/**
+ * This is a subclass of DataJoinMapperBase that is used to
+ * demonstrate the functionality of INNER JOIN between 2 data
+ * sources (TAB separated text files) based on the first column.
+ */
+public class SampleDataJoinMapper extends DataJoinMapperBase {
+
+
+  protected Text generateInputTag(String inputFile) {
+    // tag the row with input file name (data source)
+    return new Text(inputFile);
+  }
+
+  protected Text generateGroupKey(TaggedMapOutput aRecord) {
+    // first column in the input tab separated files becomes the key (to perform the JOIN)
+    String line = ((Text) aRecord.getData()).toString();
+    String groupKey = "";
+    String[] tokens = line.split("\\t", 2);
+    groupKey = tokens[0];
+    return new Text(groupKey);
+  }
+
+  protected TaggedMapOutput generateTaggedMapOutput(Object value) {
+    TaggedMapOutput retv = new SampleTaggedMapOutput((Text) value);
+    retv.setTag(new Text(this.inputTag));
+    return retv;
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinReducer.java b/hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinReducer.java
new file mode 100644
index 0000000..f8eb00d
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/test/java/SampleDataJoinReducer.java
@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.contrib.utils.join.DataJoinReducerBase;
+import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
+
+/**
+ * This is a subclass of DataJoinReducerBase that is used to
+ * demonstrate the functionality of INNER JOIN between 2 data
+ * sources (TAB separated text files) based on the first column.
+ */
+public class SampleDataJoinReducer extends DataJoinReducerBase {
+
+  /**
+   * 
+   * @param tags
+   *          a list of source tags
+   * @param values
+   *          a value per source
+   * @return combined value derived from values of the sources
+   */
+  protected TaggedMapOutput combine(Object[] tags, Object[] values) {
+    // eliminate rows which didnot match in one of the two tables (for INNER JOIN)
+    if (tags.length < 2)
+       return null;  
+    String joinedStr = ""; 
+    for (int i=0; i<tags.length; i++) {
+      if (i > 0)
+         joinedStr += "\t";
+      // strip first column as it is the key on which we joined
+      String line = ((Text) (((TaggedMapOutput) values[i]).getData())).toString();
+      String[] tokens = line.split("\\t", 2);
+      joinedStr += tokens[1];
+    }
+    TaggedMapOutput retv = new SampleTaggedMapOutput(new Text(joinedStr));
+    retv.setTag((Text) tags[0]); 
+    return retv;
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/test/java/SampleTaggedMapOutput.java b/hadoop-tools/hadoop-datajoin/src/test/java/SampleTaggedMapOutput.java
new file mode 100644
index 0000000..59f1bd1
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/test/java/SampleTaggedMapOutput.java
@@ -0,0 +1,60 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
+
+/**
+ * This is a subclass of TaggedMapOutput that is used to
+ * demonstrate the functionality of INNER JOIN between 2 data
+ * sources (TAB separated text files) based on the first column.
+ */
+public class SampleTaggedMapOutput extends TaggedMapOutput {
+
+  private Text data;
+
+  public SampleTaggedMapOutput() {
+    this.data = new Text("");
+  }
+
+  public SampleTaggedMapOutput(Text data) {
+    this.data = data;
+  }
+
+  public Writable getData() {
+    return data;
+  }
+
+  public void write(DataOutput out) throws IOException {
+    this.tag.write(out);
+    this.data.write(out);
+  }
+
+  public void readFields(DataInput in) throws IOException {
+    this.tag.readFields(in);
+    this.data.readFields(in);
+  }
+}
diff --git a/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/TestDataJoin.java b/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/TestDataJoin.java
new file mode 100644
index 0000000..37df5b0
--- /dev/null
+++ b/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/TestDataJoin.java
@@ -0,0 +1,155 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.contrib.utils.join;
+
+import java.io.IOException;
+
+import junit.framework.Test;
+import junit.framework.TestCase;
+import junit.framework.TestSuite;
+import junit.extensions.TestSetup;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.*;
+
+public class TestDataJoin extends TestCase {
+
+  private static MiniDFSCluster cluster = null;
+  public static Test suite() {
+    TestSetup setup = new TestSetup(new TestSuite(TestDataJoin.class)) {
+      protected void setUp() throws Exception {
+        Configuration conf = new Configuration();
+        cluster = new MiniDFSCluster(conf, 2, true, null);
+      }
+      protected void tearDown() throws Exception {
+        if (cluster != null) {
+          cluster.shutdown();
+        }
+      }
+    };
+    return setup;
+  }
+
+  public void testDataJoin() throws Exception {
+    final int srcs = 4;
+    JobConf job = new JobConf();
+    job.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);
+    Path base = cluster.getFileSystem().makeQualified(new Path("/inner"));
+    Path[] src = writeSimpleSrc(base, job, srcs);
+    job.setInputFormat(SequenceFileInputFormat.class);
+    Path outdir = new Path(base, "out");
+    FileOutputFormat.setOutputPath(job, outdir);
+
+    job.setMapperClass(SampleDataJoinMapper.class);
+    job.setReducerClass(SampleDataJoinReducer.class);
+    job.setMapOutputKeyClass(Text.class);
+    job.setMapOutputValueClass(SampleTaggedMapOutput.class);
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(Text.class);
+    job.setOutputFormat(TextOutputFormat.class);
+    job.setNumMapTasks(1);
+    job.setNumReduceTasks(1);
+    FileInputFormat.setInputPaths(job, src);
+    try {
+      JobClient.runJob(job);
+      confirmOutput(outdir, job, srcs);
+    } finally {
+      base.getFileSystem(job).delete(base, true);
+    }
+  }
+
+  private static void confirmOutput(Path out, JobConf job, int srcs)
+      throws IOException {
+    FileSystem fs = out.getFileSystem(job);
+    FileStatus[] outlist = fs.listStatus(out);
+    assertEquals(1, outlist.length);
+    assertTrue(0 < outlist[0].getLen());
+    FSDataInputStream in = fs.open(outlist[0].getPath());
+    LineRecordReader rr = new LineRecordReader(in, 0, Integer.MAX_VALUE, job);
+    LongWritable k = new LongWritable();
+    Text v = new Text();
+    int count = 0;
+    while (rr.next(k, v)) {
+      String[] vals = v.toString().split("\t");
+      assertEquals(srcs + 1, vals.length);
+      int[] ivals = new int[vals.length];
+      for (int i = 0; i < vals.length; ++i)
+        ivals[i] = Integer.parseInt(vals[i]);
+      assertEquals(0, ivals[0] % (srcs * srcs));
+      for (int i = 1; i < vals.length; ++i) {
+        assertEquals((ivals[i] - (i - 1)) * srcs, 10 * ivals[0]);
+      }
+      ++count;
+    }
+    assertEquals(4, count);
+  }
+
+  private static SequenceFile.Writer[] createWriters(Path testdir,
+      JobConf conf, int srcs, Path[] src) throws IOException {
+    for (int i = 0; i < srcs; ++i) {
+      src[i] = new Path(testdir, Integer.toString(i + 10, 36));
+    }
+    SequenceFile.Writer out[] = new SequenceFile.Writer[srcs];
+    for (int i = 0; i < srcs; ++i) {
+      out[i] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf,
+          src[i], Text.class, Text.class);
+    }
+    return out;
+  }
+
+  private static Path[] writeSimpleSrc(Path testdir, JobConf conf,
+      int srcs) throws IOException {
+    SequenceFile.Writer out[] = null;
+    Path[] src = new Path[srcs];
+    try {
+      out = createWriters(testdir, conf, srcs, src);
+      final int capacity = srcs * 2 + 1;
+      Text key = new Text();
+      key.set("ignored");
+      Text val = new Text();
+      for (int k = 0; k < capacity; ++k) {
+        for (int i = 0; i < srcs; ++i) {
+          val.set(Integer.toString(k % srcs == 0 ? k * srcs : k * srcs + i) +
+              "\t" + Integer.toString(10 * k + i));
+          out[i].append(key, val);
+          if (i == k) {
+            // add duplicate key
+            out[i].append(key, val);
+          }
+        }
+      }
+    } finally {
+      if (out != null) {
+        for (int i = 0; i < srcs; ++i) {
+          if (out[i] != null)
+            out[i].close();
+        }
+      }
+    }
+    return src;
+  }
+}
diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index 1b56c49..38343a7 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -54,6 +54,11 @@
     </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-datajoin</artifactId>
+      <scope>compile</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-extras</artifactId>
       <scope>compile</scope>
     </dependency>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index 6ff170a..95221a9 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -33,6 +33,7 @@
     <module>hadoop-archives</module>
     <module>hadoop-rumen</module>
     <module>hadoop-gridmix</module>
+    <module>hadoop-datajoin</module>
     <module>hadoop-tools-dist</module>
     <module>hadoop-extras</module>
     <module>hadoop-pipes</module>
-- 
1.7.0.4

